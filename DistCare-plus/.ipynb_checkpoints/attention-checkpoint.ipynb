{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d34d54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcb01525",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = \"TJ\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291e3ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttention(nn.Module):\n",
    "    def __init__(self, attention_input_dim, attention_hidden_dim, attention_type='add', demographic_dim=12, time_aware=False, use_demographic=False):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        \n",
    "        self.attention_type = attention_type\n",
    "        self.attention_hidden_dim = attention_hidden_dim\n",
    "        self.attention_input_dim = attention_input_dim\n",
    "        self.use_demographic = use_demographic\n",
    "        self.demographic_dim = demographic_dim\n",
    "        self.time_aware = time_aware\n",
    "\n",
    "        # batch_time = torch.arange(0, batch_mask.size()[1], dtype=torch.float32).reshape(1, batch_mask.size()[1], 1)\n",
    "        # batch_time = batch_time.repeat(batch_mask.size()[0], 1, 1)\n",
    "        \n",
    "        if attention_type == 'add':\n",
    "            if self.time_aware == True:\n",
    "                # self.Wx = nn.Parameter(torch.randn(attention_input_dim+1, attention_hidden_dim))\n",
    "                self.Wx = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "                self.Wtime_aware = nn.Parameter(torch.randn(1, attention_hidden_dim))\n",
    "                nn.init.kaiming_uniform_(self.Wtime_aware, a=math.sqrt(5))\n",
    "            else:\n",
    "                self.Wx = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "            self.Wt = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "            self.Wd = nn.Parameter(torch.randn(demographic_dim, attention_hidden_dim))\n",
    "            self.bh = nn.Parameter(torch.zeros(attention_hidden_dim,))\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wd, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wx, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wt, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        elif attention_type == 'mul':\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_input_dim, attention_input_dim))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        elif attention_type == 'concat':\n",
    "            if self.time_aware == True:\n",
    "                self.Wh = nn.Parameter(torch.randn(2*attention_input_dim+1, attention_hidden_dim))\n",
    "            else:\n",
    "                self.Wh = nn.Parameter(torch.randn(2*attention_input_dim, attention_hidden_dim))\n",
    "\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wh, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        else:\n",
    "            raise RuntimeError('Wrong attention type.')\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, input, demo=None):\n",
    " \n",
    "        batch_size, time_step, input_dim = input.size() # batch_size * time_step * hidden_dim(i)\n",
    "        time_decays = torch.tensor(range(time_step-1,-1,-1), dtype=torch.float32).unsqueeze(-1).unsqueeze(0).to(device)# 1*t*1\n",
    "        b_time_decays = time_decays.repeat(batch_size,1,1)# b t 1\n",
    "        \n",
    "        if self.attention_type == 'add': #B*T*I  @ H*I\n",
    "            q = torch.matmul(input[:,-1,:], self.Wt)# b h\n",
    "            q = torch.reshape(q, (batch_size, 1, self.attention_hidden_dim)) #B*1*H\n",
    "            if self.time_aware == True:\n",
    "                # k_input = torch.cat((input, time), dim=-1)\n",
    "                k = torch.matmul(input, self.Wx)#b t h\n",
    "                # k = torch.reshape(k, (batch_size, 1, time_step, self.attention_hidden_dim)) #B*1*T*H\n",
    "                time_hidden = torch.matmul(b_time_decays, self.Wtime_aware)#  b t h\n",
    "            else:\n",
    "                k = torch.matmul(input, self.Wx)# b t h\n",
    "                # k = torch.reshape(k, (batch_size, 1, time_step, self.attention_hidden_dim)) #B*1*T*H\n",
    "            if self.use_demographic == True:\n",
    "                d = torch.matmul(demo, self.Wd) #B*H\n",
    "                d = torch.reshape(d, (batch_size, 1, self.attention_hidden_dim)) # b 1 h\n",
    "            h = q + k + self.bh # b t h\n",
    "            if self.time_aware == True:\n",
    "                h += time_hidden\n",
    "            h = self.tanh(h) #B*T*H\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step))# b t\n",
    "        elif self.attention_type == 'mul':\n",
    "            e = torch.matmul(input[:,-1,:], self.Wa)#b i\n",
    "            e = torch.matmul(e.unsqueeze(1), input.permute(0,2,1)).squeeze() + self.ba #b t\n",
    "        elif self.attention_type == 'concat':\n",
    "            q = input[:,-1,:].unsqueeze(1).repeat(1,time_step,1)# b t i\n",
    "            k = input\n",
    "            c = torch.cat((q, k), dim=-1) #B*T*2I\n",
    "            if self.time_aware == True:\n",
    "                c = torch.cat((c, b_time_decays), dim=-1) #B*T*2I+1\n",
    "            h = torch.matmul(c, self.Wh)\n",
    "            h = self.tanh(h)\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step)) # b t \n",
    "        \n",
    "        a = self.softmax(e) #B*T\n",
    "        v = torch.matmul(a.unsqueeze(1), input).squeeze() #B*I\n",
    "\n",
    "        return v, a\n",
    "\n",
    "class FinalAttentionQKV(nn.Module):\n",
    "    def __init__(self, attention_input_dim, attention_hidden_dim, attention_type='add', dropout=None):\n",
    "        super(FinalAttentionQKV, self).__init__()\n",
    "        \n",
    "        self.attention_type = attention_type\n",
    "        self.attention_hidden_dim = attention_hidden_dim\n",
    "        self.attention_input_dim = attention_input_dim\n",
    "\n",
    "\n",
    "        self.W_q = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "        self.W_k = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "        self.W_v = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "\n",
    "        self.W_out = nn.Linear(attention_hidden_dim, 1)\n",
    "\n",
    "        self.b_in = nn.Parameter(torch.zeros(1,))\n",
    "        self.b_out = nn.Parameter(torch.zeros(1,))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.W_q.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_k.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_v.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_out.weight, a=math.sqrt(5))\n",
    "\n",
    "        self.Wh = nn.Parameter(torch.randn(2*attention_input_dim, attention_hidden_dim))\n",
    "        self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "        self.ba = nn.Parameter(torch.zeros(1,))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.Wh, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input):\n",
    " \n",
    "        batch_size, time_step, input_dim = input.size() # batch_size * input_dim + 1 * hidden_dim(i)\n",
    "        input_q = self.W_q(torch.mean(input, dim=1)) # b h\n",
    "        input_k = self.W_k(input)# b t h\n",
    "        input_v = self.W_v(input)# b t h\n",
    "\n",
    "        if self.attention_type == 'add': #B*T*I  @ H*I\n",
    "\n",
    "            q = torch.reshape(input_q, (batch_size, 1, self.attention_hidden_dim)) #B*1*H\n",
    "            h = q + input_k + self.b_in # b t h\n",
    "            h = self.tanh(h) #B*T*H\n",
    "            e = self.W_out(h) # b t 1\n",
    "            e = torch.reshape(e, (batch_size, time_step))# b t\n",
    "\n",
    "        elif self.attention_type == 'mul':\n",
    "            q = torch.reshape(input_q, (batch_size, self.attention_hidden_dim, 1)) #B*h 1\n",
    "            e = torch.matmul(input_k, q).squeeze()#b t\n",
    "            \n",
    "        elif self.attention_type == 'concat':\n",
    "            q = input_q.unsqueeze(1).repeat(1,time_step,1)# b t h\n",
    "            k = input_k\n",
    "            c = torch.cat((q, k), dim=-1) #B*T*2I\n",
    "            h = torch.matmul(c, self.Wh)\n",
    "            h = self.tanh(h)\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step)) # b t \n",
    "        \n",
    "        a = self.softmax(e) #B*T\n",
    "        if self.dropout is not None:\n",
    "            a = self.dropout(a)\n",
    "        v = torch.matmul(a.unsqueeze(1), input_v).squeeze() #B*I\n",
    "\n",
    "        return v, a\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def tile(a, dim, n_tile):\n",
    "    init_dim = a.size(dim)\n",
    "    repeat_idx = [1] * a.dim()\n",
    "    repeat_idx[dim] = n_tile\n",
    "    a = a.repeat(*(repeat_idx))\n",
    "    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).to(device)\n",
    "    return torch.index_select(a, dim, order_index).to(device)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module): # new added\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x)))), None\n",
    "\n",
    "    \n",
    "class PositionalEncoding(nn.Module): # new added / not use anymore\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=400):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0 \n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)# b h t d_k\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k) # b h t t\n",
    "    if mask is not None:# 1 1 t t\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)# b h t t \n",
    "    p_attn = F.softmax(scores, dim = -1)# b h t t\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn # b h t v (d_k) \n",
    "    \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, self.d_k * self.h), 3)\n",
    "        self.final_linear = nn.Linear(d_model, d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1) # 1 1 t t\n",
    "\n",
    "        nbatches = query.size(0)# b\n",
    "        input_dim = query.size(1)# i+1\n",
    "        feature_dim = query.size(-1)# i+1\n",
    "\n",
    "        #input size -> # batch_size * d_input * hidden_dim\n",
    "        \n",
    "        # d_model => h * d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))] # b num_head d_input d_k\n",
    "        \n",
    "       \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)# b num_head d_input d_v (d_k) \n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)# batch_size * d_input * hidden_dim\n",
    "\n",
    "        #DeCov \n",
    "        DeCov_contexts = x.transpose(0, 1).transpose(1, 2) # I+1 H B\n",
    "#         print(DeCov_contexts.shape)\n",
    "        Covs = cov(DeCov_contexts[0,:,:])\n",
    "        DeCov_loss = 0.5 * (torch.norm(Covs, p = 'fro')**2 - torch.norm(torch.diag(Covs))**2 ) \n",
    "        for i in range(11 -1):\n",
    "            Covs = cov(DeCov_contexts[i+1,:,:])\n",
    "            DeCov_loss += 0.5 * (torch.norm(Covs, p = 'fro')**2 - torch.norm(torch.diag(Covs))**2 ) \n",
    "\n",
    "\n",
    "        return self.final_linear(x), DeCov_loss\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-7):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "def cov(m, y=None):\n",
    "    if y is not None:\n",
    "        m = torch.cat((m, y), dim=0)\n",
    "    m_exp = torch.mean(m, dim=1)\n",
    "    x = m - m_exp[:, None]\n",
    "    cov = 1 / (x.size(1) - 1) * x.mm(x.t())\n",
    "    return cov\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        returned_value = sublayer(self.norm(x))\n",
    "        return x + self.dropout(returned_value[0]) , returned_value[1]\n",
    "\n",
    "class distcare_teacher(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(distcare_teacher, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 400)\n",
    "\n",
    "        self.GRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        self.LastStepAttentions = clones(SingleAttention(self.hidden_dim, 8, attention_type='concat', demographic_dim=12, time_aware=True, use_demographic=False),self.input_dim)\n",
    "        \n",
    "        self.FinalAttentionQKV = FinalAttentionQKV(self.hidden_dim, self.hidden_dim, attention_type='mul',dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model,dropout = 1 - self.keep_prob)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.output = nn.Linear(self.input_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, input, lens):\n",
    "        lens = lens.to('cpu')\n",
    "        # input shape [batch_size, timestep, feature_dim]\n",
    "#         demo_main = self.tanh(self.demo_proj_main(demo_input)).unsqueeze(1)# b hidden_dim\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "\n",
    "        \n",
    "        GRU_embeded_input = self.GRUs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "#         print(GRU_embeded_input.shape)\n",
    "        for i in range(feature_dim-1):\n",
    "            embeded_input = self.GRUs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "            GRU_embeded_input = torch.cat((GRU_embeded_input, embeded_input), 1)\n",
    "\n",
    "#         GRU_embeded_input = torch.cat((GRU_embeded_input, demo_main), 1)# b i+1 h\n",
    "        posi_input = self.dropout(GRU_embeded_input) # batch_size * d_input * hidden_dim\n",
    "\n",
    "\n",
    "#         #mask = subsequent_mask(time_step).to(device) # 1 t t \n",
    "#         contexts = self.SublayerConnection(posi_input, lambda x: self.MultiHeadedAttention(posi_input, posi_input, posi_input, None))# # batch_size * d_input * hidden_dim\n",
    "    \n",
    "#         DeCov_loss = contexts[1]\n",
    "#         contexts = contexts[0]\n",
    "\n",
    "#         contexts = self.SublayerConnection(contexts, lambda x: self.PositionwiseFeedForward(contexts))[0]# # batch_size * d_input * hidden_dim\n",
    "#         #contexts = contexts.view(batch_size, feature_dim * self.hidden_dim)#\n",
    "#         # contexts = torch.matmul(self.Wproj, contexts) + self.bproj\n",
    "#         # contexts = contexts.squeeze()\n",
    "#         # demo_key = self.demo_proj(demo_input)# b hidden_dim\n",
    "#         # demo_key = self.relu(demo_key)\n",
    "#         # input_dim_scores = torch.matmul(contexts, demo_key.unsqueeze(-1)).squeeze() # b i\n",
    "#         # input_dim_scores = self.dropout(self.sigmoid(input_dim_scores)).unsqueeze(1)# b i\n",
    "        \n",
    "#         # weighted_contexts = torch.matmul(input_dim_scores, contexts).squeeze()\n",
    "# #         print(contexts.shape)\n",
    "\n",
    "#         weighted_contexts = self.FinalAttentionQKV(contexts)[0]\n",
    "        contexts = self.Linear(posi_input).squeeze()# b i\n",
    "        output = self.output(self.dropout(contexts))# b 1\n",
    "        output = self.sigmoid(output)\n",
    "#         print(weighted_contexts.shape)\n",
    "          \n",
    "        return output, None, contexts\n",
    "    #, self.MultiHeadedAttention.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e11efa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, keep_prob=0.7):\n",
    "        super(FeatureAttention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)  # 注意力权重计算\n",
    "        nn.init.xavier_uniform_(self.attention.weight)\n",
    "        self.dropout = nn.Dropout(p=1 - keep_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, time_step, hidden_dim)\n",
    "        attention_weights = torch.softmax(self.attention(x), dim=1)  # (batch_size, time_step, 1)\n",
    "        weighted_output = attention_weights * x  # (batch_size, time_step, hidden_dim)\n",
    "        weighted_output = self.dropout(weighted_output)\n",
    "        return weighted_output, attention_weights\n",
    "\n",
    "class distcare_target(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.7):\n",
    "        super(distcare_target, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 400)\n",
    "\n",
    "        self.GRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        self.FeatureAttentions = clones(FeatureAttention(self.hidden_dim, self.keep_prob), self.input_dim)\n",
    "        self.LastStepAttentions = clones(SingleAttention(self.hidden_dim, 16, attention_type='concat', demographic_dim=12, time_aware=True, use_demographic=False),self.input_dim)\n",
    "        \n",
    "        self.FinalAttentionQKV = FinalAttentionQKV(self.hidden_dim, self.hidden_dim, attention_type='mul',dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model,dropout = 1 - self.keep_prob)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.output = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.FC_embed = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.Linear_los = nn.Linear(self.input_dim, self.output_dim)\n",
    "        self.Linear_outcome = nn.Linear(self.input_dim, self.output_dim)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, input, lens):\n",
    "        lens = lens.to('cpu')\n",
    "        # input shape [batch_size, timestep, feature_dim]\n",
    "#         demo_main = self.tanh(self.demo_proj_main(demo_input)).unsqueeze(1)# b hidden_dim\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "\n",
    "        # Initialization\n",
    "        #cur_hs = Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0))\n",
    "\n",
    "        # forward\n",
    "        # GRU_embeded_input = self.GRUs[0](input[:,:,0].unsqueeze(-1), Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device))[0] # b t h\n",
    "        # Attention_embeded_input = self.LastStepAttentions[0](GRU_embeded_input)[0].unsqueeze(1)# b 1 h\n",
    "        # for i in range(feature_dim-1):\n",
    "        #     embeded_input = self.GRUs[i+1](input[:,:,i+1].unsqueeze(-1), Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device))[0] # b 1 h\n",
    "        #     embeded_input = self.LastStepAttentions[i+1](embeded_input)[0].unsqueeze(1)# b 1 h\n",
    "        #     Attention_embeded_input = torch.cat((Attention_embeded_input, embeded_input), 1)# b i h\n",
    "\n",
    "        # Attention_embeded_input = torch.cat((Attention_embeded_input, demo_main), 1)# b i+1 h\n",
    "        # posi_input = self.dropout(Attention_embeded_input) # batch_size * d_input+1 * hidden_dim\n",
    "\n",
    "#         input = pack_padded_sequence(input, lens, batch_first=True)\n",
    "        \n",
    "        GRU_embeded_input = []\n",
    "        feature_importance = torch.zeros(batch_size, feature_dim).to(input.device)\n",
    "        for i in range(feature_dim):\n",
    "            embeded_input = self.GRUs[i](pack_padded_sequence(input[:,:,i].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "            weighted_input, attention_weights = self.FeatureAttentions[i](embeded_input)\n",
    "            GRU_embeded_input.append(weighted_input)\n",
    "            feature_importance[:, i] = torch.mean(attention_weights.squeeze(-1), dim=1)\n",
    "        \n",
    "\n",
    "        GRU_embeded_input = torch.cat(GRU_embeded_input, 1)\n",
    "        posi_input = self.dropout(GRU_embeded_input) # batch_size * d_input * hidden_dim\n",
    "\n",
    "\n",
    "#         #mask = subsequent_mask(time_step).to(device) # 1 t t 下三角 N to 1任务不用mask\n",
    "#         contexts = self.SublayerConnection(posi_input, lambda x: self.MultiHeadedAttention(posi_input, posi_input, posi_input, None))# # batch_size * d_input * hidden_dim\n",
    "    \n",
    "#         DeCov_loss = contexts[1]\n",
    "#         contexts = contexts[0]\n",
    "\n",
    "#         contexts = self.SublayerConnection(contexts, lambda x: self.PositionwiseFeedForward(contexts))[0]# # batch_size * d_input * hidden_dim\n",
    "#         #contexts = contexts.view(batch_size, feature_dim * self.hidden_dim)#\n",
    "#         # contexts = torch.matmul(self.Wproj, contexts) + self.bproj\n",
    "#         # contexts = contexts.squeeze()\n",
    "#         # demo_key = self.demo_proj(demo_input)# b hidden_dim\n",
    "#         # demo_key = self.relu(demo_key)\n",
    "#         # input_dim_scores = torch.matmul(contexts, demo_key.unsqueeze(-1)).squeeze() # b i\n",
    "#         # input_dim_scores = self.dropout(self.sigmoid(input_dim_scores)).unsqueeze(1)# b i\n",
    "        \n",
    "#         # weighted_contexts = torch.matmul(input_dim_scores, contexts).squeeze()\n",
    "# #         print(contexts.shape)\n",
    "\n",
    "#         weighted_contexts = self.FinalAttentionQKV(contexts)[0]\n",
    "#         #output_embed = self.FC_embed(weighted_contexts)\n",
    "        contexts = self.Linear(posi_input).squeeze()# b i\n",
    "        output = self.Linear_los(self.dropout(contexts))# b 1\n",
    "        outcome = self.Linear_outcome(self.dropout(contexts))# b 1\n",
    "        outcome = F.sigmoid(outcome)\n",
    "#         if self.output_dim != 1:\n",
    "#             output = F.softmax(output, dim=1)\n",
    "#         print(weighted_contexts.shape)\n",
    "          \n",
    "        return output, None, None, outcome, feature_importance\n",
    "    #, self.MultiHeadedAttention.attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "054b5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_dataset == 'TJ':\n",
    "    data_path = './data/Tongji/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    for i in range(len(all_time)):\n",
    "        for j in range(len(all_time[i])):\n",
    "            all_time[i][j] = all_time[i][j][-1]\n",
    "            all_y[i][j] = all_y[i][j][0]\n",
    "\n",
    "    tar_subset_idx = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "    tar_other_idx = list(range(75))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "elif target_dataset == 'HM':\n",
    "    data_path = './data/CDSL/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    for i in range(len(all_time)):\n",
    "        for j in range(len(all_time[i])):\n",
    "            all_time[i][j] = all_time[i][j][-1]\n",
    "            all_y[i][j] = all_y[i][j][0]\n",
    "\n",
    "    tar_subset_idx = [5, 6, 4, 2, 3, 48, 79, 76, 87, 25, 30, 31, 18, 43, 58, 66, 40, 57, 23, 92, 50, 54, 91, 60, 39, 81]\n",
    "    tar_other_idx = list(range(99))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d4a005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_x = all_x\n",
    "long_y = all_y\n",
    "long_y_kfold = [each[-1] for each in all_y]\n",
    "long_time = all_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf4118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n2n_data(x, y, x_len, outcome=None):\n",
    "    length = len(x)\n",
    "    assert length == len(y)\n",
    "    assert length == len(outcome)\n",
    "    assert length == len(x_len)\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    new_outcome = []\n",
    "    new_x_len = []\n",
    "    for i in range(length):\n",
    "        for j in range(len(x[i])):\n",
    "            new_x.append(x[i][:j+1])\n",
    "            new_y.append(y[i][j])\n",
    "            new_outcome.append(outcome[i][j])\n",
    "            new_x_len.append(j+1)\n",
    "    return new_x, new_y, new_x_len, new_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef7e115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_dataset == 'TJ':\n",
    "    input_dim = 75\n",
    "elif target_dataset == 'HM':\n",
    "    input_dim = 99\n",
    "    \n",
    "cell = 'GRU'\n",
    "hidden_dim = 32\n",
    "d_model = 32\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06c65a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = distcare_target(input_dim = input_dim,output_dim=output_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, hidden_dim=hidden_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1389b535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distcare_target(\n",
       "  (PositionalEncoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (GRUs): ModuleList(\n",
       "    (0): GRU(1, 32, batch_first=True)\n",
       "    (1): GRU(1, 32, batch_first=True)\n",
       "    (2): GRU(1, 32, batch_first=True)\n",
       "    (3): GRU(1, 32, batch_first=True)\n",
       "    (4): GRU(1, 32, batch_first=True)\n",
       "    (5): GRU(1, 32, batch_first=True)\n",
       "    (6): GRU(1, 32, batch_first=True)\n",
       "    (7): GRU(1, 32, batch_first=True)\n",
       "    (8): GRU(1, 32, batch_first=True)\n",
       "    (9): GRU(1, 32, batch_first=True)\n",
       "    (10): GRU(1, 32, batch_first=True)\n",
       "    (11): GRU(1, 32, batch_first=True)\n",
       "    (12): GRU(1, 32, batch_first=True)\n",
       "    (13): GRU(1, 32, batch_first=True)\n",
       "    (14): GRU(1, 32, batch_first=True)\n",
       "    (15): GRU(1, 32, batch_first=True)\n",
       "    (16): GRU(1, 32, batch_first=True)\n",
       "    (17): GRU(1, 32, batch_first=True)\n",
       "    (18): GRU(1, 32, batch_first=True)\n",
       "    (19): GRU(1, 32, batch_first=True)\n",
       "    (20): GRU(1, 32, batch_first=True)\n",
       "    (21): GRU(1, 32, batch_first=True)\n",
       "    (22): GRU(1, 32, batch_first=True)\n",
       "    (23): GRU(1, 32, batch_first=True)\n",
       "    (24): GRU(1, 32, batch_first=True)\n",
       "    (25): GRU(1, 32, batch_first=True)\n",
       "    (26): GRU(1, 32, batch_first=True)\n",
       "    (27): GRU(1, 32, batch_first=True)\n",
       "    (28): GRU(1, 32, batch_first=True)\n",
       "    (29): GRU(1, 32, batch_first=True)\n",
       "    (30): GRU(1, 32, batch_first=True)\n",
       "    (31): GRU(1, 32, batch_first=True)\n",
       "    (32): GRU(1, 32, batch_first=True)\n",
       "    (33): GRU(1, 32, batch_first=True)\n",
       "    (34): GRU(1, 32, batch_first=True)\n",
       "    (35): GRU(1, 32, batch_first=True)\n",
       "    (36): GRU(1, 32, batch_first=True)\n",
       "    (37): GRU(1, 32, batch_first=True)\n",
       "    (38): GRU(1, 32, batch_first=True)\n",
       "    (39): GRU(1, 32, batch_first=True)\n",
       "    (40): GRU(1, 32, batch_first=True)\n",
       "    (41): GRU(1, 32, batch_first=True)\n",
       "    (42): GRU(1, 32, batch_first=True)\n",
       "    (43): GRU(1, 32, batch_first=True)\n",
       "    (44): GRU(1, 32, batch_first=True)\n",
       "    (45): GRU(1, 32, batch_first=True)\n",
       "    (46): GRU(1, 32, batch_first=True)\n",
       "    (47): GRU(1, 32, batch_first=True)\n",
       "    (48): GRU(1, 32, batch_first=True)\n",
       "    (49): GRU(1, 32, batch_first=True)\n",
       "    (50): GRU(1, 32, batch_first=True)\n",
       "    (51): GRU(1, 32, batch_first=True)\n",
       "    (52): GRU(1, 32, batch_first=True)\n",
       "    (53): GRU(1, 32, batch_first=True)\n",
       "    (54): GRU(1, 32, batch_first=True)\n",
       "    (55): GRU(1, 32, batch_first=True)\n",
       "    (56): GRU(1, 32, batch_first=True)\n",
       "    (57): GRU(1, 32, batch_first=True)\n",
       "    (58): GRU(1, 32, batch_first=True)\n",
       "    (59): GRU(1, 32, batch_first=True)\n",
       "    (60): GRU(1, 32, batch_first=True)\n",
       "    (61): GRU(1, 32, batch_first=True)\n",
       "    (62): GRU(1, 32, batch_first=True)\n",
       "    (63): GRU(1, 32, batch_first=True)\n",
       "    (64): GRU(1, 32, batch_first=True)\n",
       "    (65): GRU(1, 32, batch_first=True)\n",
       "    (66): GRU(1, 32, batch_first=True)\n",
       "    (67): GRU(1, 32, batch_first=True)\n",
       "    (68): GRU(1, 32, batch_first=True)\n",
       "    (69): GRU(1, 32, batch_first=True)\n",
       "    (70): GRU(1, 32, batch_first=True)\n",
       "    (71): GRU(1, 32, batch_first=True)\n",
       "    (72): GRU(1, 32, batch_first=True)\n",
       "    (73): GRU(1, 32, batch_first=True)\n",
       "    (74): GRU(1, 32, batch_first=True)\n",
       "  )\n",
       "  (FeatureAttentions): ModuleList(\n",
       "    (0): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (1): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (2): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (3): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (4): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (5): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (6): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (7): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (8): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (9): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (10): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (11): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (12): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (13): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (14): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (15): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (16): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (17): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (18): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (19): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (20): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (21): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (22): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (23): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (24): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (25): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (26): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (27): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (28): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (29): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (30): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (31): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (32): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (33): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (34): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (35): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (36): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (37): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (38): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (39): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (40): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (41): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (42): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (43): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (44): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (45): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (46): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (47): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (48): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (49): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (50): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (51): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (52): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (53): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (54): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (55): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (56): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (57): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (58): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (59): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (60): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (61): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (62): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (63): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (64): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (65): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (66): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (67): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (68): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (69): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (70): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (71): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (72): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (73): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "    (74): FeatureAttention(\n",
       "      (attention): Linear(in_features=32, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (LastStepAttentions): ModuleList(\n",
       "    (0): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (1): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (2): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (3): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (4): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (5): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (6): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (7): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (8): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (9): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (10): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (11): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (12): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (13): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (14): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (15): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (16): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (17): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (18): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (19): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (20): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (21): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (22): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (23): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (24): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (25): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (26): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (27): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (28): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (29): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (30): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (31): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (32): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (33): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (34): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (35): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (36): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (37): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (38): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (39): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (40): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (41): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (42): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (43): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (44): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (45): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (46): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (47): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (48): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (49): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (50): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (51): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (52): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (53): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (54): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (55): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (56): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (57): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (58): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (59): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (60): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (61): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (62): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (63): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (64): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (65): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (66): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (67): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (68): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (69): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (70): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (71): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (72): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (73): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (74): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "  )\n",
       "  (FinalAttentionQKV): FinalAttentionQKV(\n",
       "    (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_k): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_v): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_out): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    (tanh): Tanh()\n",
       "    (softmax): Softmax(dim=None)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (MultiHeadedAttention): MultiHeadedAttention(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (final_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "  )\n",
       "  (SublayerConnection): SublayerConnection(\n",
       "    (norm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "  )\n",
       "  (PositionwiseFeedForward): PositionwiseFeedForward(\n",
       "    (w_1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (w_2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (demo_proj_main): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (demo_proj): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (output): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "  (FC_embed): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (Linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (Linear_los): Linear(in_features=75, out_features=1, bias=True)\n",
       "  (Linear_outcome): Linear(in_features=75, out_features=1, bias=True)\n",
       "  (softmax): Softmax(dim=None)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if target_dataset == 'TJ':    \n",
    "    file_name = './model/covid/distcare-trans-5-fold-LOS-regression-Attention1'\n",
    "    \n",
    "checkpoint = torch.load(file_name, map_location=torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu') )\n",
    "save_epoch = checkpoint['epoch']\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4d8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
