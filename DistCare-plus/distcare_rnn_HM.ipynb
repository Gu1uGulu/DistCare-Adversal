{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:55:29.996921Z",
     "start_time": "2021-02-10T14:55:28.704721Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "import pickle5 as pickle\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import logging\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from utils import utils\n",
    "from utils.readers import InHospitalMortalityReader\n",
    "from utils.preprocessing import Discretizer, Normalizer\n",
    "from utils import metrics\n",
    "from utils import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:55:52.924543Z",
     "start_time": "2021-02-10T14:55:52.918220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "# Select the target dataset: COVID-19 Dataset from TJ Hospital or HM Hospital\n",
    "target_dataset = 'HM' # 'TJ' or 'HM'\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() == True else 'cpu')\n",
    "print(\"available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:08:55,038 - INFO - 这是希望输出的info内容\n",
      "2023-11-08 12:08:55,039 - WARNING - 这是希望输出的warning内容\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_logger(name):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # 以下两行是为了在jupyter notebook 中不重复输出日志\n",
    "    if logger.root.handlers:\n",
    "        logger.root.handlers[0].setLevel(logging.WARNING)\n",
    " \n",
    "    handler_stdout = logging.StreamHandler()\n",
    "    handler_stdout.setLevel(logging.INFO)\n",
    "    handler_stdout.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler_stdout)\n",
    " \n",
    "    handler_file = logging.FileHandler('log_file_rnn_hm.log', encoding='utf-8')\n",
    "    handler_file.setLevel(logging.DEBUG)\n",
    "    handler_file.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler_file)\n",
    " \n",
    "    return logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "logger.debug('这是希望输出的debug内容')\n",
    "logger.info('这是希望输出的info内容')\n",
    "logger.warning('这是希望输出的warning内容')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Source Data & Model: \n",
    "#### Teacher Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:55:32.579746Z",
     "start_time": "2021-02-10T14:55:32.571939Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = './data/Challenge/'\n",
    "small_part = False\n",
    "teacher_flag = False\n",
    "arg_timestep = 1.0\n",
    "batch_size = 256\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T04:27:29.073353Z",
     "start_time": "2021-01-30T04:27:12.159717Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:09:18,541 - INFO - [[-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 1.2605692444279462, 0.585371321489137, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.9546043520029015, 0.2987563180527675, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 1.0769903089729194, 0.4420638197709522, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.566534136852991, 1.803485086093707, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, 0.46506052412282983, -0.05951243624269434, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, 0.281481588667803, -0.2744736888199714, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.8934113735178925, 0.9436400757845987, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:09:18,558 - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "2023-11-08 12:09:18,559 - INFO - 110609\n",
      "2023-11-08 12:09:18,561 - INFO - [[-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8962118618028821, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8617355345389544, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8272592072750267, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7927828800110989, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7583065527471711, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7238302254832434, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6893538982193156, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6548775709553878, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.62040124369146, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5859249164275323, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5514485891636045, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5169722618996767, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.48249593463574897, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.4480196073718212, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.41354328010789343, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3790669528439657, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3445906255800379, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.31011429831611015, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.27563797105218235, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.2411616437882546, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.20668531652432684, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.17220898926039907, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.1377326619964713, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.10325633473254354, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.06878000746861578, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.034303680204688006, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.0001726470592397616, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.034648974323167527, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.06912530158709529, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.10360162885102306, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.13807795611495083, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.1725542833788786, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.20703061064280637, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.24150693790673414, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.2759832651706619, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.31045959243458965, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.34493591969851745, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.3794122469624452, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.413888574226373, '0']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 1.2605692444279462, 0.585371321489137, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.9546043520029015, 0.2987563180527675, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 1.0769903089729194, 0.4420638197709522, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.566534136852991, 1.803485086093707, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, 0.46506052412282983, -0.05951243624269434, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, 0.281481588667803, -0.2744736888199714, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.8934113735178925, 0.9436400757845987, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "110609\n",
      "[[-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8962118618028821, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8617355345389544, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8272592072750267, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7927828800110989, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7583065527471711, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7238302254832434, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6893538982193156, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6548775709553878, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.62040124369146, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5859249164275323, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5514485891636045, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5169722618996767, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.48249593463574897, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.4480196073718212, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.41354328010789343, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3790669528439657, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3445906255800379, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.31011429831611015, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.27563797105218235, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.2411616437882546, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.20668531652432684, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.17220898926039907, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.1377326619964713, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.10325633473254354, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.06878000746861578, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.034303680204688006, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.0001726470592397616, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.034648974323167527, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.06912530158709529, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.10360162885102306, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.13807795611495083, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.1725542833788786, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.20703061064280637, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.24150693790673414, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.2759832651706619, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.31045959243458965, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.34493591969851745, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.3794122469624452, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.413888574226373, '0']]\n"
     ]
    }
   ],
   "source": [
    "all_x = pickle.load(open(data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "all_y = pickle.load(open(data_path + 'new_y_front_fill.dat', 'rb'))\n",
    "all_names = pickle.load(open(data_path + 'new_name.dat', 'rb'))\n",
    "static = pickle.load(open(data_path + 'new_demo_front_fill.dat', 'rb'))\n",
    "mask_x = pickle.load(open(data_path + 'new_mask_x.dat', 'rb'))\n",
    "mask_demo = pickle.load(open(data_path + 'new_mask_demo.dat', 'rb'))\n",
    "all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "print(all_x[0])\n",
    "print(mask_x[0])\n",
    "print(all_names[0])\n",
    "print(static[0])\n",
    "logger.info(all_x[0])\n",
    "logger.info(mask_x[0])\n",
    "logger.info(all_names[0])\n",
    "logger.info(static[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T04:27:41.051036Z",
     "start_time": "2021-01-30T04:27:29.075798Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:09:38,831 - INFO - [[-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 0.585371321489137, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 1.2605692444279462, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.2987563180527675, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.9546043520029015, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 0.4420638197709522, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 1.0769903089729194, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.803485086093707, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 1.566534136852991, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.281481588667803, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.9436400757845987, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.8934113735178925, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.22028861018279403, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.22028861018279403, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.22028861018279403, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.22028861018279403, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.5610886922563408, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, -0.26925521769727756, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.5610886922563408, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, -0.26925521769727756, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 0.585371321489137, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 1.0157973304879104, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 0.585371321489137, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.0157973304879104, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.08379506547549039, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.6486394595778567, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.08379506547549039, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.6486394595778567, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.08379506547549039, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.6486394595778567, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.08379506547549039, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.6486394595778567, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.373562580939153, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.627727115338, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.373562580939153, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.627727115338, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.373562580939153, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.627727115338, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.373562580939153, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.627727115338, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.2271025671936751, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.8934113735178925, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.2271025671936751, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.8934113735178925, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.2271025671936751, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.8934113735178925, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.2271025671936751, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.8934113735178925, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.6601775843755224, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.1993762659429372, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.6601775843755224, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.1993762659429372, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:09:38,838 - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "2023-11-08 12:09:38,839 - INFO - 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 0.585371321489137, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 1.2605692444279462, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.2987563180527675, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.9546043520029015, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 0.4420638197709522, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 1.0769903089729194, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.803485086093707, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 1.566534136852991, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.281481588667803, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.9436400757845987, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.8934113735178925, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.22028861018279403, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.22028861018279403, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.22028861018279403, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.22028861018279403, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.5610886922563408, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, -0.26925521769727756, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.5610886922563408, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, -0.26925521769727756, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 0.585371321489137, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 1.0157973304879104, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 0.585371321489137, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.0157973304879104, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.08379506547549039, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.6486394595778567, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.08379506547549039, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.6486394595778567, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.08379506547549039, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.6486394595778567, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.08379506547549039, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.6486394595778567, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.373562580939153, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.627727115338, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.373562580939153, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.627727115338, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.373562580939153, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.627727115338, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.373562580939153, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.627727115338, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.2271025671936751, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.8934113735178925, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.2271025671936751, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.8934113735178925, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.2271025671936751, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.8934113735178925, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.2271025671936751, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.8934113735178925, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.6601775843755224, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.1993762659429372, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.6601775843755224, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.1993762659429372, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "if target_dataset == 'TJ':\n",
    "    subset_idx = [27, 29, 18, 16, 26, 33, 28, 31, 32, 15, 11, 25, 21, 20, 9, 17, 30, 19]\n",
    "elif target_dataset == 'HM':\n",
    "    subset_idx = [0, 1, 2, 3, 5, 9, 11, 12, 13, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
    "\n",
    "subset_cnt = len(subset_idx)\n",
    "other_idx = []\n",
    "for i in range(len(all_x[0][0])):\n",
    "    if i not in subset_idx:\n",
    "        other_idx.append(i)\n",
    "\n",
    "for i in range(len(all_x)):\n",
    "    cur = np.array(all_x[i], dtype=float)\n",
    "    cur_mask = np.array(mask_x[i])\n",
    "    cur_subset = cur[:, subset_idx]\n",
    "    cur_other = cur[:, other_idx]\n",
    "    cur_mask_subset = cur_mask[:, subset_idx]\n",
    "    cur_mask_other = cur_mask[:, other_idx]\n",
    "    all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    mask_x[i] = np.concatenate((cur_mask_subset, cur_mask_other), axis=1).tolist()\n",
    "print(all_x[0])\n",
    "print(mask_x[0])\n",
    "print(len(all_x[0][0]))\n",
    "logger.info(all_x[0])\n",
    "logger.info(mask_x[0])\n",
    "logger.info(len(all_x[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T04:28:59.378928Z",
     "start_time": "2021-01-30T04:28:57.102580Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:09:41,121 - INFO - 32269\n",
      "2023-11-08 12:09:41,123 - INFO - 4034\n",
      "2023-11-08 12:09:41,124 - INFO - 4033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32269\n",
      "4034\n",
      "4033\n"
     ]
    }
   ],
   "source": [
    "train_num =int( len(all_x) * 0.8) + 1\n",
    "print(train_num)\n",
    "logger.info(train_num)\n",
    "dev_num =int( len(all_x) * 0.1) + 1\n",
    "print(dev_num)\n",
    "logger.info(dev_num)\n",
    "test_num =int( len(all_x) * 0.1)\n",
    "print(test_num)\n",
    "logger.info(test_num)\n",
    "assert(train_num+dev_num+test_num == len(all_x))\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "train_names = []\n",
    "train_static = []\n",
    "train_x_len = []\n",
    "train_mask_x = []\n",
    "for idx in range(train_num):\n",
    "    train_x.append(all_x[idx])\n",
    "    train_y.append(int(all_y[idx][-1]))\n",
    "    train_names.append(all_names[idx])\n",
    "    train_static.append(static[idx])\n",
    "    train_x_len.append(all_x_len[idx])\n",
    "    train_mask_x.append(mask_x[idx])\n",
    "\n",
    "dev_x = []\n",
    "dev_y = []\n",
    "dev_names = []\n",
    "dev_static = []\n",
    "dev_x_len = []\n",
    "dev_mask_x = []\n",
    "for idx in range(train_num, train_num + dev_num):\n",
    "    dev_x.append(all_x[idx])\n",
    "    dev_y.append(int(all_y[idx][-1]))\n",
    "    dev_names.append(all_names[idx])\n",
    "    dev_static.append(static[idx])\n",
    "    dev_x_len.append(all_x_len[idx])\n",
    "    dev_mask_x.append(mask_x[idx])\n",
    "\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "test_names = []\n",
    "test_static = []\n",
    "test_x_len = []\n",
    "test_mask_x = []\n",
    "for idx in range(train_num + dev_num, train_num + dev_num + test_num):\n",
    "    test_x.append(all_x[idx])\n",
    "    test_y.append(int(all_y[idx][-1]))\n",
    "    test_names.append(all_names[idx])\n",
    "    test_static.append(static[idx])\n",
    "    test_x_len.append(all_x_len[idx])\n",
    "    test_mask_x.append(mask_x[idx])\n",
    "\n",
    "\n",
    "assert(len(train_x) == train_num)\n",
    "assert(len(dev_x) == dev_num)\n",
    "assert(len(test_x) == test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T04:29:02.456915Z",
     "start_time": "2021-01-30T04:29:02.443296Z"
    }
   },
   "outputs": [],
   "source": [
    "long_x = all_x\n",
    "long_y = [y[-1] for y in all_y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:38.783950Z",
     "start_time": "2021-02-10T15:05:38.778517Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_loss(y_pred, y_true):\n",
    "    loss = torch.nn.BCELoss()\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:39.004588Z",
     "start_time": "2021-02-10T15:05:38.999638Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_re_loss(y_pred, y_true):\n",
    "    loss = torch.nn.MSELoss()\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:39.151905Z",
     "start_time": "2021-02-10T15:05:39.147577Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_kl_loss(x_pred, x_target):\n",
    "    loss = torch.nn.KLDivLoss(reduce=True, size_average=True)\n",
    "    return loss(x_pred, x_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:39.322887Z",
     "start_time": "2021-02-10T15:05:39.313036Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wass_dist(x_pred, x_target):\n",
    "    m1 = torch.mean(x_pred, dim=0)\n",
    "    m2 = torch.mean(x_target, dim=0)\n",
    "    v1 = torch.var(x_pred, dim=0)\n",
    "    v2 = torch.var(x_target, dim=0)\n",
    "    p1 = torch.sum(torch.pow((m1 - m2), 2))\n",
    "    p2 = torch.sum(torch.pow(torch.pow(v1, 1/2) - torch.pow(v2, 1/2), 2))\n",
    "    return torch.pow(p1+p2, 1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:41.142115Z",
     "start_time": "2021-02-10T15:05:41.134517Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_sents(sents, pad_token):\n",
    "\n",
    "    sents_padded = []\n",
    "\n",
    "    max_length = max([len(_) for _ in sents])\n",
    "    for i in sents:\n",
    "        padded = list(i) + [pad_token]*(max_length-len(i))\n",
    "        sents_padded.append(np.array(padded))\n",
    "\n",
    "\n",
    "    return np.array(sents_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:41.301945Z",
     "start_time": "2021-02-10T15:05:41.287538Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_iter(x, y, mask, lens, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(x) / batch_size) # 向下取整\n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size] #  fetch out all the induces\n",
    "        \n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx], mask[idx], lens[idx]))\n",
    "       \n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    \n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "        batch_mask_x = [e[2] for e in examples]\n",
    "#         batch_name = [e[2] for e in examples]\n",
    "        batch_lens = [e[3] for e in examples]\n",
    "\n",
    "        yield batch_x, batch_y, batch_mask_x, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:41.394432Z",
     "start_time": "2021-02-10T15:05:41.386828Z"
    }
   },
   "outputs": [],
   "source": [
    "def length_to_mask(length, max_len=None, dtype=None):\n",
    "    \"\"\"length: B.\n",
    "    return B x max_len.\n",
    "    If max_len is None, then max of length will be used.\n",
    "    \"\"\"\n",
    "    assert len(length.shape) == 1, 'Length shape should be 1 dimensional.'\n",
    "    max_len = max_len or length.max().item()\n",
    "    mask = torch.arange(max_len, device=length.device,\n",
    "                        dtype=length.dtype).expand(len(length), max_len) < length.unsqueeze(1)\n",
    "    if dtype is not None:\n",
    "        mask = torch.as_tensor(mask, dtype=dtype, device=length.device)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:41.786588Z",
     "start_time": "2021-02-10T15:05:41.673200Z"
    }
   },
   "outputs": [],
   "source": [
    "class SingleAttention(nn.Module):\n",
    "    def __init__(self, attention_input_dim, attention_hidden_dim, attention_type='add', demographic_dim=12, time_aware=False, use_demographic=False):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        \n",
    "        self.attention_type = attention_type\n",
    "        self.attention_hidden_dim = attention_hidden_dim\n",
    "        self.attention_input_dim = attention_input_dim\n",
    "        self.use_demographic = use_demographic\n",
    "        self.demographic_dim = demographic_dim\n",
    "        self.time_aware = time_aware\n",
    "\n",
    "        # batch_time = torch.arange(0, batch_mask.size()[1], dtype=torch.float32).reshape(1, batch_mask.size()[1], 1)\n",
    "        # batch_time = batch_time.repeat(batch_mask.size()[0], 1, 1)\n",
    "        \n",
    "        if attention_type == 'add':\n",
    "            if self.time_aware == True:\n",
    "                # self.Wx = nn.Parameter(torch.randn(attention_input_dim+1, attention_hidden_dim))\n",
    "                self.Wx = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "                self.Wtime_aware = nn.Parameter(torch.randn(1, attention_hidden_dim))\n",
    "                nn.init.kaiming_uniform_(self.Wtime_aware, a=math.sqrt(5))\n",
    "            else:\n",
    "                self.Wx = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "            self.Wt = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "            self.Wd = nn.Parameter(torch.randn(demographic_dim, attention_hidden_dim))\n",
    "            self.bh = nn.Parameter(torch.zeros(attention_hidden_dim,))\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wd, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wx, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wt, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        elif attention_type == 'mul':\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_input_dim, attention_input_dim))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        elif attention_type == 'concat':\n",
    "            if self.time_aware == True:\n",
    "                self.Wh = nn.Parameter(torch.randn(2*attention_input_dim+1, attention_hidden_dim))\n",
    "            else:\n",
    "                self.Wh = nn.Parameter(torch.randn(2*attention_input_dim, attention_hidden_dim))\n",
    "\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wh, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        else:\n",
    "            raise RuntimeError('Wrong attention type.')\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, input, demo=None):\n",
    " \n",
    "        batch_size, time_step, input_dim = input.size() # batch_size * time_step * hidden_dim(i)\n",
    "        time_decays = torch.tensor(range(time_step-1,-1,-1), dtype=torch.float32).unsqueeze(-1).unsqueeze(0).to(device)# 1*t*1\n",
    "        b_time_decays = time_decays.repeat(batch_size,1,1)# b t 1\n",
    "        \n",
    "        if self.attention_type == 'add': #B*T*I  @ H*I\n",
    "            q = torch.matmul(input[:,-1,:], self.Wt)# b h\n",
    "            q = torch.reshape(q, (batch_size, 1, self.attention_hidden_dim)) #B*1*H\n",
    "            if self.time_aware == True:\n",
    "                # k_input = torch.cat((input, time), dim=-1)\n",
    "                k = torch.matmul(input, self.Wx)#b t h\n",
    "                # k = torch.reshape(k, (batch_size, 1, time_step, self.attention_hidden_dim)) #B*1*T*H\n",
    "                time_hidden = torch.matmul(b_time_decays, self.Wtime_aware)#  b t h\n",
    "            else:\n",
    "                k = torch.matmul(input, self.Wx)# b t h\n",
    "                # k = torch.reshape(k, (batch_size, 1, time_step, self.attention_hidden_dim)) #B*1*T*H\n",
    "            if self.use_demographic == True:\n",
    "                d = torch.matmul(demo, self.Wd) #B*H\n",
    "                d = torch.reshape(d, (batch_size, 1, self.attention_hidden_dim)) # b 1 h\n",
    "            h = q + k + self.bh # b t h\n",
    "            if self.time_aware == True:\n",
    "                h += time_hidden\n",
    "            h = self.tanh(h) #B*T*H\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step))# b t\n",
    "        elif self.attention_type == 'mul':\n",
    "            e = torch.matmul(input[:,-1,:], self.Wa)#b i\n",
    "            e = torch.matmul(e.unsqueeze(1), input.permute(0,2,1)).squeeze() + self.ba #b t\n",
    "        elif self.attention_type == 'concat':\n",
    "            q = input[:,-1,:].unsqueeze(1).repeat(1,time_step,1)# b t i\n",
    "            k = input\n",
    "            c = torch.cat((q, k), dim=-1) #B*T*2I\n",
    "            if self.time_aware == True:\n",
    "                c = torch.cat((c, b_time_decays), dim=-1) #B*T*2I+1\n",
    "            h = torch.matmul(c, self.Wh)\n",
    "            h = self.tanh(h)\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step)) # b t \n",
    "        \n",
    "        a = self.softmax(e) #B*T\n",
    "        v = torch.matmul(a.unsqueeze(1), input).squeeze() #B*I\n",
    "\n",
    "        return v, a\n",
    "\n",
    "class FinalAttentionQKV(nn.Module):\n",
    "    def __init__(self, attention_input_dim, attention_hidden_dim, attention_type='add', dropout=None):\n",
    "        super(FinalAttentionQKV, self).__init__()\n",
    "        \n",
    "        self.attention_type = attention_type\n",
    "        self.attention_hidden_dim = attention_hidden_dim\n",
    "        self.attention_input_dim = attention_input_dim\n",
    "\n",
    "\n",
    "        self.W_q = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "        self.W_k = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "        self.W_v = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "\n",
    "        self.W_out = nn.Linear(attention_hidden_dim, 1)\n",
    "\n",
    "        self.b_in = nn.Parameter(torch.zeros(1,))\n",
    "        self.b_out = nn.Parameter(torch.zeros(1,))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.W_q.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_k.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_v.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_out.weight, a=math.sqrt(5))\n",
    "\n",
    "        self.Wh = nn.Parameter(torch.randn(2*attention_input_dim, attention_hidden_dim))\n",
    "        self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "        self.ba = nn.Parameter(torch.zeros(1,))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.Wh, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input):\n",
    " \n",
    "        batch_size, time_step, input_dim = input.size() # batch_size * input_dim + 1 * hidden_dim(i)\n",
    "        input_q = self.W_q(torch.mean(input, dim=1)) # b h\n",
    "        input_k = self.W_k(input)# b t h\n",
    "        input_v = self.W_v(input)# b t h\n",
    "\n",
    "        if self.attention_type == 'add': #B*T*I  @ H*I\n",
    "\n",
    "            q = torch.reshape(input_q, (batch_size, 1, self.attention_hidden_dim)) #B*1*H\n",
    "            h = q + input_k + self.b_in # b t h\n",
    "            h = self.tanh(h) #B*T*H\n",
    "            e = self.W_out(h) # b t 1\n",
    "            e = torch.reshape(e, (batch_size, time_step))# b t\n",
    "\n",
    "        elif self.attention_type == 'mul':\n",
    "            q = torch.reshape(input_q, (batch_size, self.attention_hidden_dim, 1)) #B*h 1\n",
    "            e = torch.matmul(input_k, q).squeeze()#b t\n",
    "            \n",
    "        elif self.attention_type == 'concat':\n",
    "            q = input_q.unsqueeze(1).repeat(1,time_step,1)# b t h\n",
    "            k = input_k\n",
    "            c = torch.cat((q, k), dim=-1) #B*T*2I\n",
    "            h = torch.matmul(c, self.Wh)\n",
    "            h = self.tanh(h)\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step)) # b t \n",
    "        \n",
    "        a = self.softmax(e) #B*T\n",
    "        if self.dropout is not None:\n",
    "            a = self.dropout(a)\n",
    "        v = torch.matmul(a.unsqueeze(1), input_v).squeeze() #B*I\n",
    "\n",
    "        return v, a\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def tile(a, dim, n_tile):\n",
    "    init_dim = a.size(dim)\n",
    "    repeat_idx = [1] * a.dim()\n",
    "    repeat_idx[dim] = n_tile\n",
    "    a = a.repeat(*(repeat_idx))\n",
    "    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).to(device)\n",
    "    return torch.index_select(a, dim, order_index).to(device)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module): # new added\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x)))), None\n",
    "\n",
    "    \n",
    "class PositionalEncoding(nn.Module): # new added / not use anymore\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=400):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0 \n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)# b h t d_k\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k) # b h t t\n",
    "    if mask is not None:# 1 1 t t\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)# b h t t \n",
    "    p_attn = F.softmax(scores, dim = -1)# b h t t\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn # b h t v (d_k) \n",
    "    \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, self.d_k * self.h), 3)\n",
    "        self.final_linear = nn.Linear(d_model, d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1) # 1 1 t t\n",
    "\n",
    "        nbatches = query.size(0)# b\n",
    "        input_dim = query.size(1)# i+1\n",
    "        feature_dim = query.size(-1)# i+1\n",
    "\n",
    "        #input size -> # batch_size * d_input * hidden_dim\n",
    "        \n",
    "        # d_model => h * d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))] # b num_head d_input d_k\n",
    "        \n",
    "       \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)# b num_head d_input d_v (d_k) \n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)# batch_size * d_input * hidden_dim\n",
    "\n",
    "        #DeCov \n",
    "        DeCov_contexts = x.transpose(0, 1).transpose(1, 2) # I+1 H B\n",
    "#         print(DeCov_contexts.shape)\n",
    "        Covs = cov(DeCov_contexts[0,:,:])\n",
    "        DeCov_loss = 0.5 * (torch.norm(Covs, p = 'fro')**2 - torch.norm(torch.diag(Covs))**2 ) \n",
    "        for i in range(11 -1):\n",
    "            Covs = cov(DeCov_contexts[i+1,:,:])\n",
    "            DeCov_loss += 0.5 * (torch.norm(Covs, p = 'fro')**2 - torch.norm(torch.diag(Covs))**2 ) \n",
    "\n",
    "\n",
    "        return self.final_linear(x), DeCov_loss\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-7):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "def cov(m, y=None):\n",
    "    if y is not None:\n",
    "        m = torch.cat((m, y), dim=0)\n",
    "    m_exp = torch.mean(m, dim=1)\n",
    "    x = m - m_exp[:, None]\n",
    "    cov = 1 / (x.size(1) - 1) * x.mm(x.t())\n",
    "    return cov\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        returned_value = sublayer(self.norm(x))\n",
    "        return x + self.dropout(returned_value[0]) , returned_value[1]\n",
    "\n",
    "class distcare_teacher(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(distcare_teacher, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 400)\n",
    "\n",
    "        self.RNNs = clones(nn.RNN(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        self.LastStepAttentions = clones(SingleAttention(self.hidden_dim, 8, attention_type='concat', demographic_dim=12, time_aware=True, use_demographic=False),self.input_dim)\n",
    "        \n",
    "        self.FinalAttentionQKV = FinalAttentionQKV(self.hidden_dim, self.hidden_dim, attention_type='mul',dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model,dropout = 1 - self.keep_prob)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.output = nn.Linear(self.input_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, input, lens):\n",
    "        lens = lens.to('cpu')\n",
    "        # input shape [batch_size, timestep, feature_dim]\n",
    "#         demo_main = self.tanh(self.demo_proj_main(demo_input)).unsqueeze(1)# b hidden_dim\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "        \n",
    "        RNN_embeded_input = self.RNNs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1][0].squeeze().unsqueeze(1) # b 1 h\n",
    "#         print(RNN_embeded_input.shape)\n",
    "        for i in range(feature_dim-1):\n",
    "            embeded_input = self.RNNs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1][0].squeeze().unsqueeze(1) # b 1 h\n",
    "            RNN_embeded_input = torch.cat((RNN_embeded_input, embeded_input), 1)\n",
    "\n",
    "#         RNN_embeded_input = torch.cat((RNN_embeded_input, demo_main), 1)# b i+1 h\n",
    "        posi_input = self.dropout(RNN_embeded_input) # batch_size * d_input * hidden_dim\n",
    "\n",
    "\n",
    "#         #mask = subsequent_mask(time_step).to(device) # 1 t t \n",
    "#         contexts = self.SublayerConnection(posi_input, lambda x: self.MultiHeadedAttention(posi_input, posi_input, posi_input, None))# # batch_size * d_input * hidden_dim\n",
    "    \n",
    "#         DeCov_loss = contexts[1]\n",
    "#         contexts = contexts[0]\n",
    "\n",
    "#         contexts = self.SublayerConnection(contexts, lambda x: self.PositionwiseFeedForward(contexts))[0]# # batch_size * d_input * hidden_dim\n",
    "#         #contexts = contexts.view(batch_size, feature_dim * self.hidden_dim)#\n",
    "#         # contexts = torch.matmul(self.Wproj, contexts) + self.bproj\n",
    "#         # contexts = contexts.squeeze()\n",
    "#         # demo_key = self.demo_proj(demo_input)# b hidden_dim\n",
    "#         # demo_key = self.relu(demo_key)\n",
    "#         # input_dim_scores = torch.matmul(contexts, demo_key.unsqueeze(-1)).squeeze() # b i\n",
    "#         # input_dim_scores = self.dropout(self.sigmoid(input_dim_scores)).unsqueeze(1)# b i\n",
    "        \n",
    "#         # weighted_contexts = torch.matmul(input_dim_scores, contexts).squeeze()\n",
    "# #         print(contexts.shape)\n",
    "\n",
    "#         weighted_contexts = self.FinalAttentionQKV(contexts)[0]\n",
    "        contexts = self.Linear(posi_input).squeeze()# b i\n",
    "        output = self.output(self.dropout(contexts))# b 1\n",
    "        output = self.sigmoid(output)\n",
    "#         print(weighted_contexts.shape)\n",
    "          \n",
    "        return output, None, contexts\n",
    "    #, self.MultiHeadedAttention.attn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:51.991644Z",
     "start_time": "2021-02-10T15:05:42.504740Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED) #numpy\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED) # cpu\n",
    "torch.cuda.manual_seed(RANDOM_SEED) #gpu\n",
    "torch.backends.cudnn.deterministic=True # cudnn\n",
    "    \n",
    "epochs = 150\n",
    "batch_size = 256\n",
    "input_dim = 34\n",
    "hidden_dim = 32\n",
    "d_model = 32\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1\n",
    "\n",
    "model = distcare_teacher(input_dim = input_dim, hidden_dim = hidden_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, output_dim = output_dim).to(device)\n",
    "# input_dim, d_model, d_k, d_v, MHD_num_head, d_ff, output_dim\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T09:16:04.789735Z",
     "start_time": "2021-01-30T04:30:23.650979Z"
    },
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Training Teacher\n",
    "# # If you don't want to train Teacher Model:\n",
    "# # - The pretrained taecher model is in direcrtory './model/', and can be directly loaded, \n",
    "# # - Simply skip this cell and load the model to validate on Dev Dataset.\n",
    "# logger.info('Training Teacher')\n",
    "\n",
    "# total_train_loss = []\n",
    "# total_valid_loss = []\n",
    "# global_best = 0\n",
    "# auroc = []\n",
    "# auprc = []\n",
    "# minpse = []\n",
    "# history = []\n",
    "\n",
    "# pad_token = np.zeros(input_dim)\n",
    "# # begin_time = time.time()\n",
    "# best_auroc = 0\n",
    "# best_auprc = 0\n",
    "# best_minpse = 0\n",
    "    \n",
    "# if target_dataset == 'TJ':\n",
    "#     file_name = './model/pretrained-challenge-front-fill-teacher-2covid-rnn'\n",
    "# elif target_dataset == 'HM':\n",
    "#     file_name = './model/pretrained-challenge-front-fill-teacher-2spain-rnn'\n",
    "\n",
    "# for each_epoch in range(epochs):\n",
    "\n",
    "#     epoch_loss = []\n",
    "#     counter_batch = 0\n",
    "#     model.train()  \n",
    "\n",
    "#     for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(train_x, train_y, train_mask_x, train_x_len, batch_size, shuffle=True)):  \n",
    "#         optimizer.zero_grad()\n",
    "#         batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "#         batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "#         batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#         batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "# #        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "#         opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "\n",
    "#         BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1)) # b t 1\n",
    "# #             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "#         loss = BCE_Loss #+ 1000 * decov_loss\n",
    "\n",
    "#         epoch_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if step % 20 == 0:\n",
    "#             print('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "#             logger.info('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "\n",
    "#     epoch_loss = np.mean(epoch_loss)\n",
    "#     total_train_loss.append(epoch_loss)\n",
    "\n",
    "#     #Validation\n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         valid_loss = []\n",
    "#         valid_true = []\n",
    "#         valid_pred = []\n",
    "#         for batch_x, batch_y, batch_mask_x, batch_lens in batch_iter(dev_x, dev_y, dev_mask_x, dev_x_len, batch_size):\n",
    "#             batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "#             batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "#             batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#             batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "# #            masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "\n",
    "#             opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "\n",
    "#             BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "# #                 REC_Loss = F.mse_loss(recon, batch_x, reduction='mean').to(device)\n",
    "\n",
    "#             valid_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "\n",
    "#             y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "#             y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "#         valid_loss = np.mean(valid_loss)\n",
    "#         total_valid_loss.append(valid_loss)\n",
    "#         ret = metrics.print_metrics_binary(y_true, y_pred,verbose = 0)\n",
    "#         history.append(ret)\n",
    "#         #print()\n",
    "\n",
    "#         print('Epoch %d: Loss = %.4f Valid loss = %.4f roc = %.4f'%(each_epoch, total_train_loss[-1], total_valid_loss[-1], ret['auroc']))\n",
    "#         logger.info('Epoch %d: Loss = %.4f Valid loss = %.4f roc = %.4f'%(each_epoch, total_train_loss[-1], total_valid_loss[-1], ret['auroc']))\n",
    "#         metrics.print_metrics_binary(y_true, y_pred)\n",
    "\n",
    "#         cur_auroc = ret['auroc']\n",
    "#         if cur_auroc > best_auroc:\n",
    "#             best_auroc = cur_auroc\n",
    "#             best_auprc = ret['auprc']\n",
    "#             best_minpse = ret['minpse']\n",
    "#             state = {\n",
    "#                 'net': model.state_dict(),\n",
    "#                 'optimizer': optimizer.state_dict(),\n",
    "#                 'epoch': each_epoch\n",
    "#             }\n",
    "#             torch.save(state, file_name)\n",
    "#             print('------------ Save best model - AUROC: %.4f ------------'%cur_auroc)       \n",
    "\n",
    "# print('auroc %.4f'%(best_auroc))\n",
    "# print('auprc %.4f'%(best_auprc))\n",
    "# print('minpse %.4f'%(best_minpse))  \n",
    "# logger.info('auroc %.4f'%(best_auroc))\n",
    "# logger.info('auprc %.4f'%(best_auprc))\n",
    "# logger.info('minpse %.4f'%(best_minpse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T10:41:54.613732Z",
     "start_time": "2021-01-30T10:41:54.090698Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:09:49,819 - INFO - last saved model is in epoch 126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last saved model is in epoch 126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "distcare_teacher(\n",
       "  (PositionalEncoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (RNNs): ModuleList(\n",
       "    (0): RNN(1, 32, batch_first=True)\n",
       "    (1): RNN(1, 32, batch_first=True)\n",
       "    (2): RNN(1, 32, batch_first=True)\n",
       "    (3): RNN(1, 32, batch_first=True)\n",
       "    (4): RNN(1, 32, batch_first=True)\n",
       "    (5): RNN(1, 32, batch_first=True)\n",
       "    (6): RNN(1, 32, batch_first=True)\n",
       "    (7): RNN(1, 32, batch_first=True)\n",
       "    (8): RNN(1, 32, batch_first=True)\n",
       "    (9): RNN(1, 32, batch_first=True)\n",
       "    (10): RNN(1, 32, batch_first=True)\n",
       "    (11): RNN(1, 32, batch_first=True)\n",
       "    (12): RNN(1, 32, batch_first=True)\n",
       "    (13): RNN(1, 32, batch_first=True)\n",
       "    (14): RNN(1, 32, batch_first=True)\n",
       "    (15): RNN(1, 32, batch_first=True)\n",
       "    (16): RNN(1, 32, batch_first=True)\n",
       "    (17): RNN(1, 32, batch_first=True)\n",
       "    (18): RNN(1, 32, batch_first=True)\n",
       "    (19): RNN(1, 32, batch_first=True)\n",
       "    (20): RNN(1, 32, batch_first=True)\n",
       "    (21): RNN(1, 32, batch_first=True)\n",
       "    (22): RNN(1, 32, batch_first=True)\n",
       "    (23): RNN(1, 32, batch_first=True)\n",
       "    (24): RNN(1, 32, batch_first=True)\n",
       "    (25): RNN(1, 32, batch_first=True)\n",
       "    (26): RNN(1, 32, batch_first=True)\n",
       "    (27): RNN(1, 32, batch_first=True)\n",
       "    (28): RNN(1, 32, batch_first=True)\n",
       "    (29): RNN(1, 32, batch_first=True)\n",
       "    (30): RNN(1, 32, batch_first=True)\n",
       "    (31): RNN(1, 32, batch_first=True)\n",
       "    (32): RNN(1, 32, batch_first=True)\n",
       "    (33): RNN(1, 32, batch_first=True)\n",
       "  )\n",
       "  (LastStepAttentions): ModuleList(\n",
       "    (0): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (1): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (2): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (3): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (4): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (5): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (6): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (7): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (8): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (9): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (10): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (11): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (12): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (13): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (14): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (15): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (16): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (17): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (18): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (19): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (20): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (21): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (22): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (23): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (24): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (25): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (26): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (27): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (28): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (29): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (30): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (31): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (32): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (33): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "  )\n",
       "  (FinalAttentionQKV): FinalAttentionQKV(\n",
       "    (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_k): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_v): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_out): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (tanh): Tanh()\n",
       "    (softmax): Softmax(dim=None)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (MultiHeadedAttention): MultiHeadedAttention(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (final_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (SublayerConnection): SublayerConnection(\n",
       "    (norm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (PositionwiseFeedForward): PositionwiseFeedForward(\n",
       "    (w_1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (w_2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (demo_proj_main): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (demo_proj): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (Linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (output): Linear(in_features=34, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (tanh): Tanh()\n",
       "  (softmax): Softmax(dim=None)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if target_dataset == 'TJ':    \n",
    "    file_name = './model/pretrained-challenge-front-fill-teacher-2covid-rnn'\n",
    "elif target_dataset == 'HM':\n",
    "    file_name = './model/pretrained-challenge-front-fill-teacher-2spain-rnn'\n",
    "    \n",
    "checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:3\" if torch.cuda.is_available() == True else 'cpu') )\n",
    "save_epoch = checkpoint['epoch']\n",
    "print(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "logger.info(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T10:42:04.714142Z",
     "start_time": "2021-01-30T10:41:56.611967Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:09:50,603 - INFO - Batch 0: Test Loss = 0.1756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Test Loss = 0.1756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:09:56,973 - INFO - \n",
      "==>Predicting on test\n",
      "2023-11-08 12:09:56,975 - INFO - Test Loss = 0.1938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>Predicting on test\n",
      "Test Loss = 0.1938\n",
      "confusion matrix:\n",
      "[[3726   10]\n",
      " [ 256   41]]\n",
      "accuracy = 0.9340441226959229\n",
      "precision class 0 = 0.9357106685638428\n",
      "precision class 1 = 0.8039215803146362\n",
      "recall class 0 = 0.9973233342170715\n",
      "recall class 1 = 0.13804714381694794\n",
      "AUC of ROC = 0.8778560047296663\n",
      "AUC of PRC = 0.47684635532547415\n",
      "min(+P, Se) = 0.4563758389261745\n",
      "f1_score = 0.2356321890877203\n"
     ]
    }
   ],
   "source": [
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "pad_token = np.zeros(34)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(test_x, test_y, test_mask_x, test_x_len, batch_size, shuffle=True)):  \n",
    "        optimizer.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "        opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "\n",
    "        loss = model_loss\n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "            logger.info('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "        y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "        y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "logger.info(\"\\n==>Predicting on test\")\n",
    "logger.info('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "test_res = metrics.print_metrics_binary(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:02.685007Z",
     "start_time": "2021-02-10T15:06:02.655919Z"
    }
   },
   "outputs": [],
   "source": [
    "class distcare_student(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(distcare_student, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 400)\n",
    "\n",
    "        self.RNNs = clones(nn.RNN(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        self.generalRNN =  nn.RNN(1, self.hidden_dim, batch_first = True)\n",
    "        self.LastStepAttentions = clones(SingleAttention(self.hidden_dim, 8, attention_type='concat', demographic_dim=12, time_aware=True, use_demographic=False),self.input_dim)\n",
    "        \n",
    "        self.FinalAttentionQKV = FinalAttentionQKV(self.hidden_dim, self.hidden_dim, attention_type='mul',dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model,dropout = 1 - self.keep_prob)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.output = nn.Linear(34, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.FC_embed = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.to_MMD = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    def forward(self, input, input_diff, lens, tar, train):\n",
    "        lens = lens.to('cpu')\n",
    "        tar_x, tar_lens = tar\n",
    "        # input shape [batch_size, timestep, feature_dim]\n",
    "#         demo_main = self.tanh(self.demo_proj_main(demo_input)).unsqueeze(1)# b hidden_dim\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        feature_dim_diff = input_diff.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "\n",
    "        # Initialization\n",
    "        #cur_hs = Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0))\n",
    "\n",
    "        # forward\n",
    "        # RNN_embeded_input = self.RNNs[0](input[:,:,0].unsqueeze(-1), Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device))[0] # b t h\n",
    "        # Attention_embeded_input = self.LastStepAttentions[0](RNN_embeded_input)[0].unsqueeze(1)# b 1 h\n",
    "        # for i in range(feature_dim-1):\n",
    "        #     embeded_input = self.RNNs[i+1](input[:,:,i+1].unsqueeze(-1), Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device))[0] # b 1 h\n",
    "        #     embeded_input = self.LastStepAttentions[i+1](embeded_input)[0].unsqueeze(1)# b 1 h\n",
    "        #     Attention_embeded_input = torch.cat((Attention_embeded_input, embeded_input), 1)# b i h\n",
    "\n",
    "        # Attention_embeded_input = torch.cat((Attention_embeded_input, demo_main), 1)# b i+1 h\n",
    "        # posi_input = self.dropout(Attention_embeded_input) # batch_size * d_input+1 * hidden_dim\n",
    "\n",
    "#         input = pack_padded_sequence(input, lens, batch_first=True)\n",
    "        \n",
    "        RNN_embeded_input = self.RNNs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1][0].squeeze().unsqueeze(1) # b 1 h\n",
    "#         print(RNN_embeded_input.shape)\n",
    "        for i in range(feature_dim-1):\n",
    "            embeded_input = self.RNNs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1][0].squeeze().unsqueeze(1) # b 1 h\n",
    "            RNN_embeded_input = torch.cat((RNN_embeded_input, embeded_input), 1)\n",
    "\n",
    "#         RNN_embeded_input = torch.cat((RNN_embeded_input, demo_main), 1)# b i+1 h\n",
    "        General_RNN_embeded_input = self.generalRNN(pack_padded_sequence(input_diff[:,:,0].unsqueeze(-1), lens, batch_first=True))[1][0].squeeze().unsqueeze(1) # b 1 h\n",
    "        for i in range(1,feature_dim_diff):\n",
    "            general_embeded_input = self.generalRNN(pack_padded_sequence(input_diff[:,:,i].unsqueeze(-1), lens, batch_first=True))[1][0].squeeze().unsqueeze(1) # b 1 h\n",
    "            General_RNN_embeded_input = torch.cat((General_RNN_embeded_input,general_embeded_input), 1)\n",
    "        \n",
    "#         posi_input = self.dropout(RNN_embeded_input) # batch_size * d_input * hidden_dim\n",
    "        posi_input = self.dropout(torch.cat((RNN_embeded_input, General_RNN_embeded_input), 1)) # batch_size * d_input * hidden_dim\n",
    "    \n",
    "        # cul tar loss\n",
    "        if train:\n",
    "            tar_lens = tar_lens.to('cpu')\n",
    "            RNN_tar_input = self.RNNs[0](pack_padded_sequence(tar_x[:,:,0].unsqueeze(-1), tar_lens, batch_first=True))[1][0].squeeze().unsqueeze(1) # b 1 h\n",
    "            for i in range(feature_dim-1):\n",
    "                tar_input = self.RNNs[i+1](pack_padded_sequence(tar_x[:,:,i+1].unsqueeze(-1), tar_lens, batch_first=True))[1][0].squeeze().unsqueeze(1) # b 1 h\n",
    "                RNN_tar_input = torch.cat((RNN_tar_input, tar_input), 1)\n",
    "            RNN_embeded_output = self.to_MMD(RNN_embeded_input)\n",
    "            RNN_tar_output = self.to_MMD(RNN_tar_input)\n",
    "        else:\n",
    "            RNN_embeded_output = []\n",
    "            RNN_tar_output = []\n",
    "        \n",
    "        #mask = subsequent_mask(time_step).to(device) # 1 t t 下三角 N to 1任务不用mask\n",
    "#         contexts = self.SublayerConnection(posi_input, lambda x: self.MultiHeadedAttention(posi_input, posi_input, posi_input, None))# # batch_size * d_input * hidden_dim\n",
    "    \n",
    "#         DeCov_loss = contexts[1]\n",
    "#         contexts = contexts[0]\n",
    "\n",
    "#         contexts = self.SublayerConnection(contexts, lambda x: self.PositionwiseFeedForward(contexts))[0]# # batch_size * d_input * hidden_dim\n",
    "#         #contexts = contexts.view(batch_size, feature_dim * self.hidden_dim)#\n",
    "#         # contexts = torch.matmul(self.Wproj, contexts) + self.bproj\n",
    "#         # contexts = contexts.squeeze()\n",
    "#         # demo_key = self.demo_proj(demo_input)# b hidden_dim\n",
    "#         # demo_key = self.relu(demo_key)\n",
    "#         # input_dim_scores = torch.matmul(contexts, demo_key.unsqueeze(-1)).squeeze() # b i\n",
    "#         # input_dim_scores = self.dropout(self.sigmoid(input_dim_scores)).unsqueeze(1)# b i\n",
    "        \n",
    "#         # weighted_contexts = torch.matmul(input_dim_scores, contexts).squeeze()\n",
    "# #         print(contexts.shape)\n",
    "\n",
    "#         weighted_contexts = self.FinalAttentionQKV(contexts)[0]\n",
    "#         output_embed = self.FC_embed(weighted_contexts)\n",
    "#         output = self.output(self.dropout(weighted_contexts))# b 1\n",
    "#         output = self.sigmoid(output)\n",
    "        contexts = self.Linear(posi_input).squeeze()# b i\n",
    "        output = self.output(self.dropout(contexts))# b 1\n",
    "        output = self.sigmoid(output)\n",
    "#         print(weighted_contexts.shape)\n",
    "          \n",
    "        return output, None, contexts, [RNN_embeded_output, RNN_tar_output]\n",
    "    #, self.MultiHeadedAttention.attn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:09:57,041 - INFO - load target data\n",
      "2023-11-08 12:10:00,421 - INFO - [[ 0.35672857  0.36219378 -0.04155072 -0.39134566  0.76306464 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [ 2.89139623 -0.16505646 -0.7197426  -0.5274138   0.76306464 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.72955757  0.88944403  1.54089701 -1.13972045  0.01202093 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.72955757  0.36219378  1.54089701 -1.13972045  0.01202093 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.09589066  1.41669427  0.63664116 -0.93561823 -0.09527102 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [ 2.07668162  1.41669427  0.63664116 -1.75202709 -0.09527102 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [ 0.08515703  0.88944403  0.18451324 -0.32331159 -0.09527102 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [ 1.08091933  1.41669427  0.4105772   0.9013017   1.19223247 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.54850988  1.94394452  0.18451324  1.98984685  1.08494051 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.54850988  1.41669427 -1.62399845 -1.0036523  -1.1681906  -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-1.72531987  1.41669427  0.4105772   0.28899506  1.51410834 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [ 0.17568088  1.41669427  0.18451324  0.56113134  2.05056813 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.63903373  1.41669427 -1.17187053 -1.0036523   0.54848072 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-1.27270064  1.41669427 -2.52825429  0.83326763 -0.30985494 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-1.27270064  0.88944403 -2.52825429  0.83326763 -0.30985494 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-1.1821768   1.94394452 -0.04155072  0.35702913  1.51410834 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.09589066  1.41669427 -0.49367864  0.4250632   0.11931289 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-1.27270064  0.88944403 -0.04155072  0.83326763  0.11931289 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [ 0.17568088  1.94394452  0.86270513 -0.79955009  0.97764855 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.91060526  1.94394452 -1.17187053  0.15292691  1.19223247 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.27693835  0.88944403 -0.94580656 -2.09219745 -1.38277452 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-1.54427218  0.88944403  0.18451324  1.58164242 -1.1681906  -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.27693835  1.41669427 -0.94580656  0.22096099  0.44118876 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-3.62632062  0.36219378 -1.39793449 -1.88809523  0.01202093 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.63903373  1.41669427 -1.17187053  0.01685877 -0.84631473 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.91060526  1.41669427 -0.26761468  0.49309727  0.01202093 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-1.09165295  0.88944403  0.4105772   1.51360835  0.22660485 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-2.35898678  1.41669427  0.4105772   1.3775402   1.51410834 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:10:00,434 - INFO - 26\n",
      "2023-11-08 12:10:00,435 - INFO - 4255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.35672857  0.36219378 -0.04155072 -0.39134566  0.76306464 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [ 2.89139623 -0.16505646 -0.7197426  -0.5274138   0.76306464 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.72955757  0.88944403  1.54089701 -1.13972045  0.01202093 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.72955757  0.36219378  1.54089701 -1.13972045  0.01202093 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.09589066  1.41669427  0.63664116 -0.93561823 -0.09527102 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [ 2.07668162  1.41669427  0.63664116 -1.75202709 -0.09527102 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [ 0.08515703  0.88944403  0.18451324 -0.32331159 -0.09527102 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [ 1.08091933  1.41669427  0.4105772   0.9013017   1.19223247 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.54850988  1.94394452  0.18451324  1.98984685  1.08494051 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.54850988  1.41669427 -1.62399845 -1.0036523  -1.1681906  -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-1.72531987  1.41669427  0.4105772   0.28899506  1.51410834 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [ 0.17568088  1.41669427  0.18451324  0.56113134  2.05056813 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.63903373  1.41669427 -1.17187053 -1.0036523   0.54848072 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-1.27270064  1.41669427 -2.52825429  0.83326763 -0.30985494 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-1.27270064  0.88944403 -2.52825429  0.83326763 -0.30985494 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-1.1821768   1.94394452 -0.04155072  0.35702913  1.51410834 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.09589066  1.41669427 -0.49367864  0.4250632   0.11931289 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-1.27270064  0.88944403 -0.04155072  0.83326763  0.11931289 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [ 0.17568088  1.94394452  0.86270513 -0.79955009  0.97764855 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.91060526  1.94394452 -1.17187053  0.15292691  1.19223247 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.27693835  0.88944403 -0.94580656 -2.09219745 -1.38277452 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-1.54427218  0.88944403  0.18451324  1.58164242 -1.1681906  -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.27693835  1.41669427 -0.94580656  0.22096099  0.44118876 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-3.62632062  0.36219378 -1.39793449 -1.88809523  0.01202093 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.63903373  1.41669427 -1.17187053  0.01685877 -0.84631473 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-0.91060526  1.41669427 -0.26761468  0.49309727  0.01202093 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-1.09165295  0.88944403  0.4105772   1.51360835  0.22660485 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]\n",
      " [-2.35898678  1.41669427  0.4105772   1.3775402   1.51410834 -0.05394446\n",
      "   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824\n",
      "  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609\n",
      "  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885\n",
      "  -0.04331355 -0.14866188]]\n",
      "26\n",
      "4255\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"load target data\")\n",
    "if target_dataset == 'TJ':\n",
    "    data_path = './data/Tongji/'\n",
    "    tar_all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    tar_all_y = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    tar_all_time = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    tar_all_x_len = [len(i) for i in tar_all_x]\n",
    "\n",
    "    for i in range(len(tar_all_time)):\n",
    "        for j in range(len(tar_all_time[i])):\n",
    "            tar_all_time[i][j] = tar_all_time[i][j][-1]\n",
    "            tar_all_y[i][j] = tar_all_y[i][j][0]\n",
    "\n",
    "    tar_subset_idx = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "    tar_other_idx = list(range(75))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(tar_all_x)):\n",
    "        cur = np.array(tar_all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "    #     tar_all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "        tar_all_x[i] = cur_subset\n",
    "elif target_dataset == 'HM':\n",
    "    data_path = './data/CDSL/'\n",
    "    tar_all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    tar_all_y = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    tar_all_time = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    tar_all_x_len = [len(i) for i in tar_all_x]\n",
    "\n",
    "    for i in range(len(tar_all_time)):\n",
    "        for j in range(len(tar_all_time[i])):\n",
    "            tar_all_time[i][j] = tar_all_time[i][j][-1]\n",
    "            tar_all_y[i][j] = tar_all_y[i][j][0]\n",
    "\n",
    "    tar_subset_idx = [5, 6, 4, 2, 3, 48, 79, 76, 87, 25, 30, 31, 18, 43, 58, 66, 40, 57, 23, 92, 50, 54, 91, 60, 39, 81]\n",
    "    tar_other_idx = list(range(99))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(tar_all_x)):\n",
    "        cur = np.array(tar_all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "    #     tar_all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "        tar_all_x[i] = cur_subset\n",
    "    \n",
    "print(tar_all_x[0])\n",
    "print(len(tar_all_x[0][0]))\n",
    "print(len(tar_all_x))\n",
    "logger.info(tar_all_x[0])\n",
    "logger.info(len(tar_all_x[0][0]))\n",
    "logger.info(len(tar_all_x))\n",
    "\n",
    "assert(subset_cnt == len(tar_subset_idx))\n",
    "\n",
    "examples = []\n",
    "for idx in range(len(tar_all_x)):\n",
    "    examples.append((tar_all_x[idx], tar_all_y[idx], tar_all_time[idx], tar_all_x_len[idx]))\n",
    "examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "tar_all_x = [e[0] for e in examples]\n",
    "tar_all_y = [e[1] for e in examples]\n",
    "tar_all_time = [e[2] for e in examples]\n",
    "tar_all_x_len = [e[3] for e in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:06.077135Z",
     "start_time": "2021-02-10T15:06:05.101451Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 256\n",
    "input_dim = subset_cnt\n",
    "hidden_dim = 32\n",
    "d_model = 32\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1\n",
    "\n",
    "model_student = distcare_student(input_dim = input_dim, hidden_dim = hidden_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, output_dim = output_dim).to(device)\n",
    "# input_dim, d_model, d_k, d_v, MHD_num_head, d_ff, output_dim\n",
    "optimizer_student = torch.optim.Adam(model_student.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T10:43:10.804571Z",
     "start_time": "2021-01-30T10:42:10.264393Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:10:01,096 - INFO - Batch 0: Test Loss = 0.1907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Test Loss = 0.1907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:10:10,422 - INFO - Batch 20: Test Loss = 0.2129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20: Test Loss = 0.2129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:10:19,092 - INFO - Batch 40: Test Loss = 0.2256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40: Test Loss = 0.2256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:10:27,319 - INFO - Batch 60: Test Loss = 0.2093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 60: Test Loss = 0.2093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:10:36,045 - INFO - Batch 80: Test Loss = 0.1635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 80: Test Loss = 0.1635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:10:45,063 - INFO - Batch 100: Test Loss = 0.1511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100: Test Loss = 0.1511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:10:54,333 - INFO - Batch 120: Test Loss = 0.1866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 120: Test Loss = 0.1866\n"
     ]
    }
   ],
   "source": [
    "#Generate Teacher model embedding\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.eval()\n",
    "\n",
    "train_teacher_emb = []\n",
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "pad_token = np.zeros(34)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(train_x, train_y, train_mask_x, train_x_len, batch_size, shuffle=False)):  \n",
    "        optimizer.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "        opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "        train_teacher_emb.append(emb.cpu().detach().numpy())\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, model_loss.cpu().detach().numpy()))\n",
    "            logger.info('Batch %d: Test Loss = %.4f'%(step, model_loss.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_all_x = torch.tensor(pad_sents(tar_all_x, np.zeros(len(tar_subset_idx))), dtype=torch.float32).to(device)\n",
    "tar_all_x_len = torch.tensor(tar_all_x_len, dtype=torch.float32).to(device).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5, fix_sigma=None, **kwargs):\n",
    "        super(MMDLoss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = None\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul, kernel_num, fix_sigma):\n",
    "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2)\n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
    "                          for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
    "                      for bandwidth_temp in bandwidth_list]\n",
    "        return sum(kernel_val)\n",
    "\n",
    "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
    "        loss = 0.0\n",
    "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
    "        loss = delta.dot(delta.T)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        if self.kernel_type == 'linear':\n",
    "            return self.linear_mmd2(source, target)\n",
    "        elif self.kernel_type == 'rbf':\n",
    "            batch_size = int(source.size()[0])\n",
    "            kernels = self.guassian_kernel(\n",
    "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
    "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
    "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
    "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
    "            loss = torch.mean(XX + YY - XY - YX)\n",
    "            return loss\n",
    "\n",
    "class MultitaskLoss(nn.Module):\n",
    "    def __init__(self, task_num=3):\n",
    "        super(MultitaskLoss, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.alpha = nn.Parameter(torch.ones((task_num)), requires_grad=True)\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.kl = nn.KLDivLoss(reduce=True, size_average=True)\n",
    "        self.tar = MMDLoss()\n",
    "\n",
    "    def forward(self, opt_student, batch_y, emb_student, emb_teacher, tar_source, tar_tar):\n",
    "        BCE_Loss = self.bce(opt_student, batch_y)\n",
    "        emb_Loss = self.kl(emb_student, emb_teacher)\n",
    "        tar_Loss = self.tar(source=tar_source, target=tar_tar)\n",
    "        if teacher_flag:\n",
    "            return BCE_Loss * self.alpha[0] + emb_Loss * self.alpha[1] + tar_Loss * self.alpha[2]\n",
    "        else:\n",
    "            return BCE_Loss * self.alpha[0] + tar_Loss * self.alpha[2]\n",
    "\n",
    "def get_multitask_loss(opt_student, batch_y, emb_student, emb_teacher, tar_source, tar_tar):\n",
    "    mtl = MultitaskLoss(task_num=3)\n",
    "    return mtl(opt_student, batch_y, emb_student, emb_teacher, tar_source, tar_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T15:39:24.489286Z",
     "start_time": "2021-01-30T10:43:58.619303Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:10:57,077 - INFO - Training Student\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:10:58,379 - INFO - Epoch 0 Batch 0: Train Loss = 0.8607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0: Train Loss = 0.8607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:11:16,728 - INFO - Epoch 0 Batch 20: Train Loss = 0.4317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 20: Train Loss = 0.4317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:11:34,102 - INFO - Epoch 0 Batch 40: Train Loss = 0.3488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 40: Train Loss = 0.3488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:11:52,660 - INFO - Epoch 0 Batch 60: Train Loss = 0.3605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 60: Train Loss = 0.3605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:12:12,549 - INFO - Epoch 0 Batch 80: Train Loss = 0.2579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 80: Train Loss = 0.2579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:12:32,095 - INFO - Epoch 0 Batch 100: Train Loss = 0.2274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 100: Train Loss = 0.2274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:12:50,018 - INFO - Epoch 0 Batch 120: Train Loss = 0.2924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 120: Train Loss = 0.2924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/DistCare-plus/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
      "/home/zzj/DistCare-plus/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
      "2023-11-08 12:13:02,814 - INFO - ------------ Save best model - AUROC: 0.7256 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.0605 Valid loss = 0.2536 roc = 0.7256\n",
      "confusion matrix:\n",
      "[[3742    0]\n",
      " [ 292    0]]\n",
      "accuracy = 0.9276152849197388\n",
      "precision class 0 = 0.9276152849197388\n",
      "precision class 1 = nan\n",
      "recall class 0 = 1.0\n",
      "recall class 1 = 0.0\n",
      "AUC of ROC = 0.7255670544650505\n",
      "AUC of PRC = 0.1546744785302498\n",
      "min(+P, Se) = 0.19937694704049844\n",
      "f1_score = nan\n",
      "------------ Save best model - AUROC: 0.7256 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:13:03,705 - INFO - Epoch 1 Batch 0: Train Loss = 0.2870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0: Train Loss = 0.2870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:13:22,021 - INFO - Epoch 1 Batch 20: Train Loss = 0.3166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 20: Train Loss = 0.3166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:13:39,314 - INFO - Epoch 1 Batch 40: Train Loss = 0.3071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 40: Train Loss = 0.3071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:13:59,059 - INFO - Epoch 1 Batch 60: Train Loss = 0.3493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 60: Train Loss = 0.3493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:14:17,610 - INFO - Epoch 1 Batch 80: Train Loss = 0.2560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 80: Train Loss = 0.2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:14:36,087 - INFO - Epoch 1 Batch 100: Train Loss = 0.2181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 100: Train Loss = 0.2181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:14:55,526 - INFO - Epoch 1 Batch 120: Train Loss = 0.2732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 120: Train Loss = 0.2732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/DistCare-plus/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
      "/home/zzj/DistCare-plus/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
      "2023-11-08 12:15:07,557 - INFO - ------------ Save best model - AUROC: 0.7640 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.2676 Valid loss = 0.2495 roc = 0.7640\n",
      "confusion matrix:\n",
      "[[3742    0]\n",
      " [ 292    0]]\n",
      "accuracy = 0.9276152849197388\n",
      "precision class 0 = 0.9276152849197388\n",
      "precision class 1 = nan\n",
      "recall class 0 = 1.0\n",
      "recall class 1 = 0.0\n",
      "AUC of ROC = 0.7639667821031899\n",
      "AUC of PRC = 0.17153120618901008\n",
      "min(+P, Se) = 0.22259136212624583\n",
      "f1_score = nan\n",
      "------------ Save best model - AUROC: 0.7640 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:15:08,370 - INFO - Epoch 2 Batch 0: Train Loss = 0.2864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 0: Train Loss = 0.2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:15:26,143 - INFO - Epoch 2 Batch 20: Train Loss = 0.2989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 20: Train Loss = 0.2989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:15:44,627 - INFO - Epoch 2 Batch 40: Train Loss = 0.2976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 40: Train Loss = 0.2976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:16:02,598 - INFO - Epoch 2 Batch 60: Train Loss = 0.3468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 60: Train Loss = 0.3468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:16:20,556 - INFO - Epoch 2 Batch 80: Train Loss = 0.2469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 80: Train Loss = 0.2469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:16:39,167 - INFO - Epoch 2 Batch 100: Train Loss = 0.2417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 100: Train Loss = 0.2417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:16:58,026 - INFO - Epoch 2 Batch 120: Train Loss = 0.2583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 120: Train Loss = 0.2583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/DistCare-plus/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
      "/home/zzj/DistCare-plus/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
      "2023-11-08 12:17:10,407 - INFO - ------------ Save best model - AUROC: 0.7679 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss = 0.2666 Valid loss = 0.2448 roc = 0.7679\n",
      "confusion matrix:\n",
      "[[3742    0]\n",
      " [ 292    0]]\n",
      "accuracy = 0.9276152849197388\n",
      "precision class 0 = 0.9276152849197388\n",
      "precision class 1 = nan\n",
      "recall class 0 = 1.0\n",
      "recall class 1 = 0.0\n",
      "AUC of ROC = 0.7678893054040401\n",
      "AUC of PRC = 0.21304411498412246\n",
      "min(+P, Se) = 0.2901023890784983\n",
      "f1_score = nan\n",
      "------------ Save best model - AUROC: 0.7679 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:17:11,643 - INFO - Epoch 3 Batch 0: Train Loss = 0.2910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 0: Train Loss = 0.2910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:17:31,723 - INFO - Epoch 3 Batch 20: Train Loss = 0.2862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 20: Train Loss = 0.2862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:17:50,656 - INFO - Epoch 3 Batch 40: Train Loss = 0.2933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 40: Train Loss = 0.2933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:18:10,090 - INFO - Epoch 3 Batch 60: Train Loss = 0.3088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 60: Train Loss = 0.3088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:18:29,117 - INFO - Epoch 3 Batch 80: Train Loss = 0.2310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 80: Train Loss = 0.2310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:18:47,419 - INFO - Epoch 3 Batch 100: Train Loss = 0.2128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 100: Train Loss = 0.2128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:19:06,120 - INFO - Epoch 3 Batch 120: Train Loss = 0.2740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 120: Train Loss = 0.2740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/DistCare-plus/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
      "/home/zzj/DistCare-plus/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
      "2023-11-08 12:19:18,171 - INFO - ------------ Save best model - AUROC: 0.7905 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss = 0.2628 Valid loss = 0.2470 roc = 0.7905\n",
      "confusion matrix:\n",
      "[[3742    0]\n",
      " [ 292    0]]\n",
      "accuracy = 0.9276152849197388\n",
      "precision class 0 = 0.9276152849197388\n",
      "precision class 1 = nan\n",
      "recall class 0 = 1.0\n",
      "recall class 1 = 0.0\n",
      "AUC of ROC = 0.7905197755211116\n",
      "AUC of PRC = 0.28271342543662353\n",
      "min(+P, Se) = 0.3197278911564626\n",
      "f1_score = nan\n",
      "------------ Save best model - AUROC: 0.7905 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:19:19,067 - INFO - Epoch 4 Batch 0: Train Loss = 0.2773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 0: Train Loss = 0.2773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:19:38,099 - INFO - Epoch 4 Batch 20: Train Loss = 0.2784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 20: Train Loss = 0.2784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:19:55,147 - INFO - Epoch 4 Batch 40: Train Loss = 0.2775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 40: Train Loss = 0.2775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:20:13,215 - INFO - Epoch 4 Batch 60: Train Loss = 0.3136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 60: Train Loss = 0.3136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:20:31,520 - INFO - Epoch 4 Batch 80: Train Loss = 0.2338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 80: Train Loss = 0.2338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:20:50,222 - INFO - Epoch 4 Batch 100: Train Loss = 0.2086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 100: Train Loss = 0.2086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:21:08,132 - INFO - Epoch 4 Batch 120: Train Loss = 0.2547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 120: Train Loss = 0.2547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/DistCare-plus/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
      "/home/zzj/DistCare-plus/utils/metrics.py:23: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
      "2023-11-08 12:21:19,352 - INFO - ------------ Save best model - AUROC: 0.7920 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss = 0.2587 Valid loss = 0.2282 roc = 0.7920\n",
      "confusion matrix:\n",
      "[[3742    0]\n",
      " [ 292    0]]\n",
      "accuracy = 0.9276152849197388\n",
      "precision class 0 = 0.9276152849197388\n",
      "precision class 1 = nan\n",
      "recall class 0 = 1.0\n",
      "recall class 1 = 0.0\n",
      "AUC of ROC = 0.7919831714049332\n",
      "AUC of PRC = 0.34141318234021406\n",
      "min(+P, Se) = 0.3424657534246575\n",
      "f1_score = nan\n",
      "------------ Save best model - AUROC: 0.7920 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:21:20,167 - INFO - Epoch 5 Batch 0: Train Loss = 0.2860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 0: Train Loss = 0.2860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:21:38,981 - INFO - Epoch 5 Batch 20: Train Loss = 0.2752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 20: Train Loss = 0.2752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:21:57,366 - INFO - Epoch 5 Batch 40: Train Loss = 0.2757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 40: Train Loss = 0.2757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:22:15,127 - INFO - Epoch 5 Batch 60: Train Loss = 0.2705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 60: Train Loss = 0.2705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:22:32,810 - INFO - Epoch 5 Batch 80: Train Loss = 0.2216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 80: Train Loss = 0.2216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:22:51,398 - INFO - Epoch 5 Batch 100: Train Loss = 0.1881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 100: Train Loss = 0.1881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:23:09,991 - INFO - Epoch 5 Batch 120: Train Loss = 0.2311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 120: Train Loss = 0.2311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:23:22,492 - INFO - ------------ Save best model - AUROC: 0.8060 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 0.2497 Valid loss = 0.2201 roc = 0.8060\n",
      "confusion matrix:\n",
      "[[3741    1]\n",
      " [ 286    6]]\n",
      "accuracy = 0.928854763507843\n",
      "precision class 0 = 0.9289793968200684\n",
      "precision class 1 = 0.8571428656578064\n",
      "recall class 0 = 0.9997327923774719\n",
      "recall class 1 = 0.02054794505238533\n",
      "AUC of ROC = 0.8060062379651934\n",
      "AUC of PRC = 0.36023900345037907\n",
      "min(+P, Se) = 0.3561643835616438\n",
      "f1_score = 0.04013377983325203\n",
      "------------ Save best model - AUROC: 0.8060 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:23:23,522 - INFO - Epoch 6 Batch 0: Train Loss = 0.2585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 0: Train Loss = 0.2585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:23:43,975 - INFO - Epoch 6 Batch 20: Train Loss = 0.2565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 20: Train Loss = 0.2565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:24:02,847 - INFO - Epoch 6 Batch 40: Train Loss = 0.2890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 40: Train Loss = 0.2890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:24:21,540 - INFO - Epoch 6 Batch 60: Train Loss = 0.3060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 60: Train Loss = 0.3060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:24:39,752 - INFO - Epoch 6 Batch 80: Train Loss = 0.2189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 80: Train Loss = 0.2189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:24:57,781 - INFO - Epoch 6 Batch 100: Train Loss = 0.1945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 100: Train Loss = 0.1945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:25:16,701 - INFO - Epoch 6 Batch 120: Train Loss = 0.2367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 120: Train Loss = 0.2367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:25:29,118 - INFO - ------------ Save best model - AUROC: 0.8136 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss = 0.2393 Valid loss = 0.2182 roc = 0.8136\n",
      "confusion matrix:\n",
      "[[3741    1]\n",
      " [ 276   16]]\n",
      "accuracy = 0.9313336610794067\n",
      "precision class 0 = 0.9312919974327087\n",
      "precision class 1 = 0.9411764740943909\n",
      "recall class 0 = 0.9997327923774719\n",
      "recall class 1 = 0.054794520139694214\n",
      "AUC of ROC = 0.8135629983233639\n",
      "AUC of PRC = 0.3711900557659712\n",
      "min(+P, Se) = 0.363013698630137\n",
      "f1_score = 0.10355987294106467\n",
      "------------ Save best model - AUROC: 0.8136 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:25:30,086 - INFO - Epoch 7 Batch 0: Train Loss = 0.2395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 0: Train Loss = 0.2395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:25:48,075 - INFO - Epoch 7 Batch 20: Train Loss = 0.2702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 20: Train Loss = 0.2702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:26:05,845 - INFO - Epoch 7 Batch 40: Train Loss = 0.2833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 40: Train Loss = 0.2833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:26:23,049 - INFO - Epoch 7 Batch 60: Train Loss = 0.2921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 60: Train Loss = 0.2921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:26:42,521 - INFO - Epoch 7 Batch 80: Train Loss = 0.2226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 80: Train Loss = 0.2226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:27:01,357 - INFO - Epoch 7 Batch 100: Train Loss = 0.2140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 100: Train Loss = 0.2140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:27:19,066 - INFO - Epoch 7 Batch 120: Train Loss = 0.2547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 120: Train Loss = 0.2547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:27:30,689 - INFO - ------------ Save best model - AUROC: 0.8186 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss = 0.2365 Valid loss = 0.2156 roc = 0.8186\n",
      "confusion matrix:\n",
      "[[3741    1]\n",
      " [ 276   16]]\n",
      "accuracy = 0.9313336610794067\n",
      "precision class 0 = 0.9312919974327087\n",
      "precision class 1 = 0.9411764740943909\n",
      "recall class 0 = 0.9997327923774719\n",
      "recall class 1 = 0.054794520139694214\n",
      "AUC of ROC = 0.8186176171265822\n",
      "AUC of PRC = 0.3820430320209147\n",
      "min(+P, Se) = 0.3898305084745763\n",
      "f1_score = 0.10355987294106467\n",
      "------------ Save best model - AUROC: 0.8186 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:27:31,655 - INFO - Epoch 8 Batch 0: Train Loss = 0.2557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 0: Train Loss = 0.2557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:27:49,595 - INFO - Epoch 8 Batch 20: Train Loss = 0.2471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 20: Train Loss = 0.2471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:28:07,472 - INFO - Epoch 8 Batch 40: Train Loss = 0.2803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 40: Train Loss = 0.2803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:28:24,470 - INFO - Epoch 8 Batch 60: Train Loss = 0.2702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 60: Train Loss = 0.2702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:28:43,177 - INFO - Epoch 8 Batch 80: Train Loss = 0.2236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 80: Train Loss = 0.2236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:29:02,763 - INFO - Epoch 8 Batch 100: Train Loss = 0.1791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 100: Train Loss = 0.1791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:29:21,779 - INFO - Epoch 8 Batch 120: Train Loss = 0.2602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 120: Train Loss = 0.2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:29:34,713 - INFO - ------------ Save best model - AUROC: 0.8202 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss = 0.2327 Valid loss = 0.2149 roc = 0.8202\n",
      "confusion matrix:\n",
      "[[3741    1]\n",
      " [ 276   16]]\n",
      "accuracy = 0.9313336610794067\n",
      "precision class 0 = 0.9312919974327087\n",
      "precision class 1 = 0.9411764740943909\n",
      "recall class 0 = 0.9997327923774719\n",
      "recall class 1 = 0.054794520139694214\n",
      "AUC of ROC = 0.820222868146109\n",
      "AUC of PRC = 0.3883491943335836\n",
      "min(+P, Se) = 0.3924914675767918\n",
      "f1_score = 0.10355987294106467\n",
      "------------ Save best model - AUROC: 0.8202 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:29:35,715 - INFO - Epoch 9 Batch 0: Train Loss = 0.2409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 0: Train Loss = 0.2409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:29:54,224 - INFO - Epoch 9 Batch 20: Train Loss = 0.2616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 20: Train Loss = 0.2616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:30:11,935 - INFO - Epoch 9 Batch 40: Train Loss = 0.2807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 40: Train Loss = 0.2807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:30:29,257 - INFO - Epoch 9 Batch 60: Train Loss = 0.2577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 60: Train Loss = 0.2577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:30:47,669 - INFO - Epoch 9 Batch 80: Train Loss = 0.2394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 80: Train Loss = 0.2394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:31:06,673 - INFO - Epoch 9 Batch 100: Train Loss = 0.1866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 100: Train Loss = 0.1866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:31:25,159 - INFO - Epoch 9 Batch 120: Train Loss = 0.2485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 120: Train Loss = 0.2485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:31:37,759 - INFO - ------------ Save best model - AUROC: 0.8225 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss = 0.2274 Valid loss = 0.2135 roc = 0.8225\n",
      "confusion matrix:\n",
      "[[3741    1]\n",
      " [ 273   19]]\n",
      "accuracy = 0.9320773482322693\n",
      "precision class 0 = 0.9319880604743958\n",
      "precision class 1 = 0.949999988079071\n",
      "recall class 0 = 0.9997327923774719\n",
      "recall class 1 = 0.06506849080324173\n",
      "AUC of ROC = 0.8224943807062371\n",
      "AUC of PRC = 0.38741528952628124\n",
      "min(+P, Se) = 0.4080267558528428\n",
      "f1_score = 0.121794861326831\n",
      "------------ Save best model - AUROC: 0.8225 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:31:38,524 - INFO - Epoch 10 Batch 0: Train Loss = 0.2604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 0: Train Loss = 0.2604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:31:57,671 - INFO - Epoch 10 Batch 20: Train Loss = 0.2667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 20: Train Loss = 0.2667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:32:15,019 - INFO - Epoch 10 Batch 40: Train Loss = 0.2680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 40: Train Loss = 0.2680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:32:32,935 - INFO - Epoch 10 Batch 60: Train Loss = 0.2717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 60: Train Loss = 0.2717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:32:50,539 - INFO - Epoch 10 Batch 80: Train Loss = 0.2364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 80: Train Loss = 0.2364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:33:09,041 - INFO - Epoch 10 Batch 100: Train Loss = 0.1919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 100: Train Loss = 0.1919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:33:26,801 - INFO - Epoch 10 Batch 120: Train Loss = 0.2358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 120: Train Loss = 0.2358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:33:38,935 - INFO - ------------ Save best model - AUROC: 0.8241 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 0.2290 Valid loss = 0.2141 roc = 0.8241\n",
      "confusion matrix:\n",
      "[[3741    1]\n",
      " [ 273   19]]\n",
      "accuracy = 0.9320773482322693\n",
      "precision class 0 = 0.9319880604743958\n",
      "precision class 1 = 0.949999988079071\n",
      "recall class 0 = 0.9997327923774719\n",
      "recall class 1 = 0.06506849080324173\n",
      "AUC of ROC = 0.8241280027529049\n",
      "AUC of PRC = 0.3837634986697262\n",
      "min(+P, Se) = 0.4075342465753425\n",
      "f1_score = 0.121794861326831\n",
      "------------ Save best model - AUROC: 0.8241 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:33:39,841 - INFO - Epoch 11 Batch 0: Train Loss = 0.2450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Batch 0: Train Loss = 0.2450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:33:58,653 - INFO - Epoch 11 Batch 20: Train Loss = 0.2783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Batch 20: Train Loss = 0.2783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:34:15,908 - INFO - Epoch 11 Batch 40: Train Loss = 0.2837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Batch 40: Train Loss = 0.2837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:34:33,864 - INFO - Epoch 11 Batch 60: Train Loss = 0.2764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Batch 60: Train Loss = 0.2764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:34:53,280 - INFO - Epoch 11 Batch 80: Train Loss = 0.2138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Batch 80: Train Loss = 0.2138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:35:12,172 - INFO - Epoch 11 Batch 100: Train Loss = 0.1754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Batch 100: Train Loss = 0.1754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:35:30,188 - INFO - Epoch 11 Batch 120: Train Loss = 0.2291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Batch 120: Train Loss = 0.2291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:35:41,670 - INFO - ------------ Save best model - AUROC: 0.8252 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss = 0.2310 Valid loss = 0.2122 roc = 0.8252\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 271   21]]\n",
      "accuracy = 0.9323252439498901\n",
      "precision class 0 = 0.9324358105659485\n",
      "precision class 1 = 0.9130434989929199\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.0719178095459938\n",
      "AUC of ROC = 0.8252179993117738\n",
      "AUC of PRC = 0.3941460470208352\n",
      "min(+P, Se) = 0.40939597315436244\n",
      "f1_score = 0.13333333280892065\n",
      "------------ Save best model - AUROC: 0.8252 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:35:42,742 - INFO - Epoch 12 Batch 0: Train Loss = 0.2494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 0: Train Loss = 0.2494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:36:01,156 - INFO - Epoch 12 Batch 20: Train Loss = 0.2427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 20: Train Loss = 0.2427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:36:19,213 - INFO - Epoch 12 Batch 40: Train Loss = 0.2787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 40: Train Loss = 0.2787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:36:36,702 - INFO - Epoch 12 Batch 60: Train Loss = 0.2699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 60: Train Loss = 0.2699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:36:55,557 - INFO - Epoch 12 Batch 80: Train Loss = 0.2209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 80: Train Loss = 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:37:14,133 - INFO - Epoch 12 Batch 100: Train Loss = 0.1809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 100: Train Loss = 0.1809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:37:31,750 - INFO - Epoch 12 Batch 120: Train Loss = 0.2248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 120: Train Loss = 0.2248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:37:44,184 - INFO - ------------ Save best model - AUROC: 0.8256 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss = 0.2236 Valid loss = 0.2117 roc = 0.8256\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 274   18]]\n",
      "accuracy = 0.9315815567970276\n",
      "precision class 0 = 0.9317389130592346\n",
      "precision class 1 = 0.8999999761581421\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.06164383515715599\n",
      "AUC of ROC = 0.8255767555259439\n",
      "AUC of PRC = 0.4006235383809643\n",
      "min(+P, Se) = 0.4178082191780822\n",
      "f1_score = 0.11538461393711599\n",
      "------------ Save best model - AUROC: 0.8256 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:37:44,992 - INFO - Epoch 13 Batch 0: Train Loss = 0.2352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 0: Train Loss = 0.2352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:38:03,779 - INFO - Epoch 13 Batch 20: Train Loss = 0.2744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 20: Train Loss = 0.2744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:38:22,000 - INFO - Epoch 13 Batch 40: Train Loss = 0.2706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 40: Train Loss = 0.2706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:38:39,989 - INFO - Epoch 13 Batch 60: Train Loss = 0.2886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 60: Train Loss = 0.2886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:38:58,177 - INFO - Epoch 13 Batch 80: Train Loss = 0.2281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 80: Train Loss = 0.2281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:39:16,369 - INFO - Epoch 13 Batch 100: Train Loss = 0.1860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 100: Train Loss = 0.1860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:39:34,743 - INFO - Epoch 13 Batch 120: Train Loss = 0.2161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 120: Train Loss = 0.2161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:39:46,100 - INFO - ------------ Save best model - AUROC: 0.8276 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss = 0.2209 Valid loss = 0.2130 roc = 0.8276\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 270   22]]\n",
      "accuracy = 0.932573139667511\n",
      "precision class 0 = 0.9326683282852173\n",
      "precision class 1 = 0.9166666865348816\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.07534246891736984\n",
      "AUC of ROC = 0.8276176390912485\n",
      "AUC of PRC = 0.3971687525183765\n",
      "min(+P, Se) = 0.4197952218430034\n",
      "f1_score = 0.13924051091573955\n",
      "------------ Save best model - AUROC: 0.8276 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:39:46,967 - INFO - Epoch 14 Batch 0: Train Loss = 0.2438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 0: Train Loss = 0.2438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:40:05,852 - INFO - Epoch 14 Batch 20: Train Loss = 0.2498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 20: Train Loss = 0.2498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:40:24,287 - INFO - Epoch 14 Batch 40: Train Loss = 0.2739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 40: Train Loss = 0.2739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:40:43,373 - INFO - Epoch 14 Batch 60: Train Loss = 0.2598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 60: Train Loss = 0.2598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:41:02,167 - INFO - Epoch 14 Batch 80: Train Loss = 0.2002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 80: Train Loss = 0.2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:41:20,291 - INFO - Epoch 14 Batch 100: Train Loss = 0.1874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 100: Train Loss = 0.1874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:41:37,980 - INFO - Epoch 14 Batch 120: Train Loss = 0.2374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 120: Train Loss = 0.2374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:41:51,033 - INFO - ------------ Save best model - AUROC: 0.8305 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss = 0.2275 Valid loss = 0.2100 roc = 0.8305\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 264   28]]\n",
      "accuracy = 0.9340605139732361\n",
      "precision class 0 = 0.9340659379959106\n",
      "precision class 1 = 0.9333333373069763\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.09589041024446487\n",
      "AUC of ROC = 0.8305046199014519\n",
      "AUC of PRC = 0.40047828597259183\n",
      "min(+P, Se) = 0.41638225255972694\n",
      "f1_score = 0.17391303355948376\n",
      "------------ Save best model - AUROC: 0.8305 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:41:51,898 - INFO - Epoch 15 Batch 0: Train Loss = 0.2431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 0: Train Loss = 0.2431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:42:10,583 - INFO - Epoch 15 Batch 20: Train Loss = 0.2519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 20: Train Loss = 0.2519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:42:28,061 - INFO - Epoch 15 Batch 40: Train Loss = 0.2795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 40: Train Loss = 0.2795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:42:47,033 - INFO - Epoch 15 Batch 60: Train Loss = 0.2614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 60: Train Loss = 0.2614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:43:05,619 - INFO - Epoch 15 Batch 80: Train Loss = 0.2200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 80: Train Loss = 0.2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:43:23,508 - INFO - Epoch 15 Batch 100: Train Loss = 0.1824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 100: Train Loss = 0.1824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:43:40,482 - INFO - Epoch 15 Batch 120: Train Loss = 0.2212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 120: Train Loss = 0.2212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:45:05,014 - INFO - Epoch 16 Batch 80: Train Loss = 0.2036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Batch 80: Train Loss = 0.2036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:45:23,105 - INFO - Epoch 16 Batch 100: Train Loss = 0.1836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Batch 100: Train Loss = 0.1836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:45:42,027 - INFO - Epoch 16 Batch 120: Train Loss = 0.2397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Batch 120: Train Loss = 0.2397\n",
      "Epoch 16: Loss = 0.2227 Valid loss = 0.2102 roc = 0.8324\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 265   27]]\n",
      "accuracy = 0.9338126182556152\n",
      "precision class 0 = 0.9338327050209045\n",
      "precision class 1 = 0.931034505367279\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.09246575087308884\n",
      "AUC of ROC = 0.8323546854293725\n",
      "AUC of PRC = 0.40101438149217544\n",
      "min(+P, Se) = 0.3972602739726027\n",
      "f1_score = 0.16822430378388303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:45:53,795 - INFO - ------------ Save best model - AUROC: 0.8324 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save best model - AUROC: 0.8324 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:45:54,634 - INFO - Epoch 17 Batch 0: Train Loss = 0.2395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Batch 0: Train Loss = 0.2395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:46:13,393 - INFO - Epoch 17 Batch 20: Train Loss = 0.2682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Batch 20: Train Loss = 0.2682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:46:32,841 - INFO - Epoch 17 Batch 40: Train Loss = 0.2835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Batch 40: Train Loss = 0.2835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:46:51,556 - INFO - Epoch 17 Batch 60: Train Loss = 0.2333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Batch 60: Train Loss = 0.2333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:47:09,478 - INFO - Epoch 17 Batch 80: Train Loss = 0.2251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Batch 80: Train Loss = 0.2251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:47:28,567 - INFO - Epoch 17 Batch 100: Train Loss = 0.1799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Batch 100: Train Loss = 0.1799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:47:46,378 - INFO - Epoch 17 Batch 120: Train Loss = 0.2156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Batch 120: Train Loss = 0.2156\n",
      "Epoch 17: Loss = 0.2193 Valid loss = 0.2091 roc = 0.8317\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 264   28]]\n",
      "accuracy = 0.9340605139732361\n",
      "precision class 0 = 0.9340659379959106\n",
      "precision class 1 = 0.9333333373069763\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.09589041024446487\n",
      "AUC of ROC = 0.8317094733605208\n",
      "AUC of PRC = 0.402797157454161\n",
      "min(+P, Se) = 0.4232081911262799\n",
      "f1_score = 0.17391303355948376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:47:58,687 - INFO - Epoch 18 Batch 0: Train Loss = 0.2340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Batch 0: Train Loss = 0.2340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:48:17,242 - INFO - Epoch 18 Batch 20: Train Loss = 0.2626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Batch 20: Train Loss = 0.2626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:48:34,919 - INFO - Epoch 18 Batch 40: Train Loss = 0.2427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Batch 40: Train Loss = 0.2427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:48:53,292 - INFO - Epoch 18 Batch 60: Train Loss = 0.2476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Batch 60: Train Loss = 0.2476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:49:11,382 - INFO - Epoch 18 Batch 80: Train Loss = 0.1891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Batch 80: Train Loss = 0.1891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:49:29,362 - INFO - Epoch 18 Batch 100: Train Loss = 0.1874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Batch 100: Train Loss = 0.1874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:49:48,022 - INFO - Epoch 18 Batch 120: Train Loss = 0.2202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Batch 120: Train Loss = 0.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:50:00,218 - INFO - ------------ Save best model - AUROC: 0.8334 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Loss = 0.2216 Valid loss = 0.2101 roc = 0.8334\n",
      "confusion matrix:\n",
      "[[3741    1]\n",
      " [ 273   19]]\n",
      "accuracy = 0.9320773482322693\n",
      "precision class 0 = 0.9319880604743958\n",
      "precision class 1 = 0.949999988079071\n",
      "recall class 0 = 0.9997327923774719\n",
      "recall class 1 = 0.06506849080324173\n",
      "AUC of ROC = 0.8333916007116551\n",
      "AUC of PRC = 0.41416118816772407\n",
      "min(+P, Se) = 0.4212328767123288\n",
      "f1_score = 0.121794861326831\n",
      "------------ Save best model - AUROC: 0.8334 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:50:00,990 - INFO - Epoch 19 Batch 0: Train Loss = 0.2252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Batch 0: Train Loss = 0.2252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:50:19,797 - INFO - Epoch 19 Batch 20: Train Loss = 0.2801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Batch 20: Train Loss = 0.2801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:50:37,850 - INFO - Epoch 19 Batch 40: Train Loss = 0.2615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Batch 40: Train Loss = 0.2615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:50:55,095 - INFO - Epoch 19 Batch 60: Train Loss = 0.2936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Batch 60: Train Loss = 0.2936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:51:13,658 - INFO - Epoch 19 Batch 80: Train Loss = 0.2251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Batch 80: Train Loss = 0.2251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:51:32,235 - INFO - Epoch 19 Batch 100: Train Loss = 0.1801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Batch 100: Train Loss = 0.1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:51:50,982 - INFO - Epoch 19 Batch 120: Train Loss = 0.2268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Batch 120: Train Loss = 0.2268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:52:03,206 - INFO - ------------ Save best model - AUROC: 0.8361 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Loss = 0.2195 Valid loss = 0.2099 roc = 0.8361\n",
      "confusion matrix:\n",
      "[[3741    1]\n",
      " [ 271   21]]\n",
      "accuracy = 0.932573139667511\n",
      "precision class 0 = 0.9324526190757751\n",
      "precision class 1 = 0.9545454382896423\n",
      "recall class 0 = 0.9997327923774719\n",
      "recall class 1 = 0.0719178095459938\n",
      "AUC of ROC = 0.8361271168447024\n",
      "AUC of PRC = 0.4150879039171147\n",
      "min(+P, Se) = 0.42662116040955633\n",
      "f1_score = 0.1337579610060086\n",
      "------------ Save best model - AUROC: 0.8361 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:52:03,918 - INFO - Epoch 20 Batch 0: Train Loss = 0.2544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 0: Train Loss = 0.2544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:52:23,186 - INFO - Epoch 20 Batch 20: Train Loss = 0.2518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 20: Train Loss = 0.2518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:52:40,634 - INFO - Epoch 20 Batch 40: Train Loss = 0.2710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 40: Train Loss = 0.2710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:52:59,468 - INFO - Epoch 20 Batch 60: Train Loss = 0.2757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 60: Train Loss = 0.2757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:53:17,331 - INFO - Epoch 20 Batch 80: Train Loss = 0.2088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 80: Train Loss = 0.2088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:53:34,780 - INFO - Epoch 20 Batch 100: Train Loss = 0.1774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 100: Train Loss = 0.1774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:53:52,713 - INFO - Epoch 20 Batch 120: Train Loss = 0.2480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 120: Train Loss = 0.2480\n",
      "Epoch 20: Loss = 0.2194 Valid loss = 0.2080 roc = 0.8359\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 265   27]]\n",
      "accuracy = 0.9338126182556152\n",
      "precision class 0 = 0.9338327050209045\n",
      "precision class 1 = 0.931034505367279\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.09246575087308884\n",
      "AUC of ROC = 0.8358900814889114\n",
      "AUC of PRC = 0.40798102450871665\n",
      "min(+P, Se) = 0.4280821917808219\n",
      "f1_score = 0.16822430378388303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:54:05,728 - INFO - Epoch 21 Batch 0: Train Loss = 0.2336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Batch 0: Train Loss = 0.2336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:54:24,329 - INFO - Epoch 21 Batch 20: Train Loss = 0.2508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Batch 20: Train Loss = 0.2508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:54:41,545 - INFO - Epoch 21 Batch 40: Train Loss = 0.3017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Batch 40: Train Loss = 0.3017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:55:00,173 - INFO - Epoch 21 Batch 60: Train Loss = 0.2669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Batch 60: Train Loss = 0.2669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:55:18,131 - INFO - Epoch 21 Batch 80: Train Loss = 0.2142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Batch 80: Train Loss = 0.2142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:55:35,814 - INFO - Epoch 21 Batch 100: Train Loss = 0.1846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Batch 100: Train Loss = 0.1846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:55:53,387 - INFO - Epoch 21 Batch 120: Train Loss = 0.2049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Batch 120: Train Loss = 0.2049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:56:05,513 - INFO - ------------ Save best model - AUROC: 0.8369 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Loss = 0.2198 Valid loss = 0.2068 roc = 0.8369\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 268   24]]\n",
      "accuracy = 0.9330689311027527\n",
      "precision class 0 = 0.9331337213516235\n",
      "precision class 1 = 0.9230769276618958\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.08219178020954132\n",
      "AUC of ROC = 0.836871169911336\n",
      "AUC of PRC = 0.41402699552937017\n",
      "min(+P, Se) = 0.4266666666666667\n",
      "f1_score = 0.15094339749249217\n",
      "------------ Save best model - AUROC: 0.8369 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:56:06,310 - INFO - Epoch 22 Batch 0: Train Loss = 0.2405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Batch 0: Train Loss = 0.2405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:56:24,527 - INFO - Epoch 22 Batch 20: Train Loss = 0.2376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Batch 20: Train Loss = 0.2376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:56:42,323 - INFO - Epoch 22 Batch 40: Train Loss = 0.2424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Batch 40: Train Loss = 0.2424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:57:00,266 - INFO - Epoch 22 Batch 60: Train Loss = 0.2532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Batch 60: Train Loss = 0.2532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:57:19,183 - INFO - Epoch 22 Batch 80: Train Loss = 0.2230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Batch 80: Train Loss = 0.2230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:57:38,108 - INFO - Epoch 22 Batch 100: Train Loss = 0.1761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Batch 100: Train Loss = 0.1761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:57:57,297 - INFO - Epoch 22 Batch 120: Train Loss = 0.2168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Batch 120: Train Loss = 0.2168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:58:09,973 - INFO - ------------ Save best model - AUROC: 0.8395 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Loss = 0.2166 Valid loss = 0.2079 roc = 0.8395\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 268   24]]\n",
      "accuracy = 0.9330689311027527\n",
      "precision class 0 = 0.9331337213516235\n",
      "precision class 1 = 0.9230769276618958\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.08219178020954132\n",
      "AUC of ROC = 0.839495032324667\n",
      "AUC of PRC = 0.41989071104297193\n",
      "min(+P, Se) = 0.4232081911262799\n",
      "f1_score = 0.15094339749249217\n",
      "------------ Save best model - AUROC: 0.8395 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 12:58:10,815 - INFO - Epoch 23 Batch 0: Train Loss = 0.2343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Batch 0: Train Loss = 0.2343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:58:29,569 - INFO - Epoch 23 Batch 20: Train Loss = 0.2369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Batch 20: Train Loss = 0.2369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:58:46,663 - INFO - Epoch 23 Batch 40: Train Loss = 0.2635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Batch 40: Train Loss = 0.2635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:59:04,114 - INFO - Epoch 23 Batch 60: Train Loss = 0.3109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Batch 60: Train Loss = 0.3109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:59:21,503 - INFO - Epoch 23 Batch 80: Train Loss = 0.2073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Batch 80: Train Loss = 0.2073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:59:39,181 - INFO - Epoch 23 Batch 100: Train Loss = 0.1774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Batch 100: Train Loss = 0.1774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 12:59:57,645 - INFO - Epoch 23 Batch 120: Train Loss = 0.2055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Batch 120: Train Loss = 0.2055\n",
      "Epoch 23: Loss = 0.2163 Valid loss = 0.2091 roc = 0.8389\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 268   24]]\n",
      "accuracy = 0.9330689311027527\n",
      "precision class 0 = 0.9331337213516235\n",
      "precision class 1 = 0.9230769276618958\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.08219178020954132\n",
      "AUC of ROC = 0.8388681241442932\n",
      "AUC of PRC = 0.41977362287038655\n",
      "min(+P, Se) = 0.4271186440677966\n",
      "f1_score = 0.15094339749249217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:00:10,092 - INFO - Epoch 24 Batch 0: Train Loss = 0.2485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Batch 0: Train Loss = 0.2485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:00:28,396 - INFO - Epoch 24 Batch 20: Train Loss = 0.2190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Batch 20: Train Loss = 0.2190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:00:46,497 - INFO - Epoch 24 Batch 40: Train Loss = 0.3017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Batch 40: Train Loss = 0.3017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:01:04,046 - INFO - Epoch 24 Batch 60: Train Loss = 0.2747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Batch 60: Train Loss = 0.2747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:01:22,859 - INFO - Epoch 24 Batch 80: Train Loss = 0.2281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Batch 80: Train Loss = 0.2281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:01:41,130 - INFO - Epoch 24 Batch 100: Train Loss = 0.1884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Batch 100: Train Loss = 0.1884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:01:58,543 - INFO - Epoch 24 Batch 120: Train Loss = 0.2550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Batch 120: Train Loss = 0.2550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:02:10,391 - INFO - ------------ Save best model - AUROC: 0.8404 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Loss = 0.2153 Valid loss = 0.2059 roc = 0.8404\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 262   30]]\n",
      "accuracy = 0.934556245803833\n",
      "precision class 0 = 0.9345327615737915\n",
      "precision class 1 = 0.9375\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.10273972898721695\n",
      "AUC of ROC = 0.8403553150831362\n",
      "AUC of PRC = 0.41709386174318264\n",
      "min(+P, Se) = 0.4212328767123288\n",
      "f1_score = 0.18518519662508023\n",
      "------------ Save best model - AUROC: 0.8404 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:02:11,299 - INFO - Epoch 25 Batch 0: Train Loss = 0.2192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Batch 0: Train Loss = 0.2192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:02:27,928 - INFO - Epoch 25 Batch 20: Train Loss = 0.2442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Batch 20: Train Loss = 0.2442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:02:44,572 - INFO - Epoch 25 Batch 40: Train Loss = 0.2655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Batch 40: Train Loss = 0.2655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:03:02,150 - INFO - Epoch 25 Batch 60: Train Loss = 0.2809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Batch 60: Train Loss = 0.2809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:03:19,770 - INFO - Epoch 25 Batch 80: Train Loss = 0.2147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Batch 80: Train Loss = 0.2147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:03:37,547 - INFO - Epoch 25 Batch 100: Train Loss = 0.1930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Batch 100: Train Loss = 0.1930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:03:53,675 - INFO - Epoch 25 Batch 120: Train Loss = 0.2299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Batch 120: Train Loss = 0.2299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:04:04,196 - INFO - ------------ Save best model - AUROC: 0.8439 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Loss = 0.2142 Valid loss = 0.2139 roc = 0.8439\n",
      "confusion matrix:\n",
      "[[3741    1]\n",
      " [ 283    9]]\n",
      "accuracy = 0.9295983910560608\n",
      "precision class 0 = 0.9296719431877136\n",
      "precision class 1 = 0.8999999761581421\n",
      "recall class 0 = 0.9997327923774719\n",
      "recall class 1 = 0.030821917578577995\n",
      "AUC of ROC = 0.843873322448621\n",
      "AUC of PRC = 0.43630322843983715\n",
      "min(+P, Se) = 0.44368600682593856\n",
      "f1_score = 0.059602648405700724\n",
      "------------ Save best model - AUROC: 0.8439 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:04:04,916 - INFO - Epoch 26 Batch 0: Train Loss = 0.2531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Batch 0: Train Loss = 0.2531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:04:21,703 - INFO - Epoch 26 Batch 20: Train Loss = 0.2387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Batch 20: Train Loss = 0.2387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:04:38,582 - INFO - Epoch 26 Batch 40: Train Loss = 0.3130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Batch 40: Train Loss = 0.3130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:04:55,558 - INFO - Epoch 26 Batch 60: Train Loss = 0.2663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Batch 60: Train Loss = 0.2663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:05:13,152 - INFO - Epoch 26 Batch 80: Train Loss = 0.2166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Batch 80: Train Loss = 0.2166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:05:30,641 - INFO - Epoch 26 Batch 100: Train Loss = 0.1904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Batch 100: Train Loss = 0.1904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:05:47,101 - INFO - Epoch 26 Batch 120: Train Loss = 0.2319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Batch 120: Train Loss = 0.2319\n",
      "Epoch 26: Loss = 0.2205 Valid loss = 0.2085 roc = 0.8416\n",
      "confusion matrix:\n",
      "[[3741    1]\n",
      " [ 273   19]]\n",
      "accuracy = 0.9320773482322693\n",
      "precision class 0 = 0.9319880604743958\n",
      "precision class 1 = 0.949999988079071\n",
      "recall class 0 = 0.9997327923774719\n",
      "recall class 1 = 0.06506849080324173\n",
      "AUC of ROC = 0.8415569653617215\n",
      "AUC of PRC = 0.4271098040784761\n",
      "min(+P, Se) = 0.43686006825938567\n",
      "f1_score = 0.121794861326831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:05:58,950 - INFO - Epoch 27 Batch 0: Train Loss = 0.2306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Batch 0: Train Loss = 0.2306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:06:16,258 - INFO - Epoch 27 Batch 20: Train Loss = 0.2412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Batch 20: Train Loss = 0.2412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:06:32,336 - INFO - Epoch 27 Batch 40: Train Loss = 0.2743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Batch 40: Train Loss = 0.2743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:06:49,641 - INFO - Epoch 27 Batch 60: Train Loss = 0.2589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Batch 60: Train Loss = 0.2589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:07:06,280 - INFO - Epoch 27 Batch 80: Train Loss = 0.1923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Batch 80: Train Loss = 0.1923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:07:23,222 - INFO - Epoch 27 Batch 100: Train Loss = 0.1744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Batch 100: Train Loss = 0.1744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:07:38,673 - INFO - Epoch 27 Batch 120: Train Loss = 0.2252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Batch 120: Train Loss = 0.2252\n",
      "Epoch 27: Loss = 0.2218 Valid loss = 0.2069 roc = 0.8428\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 270   22]]\n",
      "accuracy = 0.932573139667511\n",
      "precision class 0 = 0.9326683282852173\n",
      "precision class 1 = 0.9166666865348816\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.07534246891736984\n",
      "AUC of ROC = 0.8428034601670779\n",
      "AUC of PRC = 0.42665463567311174\n",
      "min(+P, Se) = 0.4452054794520548\n",
      "f1_score = 0.13924051091573955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:07:50,454 - INFO - Epoch 28 Batch 0: Train Loss = 0.2258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Batch 0: Train Loss = 0.2258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:08:08,372 - INFO - Epoch 28 Batch 20: Train Loss = 0.2370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Batch 20: Train Loss = 0.2370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:08:25,469 - INFO - Epoch 28 Batch 40: Train Loss = 0.2933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Batch 40: Train Loss = 0.2933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:08:42,976 - INFO - Epoch 28 Batch 60: Train Loss = 0.2560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Batch 60: Train Loss = 0.2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:09:00,197 - INFO - Epoch 28 Batch 80: Train Loss = 0.2156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Batch 80: Train Loss = 0.2156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:09:17,011 - INFO - Epoch 28 Batch 100: Train Loss = 0.1828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Batch 100: Train Loss = 0.1828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:09:32,699 - INFO - Epoch 28 Batch 120: Train Loss = 0.2187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Batch 120: Train Loss = 0.2187\n",
      "Epoch 28: Loss = 0.2162 Valid loss = 0.2073 roc = 0.8421\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 269   23]]\n",
      "accuracy = 0.9328210353851318\n",
      "precision class 0 = 0.9329009652137756\n",
      "precision class 1 = 0.9200000166893005\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.07876712083816528\n",
      "AUC of ROC = 0.8421317074599327\n",
      "AUC of PRC = 0.42602847721382886\n",
      "min(+P, Se) = 0.4383561643835616\n",
      "f1_score = 0.14511040614547138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:09:44,827 - INFO - Epoch 29 Batch 0: Train Loss = 0.2427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Batch 0: Train Loss = 0.2427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:10:01,680 - INFO - Epoch 29 Batch 20: Train Loss = 0.2305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Batch 20: Train Loss = 0.2305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:10:19,062 - INFO - Epoch 29 Batch 40: Train Loss = 0.2949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Batch 40: Train Loss = 0.2949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:10:35,181 - INFO - Epoch 29 Batch 60: Train Loss = 0.2572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Batch 60: Train Loss = 0.2572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:10:51,493 - INFO - Epoch 29 Batch 80: Train Loss = 0.2114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Batch 80: Train Loss = 0.2114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:11:07,770 - INFO - Epoch 29 Batch 100: Train Loss = 0.1766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Batch 100: Train Loss = 0.1766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:11:24,191 - INFO - Epoch 29 Batch 120: Train Loss = 0.2208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Batch 120: Train Loss = 0.2208\n",
      "Epoch 29: Loss = 0.2151 Valid loss = 0.2064 roc = 0.8435\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 267   25]]\n",
      "accuracy = 0.9333168268203735\n",
      "precision class 0 = 0.933366596698761\n",
      "precision class 1 = 0.9259259104728699\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.08561643958091736\n",
      "AUC of ROC = 0.8435054142902118\n",
      "AUC of PRC = 0.426695407965906\n",
      "min(+P, Se) = 0.43333333333333335\n",
      "f1_score = 0.1567398183611321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:11:35,510 - INFO - Epoch 30 Batch 0: Train Loss = 0.2303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Batch 0: Train Loss = 0.2303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:11:52,985 - INFO - Epoch 30 Batch 20: Train Loss = 0.2150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Batch 20: Train Loss = 0.2150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:12:09,105 - INFO - Epoch 30 Batch 40: Train Loss = 0.2841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Batch 40: Train Loss = 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:12:26,236 - INFO - Epoch 30 Batch 60: Train Loss = 0.2662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Batch 60: Train Loss = 0.2662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:12:42,986 - INFO - Epoch 30 Batch 80: Train Loss = 0.2083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Batch 80: Train Loss = 0.2083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:12:59,923 - INFO - Epoch 30 Batch 100: Train Loss = 0.1748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Batch 100: Train Loss = 0.1748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:13:18,094 - INFO - Epoch 30 Batch 120: Train Loss = 0.2254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Batch 120: Train Loss = 0.2254\n",
      "Epoch 30: Loss = 0.2167 Valid loss = 0.2057 roc = 0.8438\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 264   28]]\n",
      "accuracy = 0.9338126182556152\n",
      "precision class 0 = 0.9340494871139526\n",
      "precision class 1 = 0.9032257795333862\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.09589041024446487\n",
      "AUC of ROC = 0.8437625839233286\n",
      "AUC of PRC = 0.42398617124674953\n",
      "min(+P, Se) = 0.43389830508474575\n",
      "f1_score = 0.17337461263231305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:13:30,597 - INFO - Epoch 31 Batch 0: Train Loss = 0.2493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Batch 0: Train Loss = 0.2493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:13:48,115 - INFO - Epoch 31 Batch 20: Train Loss = 0.2556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Batch 20: Train Loss = 0.2556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:14:05,155 - INFO - Epoch 31 Batch 40: Train Loss = 0.3015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Batch 40: Train Loss = 0.3015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:14:21,898 - INFO - Epoch 31 Batch 60: Train Loss = 0.2686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Batch 60: Train Loss = 0.2686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:14:38,014 - INFO - Epoch 31 Batch 80: Train Loss = 0.2145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Batch 80: Train Loss = 0.2145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:14:55,260 - INFO - Epoch 31 Batch 100: Train Loss = 0.1870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Batch 100: Train Loss = 0.1870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:15:11,471 - INFO - Epoch 31 Batch 120: Train Loss = 0.2346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Batch 120: Train Loss = 0.2346\n",
      "Epoch 31: Loss = 0.2123 Valid loss = 0.2071 roc = 0.8386\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 260   32]]\n",
      "accuracy = 0.9348041415214539\n",
      "precision class 0 = 0.9349837303161621\n",
      "precision class 1 = 0.9142857193946838\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.10958904027938843\n",
      "AUC of ROC = 0.8386402407327412\n",
      "AUC of PRC = 0.416435255459044\n",
      "min(+P, Se) = 0.4387755102040816\n",
      "f1_score = 0.19571865324917023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:15:22,978 - INFO - Epoch 32 Batch 0: Train Loss = 0.2277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Batch 0: Train Loss = 0.2277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:15:40,157 - INFO - Epoch 32 Batch 20: Train Loss = 0.2445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Batch 20: Train Loss = 0.2445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:15:56,117 - INFO - Epoch 32 Batch 40: Train Loss = 0.2748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Batch 40: Train Loss = 0.2748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:16:13,519 - INFO - Epoch 32 Batch 60: Train Loss = 0.2482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Batch 60: Train Loss = 0.2482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:16:30,044 - INFO - Epoch 32 Batch 80: Train Loss = 0.2090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Batch 80: Train Loss = 0.2090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:16:47,275 - INFO - Epoch 32 Batch 100: Train Loss = 0.1918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Batch 100: Train Loss = 0.1918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:17:03,381 - INFO - Epoch 32 Batch 120: Train Loss = 0.2388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Batch 120: Train Loss = 0.2388\n",
      "Epoch 32: Loss = 0.2169 Valid loss = 0.2066 roc = 0.8401\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 262   30]]\n",
      "accuracy = 0.9343083500862122\n",
      "precision class 0 = 0.9345163702964783\n",
      "precision class 1 = 0.9090909361839294\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.10273972898721695\n",
      "AUC of ROC = 0.8400889935057803\n",
      "AUC of PRC = 0.4177440545293697\n",
      "min(+P, Se) = 0.43197278911564624\n",
      "f1_score = 0.18461538587434767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:17:14,980 - INFO - Epoch 33 Batch 0: Train Loss = 0.2436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Batch 0: Train Loss = 0.2436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:17:31,452 - INFO - Epoch 33 Batch 20: Train Loss = 0.2542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Batch 20: Train Loss = 0.2542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:17:47,898 - INFO - Epoch 33 Batch 40: Train Loss = 0.2899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Batch 40: Train Loss = 0.2899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:18:04,484 - INFO - Epoch 33 Batch 60: Train Loss = 0.2597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Batch 60: Train Loss = 0.2597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:18:21,417 - INFO - Epoch 33 Batch 80: Train Loss = 0.2173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Batch 80: Train Loss = 0.2173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:18:38,624 - INFO - Epoch 33 Batch 100: Train Loss = 0.1770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Batch 100: Train Loss = 0.1770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:18:55,717 - INFO - Epoch 33 Batch 120: Train Loss = 0.2331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Batch 120: Train Loss = 0.2331\n",
      "Epoch 33: Loss = 0.2186 Valid loss = 0.2062 roc = 0.8424\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 258   34]]\n",
      "accuracy = 0.9352999329566956\n",
      "precision class 0 = 0.9354515671730042\n",
      "precision class 1 = 0.9189189076423645\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.1164383590221405\n",
      "AUC of ROC = 0.8423760643711151\n",
      "AUC of PRC = 0.4211865954504713\n",
      "min(+P, Se) = 0.4383561643835616\n",
      "f1_score = 0.2066869402575825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:19:08,642 - INFO - Epoch 34 Batch 0: Train Loss = 0.2240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Batch 0: Train Loss = 0.2240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:19:25,446 - INFO - Epoch 34 Batch 20: Train Loss = 0.2520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Batch 20: Train Loss = 0.2520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:19:42,228 - INFO - Epoch 34 Batch 40: Train Loss = 0.2700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Batch 40: Train Loss = 0.2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:19:58,897 - INFO - Epoch 34 Batch 60: Train Loss = 0.2717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Batch 60: Train Loss = 0.2717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:20:15,684 - INFO - Epoch 34 Batch 80: Train Loss = 0.2242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Batch 80: Train Loss = 0.2242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:20:32,455 - INFO - Epoch 34 Batch 100: Train Loss = 0.1727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Batch 100: Train Loss = 0.1727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:20:49,531 - INFO - Epoch 34 Batch 120: Train Loss = 0.2227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Batch 120: Train Loss = 0.2227\n",
      "Epoch 34: Loss = 0.2188 Valid loss = 0.2053 roc = 0.8393\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 261   31]]\n",
      "accuracy = 0.934556245803833\n",
      "precision class 0 = 0.9347500205039978\n",
      "precision class 1 = 0.9117646813392639\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.10616438090801239\n",
      "AUC of ROC = 0.8393129086343102\n",
      "AUC of PRC = 0.4197545682742919\n",
      "min(+P, Se) = 0.42662116040955633\n",
      "f1_score = 0.1901840415038246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:21:01,905 - INFO - Epoch 35 Batch 0: Train Loss = 0.2321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Batch 0: Train Loss = 0.2321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:21:19,079 - INFO - Epoch 35 Batch 20: Train Loss = 0.2428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Batch 20: Train Loss = 0.2428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:21:34,803 - INFO - Epoch 35 Batch 40: Train Loss = 0.2939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Batch 40: Train Loss = 0.2939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:21:51,847 - INFO - Epoch 35 Batch 60: Train Loss = 0.2561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Batch 60: Train Loss = 0.2561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:22:08,579 - INFO - Epoch 35 Batch 80: Train Loss = 0.2259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Batch 80: Train Loss = 0.2259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:22:25,330 - INFO - Epoch 35 Batch 100: Train Loss = 0.1723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Batch 100: Train Loss = 0.1723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:22:40,979 - INFO - Epoch 35 Batch 120: Train Loss = 0.2159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Batch 120: Train Loss = 0.2159\n",
      "Epoch 35: Loss = 0.2178 Valid loss = 0.2057 roc = 0.8419\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 254   38]]\n",
      "accuracy = 0.936291515827179\n",
      "precision class 0 = 0.9363886713981628\n",
      "precision class 1 = 0.9268292784690857\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.13013698160648346\n",
      "AUC of ROC = 0.8419166367703155\n",
      "AUC of PRC = 0.4171054620518752\n",
      "min(+P, Se) = 0.423728813559322\n",
      "f1_score = 0.22822821166412668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:22:52,459 - INFO - Epoch 36 Batch 0: Train Loss = 0.2153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Batch 0: Train Loss = 0.2153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:23:09,163 - INFO - Epoch 36 Batch 20: Train Loss = 0.2505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Batch 20: Train Loss = 0.2505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:23:25,280 - INFO - Epoch 36 Batch 40: Train Loss = 0.3152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Batch 40: Train Loss = 0.3152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:23:42,585 - INFO - Epoch 36 Batch 60: Train Loss = 0.2543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Batch 60: Train Loss = 0.2543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:23:59,839 - INFO - Epoch 36 Batch 80: Train Loss = 0.1970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Batch 80: Train Loss = 0.1970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:24:17,167 - INFO - Epoch 36 Batch 100: Train Loss = 0.1725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Batch 100: Train Loss = 0.1725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:24:33,067 - INFO - Epoch 36 Batch 120: Train Loss = 0.2052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Batch 120: Train Loss = 0.2052\n",
      "Epoch 36: Loss = 0.2164 Valid loss = 0.2070 roc = 0.8406\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 257   35]]\n",
      "accuracy = 0.9355478286743164\n",
      "precision class 0 = 0.9356856942176819\n",
      "precision class 1 = 0.9210526347160339\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.11986301094293594\n",
      "AUC of ROC = 0.8406069935497097\n",
      "AUC of PRC = 0.41758457816241185\n",
      "min(+P, Se) = 0.4232081911262799\n",
      "f1_score = 0.21212121548074675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:24:44,577 - INFO - Epoch 37 Batch 0: Train Loss = 0.2222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Batch 0: Train Loss = 0.2222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:25:01,781 - INFO - Epoch 37 Batch 20: Train Loss = 0.2455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Batch 20: Train Loss = 0.2455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:25:17,977 - INFO - Epoch 37 Batch 40: Train Loss = 0.2754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Batch 40: Train Loss = 0.2754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:25:34,144 - INFO - Epoch 37 Batch 60: Train Loss = 0.2310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Batch 60: Train Loss = 0.2310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:25:50,858 - INFO - Epoch 37 Batch 80: Train Loss = 0.2046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Batch 80: Train Loss = 0.2046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:26:07,121 - INFO - Epoch 37 Batch 100: Train Loss = 0.1750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Batch 100: Train Loss = 0.1750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:26:23,891 - INFO - Epoch 37 Batch 120: Train Loss = 0.2207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Batch 120: Train Loss = 0.2207\n",
      "Epoch 37: Loss = 0.2189 Valid loss = 0.2064 roc = 0.8424\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 254   38]]\n",
      "accuracy = 0.936291515827179\n",
      "precision class 0 = 0.9363886713981628\n",
      "precision class 1 = 0.9268292784690857\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.13013698160648346\n",
      "AUC of ROC = 0.8423897922874735\n",
      "AUC of PRC = 0.4189867638114468\n",
      "min(+P, Se) = 0.42424242424242425\n",
      "f1_score = 0.22822821166412668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:26:35,069 - INFO - Epoch 38 Batch 0: Train Loss = 0.2564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Batch 0: Train Loss = 0.2564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:26:51,634 - INFO - Epoch 38 Batch 20: Train Loss = 0.2510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Batch 20: Train Loss = 0.2510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:27:07,514 - INFO - Epoch 38 Batch 40: Train Loss = 0.2952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Batch 40: Train Loss = 0.2952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:27:23,801 - INFO - Epoch 38 Batch 60: Train Loss = 0.2613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Batch 60: Train Loss = 0.2613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:27:40,069 - INFO - Epoch 38 Batch 80: Train Loss = 0.2340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Batch 80: Train Loss = 0.2340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:27:56,753 - INFO - Epoch 38 Batch 100: Train Loss = 0.1793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Batch 100: Train Loss = 0.1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:28:12,892 - INFO - Epoch 38 Batch 120: Train Loss = 0.2252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Batch 120: Train Loss = 0.2252\n",
      "Epoch 38: Loss = 0.2206 Valid loss = 0.2051 roc = 0.8420\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 254   38]]\n",
      "accuracy = 0.936291515827179\n",
      "precision class 0 = 0.9363886713981628\n",
      "precision class 1 = 0.9268292784690857\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.13013698160648346\n",
      "AUC of ROC = 0.8419866491437441\n",
      "AUC of PRC = 0.41984211418212575\n",
      "min(+P, Se) = 0.4246575342465753\n",
      "f1_score = 0.22822821166412668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:28:24,715 - INFO - Epoch 39 Batch 0: Train Loss = 0.2279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Batch 0: Train Loss = 0.2279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:28:42,693 - INFO - Epoch 39 Batch 20: Train Loss = 0.2498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Batch 20: Train Loss = 0.2498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:28:59,865 - INFO - Epoch 39 Batch 40: Train Loss = 0.2653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Batch 40: Train Loss = 0.2653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:29:17,648 - INFO - Epoch 39 Batch 60: Train Loss = 0.2603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Batch 60: Train Loss = 0.2603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:29:35,074 - INFO - Epoch 39 Batch 80: Train Loss = 0.2058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Batch 80: Train Loss = 0.2058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:29:51,087 - INFO - Epoch 39 Batch 100: Train Loss = 0.1884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Batch 100: Train Loss = 0.1884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:30:07,598 - INFO - Epoch 39 Batch 120: Train Loss = 0.2285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Batch 120: Train Loss = 0.2285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:30:18,406 - INFO - ------------ Save best model - AUROC: 0.8453 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: Loss = 0.2202 Valid loss = 0.2053 roc = 0.8453\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 255   37]]\n",
      "accuracy = 0.9360436201095581\n",
      "precision class 0 = 0.9361542463302612\n",
      "precision class 1 = 0.925000011920929\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.12671232223510742\n",
      "AUC of ROC = 0.8453083473053016\n",
      "AUC of PRC = 0.4184783721705157\n",
      "min(+P, Se) = 0.423728813559322\n",
      "f1_score = 0.22289156913757324\n",
      "------------ Save best model - AUROC: 0.8453 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:30:19,322 - INFO - Epoch 40 Batch 0: Train Loss = 0.2271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Batch 0: Train Loss = 0.2271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:30:37,170 - INFO - Epoch 40 Batch 20: Train Loss = 0.2431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Batch 20: Train Loss = 0.2431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:30:53,857 - INFO - Epoch 40 Batch 40: Train Loss = 0.2724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Batch 40: Train Loss = 0.2724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:31:09,973 - INFO - Epoch 40 Batch 60: Train Loss = 0.2729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Batch 60: Train Loss = 0.2729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:31:26,548 - INFO - Epoch 40 Batch 80: Train Loss = 0.2012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Batch 80: Train Loss = 0.2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:31:42,556 - INFO - Epoch 40 Batch 100: Train Loss = 0.1723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Batch 100: Train Loss = 0.1723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:31:58,915 - INFO - Epoch 40 Batch 120: Train Loss = 0.2166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Batch 120: Train Loss = 0.2166\n",
      "Epoch 40: Loss = 0.2177 Valid loss = 0.2043 roc = 0.8427\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 258   34]]\n",
      "accuracy = 0.9352999329566956\n",
      "precision class 0 = 0.9354515671730042\n",
      "precision class 1 = 0.9189189076423645\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.1164383590221405\n",
      "AUC of ROC = 0.8427403117518285\n",
      "AUC of PRC = 0.42060954459789246\n",
      "min(+P, Se) = 0.4315068493150685\n",
      "f1_score = 0.2066869402575825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:32:11,091 - INFO - Epoch 41 Batch 0: Train Loss = 0.2076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Batch 0: Train Loss = 0.2076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:32:27,780 - INFO - Epoch 41 Batch 20: Train Loss = 0.2456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Batch 20: Train Loss = 0.2456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:32:43,866 - INFO - Epoch 41 Batch 40: Train Loss = 0.2700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Batch 40: Train Loss = 0.2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:32:59,821 - INFO - Epoch 41 Batch 60: Train Loss = 0.2576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Batch 60: Train Loss = 0.2576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:33:16,819 - INFO - Epoch 41 Batch 80: Train Loss = 0.2075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Batch 80: Train Loss = 0.2075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:33:32,894 - INFO - Epoch 41 Batch 100: Train Loss = 0.1607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Batch 100: Train Loss = 0.1607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:33:50,758 - INFO - Epoch 41 Batch 120: Train Loss = 0.2310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Batch 120: Train Loss = 0.2310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:34:01,907 - INFO - ------------ Save best model - AUROC: 0.8458 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: Loss = 0.2153 Valid loss = 0.2058 roc = 0.8458\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 258   34]]\n",
      "accuracy = 0.9352999329566956\n",
      "precision class 0 = 0.9354515671730042\n",
      "precision class 1 = 0.9189189076423645\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.1164383590221405\n",
      "AUC of ROC = 0.8458473968209806\n",
      "AUC of PRC = 0.4247885725276664\n",
      "min(+P, Se) = 0.4334470989761092\n",
      "f1_score = 0.2066869402575825\n",
      "------------ Save best model - AUROC: 0.8458 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:34:02,663 - INFO - Epoch 42 Batch 0: Train Loss = 0.2281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Batch 0: Train Loss = 0.2281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:34:19,951 - INFO - Epoch 42 Batch 20: Train Loss = 0.2538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Batch 20: Train Loss = 0.2538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:34:37,211 - INFO - Epoch 42 Batch 40: Train Loss = 0.3052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Batch 40: Train Loss = 0.3052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:34:53,234 - INFO - Epoch 42 Batch 60: Train Loss = 0.2639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Batch 60: Train Loss = 0.2639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:35:10,139 - INFO - Epoch 42 Batch 80: Train Loss = 0.2025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Batch 80: Train Loss = 0.2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:35:26,070 - INFO - Epoch 42 Batch 100: Train Loss = 0.1794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Batch 100: Train Loss = 0.1794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:35:42,262 - INFO - Epoch 42 Batch 120: Train Loss = 0.2154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Batch 120: Train Loss = 0.2154\n",
      "Epoch 42: Loss = 0.2166 Valid loss = 0.2050 roc = 0.8420\n",
      "confusion matrix:\n",
      "[[3738    4]\n",
      " [ 251   41]]\n",
      "accuracy = 0.9367873072624207\n",
      "precision class 0 = 0.937076985836029\n",
      "precision class 1 = 0.9111111164093018\n",
      "recall class 0 = 0.9989310503005981\n",
      "recall class 1 = 0.14041095972061157\n",
      "AUC of ROC = 0.842016850559733\n",
      "AUC of PRC = 0.41431169438688487\n",
      "min(+P, Se) = 0.42905405405405406\n",
      "f1_score = 0.24332345734403116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:35:54,685 - INFO - Epoch 43 Batch 0: Train Loss = 0.2328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Batch 0: Train Loss = 0.2328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:36:11,644 - INFO - Epoch 43 Batch 20: Train Loss = 0.2496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Batch 20: Train Loss = 0.2496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:36:28,355 - INFO - Epoch 43 Batch 40: Train Loss = 0.2743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Batch 40: Train Loss = 0.2743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:36:45,634 - INFO - Epoch 43 Batch 60: Train Loss = 0.2545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Batch 60: Train Loss = 0.2545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:37:02,346 - INFO - Epoch 43 Batch 80: Train Loss = 0.2064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Batch 80: Train Loss = 0.2064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:37:18,983 - INFO - Epoch 43 Batch 100: Train Loss = 0.2044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Batch 100: Train Loss = 0.2044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:37:35,147 - INFO - Epoch 43 Batch 120: Train Loss = 0.2126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Batch 120: Train Loss = 0.2126\n",
      "Epoch 43: Loss = 0.2195 Valid loss = 0.2058 roc = 0.8442\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 258   34]]\n",
      "accuracy = 0.9352999329566956\n",
      "precision class 0 = 0.9354515671730042\n",
      "precision class 1 = 0.9189189076423645\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.1164383590221405\n",
      "AUC of ROC = 0.8442201811352805\n",
      "AUC of PRC = 0.42528787663794\n",
      "min(+P, Se) = 0.4370860927152318\n",
      "f1_score = 0.2066869402575825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:37:46,741 - INFO - Epoch 44 Batch 0: Train Loss = 0.2329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Batch 0: Train Loss = 0.2329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:38:04,157 - INFO - Epoch 44 Batch 20: Train Loss = 0.2611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Batch 20: Train Loss = 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:38:20,142 - INFO - Epoch 44 Batch 40: Train Loss = 0.2828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Batch 40: Train Loss = 0.2828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:38:35,895 - INFO - Epoch 44 Batch 60: Train Loss = 0.2635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Batch 60: Train Loss = 0.2635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:38:52,871 - INFO - Epoch 44 Batch 80: Train Loss = 0.2154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Batch 80: Train Loss = 0.2154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:39:10,638 - INFO - Epoch 44 Batch 100: Train Loss = 0.2105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Batch 100: Train Loss = 0.2105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:39:28,251 - INFO - Epoch 44 Batch 120: Train Loss = 0.2240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Batch 120: Train Loss = 0.2240\n",
      "Epoch 44: Loss = 0.2168 Valid loss = 0.2036 roc = 0.8456\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 257   35]]\n",
      "accuracy = 0.9357957243919373\n",
      "precision class 0 = 0.9357017874717712\n",
      "precision class 1 = 0.9459459185600281\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.11986301094293594\n",
      "AUC of ROC = 0.845560940966299\n",
      "AUC of PRC = 0.4272617467481253\n",
      "min(+P, Se) = 0.4417808219178082\n",
      "f1_score = 0.2127659598493648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:39:40,514 - INFO - Epoch 45 Batch 0: Train Loss = 0.2138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Batch 0: Train Loss = 0.2138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:39:57,484 - INFO - Epoch 45 Batch 20: Train Loss = 0.2683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Batch 20: Train Loss = 0.2683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:40:13,994 - INFO - Epoch 45 Batch 40: Train Loss = 0.2589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Batch 40: Train Loss = 0.2589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:40:30,464 - INFO - Epoch 45 Batch 60: Train Loss = 0.2519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Batch 60: Train Loss = 0.2519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:40:46,641 - INFO - Epoch 45 Batch 80: Train Loss = 0.2057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Batch 80: Train Loss = 0.2057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:41:02,635 - INFO - Epoch 45 Batch 100: Train Loss = 0.1701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Batch 100: Train Loss = 0.1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:41:18,806 - INFO - Epoch 45 Batch 120: Train Loss = 0.2497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Batch 120: Train Loss = 0.2497\n",
      "Epoch 45: Loss = 0.2126 Valid loss = 0.2042 roc = 0.8406\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 253   39]]\n",
      "accuracy = 0.9365394115447998\n",
      "precision class 0 = 0.9366232752799988\n",
      "precision class 1 = 0.9285714030265808\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.1335616409778595\n",
      "AUC of ROC = 0.8405630642173624\n",
      "AUC of PRC = 0.4175410031282152\n",
      "min(+P, Se) = 0.4315068493150685\n",
      "f1_score = 0.23353292240272916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:41:29,785 - INFO - Epoch 46 Batch 0: Train Loss = 0.2200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Batch 0: Train Loss = 0.2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:41:46,291 - INFO - Epoch 46 Batch 20: Train Loss = 0.2508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Batch 20: Train Loss = 0.2508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:42:02,950 - INFO - Epoch 46 Batch 40: Train Loss = 0.2487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Batch 40: Train Loss = 0.2487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:42:19,512 - INFO - Epoch 46 Batch 60: Train Loss = 0.2728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Batch 60: Train Loss = 0.2728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:42:36,223 - INFO - Epoch 46 Batch 80: Train Loss = 0.1923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Batch 80: Train Loss = 0.1923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:42:53,356 - INFO - Epoch 46 Batch 100: Train Loss = 0.1846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Batch 100: Train Loss = 0.1846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:43:09,279 - INFO - Epoch 46 Batch 120: Train Loss = 0.2293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Batch 120: Train Loss = 0.2293\n",
      "Epoch 46: Loss = 0.2192 Valid loss = 0.2049 roc = 0.8409\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 256   36]]\n",
      "accuracy = 0.9357957243919373\n",
      "precision class 0 = 0.9359198808670044\n",
      "precision class 1 = 0.9230769276618958\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.12328767031431198\n",
      "AUC of ROC = 0.84094470029213\n",
      "AUC of PRC = 0.42218126681414314\n",
      "min(+P, Se) = 0.4417808219178082\n",
      "f1_score = 0.21752266814986623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:43:21,255 - INFO - Epoch 47 Batch 0: Train Loss = 0.2466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Batch 0: Train Loss = 0.2466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:43:38,934 - INFO - Epoch 47 Batch 20: Train Loss = 0.2593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Batch 20: Train Loss = 0.2593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:43:55,167 - INFO - Epoch 47 Batch 40: Train Loss = 0.2870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Batch 40: Train Loss = 0.2870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:44:12,486 - INFO - Epoch 47 Batch 60: Train Loss = 0.2691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Batch 60: Train Loss = 0.2691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:44:30,351 - INFO - Epoch 47 Batch 80: Train Loss = 0.2062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Batch 80: Train Loss = 0.2062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:44:48,399 - INFO - Epoch 47 Batch 100: Train Loss = 0.1833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Batch 100: Train Loss = 0.1833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:45:04,901 - INFO - Epoch 47 Batch 120: Train Loss = 0.2344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Batch 120: Train Loss = 0.2344\n",
      "Epoch 47: Loss = 0.2217 Valid loss = 0.2063 roc = 0.8356\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 262   30]]\n",
      "accuracy = 0.934556245803833\n",
      "precision class 0 = 0.9345327615737915\n",
      "precision class 1 = 0.9375\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.10273972898721695\n",
      "AUC of ROC = 0.8355670178572736\n",
      "AUC of PRC = 0.42350831514034076\n",
      "min(+P, Se) = 0.4178082191780822\n",
      "f1_score = 0.18518519662508023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:45:16,900 - INFO - Epoch 48 Batch 0: Train Loss = 0.2285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Batch 0: Train Loss = 0.2285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:45:34,475 - INFO - Epoch 48 Batch 20: Train Loss = 0.2657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Batch 20: Train Loss = 0.2657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:45:50,675 - INFO - Epoch 48 Batch 40: Train Loss = 0.2795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Batch 40: Train Loss = 0.2795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:46:07,104 - INFO - Epoch 48 Batch 60: Train Loss = 0.2658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Batch 60: Train Loss = 0.2658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:46:23,949 - INFO - Epoch 48 Batch 80: Train Loss = 0.2078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Batch 80: Train Loss = 0.2078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:46:40,969 - INFO - Epoch 48 Batch 100: Train Loss = 0.1784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Batch 100: Train Loss = 0.1784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:46:57,817 - INFO - Epoch 48 Batch 120: Train Loss = 0.2414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Batch 120: Train Loss = 0.2414\n",
      "Epoch 48: Loss = 0.2184 Valid loss = 0.2070 roc = 0.8411\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 259   33]]\n",
      "accuracy = 0.9350520372390747\n",
      "precision class 0 = 0.9352176189422607\n",
      "precision class 1 = 0.9166666865348816\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.11301369965076447\n",
      "AUC of ROC = 0.8411423822876932\n",
      "AUC of PRC = 0.4219529736124301\n",
      "min(+P, Se) = 0.4280936454849498\n",
      "f1_score = 0.2012195172035432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:47:09,470 - INFO - Epoch 49 Batch 0: Train Loss = 0.2272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Batch 0: Train Loss = 0.2272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:47:25,905 - INFO - Epoch 49 Batch 20: Train Loss = 0.2450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Batch 20: Train Loss = 0.2450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:47:42,858 - INFO - Epoch 49 Batch 40: Train Loss = 0.2824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Batch 40: Train Loss = 0.2824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:47:58,986 - INFO - Epoch 49 Batch 60: Train Loss = 0.2626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Batch 60: Train Loss = 0.2626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:48:15,578 - INFO - Epoch 49 Batch 80: Train Loss = 0.1960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Batch 80: Train Loss = 0.1960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:48:31,999 - INFO - Epoch 49 Batch 100: Train Loss = 0.1774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Batch 100: Train Loss = 0.1774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:48:48,929 - INFO - Epoch 49 Batch 120: Train Loss = 0.2478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Batch 120: Train Loss = 0.2478\n",
      "Epoch 49: Loss = 0.2246 Valid loss = 0.2032 roc = 0.8413\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 251   41]]\n",
      "accuracy = 0.9370352029800415\n",
      "precision class 0 = 0.9370927214622498\n",
      "precision class 1 = 0.9318181872367859\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.14041095972061157\n",
      "AUC of ROC = 0.8413373186999847\n",
      "AUC of PRC = 0.4221267318495526\n",
      "min(+P, Se) = 0.4280821917808219\n",
      "f1_score = 0.24404762046677725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:49:00,624 - INFO - Epoch 50 Batch 0: Train Loss = 0.2262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Batch 0: Train Loss = 0.2262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:49:18,873 - INFO - Epoch 50 Batch 20: Train Loss = 0.2679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Batch 20: Train Loss = 0.2679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:49:36,917 - INFO - Epoch 50 Batch 40: Train Loss = 0.3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Batch 40: Train Loss = 0.3047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:49:54,563 - INFO - Epoch 50 Batch 60: Train Loss = 0.2572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Batch 60: Train Loss = 0.2572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:50:11,177 - INFO - Epoch 50 Batch 80: Train Loss = 0.2200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Batch 80: Train Loss = 0.2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:50:27,326 - INFO - Epoch 50 Batch 100: Train Loss = 0.1685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Batch 100: Train Loss = 0.1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:50:43,387 - INFO - Epoch 50 Batch 120: Train Loss = 0.2226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Batch 120: Train Loss = 0.2226\n",
      "Epoch 50: Loss = 0.2186 Valid loss = 0.2044 roc = 0.8416\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 256   36]]\n",
      "accuracy = 0.9357957243919373\n",
      "precision class 0 = 0.9359198808670044\n",
      "precision class 1 = 0.9230769276618958\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.12328767031431198\n",
      "AUC of ROC = 0.8416219441658186\n",
      "AUC of PRC = 0.42296255522332676\n",
      "min(+P, Se) = 0.4246575342465753\n",
      "f1_score = 0.21752266814986623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:50:55,315 - INFO - Epoch 51 Batch 0: Train Loss = 0.2440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Batch 0: Train Loss = 0.2440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:51:12,124 - INFO - Epoch 51 Batch 20: Train Loss = 0.2627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Batch 20: Train Loss = 0.2627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:51:28,802 - INFO - Epoch 51 Batch 40: Train Loss = 0.3208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Batch 40: Train Loss = 0.3208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:51:44,874 - INFO - Epoch 51 Batch 60: Train Loss = 0.2459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Batch 60: Train Loss = 0.2459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:52:00,743 - INFO - Epoch 51 Batch 80: Train Loss = 0.2271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Batch 80: Train Loss = 0.2271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:52:17,617 - INFO - Epoch 51 Batch 100: Train Loss = 0.1911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Batch 100: Train Loss = 0.1911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:52:34,379 - INFO - Epoch 51 Batch 120: Train Loss = 0.2219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Batch 120: Train Loss = 0.2219\n",
      "Epoch 51: Loss = 0.2166 Valid loss = 0.2041 roc = 0.8427\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 256   36]]\n",
      "accuracy = 0.9357957243919373\n",
      "precision class 0 = 0.9359198808670044\n",
      "precision class 1 = 0.9230769276618958\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.12328767031431198\n",
      "AUC of ROC = 0.8426716721700358\n",
      "AUC of PRC = 0.42730095197953033\n",
      "min(+P, Se) = 0.4417808219178082\n",
      "f1_score = 0.21752266814986623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:52:45,677 - INFO - Epoch 52 Batch 0: Train Loss = 0.2341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Batch 0: Train Loss = 0.2341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:53:02,443 - INFO - Epoch 52 Batch 20: Train Loss = 0.2757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Batch 20: Train Loss = 0.2757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:53:18,926 - INFO - Epoch 52 Batch 40: Train Loss = 0.2722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Batch 40: Train Loss = 0.2722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:53:35,084 - INFO - Epoch 52 Batch 60: Train Loss = 0.2586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Batch 60: Train Loss = 0.2586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:53:51,170 - INFO - Epoch 52 Batch 80: Train Loss = 0.1893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Batch 80: Train Loss = 0.1893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:54:07,349 - INFO - Epoch 52 Batch 100: Train Loss = 0.1695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Batch 100: Train Loss = 0.1695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:54:24,935 - INFO - Epoch 52 Batch 120: Train Loss = 0.2614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Batch 120: Train Loss = 0.2614\n",
      "Epoch 52: Loss = 0.2207 Valid loss = 0.2039 roc = 0.8452\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 262   30]]\n",
      "accuracy = 0.9343083500862122\n",
      "precision class 0 = 0.9345163702964783\n",
      "precision class 1 = 0.9090909361839294\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.10273972898721695\n",
      "AUC of ROC = 0.8451866264469223\n",
      "AUC of PRC = 0.4334923513706081\n",
      "min(+P, Se) = 0.4349315068493151\n",
      "f1_score = 0.18461538587434767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:54:37,352 - INFO - Epoch 53 Batch 0: Train Loss = 0.2536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Batch 0: Train Loss = 0.2536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:54:54,574 - INFO - Epoch 53 Batch 20: Train Loss = 0.2565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Batch 20: Train Loss = 0.2565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:55:11,131 - INFO - Epoch 53 Batch 40: Train Loss = 0.2853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Batch 40: Train Loss = 0.2853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:55:27,671 - INFO - Epoch 53 Batch 60: Train Loss = 0.2427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Batch 60: Train Loss = 0.2427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:55:44,337 - INFO - Epoch 53 Batch 80: Train Loss = 0.1989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Batch 80: Train Loss = 0.1989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:56:01,231 - INFO - Epoch 53 Batch 100: Train Loss = 0.1938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Batch 100: Train Loss = 0.1938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:56:17,237 - INFO - Epoch 53 Batch 120: Train Loss = 0.2242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Batch 120: Train Loss = 0.2242\n",
      "Epoch 53: Loss = 0.2164 Valid loss = 0.2046 roc = 0.8450\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 259   33]]\n",
      "accuracy = 0.9350520372390747\n",
      "precision class 0 = 0.9352176189422607\n",
      "precision class 1 = 0.9166666865348816\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.11301369965076447\n",
      "AUC of ROC = 0.8450264674227391\n",
      "AUC of PRC = 0.43230559372231736\n",
      "min(+P, Se) = 0.4271186440677966\n",
      "f1_score = 0.2012195172035432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:56:28,666 - INFO - Epoch 54 Batch 0: Train Loss = 0.2085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Batch 0: Train Loss = 0.2085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:56:45,205 - INFO - Epoch 54 Batch 20: Train Loss = 0.2576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Batch 20: Train Loss = 0.2576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:57:02,142 - INFO - Epoch 54 Batch 40: Train Loss = 0.2791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Batch 40: Train Loss = 0.2791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:57:18,730 - INFO - Epoch 54 Batch 60: Train Loss = 0.2458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Batch 60: Train Loss = 0.2458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:57:35,730 - INFO - Epoch 54 Batch 80: Train Loss = 0.2249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Batch 80: Train Loss = 0.2249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:57:52,009 - INFO - Epoch 54 Batch 100: Train Loss = 0.1791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Batch 100: Train Loss = 0.1791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:58:07,941 - INFO - Epoch 54 Batch 120: Train Loss = 0.2029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Batch 120: Train Loss = 0.2029\n",
      "Epoch 54: Loss = 0.2167 Valid loss = 0.2043 roc = 0.8458\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 253   39]]\n",
      "accuracy = 0.9365394115447998\n",
      "precision class 0 = 0.9366232752799988\n",
      "precision class 1 = 0.9285714030265808\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.1335616409778595\n",
      "AUC of ROC = 0.8458432784460733\n",
      "AUC of PRC = 0.42971987672295325\n",
      "min(+P, Se) = 0.4383561643835616\n",
      "f1_score = 0.23353292240272916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 13:58:20,216 - INFO - Epoch 55 Batch 0: Train Loss = 0.2267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Batch 0: Train Loss = 0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:58:37,081 - INFO - Epoch 55 Batch 20: Train Loss = 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Batch 20: Train Loss = 0.2457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:58:52,871 - INFO - Epoch 55 Batch 40: Train Loss = 0.2872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Batch 40: Train Loss = 0.2872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:59:09,906 - INFO - Epoch 55 Batch 60: Train Loss = 0.2638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Batch 60: Train Loss = 0.2638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:59:26,214 - INFO - Epoch 55 Batch 80: Train Loss = 0.2133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Batch 80: Train Loss = 0.2133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 13:59:44,143 - INFO - Epoch 55 Batch 100: Train Loss = 0.1806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Batch 100: Train Loss = 0.1806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:00:01,641 - INFO - Epoch 55 Batch 120: Train Loss = 0.1983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Batch 120: Train Loss = 0.1983\n",
      "Epoch 55: Loss = 0.2146 Valid loss = 0.2031 roc = 0.8405\n",
      "confusion matrix:\n",
      "[[3738    4]\n",
      " [ 250   42]]\n",
      "accuracy = 0.9370352029800415\n",
      "precision class 0 = 0.9373119473457336\n",
      "precision class 1 = 0.9130434989929199\n",
      "recall class 0 = 0.9989310503005981\n",
      "recall class 1 = 0.1438356190919876\n",
      "AUC of ROC = 0.8405154741073193\n",
      "AUC of PRC = 0.42157929899900143\n",
      "min(+P, Se) = 0.4387755102040816\n",
      "f1_score = 0.24852070427630485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:00:13,850 - INFO - Epoch 56 Batch 0: Train Loss = 0.2303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Batch 0: Train Loss = 0.2303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:00:31,647 - INFO - Epoch 56 Batch 20: Train Loss = 0.2893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Batch 20: Train Loss = 0.2893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:00:47,499 - INFO - Epoch 56 Batch 40: Train Loss = 0.3050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Batch 40: Train Loss = 0.3050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:01:03,688 - INFO - Epoch 56 Batch 60: Train Loss = 0.2599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Batch 60: Train Loss = 0.2599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:01:20,180 - INFO - Epoch 56 Batch 80: Train Loss = 0.2098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Batch 80: Train Loss = 0.2098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:01:36,789 - INFO - Epoch 56 Batch 100: Train Loss = 0.1880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Batch 100: Train Loss = 0.1880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:01:53,150 - INFO - Epoch 56 Batch 120: Train Loss = 0.2093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Batch 120: Train Loss = 0.2093\n",
      "Epoch 56: Loss = 0.2170 Valid loss = 0.2038 roc = 0.8382\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 254   38]]\n",
      "accuracy = 0.936291515827179\n",
      "precision class 0 = 0.9363886713981628\n",
      "precision class 1 = 0.9268292784690857\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.13013698160648346\n",
      "AUC of ROC = 0.8381771523542461\n",
      "AUC of PRC = 0.41765514101470663\n",
      "min(+P, Se) = 0.43097643097643096\n",
      "f1_score = 0.22822821166412668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:02:05,364 - INFO - Epoch 57 Batch 0: Train Loss = 0.2276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Batch 0: Train Loss = 0.2276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:02:21,887 - INFO - Epoch 57 Batch 20: Train Loss = 0.2495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Batch 20: Train Loss = 0.2495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:02:38,254 - INFO - Epoch 57 Batch 40: Train Loss = 0.2799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Batch 40: Train Loss = 0.2799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:02:53,945 - INFO - Epoch 57 Batch 60: Train Loss = 0.2656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Batch 60: Train Loss = 0.2656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:03:10,718 - INFO - Epoch 57 Batch 80: Train Loss = 0.1985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Batch 80: Train Loss = 0.1985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:03:27,142 - INFO - Epoch 57 Batch 100: Train Loss = 0.1807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Batch 100: Train Loss = 0.1807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:03:43,752 - INFO - Epoch 57 Batch 120: Train Loss = 0.2212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Batch 120: Train Loss = 0.2212\n",
      "Epoch 57: Loss = 0.2204 Valid loss = 0.2028 roc = 0.8451\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 254   38]]\n",
      "accuracy = 0.936291515827179\n",
      "precision class 0 = 0.9363886713981628\n",
      "precision class 1 = 0.9268292784690857\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.13013698160648346\n",
      "AUC of ROC = 0.8451042589487712\n",
      "AUC of PRC = 0.43119026453385273\n",
      "min(+P, Se) = 0.4452054794520548\n",
      "f1_score = 0.22822821166412668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:03:55,572 - INFO - Epoch 58 Batch 0: Train Loss = 0.2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Batch 0: Train Loss = 0.2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:04:13,399 - INFO - Epoch 58 Batch 20: Train Loss = 0.2324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Batch 20: Train Loss = 0.2324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:04:30,635 - INFO - Epoch 58 Batch 40: Train Loss = 0.2706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Batch 40: Train Loss = 0.2706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:04:47,603 - INFO - Epoch 58 Batch 60: Train Loss = 0.2653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Batch 60: Train Loss = 0.2653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:05:04,844 - INFO - Epoch 58 Batch 80: Train Loss = 0.2003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Batch 80: Train Loss = 0.2003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:05:22,382 - INFO - Epoch 58 Batch 100: Train Loss = 0.1958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Batch 100: Train Loss = 0.1958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:05:39,336 - INFO - Epoch 58 Batch 120: Train Loss = 0.2223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Batch 120: Train Loss = 0.2223\n",
      "Epoch 58: Loss = 0.2144 Valid loss = 0.2066 roc = 0.8319\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 254   38]]\n",
      "accuracy = 0.936291515827179\n",
      "precision class 0 = 0.9363886713981628\n",
      "precision class 1 = 0.9268292784690857\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.13013698160648346\n",
      "AUC of ROC = 0.831942847938616\n",
      "AUC of PRC = 0.40585466009690113\n",
      "min(+P, Se) = 0.4080267558528428\n",
      "f1_score = 0.22822821166412668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:05:50,904 - INFO - Epoch 59 Batch 0: Train Loss = 0.2154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Batch 0: Train Loss = 0.2154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:06:08,481 - INFO - Epoch 59 Batch 20: Train Loss = 0.2601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Batch 20: Train Loss = 0.2601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:06:24,955 - INFO - Epoch 59 Batch 40: Train Loss = 0.2778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Batch 40: Train Loss = 0.2778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:06:41,159 - INFO - Epoch 59 Batch 60: Train Loss = 0.2371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Batch 60: Train Loss = 0.2371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:06:58,266 - INFO - Epoch 59 Batch 80: Train Loss = 0.2026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Batch 80: Train Loss = 0.2026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:07:15,281 - INFO - Epoch 59 Batch 100: Train Loss = 0.1760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Batch 100: Train Loss = 0.1760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:07:31,612 - INFO - Epoch 59 Batch 120: Train Loss = 0.2389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Batch 120: Train Loss = 0.2389\n",
      "Epoch 59: Loss = 0.2191 Valid loss = 0.2033 roc = 0.8454\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 254   38]]\n",
      "accuracy = 0.936291515827179\n",
      "precision class 0 = 0.9363886713981628\n",
      "precision class 1 = 0.9268292784690857\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.13013698160648346\n",
      "AUC of ROC = 0.8454437960800392\n",
      "AUC of PRC = 0.4334242460709595\n",
      "min(+P, Se) = 0.4349315068493151\n",
      "f1_score = 0.22822821166412668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:07:42,778 - INFO - Epoch 60 Batch 0: Train Loss = 0.2264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Batch 0: Train Loss = 0.2264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:07:58,803 - INFO - Epoch 60 Batch 20: Train Loss = 0.2547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Batch 20: Train Loss = 0.2547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:08:15,017 - INFO - Epoch 60 Batch 40: Train Loss = 0.2923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Batch 40: Train Loss = 0.2923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:08:32,307 - INFO - Epoch 60 Batch 60: Train Loss = 0.2674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Batch 60: Train Loss = 0.2674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:08:48,687 - INFO - Epoch 60 Batch 80: Train Loss = 0.2086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Batch 80: Train Loss = 0.2086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:09:05,373 - INFO - Epoch 60 Batch 100: Train Loss = 0.1608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Batch 100: Train Loss = 0.1608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:09:21,068 - INFO - Epoch 60 Batch 120: Train Loss = 0.2353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Batch 120: Train Loss = 0.2353\n",
      "Epoch 60: Loss = 0.2167 Valid loss = 0.2038 roc = 0.8420\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 253   39]]\n",
      "accuracy = 0.9365394115447998\n",
      "precision class 0 = 0.9366232752799988\n",
      "precision class 1 = 0.9285714030265808\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.1335616409778595\n",
      "AUC of ROC = 0.8420264601011839\n",
      "AUC of PRC = 0.4204057971767752\n",
      "min(+P, Se) = 0.4280821917808219\n",
      "f1_score = 0.23353292240272916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:09:32,799 - INFO - Epoch 61 Batch 0: Train Loss = 0.2325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Batch 0: Train Loss = 0.2325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:09:50,312 - INFO - Epoch 61 Batch 20: Train Loss = 0.2341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Batch 20: Train Loss = 0.2341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:10:07,508 - INFO - Epoch 61 Batch 40: Train Loss = 0.3057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Batch 40: Train Loss = 0.3057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:10:25,306 - INFO - Epoch 61 Batch 60: Train Loss = 0.2702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Batch 60: Train Loss = 0.2702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:10:42,492 - INFO - Epoch 61 Batch 80: Train Loss = 0.2070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Batch 80: Train Loss = 0.2070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:10:59,297 - INFO - Epoch 61 Batch 100: Train Loss = 0.1729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Batch 100: Train Loss = 0.1729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:11:15,272 - INFO - Epoch 61 Batch 120: Train Loss = 0.2027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Batch 120: Train Loss = 0.2027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:11:27,168 - INFO - ------------ Save best model - AUROC: 0.8462 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: Loss = 0.2189 Valid loss = 0.2046 roc = 0.8462\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 257   35]]\n",
      "accuracy = 0.9355478286743164\n",
      "precision class 0 = 0.9356856942176819\n",
      "precision class 1 = 0.9210526347160339\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.11986301094293594\n",
      "AUC of ROC = 0.8462299480901724\n",
      "AUC of PRC = 0.4311225286545603\n",
      "min(+P, Se) = 0.4417808219178082\n",
      "f1_score = 0.21212121548074675\n",
      "------------ Save best model - AUROC: 0.8462 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:11:28,027 - INFO - Epoch 62 Batch 0: Train Loss = 0.2299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Batch 0: Train Loss = 0.2299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:11:46,139 - INFO - Epoch 62 Batch 20: Train Loss = 0.2533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Batch 20: Train Loss = 0.2533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:12:03,565 - INFO - Epoch 62 Batch 40: Train Loss = 0.2791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Batch 40: Train Loss = 0.2791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:12:20,108 - INFO - Epoch 62 Batch 60: Train Loss = 0.2929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Batch 60: Train Loss = 0.2929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:12:36,667 - INFO - Epoch 62 Batch 80: Train Loss = 0.2383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Batch 80: Train Loss = 0.2383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:12:52,991 - INFO - Epoch 62 Batch 100: Train Loss = 0.1939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Batch 100: Train Loss = 0.1939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:13:09,274 - INFO - Epoch 62 Batch 120: Train Loss = 0.2336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Batch 120: Train Loss = 0.2336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:13:20,298 - INFO - ------------ Save best model - AUROC: 0.8475 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: Loss = 0.2152 Valid loss = 0.2038 roc = 0.8475\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 257   35]]\n",
      "accuracy = 0.9355478286743164\n",
      "precision class 0 = 0.9356856942176819\n",
      "precision class 1 = 0.9210526347160339\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.11986301094293594\n",
      "AUC of ROC = 0.8475148810613328\n",
      "AUC of PRC = 0.4343035411533047\n",
      "min(+P, Se) = 0.4452054794520548\n",
      "f1_score = 0.21212121548074675\n",
      "------------ Save best model - AUROC: 0.8475 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:13:21,249 - INFO - Epoch 63 Batch 0: Train Loss = 0.2231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Batch 0: Train Loss = 0.2231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:13:37,557 - INFO - Epoch 63 Batch 20: Train Loss = 0.2652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Batch 20: Train Loss = 0.2652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:13:53,889 - INFO - Epoch 63 Batch 40: Train Loss = 0.2711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Batch 40: Train Loss = 0.2711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:14:09,949 - INFO - Epoch 63 Batch 60: Train Loss = 0.2767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Batch 60: Train Loss = 0.2767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:14:26,643 - INFO - Epoch 63 Batch 80: Train Loss = 0.2338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Batch 80: Train Loss = 0.2338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:14:42,467 - INFO - Epoch 63 Batch 100: Train Loss = 0.1934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Batch 100: Train Loss = 0.1934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:14:58,691 - INFO - Epoch 63 Batch 120: Train Loss = 0.2284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Batch 120: Train Loss = 0.2284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:15:10,328 - INFO - ------------ Save best model - AUROC: 0.8476 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: Loss = 0.2110 Valid loss = 0.2077 roc = 0.8476\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 257   35]]\n",
      "accuracy = 0.9355478286743164\n",
      "precision class 0 = 0.9356856942176819\n",
      "precision class 1 = 0.9210526347160339\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.11986301094293594\n",
      "AUC of ROC = 0.8475826054487016\n",
      "AUC of PRC = 0.4217419903796542\n",
      "min(+P, Se) = 0.423728813559322\n",
      "f1_score = 0.21212121548074675\n",
      "------------ Save best model - AUROC: 0.8476 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:15:11,051 - INFO - Epoch 64 Batch 0: Train Loss = 0.2329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Batch 0: Train Loss = 0.2329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:15:29,103 - INFO - Epoch 64 Batch 20: Train Loss = 0.2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Batch 20: Train Loss = 0.2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:15:45,596 - INFO - Epoch 64 Batch 40: Train Loss = 0.2868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Batch 40: Train Loss = 0.2868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:16:01,957 - INFO - Epoch 64 Batch 60: Train Loss = 0.2383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Batch 60: Train Loss = 0.2383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:16:18,142 - INFO - Epoch 64 Batch 80: Train Loss = 0.2162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Batch 80: Train Loss = 0.2162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:16:34,563 - INFO - Epoch 64 Batch 100: Train Loss = 0.1800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Batch 100: Train Loss = 0.1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:16:51,393 - INFO - Epoch 64 Batch 120: Train Loss = 0.2375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Batch 120: Train Loss = 0.2375\n",
      "Epoch 64: Loss = 0.2203 Valid loss = 0.2053 roc = 0.8407\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 258   34]]\n",
      "accuracy = 0.9352999329566956\n",
      "precision class 0 = 0.9354515671730042\n",
      "precision class 1 = 0.9189189076423645\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.1164383590221405\n",
      "AUC of ROC = 0.8406747179370786\n",
      "AUC of PRC = 0.42112989618336016\n",
      "min(+P, Se) = 0.4421768707482993\n",
      "f1_score = 0.2066869402575825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:17:02,658 - INFO - Epoch 65 Batch 0: Train Loss = 0.2281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Batch 0: Train Loss = 0.2281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:17:19,725 - INFO - Epoch 65 Batch 20: Train Loss = 0.2562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Batch 20: Train Loss = 0.2562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:17:35,589 - INFO - Epoch 65 Batch 40: Train Loss = 0.2812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Batch 40: Train Loss = 0.2812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:17:51,382 - INFO - Epoch 65 Batch 60: Train Loss = 0.2566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Batch 60: Train Loss = 0.2566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:18:07,276 - INFO - Epoch 65 Batch 80: Train Loss = 0.1935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Batch 80: Train Loss = 0.1935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:18:23,053 - INFO - Epoch 65 Batch 100: Train Loss = 0.1879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Batch 100: Train Loss = 0.1879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:18:39,366 - INFO - Epoch 65 Batch 120: Train Loss = 0.2203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Batch 120: Train Loss = 0.2203\n",
      "Epoch 65: Loss = 0.2205 Valid loss = 0.2041 roc = 0.8409\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 257   35]]\n",
      "accuracy = 0.9355478286743164\n",
      "precision class 0 = 0.9356856942176819\n",
      "precision class 1 = 0.9210526347160339\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.11986301094293594\n",
      "AUC of ROC = 0.8408586720162831\n",
      "AUC of PRC = 0.42221313706412317\n",
      "min(+P, Se) = 0.4315068493150685\n",
      "f1_score = 0.21212121548074675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:18:50,770 - INFO - Epoch 66 Batch 0: Train Loss = 0.2534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Batch 0: Train Loss = 0.2534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:19:07,890 - INFO - Epoch 66 Batch 20: Train Loss = 0.2723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Batch 20: Train Loss = 0.2723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:19:24,235 - INFO - Epoch 66 Batch 40: Train Loss = 0.2661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Batch 40: Train Loss = 0.2661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:19:40,948 - INFO - Epoch 66 Batch 60: Train Loss = 0.2626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Batch 60: Train Loss = 0.2626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:19:57,540 - INFO - Epoch 66 Batch 80: Train Loss = 0.2037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Batch 80: Train Loss = 0.2037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:20:14,682 - INFO - Epoch 66 Batch 100: Train Loss = 0.2014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Batch 100: Train Loss = 0.2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:20:32,092 - INFO - Epoch 66 Batch 120: Train Loss = 0.2140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Batch 120: Train Loss = 0.2140\n",
      "Epoch 66: Loss = 0.2196 Valid loss = 0.2045 roc = 0.8389\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 255   37]]\n",
      "accuracy = 0.9360436201095581\n",
      "precision class 0 = 0.9361542463302612\n",
      "precision class 1 = 0.925000011920929\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.12671232223510742\n",
      "AUC of ROC = 0.8389056471156734\n",
      "AUC of PRC = 0.42035556985781364\n",
      "min(+P, Se) = 0.42857142857142855\n",
      "f1_score = 0.22289156913757324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:20:44,733 - INFO - Epoch 67 Batch 0: Train Loss = 0.2326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Batch 0: Train Loss = 0.2326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:21:01,639 - INFO - Epoch 67 Batch 20: Train Loss = 0.2580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Batch 20: Train Loss = 0.2580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:21:17,730 - INFO - Epoch 67 Batch 40: Train Loss = 0.2872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Batch 40: Train Loss = 0.2872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:21:33,660 - INFO - Epoch 67 Batch 60: Train Loss = 0.2625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Batch 60: Train Loss = 0.2625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:21:50,344 - INFO - Epoch 67 Batch 80: Train Loss = 0.2002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Batch 80: Train Loss = 0.2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:22:06,728 - INFO - Epoch 67 Batch 100: Train Loss = 0.1958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Batch 100: Train Loss = 0.1958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:22:22,537 - INFO - Epoch 67 Batch 120: Train Loss = 0.2077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Batch 120: Train Loss = 0.2077\n",
      "Epoch 67: Loss = 0.2166 Valid loss = 0.2042 roc = 0.8446\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 255   37]]\n",
      "accuracy = 0.9360436201095581\n",
      "precision class 0 = 0.9361542463302612\n",
      "precision class 1 = 0.925000011920929\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.12671232223510742\n",
      "AUC of ROC = 0.8445972412379287\n",
      "AUC of PRC = 0.42485360979796555\n",
      "min(+P, Se) = 0.4402730375426621\n",
      "f1_score = 0.22289156913757324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:22:34,048 - INFO - Epoch 68 Batch 0: Train Loss = 0.2309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Batch 0: Train Loss = 0.2309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:22:50,399 - INFO - Epoch 68 Batch 20: Train Loss = 0.2624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Batch 20: Train Loss = 0.2624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:23:06,518 - INFO - Epoch 68 Batch 40: Train Loss = 0.2687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Batch 40: Train Loss = 0.2687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:23:22,994 - INFO - Epoch 68 Batch 60: Train Loss = 0.2620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Batch 60: Train Loss = 0.2620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:23:39,173 - INFO - Epoch 68 Batch 80: Train Loss = 0.1789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Batch 80: Train Loss = 0.1789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:23:55,716 - INFO - Epoch 68 Batch 100: Train Loss = 0.1853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Batch 100: Train Loss = 0.1853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:24:12,540 - INFO - Epoch 68 Batch 120: Train Loss = 0.2428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Batch 120: Train Loss = 0.2428\n",
      "Epoch 68: Loss = 0.2184 Valid loss = 0.2058 roc = 0.8395\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 258   34]]\n",
      "accuracy = 0.9352999329566956\n",
      "precision class 0 = 0.9354515671730042\n",
      "precision class 1 = 0.9189189076423645\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.1164383590221405\n",
      "AUC of ROC = 0.8395394192542264\n",
      "AUC of PRC = 0.41786559027589376\n",
      "min(+P, Se) = 0.4334470989761092\n",
      "f1_score = 0.2066869402575825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:24:24,007 - INFO - Epoch 69 Batch 0: Train Loss = 0.2442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 Batch 0: Train Loss = 0.2442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:24:40,714 - INFO - Epoch 69 Batch 20: Train Loss = 0.2198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 Batch 20: Train Loss = 0.2198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:24:56,705 - INFO - Epoch 69 Batch 40: Train Loss = 0.3171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 Batch 40: Train Loss = 0.3171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:25:13,693 - INFO - Epoch 69 Batch 60: Train Loss = 0.2613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 Batch 60: Train Loss = 0.2613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:25:30,613 - INFO - Epoch 69 Batch 80: Train Loss = 0.2137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 Batch 80: Train Loss = 0.2137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:25:48,201 - INFO - Epoch 69 Batch 100: Train Loss = 0.1742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 Batch 100: Train Loss = 0.1742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:26:04,971 - INFO - Epoch 69 Batch 120: Train Loss = 0.2262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 Batch 120: Train Loss = 0.2262\n",
      "Epoch 69: Loss = 0.2176 Valid loss = 0.2046 roc = 0.8442\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 257   35]]\n",
      "accuracy = 0.9355478286743164\n",
      "precision class 0 = 0.9356856942176819\n",
      "precision class 1 = 0.9210526347160339\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.11986301094293594\n",
      "AUC of ROC = 0.8441826581639003\n",
      "AUC of PRC = 0.4225607337231854\n",
      "min(+P, Se) = 0.4300341296928328\n",
      "f1_score = 0.21212121548074675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:26:16,508 - INFO - Epoch 70 Batch 0: Train Loss = 0.2296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 Batch 0: Train Loss = 0.2296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:26:33,676 - INFO - Epoch 70 Batch 20: Train Loss = 0.2533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 Batch 20: Train Loss = 0.2533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:26:50,163 - INFO - Epoch 70 Batch 40: Train Loss = 0.2883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 Batch 40: Train Loss = 0.2883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:27:06,294 - INFO - Epoch 70 Batch 60: Train Loss = 0.2536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 Batch 60: Train Loss = 0.2536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:27:23,134 - INFO - Epoch 70 Batch 80: Train Loss = 0.1982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 Batch 80: Train Loss = 0.1982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:27:40,355 - INFO - Epoch 70 Batch 100: Train Loss = 0.1777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 Batch 100: Train Loss = 0.1777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:27:57,252 - INFO - Epoch 70 Batch 120: Train Loss = 0.2311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 Batch 120: Train Loss = 0.2311\n",
      "Epoch 70: Loss = 0.2195 Valid loss = 0.2052 roc = 0.8402\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 257   35]]\n",
      "accuracy = 0.9355478286743164\n",
      "precision class 0 = 0.9356856942176819\n",
      "precision class 1 = 0.9210526347160339\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.11986301094293594\n",
      "AUC of ROC = 0.8402107143641595\n",
      "AUC of PRC = 0.42046046116646274\n",
      "min(+P, Se) = 0.43197278911564624\n",
      "f1_score = 0.21212121548074675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:28:08,930 - INFO - Epoch 71 Batch 0: Train Loss = 0.2301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 Batch 0: Train Loss = 0.2301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:28:25,752 - INFO - Epoch 71 Batch 20: Train Loss = 0.2698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 Batch 20: Train Loss = 0.2698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:28:41,999 - INFO - Epoch 71 Batch 40: Train Loss = 0.2780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 Batch 40: Train Loss = 0.2780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:28:57,799 - INFO - Epoch 71 Batch 60: Train Loss = 0.2709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 Batch 60: Train Loss = 0.2709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:29:14,784 - INFO - Epoch 71 Batch 80: Train Loss = 0.2206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 Batch 80: Train Loss = 0.2206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:29:31,304 - INFO - Epoch 71 Batch 100: Train Loss = 0.1701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 Batch 100: Train Loss = 0.1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:29:46,830 - INFO - Epoch 71 Batch 120: Train Loss = 0.2508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 Batch 120: Train Loss = 0.2508\n",
      "Epoch 71: Loss = 0.2194 Valid loss = 0.2054 roc = 0.8392\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 253   39]]\n",
      "accuracy = 0.9365394115447998\n",
      "precision class 0 = 0.9366232752799988\n",
      "precision class 1 = 0.9285714030265808\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.1335616409778595\n",
      "AUC of ROC = 0.8391564103878228\n",
      "AUC of PRC = 0.41209946825698407\n",
      "min(+P, Se) = 0.4315068493150685\n",
      "f1_score = 0.23353292240272916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:29:59,197 - INFO - Epoch 72 Batch 0: Train Loss = 0.2547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 Batch 0: Train Loss = 0.2547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:30:16,161 - INFO - Epoch 72 Batch 20: Train Loss = 0.2543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 Batch 20: Train Loss = 0.2543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:30:33,350 - INFO - Epoch 72 Batch 40: Train Loss = 0.2459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 Batch 40: Train Loss = 0.2459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:30:51,016 - INFO - Epoch 72 Batch 60: Train Loss = 0.2750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 Batch 60: Train Loss = 0.2750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:31:08,150 - INFO - Epoch 72 Batch 80: Train Loss = 0.1993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 Batch 80: Train Loss = 0.1993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:31:24,941 - INFO - Epoch 72 Batch 100: Train Loss = 0.2044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 Batch 100: Train Loss = 0.2044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:31:39,975 - INFO - Epoch 72 Batch 120: Train Loss = 0.2362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 Batch 120: Train Loss = 0.2362\n",
      "Epoch 72: Loss = 0.2186 Valid loss = 0.2056 roc = 0.8358\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 258   34]]\n",
      "accuracy = 0.9352999329566956\n",
      "precision class 0 = 0.9354515671730042\n",
      "precision class 1 = 0.9189189076423645\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.1164383590221405\n",
      "AUC of ROC = 0.8357747669914997\n",
      "AUC of PRC = 0.4209599878150839\n",
      "min(+P, Se) = 0.4349315068493151\n",
      "f1_score = 0.2066869402575825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:31:51,412 - INFO - Epoch 73 Batch 0: Train Loss = 0.2339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 Batch 0: Train Loss = 0.2339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:32:07,136 - INFO - Epoch 73 Batch 20: Train Loss = 0.2361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 Batch 20: Train Loss = 0.2361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:32:23,191 - INFO - Epoch 73 Batch 40: Train Loss = 0.3230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 Batch 40: Train Loss = 0.3230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:32:39,427 - INFO - Epoch 73 Batch 60: Train Loss = 0.2444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 Batch 60: Train Loss = 0.2444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:32:55,273 - INFO - Epoch 73 Batch 80: Train Loss = 0.2146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 Batch 80: Train Loss = 0.2146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:33:11,323 - INFO - Epoch 73 Batch 100: Train Loss = 0.1703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 Batch 100: Train Loss = 0.1703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:33:26,876 - INFO - Epoch 73 Batch 120: Train Loss = 0.2425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 Batch 120: Train Loss = 0.2425\n",
      "Epoch 73: Loss = 0.2217 Valid loss = 0.2050 roc = 0.8403\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 254   38]]\n",
      "accuracy = 0.936291515827179\n",
      "precision class 0 = 0.9363886713981628\n",
      "precision class 1 = 0.9268292784690857\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.13013698160648346\n",
      "AUC of ROC = 0.8402592196686264\n",
      "AUC of PRC = 0.41408479011364296\n",
      "min(+P, Se) = 0.4246575342465753\n",
      "f1_score = 0.22822821166412668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:33:38,505 - INFO - Epoch 74 Batch 0: Train Loss = 0.2239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Batch 0: Train Loss = 0.2239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:33:55,380 - INFO - Epoch 74 Batch 20: Train Loss = 0.2536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Batch 20: Train Loss = 0.2536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:34:10,565 - INFO - Epoch 74 Batch 40: Train Loss = 0.2539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Batch 40: Train Loss = 0.2539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:34:27,627 - INFO - Epoch 74 Batch 60: Train Loss = 0.2560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Batch 60: Train Loss = 0.2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:34:43,811 - INFO - Epoch 74 Batch 80: Train Loss = 0.1861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Batch 80: Train Loss = 0.1861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:34:59,217 - INFO - Epoch 74 Batch 100: Train Loss = 0.1555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Batch 100: Train Loss = 0.1555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:35:15,514 - INFO - Epoch 74 Batch 120: Train Loss = 0.2137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Batch 120: Train Loss = 0.2137\n",
      "Epoch 74: Loss = 0.2183 Valid loss = 0.2054 roc = 0.8374\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 253   39]]\n",
      "accuracy = 0.9365394115447998\n",
      "precision class 0 = 0.9366232752799988\n",
      "precision class 1 = 0.9285714030265808\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.1335616409778595\n",
      "AUC of ROC = 0.8374266929266453\n",
      "AUC of PRC = 0.4124382027635655\n",
      "min(+P, Se) = 0.4334470989761092\n",
      "f1_score = 0.23353292240272916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:35:27,965 - INFO - Epoch 75 Batch 0: Train Loss = 0.2417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 Batch 0: Train Loss = 0.2417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:35:45,766 - INFO - Epoch 75 Batch 20: Train Loss = 0.2302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 Batch 20: Train Loss = 0.2302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:36:03,319 - INFO - Epoch 75 Batch 40: Train Loss = 0.2751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 Batch 40: Train Loss = 0.2751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:36:19,312 - INFO - Epoch 75 Batch 60: Train Loss = 0.2659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 Batch 60: Train Loss = 0.2659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:36:35,784 - INFO - Epoch 75 Batch 80: Train Loss = 0.2027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 Batch 80: Train Loss = 0.2027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:36:52,202 - INFO - Epoch 75 Batch 100: Train Loss = 0.1832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 Batch 100: Train Loss = 0.1832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:37:08,464 - INFO - Epoch 75 Batch 120: Train Loss = 0.2332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 Batch 120: Train Loss = 0.2332\n",
      "Epoch 75: Loss = 0.2205 Valid loss = 0.2075 roc = 0.8379\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 255   37]]\n",
      "accuracy = 0.9360436201095581\n",
      "precision class 0 = 0.9361542463302612\n",
      "precision class 1 = 0.925000011920929\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.12671232223510742\n",
      "AUC of ROC = 0.8378824597497494\n",
      "AUC of PRC = 0.4105569841526145\n",
      "min(+P, Se) = 0.41414141414141414\n",
      "f1_score = 0.22289156913757324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:37:20,647 - INFO - Epoch 76 Batch 0: Train Loss = 0.2263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Batch 0: Train Loss = 0.2263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:37:38,351 - INFO - Epoch 76 Batch 20: Train Loss = 0.2627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Batch 20: Train Loss = 0.2627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:37:54,583 - INFO - Epoch 76 Batch 40: Train Loss = 0.2741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Batch 40: Train Loss = 0.2741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:38:10,956 - INFO - Epoch 76 Batch 60: Train Loss = 0.2720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Batch 60: Train Loss = 0.2720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:38:27,270 - INFO - Epoch 76 Batch 80: Train Loss = 0.1937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Batch 80: Train Loss = 0.1937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:38:44,235 - INFO - Epoch 76 Batch 100: Train Loss = 0.1973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Batch 100: Train Loss = 0.1973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:38:59,860 - INFO - Epoch 76 Batch 120: Train Loss = 0.2245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Batch 120: Train Loss = 0.2245\n",
      "Epoch 76: Loss = 0.2232 Valid loss = 0.2043 roc = 0.8429\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 255   37]]\n",
      "accuracy = 0.9360436201095581\n",
      "precision class 0 = 0.9361542463302612\n",
      "precision class 1 = 0.925000011920929\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.12671232223510742\n",
      "AUC of ROC = 0.8429306721920005\n",
      "AUC of PRC = 0.42286380944869684\n",
      "min(+P, Se) = 0.43686006825938567\n",
      "f1_score = 0.22289156913757324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:39:11,020 - INFO - Epoch 77 Batch 0: Train Loss = 0.2093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 Batch 0: Train Loss = 0.2093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:39:28,545 - INFO - Epoch 77 Batch 20: Train Loss = 0.2321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 Batch 20: Train Loss = 0.2321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:39:44,667 - INFO - Epoch 77 Batch 40: Train Loss = 0.2599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 Batch 40: Train Loss = 0.2599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:40:00,316 - INFO - Epoch 77 Batch 60: Train Loss = 0.2664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 Batch 60: Train Loss = 0.2664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:40:16,015 - INFO - Epoch 77 Batch 80: Train Loss = 0.1958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 Batch 80: Train Loss = 0.1958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:40:32,483 - INFO - Epoch 77 Batch 100: Train Loss = 0.1872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 Batch 100: Train Loss = 0.1872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:40:49,562 - INFO - Epoch 77 Batch 120: Train Loss = 0.2459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 Batch 120: Train Loss = 0.2459\n",
      "Epoch 77: Loss = 0.2201 Valid loss = 0.2053 roc = 0.8427\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 257   35]]\n",
      "accuracy = 0.9355478286743164\n",
      "precision class 0 = 0.9356856942176819\n",
      "precision class 1 = 0.9210526347160339\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.11986301094293594\n",
      "AUC of ROC = 0.8427274990298939\n",
      "AUC of PRC = 0.4182783868551236\n",
      "min(+P, Se) = 0.4395973154362416\n",
      "f1_score = 0.21212121548074675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:41:00,999 - INFO - Epoch 78 Batch 0: Train Loss = 0.2480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 Batch 0: Train Loss = 0.2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:41:18,799 - INFO - Epoch 78 Batch 20: Train Loss = 0.2510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 Batch 20: Train Loss = 0.2510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:41:35,683 - INFO - Epoch 78 Batch 40: Train Loss = 0.2969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 Batch 40: Train Loss = 0.2969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:41:51,889 - INFO - Epoch 78 Batch 60: Train Loss = 0.2482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 Batch 60: Train Loss = 0.2482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:42:08,481 - INFO - Epoch 78 Batch 80: Train Loss = 0.2163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 Batch 80: Train Loss = 0.2163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:42:25,114 - INFO - Epoch 78 Batch 100: Train Loss = 0.1802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 Batch 100: Train Loss = 0.1802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:42:41,155 - INFO - Epoch 78 Batch 120: Train Loss = 0.2320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 Batch 120: Train Loss = 0.2320\n",
      "Epoch 78: Loss = 0.2218 Valid loss = 0.2040 roc = 0.8406\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 257   35]]\n",
      "accuracy = 0.9355478286743164\n",
      "precision class 0 = 0.9356856942176819\n",
      "precision class 1 = 0.9210526347160339\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.11986301094293594\n",
      "AUC of ROC = 0.8406252974381878\n",
      "AUC of PRC = 0.41946703344598407\n",
      "min(+P, Se) = 0.4280821917808219\n",
      "f1_score = 0.21212121548074675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:42:53,151 - INFO - Epoch 79 Batch 0: Train Loss = 0.2380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 Batch 0: Train Loss = 0.2380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:43:09,491 - INFO - Epoch 79 Batch 20: Train Loss = 0.2506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 Batch 20: Train Loss = 0.2506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:43:25,395 - INFO - Epoch 79 Batch 40: Train Loss = 0.2948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 Batch 40: Train Loss = 0.2948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:43:41,426 - INFO - Epoch 79 Batch 60: Train Loss = 0.2598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 Batch 60: Train Loss = 0.2598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:43:56,889 - INFO - Epoch 79 Batch 80: Train Loss = 0.1839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 Batch 80: Train Loss = 0.1839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:44:12,907 - INFO - Epoch 79 Batch 100: Train Loss = 0.1919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 Batch 100: Train Loss = 0.1919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:44:29,547 - INFO - Epoch 79 Batch 120: Train Loss = 0.2370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 Batch 120: Train Loss = 0.2370\n",
      "Epoch 79: Loss = 0.2177 Valid loss = 0.2040 roc = 0.8423\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 252   40]]\n",
      "accuracy = 0.9367873072624207\n",
      "precision class 0 = 0.9368579387664795\n",
      "precision class 1 = 0.930232584476471\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.13698630034923553\n",
      "AUC of ROC = 0.8422845449287245\n",
      "AUC of PRC = 0.41608705665674467\n",
      "min(+P, Se) = 0.4315068493150685\n",
      "f1_score = 0.23880596613188218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:44:42,122 - INFO - Epoch 80 Batch 0: Train Loss = 0.2193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 Batch 0: Train Loss = 0.2193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:44:58,726 - INFO - Epoch 80 Batch 20: Train Loss = 0.2624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 Batch 20: Train Loss = 0.2624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:45:14,841 - INFO - Epoch 80 Batch 40: Train Loss = 0.2726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 Batch 40: Train Loss = 0.2726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:45:31,073 - INFO - Epoch 80 Batch 60: Train Loss = 0.2629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 Batch 60: Train Loss = 0.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:45:48,636 - INFO - Epoch 80 Batch 80: Train Loss = 0.2043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 Batch 80: Train Loss = 0.2043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:46:05,757 - INFO - Epoch 80 Batch 100: Train Loss = 0.1979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 Batch 100: Train Loss = 0.1979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:46:23,210 - INFO - Epoch 80 Batch 120: Train Loss = 0.2304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 Batch 120: Train Loss = 0.2304\n",
      "Epoch 80: Loss = 0.2202 Valid loss = 0.2126 roc = 0.8469\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 283    9]]\n",
      "accuracy = 0.9293504953384399\n",
      "precision class 0 = 0.9296544790267944\n",
      "precision class 1 = 0.8181818127632141\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.030821917578577995\n",
      "AUC of ROC = 0.8468733297701765\n",
      "AUC of PRC = 0.43622039212611835\n",
      "min(+P, Se) = 0.43243243243243246\n",
      "f1_score = 0.059405940022904224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:46:34,874 - INFO - Epoch 81 Batch 0: Train Loss = 0.2398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 Batch 0: Train Loss = 0.2398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:46:50,903 - INFO - Epoch 81 Batch 20: Train Loss = 0.2451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 Batch 20: Train Loss = 0.2451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:47:06,793 - INFO - Epoch 81 Batch 40: Train Loss = 0.2795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 Batch 40: Train Loss = 0.2795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:47:22,877 - INFO - Epoch 81 Batch 60: Train Loss = 0.2631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 Batch 60: Train Loss = 0.2631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:47:40,033 - INFO - Epoch 81 Batch 80: Train Loss = 0.2068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 Batch 80: Train Loss = 0.2068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:47:56,761 - INFO - Epoch 81 Batch 100: Train Loss = 0.1770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 Batch 100: Train Loss = 0.1770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:48:13,166 - INFO - Epoch 81 Batch 120: Train Loss = 0.2232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 Batch 120: Train Loss = 0.2232\n",
      "Epoch 81: Loss = 0.2204 Valid loss = 0.2056 roc = 0.8411\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 263   29]]\n",
      "accuracy = 0.9340605139732361\n",
      "precision class 0 = 0.9342828392982483\n",
      "precision class 1 = 0.90625\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.09931506961584091\n",
      "AUC of ROC = 0.8411469582598127\n",
      "AUC of PRC = 0.4237992871532743\n",
      "min(+P, Se) = 0.4212328767123288\n",
      "f1_score = 0.1790123514818576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:48:25,439 - INFO - Epoch 82 Batch 0: Train Loss = 0.2410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 Batch 0: Train Loss = 0.2410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:48:42,170 - INFO - Epoch 82 Batch 20: Train Loss = 0.2615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 Batch 20: Train Loss = 0.2615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:48:58,212 - INFO - Epoch 82 Batch 40: Train Loss = 0.2840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 Batch 40: Train Loss = 0.2840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:49:14,005 - INFO - Epoch 82 Batch 60: Train Loss = 0.2569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 Batch 60: Train Loss = 0.2569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:49:30,821 - INFO - Epoch 82 Batch 80: Train Loss = 0.2118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 Batch 80: Train Loss = 0.2118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:49:46,754 - INFO - Epoch 82 Batch 100: Train Loss = 0.1747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 Batch 100: Train Loss = 0.1747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:50:02,520 - INFO - Epoch 82 Batch 120: Train Loss = 0.2223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 Batch 120: Train Loss = 0.2223\n",
      "Epoch 82: Loss = 0.2154 Valid loss = 0.2060 roc = 0.8422\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 259   33]]\n",
      "accuracy = 0.9350520372390747\n",
      "precision class 0 = 0.9352176189422607\n",
      "precision class 1 = 0.9166666865348816\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.11301369965076447\n",
      "AUC of ROC = 0.8422387852075295\n",
      "AUC of PRC = 0.428473369456584\n",
      "min(+P, Se) = 0.4349315068493151\n",
      "f1_score = 0.2012195172035432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:50:14,013 - INFO - Epoch 83 Batch 0: Train Loss = 0.2184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 Batch 0: Train Loss = 0.2184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:50:30,664 - INFO - Epoch 83 Batch 20: Train Loss = 0.2397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 Batch 20: Train Loss = 0.2397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:50:47,964 - INFO - Epoch 83 Batch 40: Train Loss = 0.2798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 Batch 40: Train Loss = 0.2798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:51:05,789 - INFO - Epoch 83 Batch 60: Train Loss = 0.2697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 Batch 60: Train Loss = 0.2697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:51:23,834 - INFO - Epoch 83 Batch 80: Train Loss = 0.2223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 Batch 80: Train Loss = 0.2223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:51:40,877 - INFO - Epoch 83 Batch 100: Train Loss = 0.1730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 Batch 100: Train Loss = 0.1730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:51:57,093 - INFO - Epoch 83 Batch 120: Train Loss = 0.2408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 Batch 120: Train Loss = 0.2408\n",
      "Epoch 83: Loss = 0.2187 Valid loss = 0.2067 roc = 0.8378\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 257   35]]\n",
      "accuracy = 0.9355478286743164\n",
      "precision class 0 = 0.9356856942176819\n",
      "precision class 1 = 0.9210526347160339\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.11986301094293594\n",
      "AUC of ROC = 0.8377891099185111\n",
      "AUC of PRC = 0.41679204827444705\n",
      "min(+P, Se) = 0.4334470989761092\n",
      "f1_score = 0.21212121548074675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:52:09,220 - INFO - Epoch 84 Batch 0: Train Loss = 0.2335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 Batch 0: Train Loss = 0.2335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:52:26,408 - INFO - Epoch 84 Batch 20: Train Loss = 0.2432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 Batch 20: Train Loss = 0.2432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:52:42,644 - INFO - Epoch 84 Batch 40: Train Loss = 0.2624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 Batch 40: Train Loss = 0.2624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:52:59,302 - INFO - Epoch 84 Batch 60: Train Loss = 0.2525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 Batch 60: Train Loss = 0.2525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:53:16,408 - INFO - Epoch 84 Batch 80: Train Loss = 0.2238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 Batch 80: Train Loss = 0.2238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:53:33,002 - INFO - Epoch 84 Batch 100: Train Loss = 0.1821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 Batch 100: Train Loss = 0.1821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:53:49,738 - INFO - Epoch 84 Batch 120: Train Loss = 0.2340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 Batch 120: Train Loss = 0.2340\n",
      "Epoch 84: Loss = 0.2213 Valid loss = 0.2057 roc = 0.8438\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 262   30]]\n",
      "accuracy = 0.9343083500862122\n",
      "precision class 0 = 0.9345163702964783\n",
      "precision class 1 = 0.9090909361839294\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.10273972898721695\n",
      "AUC of ROC = 0.8438211563664585\n",
      "AUC of PRC = 0.4326944311608372\n",
      "min(+P, Se) = 0.44594594594594594\n",
      "f1_score = 0.18461538587434767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:54:01,021 - INFO - Epoch 85 Batch 0: Train Loss = 0.2297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 Batch 0: Train Loss = 0.2297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:54:17,834 - INFO - Epoch 85 Batch 20: Train Loss = 0.2471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 Batch 20: Train Loss = 0.2471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:54:34,427 - INFO - Epoch 85 Batch 40: Train Loss = 0.2730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 Batch 40: Train Loss = 0.2730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:54:51,156 - INFO - Epoch 85 Batch 60: Train Loss = 0.2772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 Batch 60: Train Loss = 0.2772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:55:07,828 - INFO - Epoch 85 Batch 80: Train Loss = 0.2047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 Batch 80: Train Loss = 0.2047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:55:23,356 - INFO - Epoch 85 Batch 100: Train Loss = 0.2209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 Batch 100: Train Loss = 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:55:39,681 - INFO - Epoch 85 Batch 120: Train Loss = 0.2229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 Batch 120: Train Loss = 0.2229\n",
      "Epoch 85: Loss = 0.2179 Valid loss = 0.2070 roc = 0.8380\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 258   34]]\n",
      "accuracy = 0.9352999329566956\n",
      "precision class 0 = 0.9354515671730042\n",
      "precision class 1 = 0.9189189076423645\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.1164383590221405\n",
      "AUC of ROC = 0.8379506417343301\n",
      "AUC of PRC = 0.4179375297431575\n",
      "min(+P, Se) = 0.410958904109589\n",
      "f1_score = 0.2066869402575825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:55:51,713 - INFO - Epoch 86 Batch 0: Train Loss = 0.2264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 Batch 0: Train Loss = 0.2264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:56:09,323 - INFO - Epoch 86 Batch 20: Train Loss = 0.2518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 Batch 20: Train Loss = 0.2518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:56:27,833 - INFO - Epoch 86 Batch 40: Train Loss = 0.2856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 Batch 40: Train Loss = 0.2856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:56:44,055 - INFO - Epoch 86 Batch 60: Train Loss = 0.2829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 Batch 60: Train Loss = 0.2829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:57:00,211 - INFO - Epoch 86 Batch 80: Train Loss = 0.2095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 Batch 80: Train Loss = 0.2095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:57:16,684 - INFO - Epoch 86 Batch 100: Train Loss = 0.2037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 Batch 100: Train Loss = 0.2037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:57:32,324 - INFO - Epoch 86 Batch 120: Train Loss = 0.2286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 Batch 120: Train Loss = 0.2286\n",
      "Epoch 86: Loss = 0.2202 Valid loss = 0.2052 roc = 0.8409\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 254   38]]\n",
      "accuracy = 0.936291515827179\n",
      "precision class 0 = 0.9363886713981628\n",
      "precision class 1 = 0.9268292784690857\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.13013698160648346\n",
      "AUC of ROC = 0.8409410395144344\n",
      "AUC of PRC = 0.416190980274522\n",
      "min(+P, Se) = 0.4261744966442953\n",
      "f1_score = 0.22822821166412668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:57:43,591 - INFO - Epoch 87 Batch 0: Train Loss = 0.2473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 Batch 0: Train Loss = 0.2473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:57:59,982 - INFO - Epoch 87 Batch 20: Train Loss = 0.2364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 Batch 20: Train Loss = 0.2364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:58:16,039 - INFO - Epoch 87 Batch 40: Train Loss = 0.2994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 Batch 40: Train Loss = 0.2994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:58:32,842 - INFO - Epoch 87 Batch 60: Train Loss = 0.2799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 Batch 60: Train Loss = 0.2799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:58:49,070 - INFO - Epoch 87 Batch 80: Train Loss = 0.1985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 Batch 80: Train Loss = 0.1985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:59:05,410 - INFO - Epoch 87 Batch 100: Train Loss = 0.1719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 Batch 100: Train Loss = 0.1719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:59:21,166 - INFO - Epoch 87 Batch 120: Train Loss = 0.2145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 Batch 120: Train Loss = 0.2145\n",
      "Epoch 87: Loss = 0.2213 Valid loss = 0.2066 roc = 0.8411\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 256   36]]\n",
      "accuracy = 0.9357957243919373\n",
      "precision class 0 = 0.9359198808670044\n",
      "precision class 1 = 0.9230769276618958\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.12328767031431198\n",
      "AUC of ROC = 0.8410563540118464\n",
      "AUC of PRC = 0.4176946736537276\n",
      "min(+P, Se) = 0.4280936454849498\n",
      "f1_score = 0.21752266814986623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 14:59:32,219 - INFO - Epoch 88 Batch 0: Train Loss = 0.2330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 Batch 0: Train Loss = 0.2330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:59:49,747 - INFO - Epoch 88 Batch 20: Train Loss = 0.2581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 Batch 20: Train Loss = 0.2581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:00:06,198 - INFO - Epoch 88 Batch 40: Train Loss = 0.3007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 Batch 40: Train Loss = 0.3007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:00:24,343 - INFO - Epoch 88 Batch 60: Train Loss = 0.2587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 Batch 60: Train Loss = 0.2587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:00:40,695 - INFO - Epoch 88 Batch 80: Train Loss = 0.2107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 Batch 80: Train Loss = 0.2107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:00:56,505 - INFO - Epoch 88 Batch 100: Train Loss = 0.1916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 Batch 100: Train Loss = 0.1916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:01:13,297 - INFO - Epoch 88 Batch 120: Train Loss = 0.2106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 Batch 120: Train Loss = 0.2106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:01:24,952 - INFO - ------------ Save best model - AUROC: 0.8527 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88: Loss = 0.2187 Valid loss = 0.2010 roc = 0.8527\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 255   37]]\n",
      "accuracy = 0.9360436201095581\n",
      "precision class 0 = 0.9361542463302612\n",
      "precision class 1 = 0.925000011920929\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.12671232223510742\n",
      "AUC of ROC = 0.8526857295563868\n",
      "AUC of PRC = 0.44722274641395754\n",
      "min(+P, Se) = 0.46258503401360546\n",
      "f1_score = 0.22289156913757324\n",
      "------------ Save best model - AUROC: 0.8527 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 15:01:25,731 - INFO - Epoch 89 Batch 0: Train Loss = 0.2285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 Batch 0: Train Loss = 0.2285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:01:44,020 - INFO - Epoch 89 Batch 20: Train Loss = 0.2608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 Batch 20: Train Loss = 0.2608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:02:00,295 - INFO - Epoch 89 Batch 40: Train Loss = 0.2740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 Batch 40: Train Loss = 0.2740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:02:15,851 - INFO - Epoch 89 Batch 60: Train Loss = 0.2600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 Batch 60: Train Loss = 0.2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:02:31,886 - INFO - Epoch 89 Batch 80: Train Loss = 0.1963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 Batch 80: Train Loss = 0.1963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:02:47,613 - INFO - Epoch 89 Batch 100: Train Loss = 0.2180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 Batch 100: Train Loss = 0.2180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:03:04,738 - INFO - Epoch 89 Batch 120: Train Loss = 0.2302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 Batch 120: Train Loss = 0.2302\n",
      "Epoch 89: Loss = 0.2129 Valid loss = 0.2064 roc = 0.8443\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 264   28]]\n",
      "accuracy = 0.9338126182556152\n",
      "precision class 0 = 0.9340494871139526\n",
      "precision class 1 = 0.9032257795333862\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.09589041024446487\n",
      "AUC of ROC = 0.8442943118836164\n",
      "AUC of PRC = 0.4125461803691942\n",
      "min(+P, Se) = 0.41496598639455784\n",
      "f1_score = 0.17337461263231305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 15:03:16,292 - INFO - Epoch 90 Batch 0: Train Loss = 0.2290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 Batch 0: Train Loss = 0.2290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:03:33,168 - INFO - Epoch 90 Batch 20: Train Loss = 0.2563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 Batch 20: Train Loss = 0.2563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:03:49,147 - INFO - Epoch 90 Batch 40: Train Loss = 0.2682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 Batch 40: Train Loss = 0.2682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:04:04,962 - INFO - Epoch 90 Batch 60: Train Loss = 0.2283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 Batch 60: Train Loss = 0.2283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:04:21,281 - INFO - Epoch 90 Batch 80: Train Loss = 0.1956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 Batch 80: Train Loss = 0.1956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:04:37,836 - INFO - Epoch 90 Batch 100: Train Loss = 0.1781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 Batch 100: Train Loss = 0.1781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:04:53,800 - INFO - Epoch 90 Batch 120: Train Loss = 0.2023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 Batch 120: Train Loss = 0.2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:05:04,998 - INFO - ------------ Save best model - AUROC: 0.8779 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: Loss = 0.2168 Valid loss = 0.1916 roc = 0.8779\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 255   37]]\n",
      "accuracy = 0.9360436201095581\n",
      "precision class 0 = 0.9361542463302612\n",
      "precision class 1 = 0.925000011920929\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.12671232223510742\n",
      "AUC of ROC = 0.8778883536018393\n",
      "AUC of PRC = 0.5176542155003642\n",
      "min(+P, Se) = 0.5273972602739726\n",
      "f1_score = 0.22289156913757324\n",
      "------------ Save best model - AUROC: 0.8779 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 15:05:05,867 - INFO - Epoch 91 Batch 0: Train Loss = 0.1973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 Batch 0: Train Loss = 0.1973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:05:22,761 - INFO - Epoch 91 Batch 20: Train Loss = 0.2386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 Batch 20: Train Loss = 0.2386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:05:38,716 - INFO - Epoch 91 Batch 40: Train Loss = 0.2611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 Batch 40: Train Loss = 0.2611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:05:54,446 - INFO - Epoch 91 Batch 60: Train Loss = 0.2704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 Batch 60: Train Loss = 0.2704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:06:12,163 - INFO - Epoch 91 Batch 80: Train Loss = 0.1859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 Batch 80: Train Loss = 0.1859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:06:29,642 - INFO - Epoch 91 Batch 100: Train Loss = 0.1656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 Batch 100: Train Loss = 0.1656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:06:46,706 - INFO - Epoch 91 Batch 120: Train Loss = 0.1859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 Batch 120: Train Loss = 0.1859\n",
      "Epoch 91: Loss = 0.2094 Valid loss = 0.1851 roc = 0.8555\n",
      "confusion matrix:\n",
      "[[3736    6]\n",
      " [ 237   55]]\n",
      "accuracy = 0.9397619962692261\n",
      "precision class 0 = 0.9403473734855652\n",
      "precision class 1 = 0.9016393423080444\n",
      "recall class 0 = 0.9983965754508972\n",
      "recall class 1 = 0.1883561611175537\n",
      "AUC of ROC = 0.8555026979931617\n",
      "AUC of PRC = 0.5072902061596933\n",
      "min(+P, Se) = 0.4828767123287671\n",
      "f1_score = 0.31161472629192644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 15:06:58,526 - INFO - Epoch 92 Batch 0: Train Loss = 0.1968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 Batch 0: Train Loss = 0.1968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:07:15,385 - INFO - Epoch 92 Batch 20: Train Loss = 0.2318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 Batch 20: Train Loss = 0.2318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:07:31,371 - INFO - Epoch 92 Batch 40: Train Loss = 0.2804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 Batch 40: Train Loss = 0.2804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:07:47,935 - INFO - Epoch 92 Batch 60: Train Loss = 0.2243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 Batch 60: Train Loss = 0.2243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:08:03,924 - INFO - Epoch 92 Batch 80: Train Loss = 0.1698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 Batch 80: Train Loss = 0.1698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:08:20,048 - INFO - Epoch 92 Batch 100: Train Loss = 0.1393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 Batch 100: Train Loss = 0.1393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:08:36,146 - INFO - Epoch 92 Batch 120: Train Loss = 0.1813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 Batch 120: Train Loss = 0.1813\n",
      "Epoch 92: Loss = 0.1926 Valid loss = 0.1886 roc = 0.8505\n",
      "confusion matrix:\n",
      "[[3738    4]\n",
      " [ 253   39]]\n",
      "accuracy = 0.936291515827179\n",
      "precision class 0 = 0.9366073608398438\n",
      "precision class 1 = 0.9069767594337463\n",
      "recall class 0 = 0.9989310503005981\n",
      "recall class 1 = 0.1335616409778595\n",
      "AUC of ROC = 0.8505496657709964\n",
      "AUC of PRC = 0.5126348097267571\n",
      "min(+P, Se) = 0.511864406779661\n",
      "f1_score = 0.23283581038683482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 15:08:47,835 - INFO - Epoch 93 Batch 0: Train Loss = 0.2095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 Batch 0: Train Loss = 0.2095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:09:04,911 - INFO - Epoch 93 Batch 20: Train Loss = 0.2105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 Batch 20: Train Loss = 0.2105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:09:21,490 - INFO - Epoch 93 Batch 40: Train Loss = 0.2781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 Batch 40: Train Loss = 0.2781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:09:39,058 - INFO - Epoch 93 Batch 60: Train Loss = 0.2529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 Batch 60: Train Loss = 0.2529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:09:55,064 - INFO - Epoch 93 Batch 80: Train Loss = 0.1673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 Batch 80: Train Loss = 0.1673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:10:10,968 - INFO - Epoch 93 Batch 100: Train Loss = 0.1544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 Batch 100: Train Loss = 0.1544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:10:27,272 - INFO - Epoch 93 Batch 120: Train Loss = 0.1826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 Batch 120: Train Loss = 0.1826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:10:40,556 - INFO - ------------ Save best model - AUROC: 0.8810 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93: Loss = 0.2019 Valid loss = 0.1774 roc = 0.8810\n",
      "confusion matrix:\n",
      "[[3738    4]\n",
      " [ 239   53]]\n",
      "accuracy = 0.9397619962692261\n",
      "precision class 0 = 0.9399044513702393\n",
      "precision class 1 = 0.9298245906829834\n",
      "recall class 0 = 0.9989310503005981\n",
      "recall class 1 = 0.18150684237480164\n",
      "AUC of ROC = 0.8810137425594693\n",
      "AUC of PRC = 0.566515736176599\n",
      "min(+P, Se) = 0.5513698630136986\n",
      "f1_score = 0.3037249120671303\n",
      "------------ Save best model - AUROC: 0.8810 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 15:10:41,276 - INFO - Epoch 94 Batch 0: Train Loss = 0.1944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 Batch 0: Train Loss = 0.1944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:10:57,728 - INFO - Epoch 94 Batch 20: Train Loss = 0.2051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 Batch 20: Train Loss = 0.2051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:11:14,676 - INFO - Epoch 94 Batch 40: Train Loss = 0.2407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 Batch 40: Train Loss = 0.2407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:11:30,762 - INFO - Epoch 94 Batch 60: Train Loss = 0.2173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 Batch 60: Train Loss = 0.2173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:11:48,004 - INFO - Epoch 94 Batch 80: Train Loss = 0.1790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 Batch 80: Train Loss = 0.1790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:12:04,550 - INFO - Epoch 94 Batch 100: Train Loss = 0.1452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 Batch 100: Train Loss = 0.1452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:12:19,123 - INFO - Epoch 94 Batch 120: Train Loss = 0.1659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 Batch 120: Train Loss = 0.1659\n",
      "Epoch 94: Loss = 0.1832 Valid loss = 0.1706 roc = 0.8803\n",
      "confusion matrix:\n",
      "[[3731   11]\n",
      " [ 207   85]]\n",
      "accuracy = 0.9459593296051025\n",
      "precision class 0 = 0.9474352598190308\n",
      "precision class 1 = 0.8854166865348816\n",
      "recall class 0 = 0.9970604181289673\n",
      "recall class 1 = 0.29109588265419006\n",
      "AUC of ROC = 0.8803291771303896\n",
      "AUC of PRC = 0.5681002190713403\n",
      "min(+P, Se) = 0.5460750853242321\n",
      "f1_score = 0.4381443124443838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 15:12:30,732 - INFO - Epoch 95 Batch 0: Train Loss = 0.1883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 Batch 0: Train Loss = 0.1883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:12:46,939 - INFO - Epoch 95 Batch 20: Train Loss = 0.2270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 Batch 20: Train Loss = 0.2270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:13:03,242 - INFO - Epoch 95 Batch 40: Train Loss = 0.2956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 Batch 40: Train Loss = 0.2956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:13:18,803 - INFO - Epoch 95 Batch 60: Train Loss = 0.2007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 Batch 60: Train Loss = 0.2007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:13:35,349 - INFO - Epoch 95 Batch 80: Train Loss = 0.1788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 Batch 80: Train Loss = 0.1788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:13:51,474 - INFO - Epoch 95 Batch 100: Train Loss = 0.1186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 Batch 100: Train Loss = 0.1186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:14:07,435 - INFO - Epoch 95 Batch 120: Train Loss = 0.1323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 Batch 120: Train Loss = 0.1323\n",
      "Epoch 95: Loss = 0.1797 Valid loss = 0.1750 roc = 0.8810\n",
      "confusion matrix:\n",
      "[[3736    6]\n",
      " [ 212   80]]\n",
      "accuracy = 0.9459593296051025\n",
      "precision class 0 = 0.9463019371032715\n",
      "precision class 1 = 0.930232584476471\n",
      "recall class 0 = 0.9983965754508972\n",
      "recall class 1 = 0.27397260069847107\n",
      "AUC of ROC = 0.8809926930877195\n",
      "AUC of PRC = 0.5862504793735265\n",
      "min(+P, Se) = 0.552901023890785\n",
      "f1_score = 0.423280434046336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 15:14:19,478 - INFO - Epoch 96 Batch 0: Train Loss = 0.2187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 Batch 0: Train Loss = 0.2187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:14:37,437 - INFO - Epoch 96 Batch 20: Train Loss = 0.2519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 Batch 20: Train Loss = 0.2519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:14:52,794 - INFO - Epoch 96 Batch 40: Train Loss = 0.2624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 Batch 40: Train Loss = 0.2624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:15:09,075 - INFO - Epoch 96 Batch 60: Train Loss = 0.2429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 Batch 60: Train Loss = 0.2429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:15:25,273 - INFO - Epoch 96 Batch 80: Train Loss = 0.1565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 Batch 80: Train Loss = 0.1565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:15:42,882 - INFO - Epoch 96 Batch 100: Train Loss = 0.1497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 Batch 100: Train Loss = 0.1497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:15:58,942 - INFO - Epoch 96 Batch 120: Train Loss = 0.1540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 Batch 120: Train Loss = 0.1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:16:09,317 - INFO - ------------ Save best model - AUROC: 0.8851 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96: Loss = 0.1844 Valid loss = 0.1703 roc = 0.8851\n",
      "confusion matrix:\n",
      "[[3731   11]\n",
      " [ 196   96]]\n",
      "accuracy = 0.9486861824989319\n",
      "precision class 0 = 0.9500890970230103\n",
      "precision class 1 = 0.8971962332725525\n",
      "recall class 0 = 0.9970604181289673\n",
      "recall class 1 = 0.3287671208381653\n",
      "AUC of ROC = 0.8850955096900786\n",
      "AUC of PRC = 0.594814182907989\n",
      "min(+P, Se) = 0.5513698630136986\n",
      "f1_score = 0.4812030008088237\n",
      "------------ Save best model - AUROC: 0.8851 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 15:16:10,055 - INFO - Epoch 97 Batch 0: Train Loss = 0.2128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 Batch 0: Train Loss = 0.2128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:16:28,934 - INFO - Epoch 97 Batch 20: Train Loss = 0.2449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 Batch 20: Train Loss = 0.2449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:16:45,521 - INFO - Epoch 97 Batch 40: Train Loss = 0.2832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 Batch 40: Train Loss = 0.2832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:17:02,298 - INFO - Epoch 97 Batch 60: Train Loss = 0.2077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 Batch 60: Train Loss = 0.2077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:17:18,966 - INFO - Epoch 97 Batch 80: Train Loss = 0.1624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 Batch 80: Train Loss = 0.1624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:17:35,359 - INFO - Epoch 97 Batch 100: Train Loss = 0.1799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 Batch 100: Train Loss = 0.1799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:17:51,326 - INFO - Epoch 97 Batch 120: Train Loss = 0.2103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 Batch 120: Train Loss = 0.2103\n",
      "Epoch 97: Loss = 0.1816 Valid loss = 0.2143 roc = 0.8321\n",
      "confusion matrix:\n",
      "[[3740    2]\n",
      " [ 271   21]]\n",
      "accuracy = 0.9323252439498901\n",
      "precision class 0 = 0.9324358105659485\n",
      "precision class 1 = 0.9130434989929199\n",
      "recall class 0 = 0.9994655251502991\n",
      "recall class 1 = 0.0719178095459938\n",
      "AUC of ROC = 0.8321354963648477\n",
      "AUC of PRC = 0.41144125911521484\n",
      "min(+P, Se) = 0.4006849315068493\n",
      "f1_score = 0.13333333280892065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 15:18:03,573 - INFO - Epoch 98 Batch 0: Train Loss = 0.2319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 Batch 0: Train Loss = 0.2319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:18:20,153 - INFO - Epoch 98 Batch 20: Train Loss = 0.2749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 Batch 20: Train Loss = 0.2749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:18:35,861 - INFO - Epoch 98 Batch 40: Train Loss = 0.2745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 Batch 40: Train Loss = 0.2745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:18:51,674 - INFO - Epoch 98 Batch 60: Train Loss = 0.2560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 Batch 60: Train Loss = 0.2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:19:06,944 - INFO - Epoch 98 Batch 80: Train Loss = 0.2017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 Batch 80: Train Loss = 0.2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:19:23,111 - INFO - Epoch 98 Batch 100: Train Loss = 0.1890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 Batch 100: Train Loss = 0.1890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:19:38,889 - INFO - Epoch 98 Batch 120: Train Loss = 0.2219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98 Batch 120: Train Loss = 0.2219\n",
      "Epoch 98: Loss = 0.2228 Valid loss = 0.2111 roc = 0.8437\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 262   30]]\n",
      "accuracy = 0.9343083500862122\n",
      "precision class 0 = 0.9345163702964783\n",
      "precision class 1 = 0.9090909361839294\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.10273972898721695\n",
      "AUC of ROC = 0.8436774708419056\n",
      "AUC of PRC = 0.4137359654603383\n",
      "min(+P, Se) = 0.4315068493150685\n",
      "f1_score = 0.18461538587434767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:2905: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "2023-11-08 15:19:51,699 - INFO - Epoch 99 Batch 0: Train Loss = 0.2295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Batch 0: Train Loss = 0.2295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:20:08,322 - INFO - Epoch 99 Batch 20: Train Loss = 0.2630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Batch 20: Train Loss = 0.2630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:20:23,371 - INFO - Epoch 99 Batch 40: Train Loss = 0.2808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Batch 40: Train Loss = 0.2808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:20:39,066 - INFO - Epoch 99 Batch 60: Train Loss = 0.2513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Batch 60: Train Loss = 0.2513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:20:54,975 - INFO - Epoch 99 Batch 80: Train Loss = 0.1952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Batch 80: Train Loss = 0.1952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:21:11,118 - INFO - Epoch 99 Batch 100: Train Loss = 0.1938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Batch 100: Train Loss = 0.1938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:21:28,530 - INFO - Epoch 99 Batch 120: Train Loss = 0.2064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 Batch 120: Train Loss = 0.2064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:21:39,761 - INFO - auroc 0.8851\n",
      "2023-11-08 15:21:39,762 - INFO - auprc 0.5948\n",
      "2023-11-08 15:21:39,763 - INFO - minpse 0.5514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: Loss = 0.2233 Valid loss = 0.2088 roc = 0.8434\n",
      "confusion matrix:\n",
      "[[3739    3]\n",
      " [ 264   28]]\n",
      "accuracy = 0.9338126182556152\n",
      "precision class 0 = 0.9340494871139526\n",
      "precision class 1 = 0.9032257795333862\n",
      "recall class 0 = 0.999198317527771\n",
      "recall class 1 = 0.09589041024446487\n",
      "AUC of ROC = 0.8434468418470821\n",
      "AUC of PRC = 0.4241268346836312\n",
      "min(+P, Se) = 0.4246575342465753\n",
      "f1_score = 0.17337461263231305\n",
      "auroc 0.8851\n",
      "auprc 0.5948\n",
      "minpse 0.5514\n"
     ]
    }
   ],
   "source": [
    "# Training Student\n",
    "# If you don't want to train Student Model:\n",
    "# - The pretrained student model is in direcrtory './model/', and can be directly loaded, \n",
    "# - Simply skip this cell and load the model to validate on Dev Dataset.\n",
    "\n",
    "logger.info('Training Student')\n",
    "total_train_loss = []\n",
    "total_valid_loss = []\n",
    "global_best = 0\n",
    "auroc = []\n",
    "auprc = []\n",
    "minpse = []\n",
    "history = []\n",
    "\n",
    "pad_token = np.zeros(34)\n",
    "# begin_time = time.time()\n",
    "best_auroc = 0\n",
    "best_auprc = 0\n",
    "best_minpse = 0\n",
    "\n",
    "if target_dataset == 'TJ':    \n",
    "    data_str = 'covid'\n",
    "elif target_dataset == 'HM':\n",
    "    data_str = 'spain'\n",
    "\n",
    "if teacher_flag:\n",
    "    file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-rnn'\n",
    "else: \n",
    "    file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher' + '-rnn'\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "\n",
    "    epoch_loss = []\n",
    "    counter_batch = 0\n",
    "    model_student.train()  \n",
    "    model.eval()\n",
    "    for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(train_x, train_y, train_mask_x, train_x_len, batch_size, shuffle=False)):  \n",
    "        optimizer_student.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "#        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "        opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "        emb_teacher = torch.tensor(train_teacher_emb[step], dtype=torch.float32).to(device)\n",
    "        # opt_teacher, decov_loss_teacher, emb_teacher = model(batch_x, batch_lens)\n",
    "#         BCE_Loss = get_loss(opt_student, batch_y.unsqueeze(-1)) # b t 1\n",
    "        #emb_Loss = 0.1 * get_re_loss(emb_student, emb_teacher.detach())\n",
    "        emb_student = F.log_softmax(emb_student, dim=1)\n",
    "        emb_teacher = F.softmax(emb_teacher.detach(), dim=1)\n",
    "#             emb_Loss = get_kl_loss(emb_student, emb_teacher)\n",
    "            \n",
    "        tar_source = F.softmax(tar_result[0].squeeze(), dim=-1)\n",
    "        tar_tar = F.softmax(tar_result[1].squeeze(), dim=-1)\n",
    "#             shrink_indices = torch.tensor(np.random.choice(range(len(tar_tar)), len(tar_source))).to(device)\n",
    "#             tar_tar = torch.index_select(tar_tar, 0, shrink_indices)\n",
    "#             tar_Loss = get_kl_loss(tar_source, tar_tar)\n",
    "#             logger.info(tar_tar)\n",
    "#             logger.info(tar_source)\n",
    "#             logger.info(tar_Loss)\n",
    "        loss = get_multitask_loss(opt_student, batch_y.unsqueeze(-1), emb_student, emb_teacher, tar_source, tar_tar)\n",
    "        # emb_Loss = 0.1 * get_wass_dist(emb_student, emb_teacher.detach())\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        epoch_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_student.parameters(), 20)\n",
    "        optimizer_student.step()\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "            logger.info('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    total_train_loss.append(epoch_loss)\n",
    "\n",
    "    #Validation\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        model_student.eval()\n",
    "        valid_loss = []\n",
    "        valid_true = []\n",
    "        valid_pred = []\n",
    "        for batch_x, batch_y, batch_mask_x, batch_lens in batch_iter(dev_x, dev_y, dev_mask_x, dev_x_len, batch_size):\n",
    "            batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "            batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "#            masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "\n",
    "            opt_student, decov_loss_student, emb, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [[], []], False)\n",
    "\n",
    "            BCE_Loss = get_loss(opt_student, batch_y.unsqueeze(-1))\n",
    "#                 REC_Loss = F.mse_loss(recon, batch_x, reduction='mean').to(device)\n",
    "\n",
    "            valid_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "\n",
    "            y_pred += list(opt_student.cpu().detach().numpy().flatten())\n",
    "            y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "        valid_loss = np.mean(valid_loss)\n",
    "        total_valid_loss.append(valid_loss)\n",
    "        ret = metrics.print_metrics_binary(y_true, y_pred,verbose = 0)\n",
    "        history.append(ret)\n",
    "        #print()\n",
    "\n",
    "        print('Epoch %d: Loss = %.4f Valid loss = %.4f roc = %.4f'%(each_epoch, total_train_loss[-1], total_valid_loss[-1], ret['auroc']))\n",
    "        metrics.print_metrics_binary(y_true, y_pred)\n",
    "\n",
    "        cur_auroc = ret['auroc']\n",
    "        if cur_auroc > best_auroc:\n",
    "            best_auroc = cur_auroc\n",
    "            best_auprc = ret['auprc']\n",
    "            best_minpse = ret['minpse']\n",
    "            state = {\n",
    "                'net': model_student.state_dict(),\n",
    "                'optimizer': optimizer_student.state_dict(),\n",
    "                'epoch': each_epoch\n",
    "            }\n",
    "            torch.save(state, file_name)\n",
    "            print('------------ Save best model - AUROC: %.4f ------------'%cur_auroc)\n",
    "            logger.info('------------ Save best model - AUROC: %.4f ------------'%cur_auroc)\n",
    "\n",
    "print('auroc %.4f'%(best_auroc))\n",
    "print('auprc %.4f'%(best_auprc))\n",
    "print('minpse %.4f'%(best_minpse)) \n",
    "logger.info('auroc %.4f'%(best_auroc))\n",
    "logger.info('auprc %.4f'%(best_auprc))\n",
    "logger.info('minpse %.4f'%(best_minpse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:17.118640Z",
     "start_time": "2021-02-10T15:06:15.660556Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:21:39,919 - INFO - last saved model is in epoch 96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last saved model is in epoch 96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "distcare_student(\n",
       "  (PositionalEncoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (RNNs): ModuleList(\n",
       "    (0): RNN(1, 32, batch_first=True)\n",
       "    (1): RNN(1, 32, batch_first=True)\n",
       "    (2): RNN(1, 32, batch_first=True)\n",
       "    (3): RNN(1, 32, batch_first=True)\n",
       "    (4): RNN(1, 32, batch_first=True)\n",
       "    (5): RNN(1, 32, batch_first=True)\n",
       "    (6): RNN(1, 32, batch_first=True)\n",
       "    (7): RNN(1, 32, batch_first=True)\n",
       "    (8): RNN(1, 32, batch_first=True)\n",
       "    (9): RNN(1, 32, batch_first=True)\n",
       "    (10): RNN(1, 32, batch_first=True)\n",
       "    (11): RNN(1, 32, batch_first=True)\n",
       "    (12): RNN(1, 32, batch_first=True)\n",
       "    (13): RNN(1, 32, batch_first=True)\n",
       "    (14): RNN(1, 32, batch_first=True)\n",
       "    (15): RNN(1, 32, batch_first=True)\n",
       "    (16): RNN(1, 32, batch_first=True)\n",
       "    (17): RNN(1, 32, batch_first=True)\n",
       "    (18): RNN(1, 32, batch_first=True)\n",
       "    (19): RNN(1, 32, batch_first=True)\n",
       "    (20): RNN(1, 32, batch_first=True)\n",
       "    (21): RNN(1, 32, batch_first=True)\n",
       "    (22): RNN(1, 32, batch_first=True)\n",
       "    (23): RNN(1, 32, batch_first=True)\n",
       "    (24): RNN(1, 32, batch_first=True)\n",
       "    (25): RNN(1, 32, batch_first=True)\n",
       "  )\n",
       "  (generalRNN): RNN(1, 32, batch_first=True)\n",
       "  (LastStepAttentions): ModuleList(\n",
       "    (0): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (1): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (2): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (3): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (4): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (5): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (6): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (7): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (8): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (9): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (10): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (11): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (12): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (13): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (14): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (15): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (16): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (17): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (18): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (19): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (20): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (21): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (22): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (23): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (24): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (25): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "  )\n",
       "  (FinalAttentionQKV): FinalAttentionQKV(\n",
       "    (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_k): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_v): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_out): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (tanh): Tanh()\n",
       "    (softmax): Softmax(dim=None)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (MultiHeadedAttention): MultiHeadedAttention(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (final_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (SublayerConnection): SublayerConnection(\n",
       "    (norm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (PositionwiseFeedForward): PositionwiseFeedForward(\n",
       "    (w_1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (w_2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (demo_proj_main): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (demo_proj): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (Linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (output): Linear(in_features=34, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (FC_embed): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (softmax): Softmax(dim=None)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       "  (to_MMD): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if target_dataset == 'TJ':    \n",
    "    data_str = 'covid'\n",
    "elif target_dataset == 'HM':\n",
    "    data_str = 'spain'\n",
    "\n",
    "if teacher_flag:\n",
    "    file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-rnn'\n",
    "else: \n",
    "    file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher' + '-rnn'\n",
    "\n",
    "checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:3\" if torch.cuda.is_available() == True else 'cpu') )\n",
    "save_epoch = checkpoint['epoch']\n",
    "print(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "logger.info(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "model_student.load_state_dict(checkpoint['net'])\n",
    "optimizer_student.load_state_dict(checkpoint['optimizer'])\n",
    "model_student.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T15:40:05.462185Z",
     "start_time": "2021-01-30T15:40:00.917653Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:21:40,535 - INFO - Batch 0: Test Loss = 0.2568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Test Loss = 0.2568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:21:47,622 - INFO - \n",
      "==>Predicting on test\n",
      "2023-11-08 15:21:47,624 - INFO - Test Loss = 0.1648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>Predicting on test\n",
      "Test Loss = 0.1648\n",
      "confusion matrix:\n",
      "[[3719   17]\n",
      " [ 201   96]]\n",
      "accuracy = 0.9459459185600281\n",
      "precision class 0 = 0.9487245082855225\n",
      "precision class 1 = 0.8495575189590454\n",
      "recall class 0 = 0.9954496622085571\n",
      "recall class 1 = 0.3232323229312897\n",
      "AUC of ROC = 0.8945468244183447\n",
      "AUC of PRC = 0.6162440538671333\n",
      "min(+P, Se) = 0.6026936026936027\n",
      "f1_score = 0.46829269403010054\n"
     ]
    }
   ],
   "source": [
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    model_student.eval()\n",
    "    for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(test_x, test_y, test_mask_x, test_x_len, batch_size, shuffle=True)):  \n",
    "        optimizer_student.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "        opt, decov_loss, emb, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [[], []], False)\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "\n",
    "        loss = model_loss\n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "            logger.info('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "        y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "        y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "logger.info(\"\\n==>Predicting on test\")\n",
    "logger.info('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "test_res = metrics.print_metrics_binary(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Target Dataset & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:28.889438Z",
     "start_time": "2021-02-10T15:06:28.597765Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:21:47,702 - INFO - Transfer Target Dataset & Model\n",
      "2023-11-08 15:21:50,784 - INFO - [[0.35672856748565834, 0.362193782095267, -0.0415507190085758, -0.39134565918228126, 0.7630646387621486, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [2.891396229598206, -0.1650564631881193, -0.7197426028629784, -0.5274138023823791, 0.7630646387621486, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.7295575734197193, 0.8894440273786534, 1.5408970099849981, -1.1397204467828197, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.7295575734197193, 0.362193782095267, 1.5408970099849981, -1.1397204467828197, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.09589065789158233, 1.4166942726620397, 0.6366411648458108, -0.9356182319826729, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [2.076681623919173, 1.4166942726620397, 0.6366411648458108, -1.7520270911832603, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.08515703225931394, 0.8894440273786534, 0.18451324227620902, -0.32331158758223233, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [1.0809193280892433, 1.4166942726620397, 0.41057720356100985, 0.9013017012186487, 1.1922324705555387, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.548509883268823, 1.943944517945426, 0.18451324227620902, 1.989846846819432, 1.0849405126071912, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.548509883268823, 1.4166942726620397, -1.6239984480021659, -1.0036523035827218, -1.1681906043081072, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.7253198692496488, 1.4166942726620397, 0.41057720356100985, 0.28899505681820825, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.17568087733476206, 1.4166942726620397, 0.18451324227620902, 0.561131343218404, 2.050568134142319, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.6390337283442711, 1.4166942726620397, -1.171870525432564, -1.0036523035827218, 0.5484807228654535, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 1.4166942726620397, -2.5282542931413534, 0.8332676296185998, -0.30985494072132685, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 0.8894440273786534, -2.5282542931413534, 0.8332676296185998, -0.30985494072132685, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.18217679879696, 1.943944517945426, -0.0415507190085758, 0.3570291284182572, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.09589065789158233, 1.4166942726620397, -0.49367864157817754, 0.4250632000183061, 0.11931289107206332, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 0.8894440273786534, -0.0415507190085758, 0.8332676296185998, 0.11931289107206332, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.17568087733476206, 1.943944517945426, 0.8627051261306116, -0.7995500887825749, 0.9776485546588437, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.9106052635706156, 1.943944517945426, -1.171870525432564, 0.15292691361811034, 1.1922324705555387, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.2769383480424786, 0.8894440273786534, -0.9458065641477793, -2.092197449183505, -1.3827745202048023, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.5442721790987524, 0.8894440273786534, 0.18451324227620902, 1.5816424172191383, -1.1681906043081072, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.2769383480424786, 1.4166942726620397, -0.9458065641477793, 0.22096098521815927, 0.44118876491710596, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-3.6263206158340595, 0.362193782095267, -1.3979344867173649, -1.8880952343833581, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.6390337283442711, 1.4166942726620397, -1.171870525432564, 0.01685877041801243, -0.8463147304630646, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.9106052635706156, 1.4166942726620397, -0.26761468029337665, 0.4930972716183551, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.0916529537215118, 0.8894440273786534, 0.41057720356100985, 1.5136083456190894, 0.22660484902041086, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-2.3589867847777857, 1.4166942726620397, 0.41057720356100985, 1.3775402024189913, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:21:50,794 - INFO - 99\n",
      "2023-11-08 15:21:50,795 - INFO - 4255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35672856748565834, 0.362193782095267, -0.0415507190085758, -0.39134565918228126, 0.7630646387621486, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [2.891396229598206, -0.1650564631881193, -0.7197426028629784, -0.5274138023823791, 0.7630646387621486, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.7295575734197193, 0.8894440273786534, 1.5408970099849981, -1.1397204467828197, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.7295575734197193, 0.362193782095267, 1.5408970099849981, -1.1397204467828197, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.09589065789158233, 1.4166942726620397, 0.6366411648458108, -0.9356182319826729, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [2.076681623919173, 1.4166942726620397, 0.6366411648458108, -1.7520270911832603, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.08515703225931394, 0.8894440273786534, 0.18451324227620902, -0.32331158758223233, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [1.0809193280892433, 1.4166942726620397, 0.41057720356100985, 0.9013017012186487, 1.1922324705555387, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.548509883268823, 1.943944517945426, 0.18451324227620902, 1.989846846819432, 1.0849405126071912, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.548509883268823, 1.4166942726620397, -1.6239984480021659, -1.0036523035827218, -1.1681906043081072, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.7253198692496488, 1.4166942726620397, 0.41057720356100985, 0.28899505681820825, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.17568087733476206, 1.4166942726620397, 0.18451324227620902, 0.561131343218404, 2.050568134142319, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.6390337283442711, 1.4166942726620397, -1.171870525432564, -1.0036523035827218, 0.5484807228654535, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 1.4166942726620397, -2.5282542931413534, 0.8332676296185998, -0.30985494072132685, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 0.8894440273786534, -2.5282542931413534, 0.8332676296185998, -0.30985494072132685, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.18217679879696, 1.943944517945426, -0.0415507190085758, 0.3570291284182572, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.09589065789158233, 1.4166942726620397, -0.49367864157817754, 0.4250632000183061, 0.11931289107206332, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 0.8894440273786534, -0.0415507190085758, 0.8332676296185998, 0.11931289107206332, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.17568087733476206, 1.943944517945426, 0.8627051261306116, -0.7995500887825749, 0.9776485546588437, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.9106052635706156, 1.943944517945426, -1.171870525432564, 0.15292691361811034, 1.1922324705555387, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.2769383480424786, 0.8894440273786534, -0.9458065641477793, -2.092197449183505, -1.3827745202048023, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.5442721790987524, 0.8894440273786534, 0.18451324227620902, 1.5816424172191383, -1.1681906043081072, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.2769383480424786, 1.4166942726620397, -0.9458065641477793, 0.22096098521815927, 0.44118876491710596, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-3.6263206158340595, 0.362193782095267, -1.3979344867173649, -1.8880952343833581, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.6390337283442711, 1.4166942726620397, -1.171870525432564, 0.01685877041801243, -0.8463147304630646, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.9106052635706156, 1.4166942726620397, -0.26761468029337665, 0.4930972716183551, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.0916529537215118, 0.8894440273786534, 0.41057720356100985, 1.5136083456190894, 0.22660484902041086, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-2.3589867847777857, 1.4166942726620397, 0.41057720356100985, 1.3775402024189913, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185]]\n",
      "99\n",
      "4255\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Transfer Target Dataset & Model\")\n",
    "# if target_dataset == 'TJ':\n",
    "#     data_path = './data/Tongji/'\n",
    "#     all_x = pickle.load(open(data_path + 'x.dat', 'rb'))\n",
    "#     all_y = pickle.load(open(data_path + 'y.dat', 'rb'))\n",
    "#     all_time = pickle.load(open(data_path + 'time_all.dat', 'rb'))\n",
    "#     all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "#     tar_subset_idx = [0,1,2,7,11,12,24,25,28,30,32,36,37,39,50,51,65,73]\n",
    "#     tar_other_idx = list(range(74))\n",
    "#     for i in tar_subset_idx:\n",
    "#         tar_other_idx.remove(i)\n",
    "#     for i in range(len(all_x)):\n",
    "#         cur = np.array(all_x[i], dtype=float)\n",
    "#         cur_subset = cur[:, tar_subset_idx]\n",
    "#         cur_other = cur[:, tar_other_idx]\n",
    "#         all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "#     print(all_x[0])\n",
    "#     print(len(all_x[0][0]))\n",
    "#     logger.info(all_x[0])\n",
    "#     logger.info(len(all_x[0][0]))\n",
    "    \n",
    "# elif target_dataset == 'HM':\n",
    "#     data_path = './data/Spain/'\n",
    "#     all_x = pickle.load(open(data_path + 'x.dat', 'rb'))\n",
    "#     all_y = pickle.load(open(data_path + 'y.dat', 'rb'))\n",
    "#     all_time = pickle.load(open(data_path + 'time_all.dat', 'rb'))\n",
    "#     all_x_len = [len(i) for i in all_x]\n",
    "    \n",
    "#     tar_subset_idx = [39, 35, 23, 47, 55, 51, 22, 53, 25, 15, 43, 65, 1, 2, 48, 12, 26, 44, 49]\n",
    "#     tar_other_idx = list(range(66))\n",
    "#     for i in tar_subset_idx:\n",
    "#         tar_other_idx.remove(i)\n",
    "#     for i in range(len(all_x)):\n",
    "#         cur = np.array(all_x[i], dtype=float)\n",
    "#         cur_subset = cur[:, tar_subset_idx]\n",
    "#         cur_other = cur[:, tar_other_idx]\n",
    "#         all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "#     print(all_x[0])\n",
    "#     print(len(all_x[0][0]))\n",
    "#     print(all_y[0])\n",
    "#     logger.info(all_x[0])\n",
    "#     logger.info(len(all_x[0][0]))\n",
    "#     logger.info(all_y[0])\n",
    "if target_dataset == 'TJ':\n",
    "    data_path = './data/Tongji/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    for i in range(len(all_time)):\n",
    "        for j in range(len(all_time[i])):\n",
    "            all_time[i][j] = all_time[i][j][-1]\n",
    "            all_y[i][j] = all_y[i][j][0]\n",
    "\n",
    "    tar_subset_idx = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "    tar_other_idx = list(range(75))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "elif target_dataset == 'HM':\n",
    "    data_path = './data/CDSL/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    for i in range(len(all_time)):\n",
    "        for j in range(len(all_time[i])):\n",
    "            all_time[i][j] = all_time[i][j][-1]\n",
    "            all_y[i][j] = all_y[i][j][0]\n",
    "\n",
    "    tar_subset_idx = [5, 6, 4, 2, 3, 48, 79, 76, 87, 25, 30, 31, 18, 43, 58, 66, 40, 57, 23, 92, 50, 54, 91, 60, 39, 81]\n",
    "    tar_other_idx = list(range(99))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    \n",
    "print(all_x[0])\n",
    "print(len(all_x[0][0]))\n",
    "print(len(all_x))\n",
    "logger.info(all_x[0])\n",
    "logger.info(len(all_x[0][0]))\n",
    "logger.info(len(all_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:34.621807Z",
     "start_time": "2021-02-10T15:06:34.616350Z"
    }
   },
   "outputs": [],
   "source": [
    "long_x = all_x\n",
    "long_y = all_y\n",
    "long_y_kfold = [each[-1] for each in all_y]\n",
    "long_time = all_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:34.845288Z",
     "start_time": "2021-02-10T15:06:34.837259Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_n2n_data(x, y, x_len):\n",
    "#     length = len(x)\n",
    "#     assert length == len(y)\n",
    "#     assert length == len(x_len)\n",
    "#     new_x = []\n",
    "#     new_y = []\n",
    "#     new_x_len = []\n",
    "#     for i in range(length):\n",
    "#         for j in range(len(x[i])):\n",
    "#             new_x.append(x[i][:j+1])\n",
    "#             new_y.append(y[i][j])\n",
    "#             new_x_len.append(j+1)\n",
    "#     return new_x, new_y, new_x_len\n",
    "def get_n2n_data(x, y, x_len, outcome=None):\n",
    "    length = len(x)\n",
    "    assert length == len(y)\n",
    "    assert length == len(outcome)\n",
    "    assert length == len(x_len)\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    new_outcome = []\n",
    "    new_x_len = []\n",
    "    for i in range(length):\n",
    "        for j in range(len(x[i])):\n",
    "            new_x.append(x[i][:j+1])\n",
    "            new_y.append(y[i][j])\n",
    "            new_outcome.append(outcome[i][j])\n",
    "            new_x_len.append(j+1)\n",
    "    return new_x, new_y, new_x_len, new_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:35.791565Z",
     "start_time": "2021-02-10T15:06:35.745700Z"
    }
   },
   "outputs": [],
   "source": [
    "class distcare_target(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.7):\n",
    "        super(distcare_target, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 400)\n",
    "\n",
    "        self.RNNs = clones(nn.RNN(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        self.LastStepAttentions = clones(SingleAttention(self.hidden_dim, 16, attention_type='concat', demographic_dim=12, time_aware=True, use_demographic=False),self.input_dim)\n",
    "        \n",
    "        self.FinalAttentionQKV = FinalAttentionQKV(self.hidden_dim, self.hidden_dim, attention_type='mul',dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model,dropout = 1 - self.keep_prob)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.output = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.FC_embed = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.Linear_los = nn.Linear(self.input_dim, self.output_dim)\n",
    "        self.Linear_outcome = nn.Linear(self.input_dim, self.output_dim)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, input, lens):\n",
    "        lens = lens.to('cpu')\n",
    "        # input shape [batch_size, timestep, feature_dim]\n",
    "#         demo_main = self.tanh(self.demo_proj_main(demo_input)).unsqueeze(1)# b hidden_dim\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "\n",
    "        # Initialization\n",
    "        #cur_hs = Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0))\n",
    "\n",
    "        # forward\n",
    "        # RNN_embeded_input = self.RNNs[0](input[:,:,0].unsqueeze(-1), Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device))[0] # b t h\n",
    "        # Attention_embeded_input = self.LastStepAttentions[0](RNN_embeded_input)[0].unsqueeze(1)# b 1 h\n",
    "        # for i in range(feature_dim-1):\n",
    "        #     embeded_input = self.RNNs[i+1](input[:,:,i+1].unsqueeze(-1), Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device))[0] # b 1 h\n",
    "        #     embeded_input = self.LastStepAttentions[i+1](embeded_input)[0].unsqueeze(1)# b 1 h\n",
    "        #     Attention_embeded_input = torch.cat((Attention_embeded_input, embeded_input), 1)# b i h\n",
    "\n",
    "        # Attention_embeded_input = torch.cat((Attention_embeded_input, demo_main), 1)# b i+1 h\n",
    "        # posi_input = self.dropout(Attention_embeded_input) # batch_size * d_input+1 * hidden_dim\n",
    "\n",
    "#         input = pack_padded_sequence(input, lens, batch_first=True)\n",
    "        \n",
    "        RNN_embeded_input = self.RNNs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1][0].squeeze().unsqueeze(1) # b 1 h\n",
    "#         print(RNN_embeded_input.shape)\n",
    "        for i in range(feature_dim-1):\n",
    "            embeded_input = self.RNNs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1][0].squeeze().unsqueeze(1) # b 1 h\n",
    "            RNN_embeded_input = torch.cat((RNN_embeded_input, embeded_input), 1)\n",
    "        \n",
    "\n",
    "#         RNN_embeded_input = torch.cat((RNN_embeded_input, demo_main), 1)# b i+1 h\n",
    "        posi_input = self.dropout(RNN_embeded_input) # batch_size * d_input * hidden_dim\n",
    "\n",
    "\n",
    "#         #mask = subsequent_mask(time_step).to(device) # 1 t t 下三角 N to 1任务不用mask\n",
    "#         contexts = self.SublayerConnection(posi_input, lambda x: self.MultiHeadedAttention(posi_input, posi_input, posi_input, None))# # batch_size * d_input * hidden_dim\n",
    "    \n",
    "#         DeCov_loss = contexts[1]\n",
    "#         contexts = contexts[0]\n",
    "\n",
    "#         contexts = self.SublayerConnection(contexts, lambda x: self.PositionwiseFeedForward(contexts))[0]# # batch_size * d_input * hidden_dim\n",
    "#         #contexts = contexts.view(batch_size, feature_dim * self.hidden_dim)#\n",
    "#         # contexts = torch.matmul(self.Wproj, contexts) + self.bproj\n",
    "#         # contexts = contexts.squeeze()\n",
    "#         # demo_key = self.demo_proj(demo_input)# b hidden_dim\n",
    "#         # demo_key = self.relu(demo_key)\n",
    "#         # input_dim_scores = torch.matmul(contexts, demo_key.unsqueeze(-1)).squeeze() # b i\n",
    "#         # input_dim_scores = self.dropout(self.sigmoid(input_dim_scores)).unsqueeze(1)# b i\n",
    "        \n",
    "#         # weighted_contexts = torch.matmul(input_dim_scores, contexts).squeeze()\n",
    "# #         print(contexts.shape)\n",
    "\n",
    "#         weighted_contexts = self.FinalAttentionQKV(contexts)[0]\n",
    "#         #output_embed = self.FC_embed(weighted_contexts)\n",
    "        contexts = self.Linear(posi_input).squeeze()# b i\n",
    "        output = self.Linear_los(self.dropout(contexts))# b 1\n",
    "        outcome = self.Linear_outcome(self.dropout(contexts))# b 1\n",
    "        outcome = F.sigmoid(outcome)\n",
    "#         if self.output_dim != 1:\n",
    "#             output = F.softmax(output, dim=1)\n",
    "#         print(weighted_contexts.shape)\n",
    "          \n",
    "        return output, None, None, outcome\n",
    "    #, self.MultiHeadedAttention.attn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:37.622869Z",
     "start_time": "2021-02-10T15:06:37.615260Z"
    }
   },
   "outputs": [],
   "source": [
    "# def transfer_rnn_dict(pretrain_dict, model_dict):\n",
    "#     state_dict = {}\n",
    "#     for k, v in pretrain_dict.items():\n",
    "#         if 'RNNs' in k:\n",
    "#             state_dict[k] = v\n",
    "#             print(\"transfered weight: {}\".format(k))\n",
    "#             logger.info(\"transfered weight: {}\".format(k))\n",
    "#         else:\n",
    "#             print(\"Other weight in model_dict: {}\".format(k))\n",
    "#             logger.info(\"Other weight in model_dict: {}\".format(k))\n",
    "#     return state_dict\n",
    "def transfer_rnn_dict(pretrain_dict, model_dict):\n",
    "    state_dict = {}\n",
    "    for k, v in model_dict.items():\n",
    "        if \"RNNs\" in k:\n",
    "            if k in pretrain_dict:\n",
    "                state_dict[k] = pretrain_dict[k]\n",
    "            else:\n",
    "                target_k = \"generalRNN.\"\n",
    "                point_position1 = k.find('.')\n",
    "                point_position2 = k.find('.', point_position1+1)\n",
    "                target_k += k[point_position2+1:]\n",
    "                state_dict[k] = pretrain_dict[target_k]\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:37.864449Z",
     "start_time": "2021-02-10T15:06:37.857471Z"
    }
   },
   "outputs": [],
   "source": [
    "if target_dataset == 'TJ':\n",
    "    input_dim = 75\n",
    "elif target_dataset == 'HM':\n",
    "    input_dim = 99\n",
    "    \n",
    "cell = 'RNN'\n",
    "hidden_dim = 32\n",
    "d_model = 32\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:39.629505Z",
     "start_time": "2021-02-10T15:06:39.615897Z"
    }
   },
   "outputs": [],
   "source": [
    "def ckd_batch_iter(x, y, lens, batch_size, shuffle=False, outcome=None):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(x) / batch_size) # 向下取整\n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size] #  fetch out all the induces\n",
    "        \n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx],  lens[idx], outcome[idx]))\n",
    "       \n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    \n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "#         batch_name = [e[2] for e in examples]\n",
    "        batch_lens = [e[2] for e in examples]\n",
    "        batch_outcome = [e[3] for e in examples]\n",
    "       \n",
    "\n",
    "        yield batch_x, batch_y, batch_lens, batch_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetMultitaskLoss(nn.Module):\n",
    "    def __init__(self, task_num=2):\n",
    "        super(TargetMultitaskLoss, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.alpha = nn.Parameter(torch.ones((task_num)), requires_grad=True)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, opt_student, los, outcome, outcome_y):\n",
    "        MSE_Loss = self.mse(opt_student, los)\n",
    "        BCE_Loss = self.bce(outcome, outcome_y)\n",
    "        return MSE_Loss * self.alpha[0] + BCE_Loss * self.alpha[1]\n",
    "\n",
    "def get_target_multitask_loss(opt_student, los, outcome, outcome_y):\n",
    "    mtl = TargetMultitaskLoss(task_num=2)\n",
    "    return mtl(opt_student, los, outcome, outcome_y)\n",
    "\n",
    "def reverse_los(y, los_info):\n",
    "    return y * los_info[\"los_std\"] + los_info[\"los_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:21:51,288 - INFO - {'los_mean': 5.363315937659429, 'los_std': 4.099244607743503, 'los_median': 4.333333333333333, 'large_los': 21.291666666666668, 'threshold': 3.7605509886094994}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'los_mean': 5.363315937659429, 'los_std': 4.099244607743503, 'los_median': 4.333333333333333, 'large_los': 21.291666666666668, 'threshold': 3.7605509886094994}\n"
     ]
    }
   ],
   "source": [
    "los_info = pickle.load(open(data_path + 'los_info.pkl', 'rb'))\n",
    "print(los_info)\n",
    "logger.info(los_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-10T15:08:58.832Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 15:21:56,901 - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 3.4467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 0 Batch 0: Train Loss = 3.4467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:22:32,067 - INFO - Fold 1 Epoch 0 Batch 50: Train Loss = 1.8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 0 Batch 50: Train Loss = 1.8900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:23:06,776 - INFO - Fold 1 Epoch 0 Batch 100: Train Loss = 1.8287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 0 Batch 100: Train Loss = 1.8287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:23:41,000 - INFO - Fold 1 Epoch 0 Batch 150: Train Loss = 2.8285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 0 Batch 150: Train Loss = 2.8285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:24:15,772 - INFO - Fold 1 Epoch 0 Batch 200: Train Loss = 1.9309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 0 Batch 200: Train Loss = 1.9309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:24:50,524 - INFO - Fold 1 Epoch 0 Batch 250: Train Loss = 2.1742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 0 Batch 250: Train Loss = 2.1742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:25:25,098 - INFO - Fold 1 Epoch 0 Batch 300: Train Loss = 2.3388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 0 Batch 300: Train Loss = 2.3388\n",
      "{'mad': 4.000151650582401, 'mse': 33.386029311623375, 'mape': 476.01410445638425, 'kappa': 0.2301069434179297}\n",
      "Fold 1, epoch 0: Loss = 2.2154 Valid loss = 2.2901 MSE = 33.3860 AUROC = 0.8155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:26:34,466 - INFO - Fold 1, epoch 0: Loss = 2.2154 Valid loss = 2.2901 MSE = 33.3860 AUROC = 0.8155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save FOLD-BEST model - MSE: 33.3860 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:26:34,469 - INFO - ------------ Save FOLD-BEST model - MSE: 33.3860 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[22918  5403     0     0]\n",
      " [ 7375  5076     0     0]\n",
      " [   23   125     0     0]\n",
      " [    6    13     0     0]]\n",
      "Mean absolute deviation (MAD) = 4.000151650582401\n",
      "Mean squared error (MSE) = 33.386029311623375\n",
      "Mean absolute percentage error (MAPE) = 476.01410445638425\n",
      "Cohen kappa score = 0.2301069434179297\n",
      "------------ Save best model - MSE: 33.3860 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:26:35,193 - INFO - ------------ Save best model - MSE: 33.3860 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ auroc-BEST model - auroc: 0.8155 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:26:35,195 - INFO - ------------ auroc-BEST model - auroc: 0.8155 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 33.3860, mad = 4.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:26:35,197 - INFO - Fold 1, mse = 33.3860, mad = 4.0002\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 15:26:35,953 - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 2.4450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1 Batch 0: Train Loss = 2.4450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:27:11,975 - INFO - Fold 1 Epoch 1 Batch 50: Train Loss = 2.4769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1 Batch 50: Train Loss = 2.4769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:27:46,215 - INFO - Fold 1 Epoch 1 Batch 100: Train Loss = 2.3444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1 Batch 100: Train Loss = 2.3444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:28:21,217 - INFO - Fold 1 Epoch 1 Batch 150: Train Loss = 1.5826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1 Batch 150: Train Loss = 1.5826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:28:55,931 - INFO - Fold 1 Epoch 1 Batch 200: Train Loss = 1.7267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1 Batch 200: Train Loss = 1.7267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:29:30,834 - INFO - Fold 1 Epoch 1 Batch 250: Train Loss = 1.3182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1 Batch 250: Train Loss = 1.3182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:30:06,617 - INFO - Fold 1 Epoch 1 Batch 300: Train Loss = 1.7561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1 Batch 300: Train Loss = 1.7561\n",
      "{'mad': 4.0727060104723, 'mse': 33.06695030705997, 'mape': 523.6522413350256, 'kappa': 0.2521325051281137}\n",
      "------------ Save FOLD-BEST model - MSE: 33.0670 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:31:15,257 - INFO - ------------ Save FOLD-BEST model - MSE: 33.0670 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[21180  7141     0     0]\n",
      " [ 6135  6316     0     0]\n",
      " [   13   135     0     0]\n",
      " [    4    15     0     0]]\n",
      "Mean absolute deviation (MAD) = 4.0727060104723\n",
      "Mean squared error (MSE) = 33.06695030705997\n",
      "Mean absolute percentage error (MAPE) = 523.6522413350256\n",
      "Cohen kappa score = 0.2521325051281137\n",
      "------------ Save best model - MSE: 33.0670 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:31:15,836 - INFO - ------------ Save best model - MSE: 33.0670 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ auroc-BEST model - auroc: 0.8673 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:31:15,838 - INFO - ------------ auroc-BEST model - auroc: 0.8673 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 33.0670, mad = 4.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:31:15,839 - INFO - Fold 1, mse = 33.0670, mad = 4.0727\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 15:31:16,563 - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 1.6060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 2 Batch 0: Train Loss = 1.6060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:31:53,763 - INFO - Fold 1 Epoch 2 Batch 50: Train Loss = 2.4943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 2 Batch 50: Train Loss = 2.4943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:32:28,081 - INFO - Fold 1 Epoch 2 Batch 100: Train Loss = 2.2757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 2 Batch 100: Train Loss = 2.2757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:33:02,296 - INFO - Fold 1 Epoch 2 Batch 150: Train Loss = 2.2057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 2 Batch 150: Train Loss = 2.2057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:33:37,273 - INFO - Fold 1 Epoch 2 Batch 200: Train Loss = 2.0353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 2 Batch 200: Train Loss = 2.0353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:34:11,420 - INFO - Fold 1 Epoch 2 Batch 250: Train Loss = 2.2641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 2 Batch 250: Train Loss = 2.2641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:34:46,190 - INFO - Fold 1 Epoch 2 Batch 300: Train Loss = 2.0765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 2 Batch 300: Train Loss = 2.0765\n",
      "{'mad': 3.9698579991620413, 'mse': 33.21462204220781, 'mape': 467.9653172757859, 'kappa': 0.2205914228458934}\n",
      "------------ auroc-BEST model - auroc: 0.8696 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:35:52,165 - INFO - ------------ auroc-BEST model - auroc: 0.8696 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 33.2146, mad = 3.9699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:35:52,169 - INFO - Fold 1, mse = 33.2146, mad = 3.9699\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 15:35:52,896 - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 1.6039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 3 Batch 0: Train Loss = 1.6039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:36:30,406 - INFO - Fold 1 Epoch 3 Batch 50: Train Loss = 1.8141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 3 Batch 50: Train Loss = 1.8141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:37:06,906 - INFO - Fold 1 Epoch 3 Batch 100: Train Loss = 1.9703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 3 Batch 100: Train Loss = 1.9703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:37:42,631 - INFO - Fold 1 Epoch 3 Batch 150: Train Loss = 1.8529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 3 Batch 150: Train Loss = 1.8529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:38:16,761 - INFO - Fold 1 Epoch 3 Batch 200: Train Loss = 1.8739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 3 Batch 200: Train Loss = 1.8739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:38:51,256 - INFO - Fold 1 Epoch 3 Batch 250: Train Loss = 2.4635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 3 Batch 250: Train Loss = 2.4635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:39:26,244 - INFO - Fold 1 Epoch 3 Batch 300: Train Loss = 2.4539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 3 Batch 300: Train Loss = 2.4539\n",
      "{'mad': 3.9483620625338216, 'mse': 33.10764229275207, 'mape': 449.7218620190331, 'kappa': 0.21575284919166338}\n",
      "------------ auroc-BEST model - auroc: 0.8825 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:40:32,169 - INFO - ------------ auroc-BEST model - auroc: 0.8825 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 33.1076, mad = 3.9484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:40:32,171 - INFO - Fold 1, mse = 33.1076, mad = 3.9484\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 15:40:32,851 - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 1.9403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 4 Batch 0: Train Loss = 1.9403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:41:09,567 - INFO - Fold 1 Epoch 4 Batch 50: Train Loss = 1.9377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 4 Batch 50: Train Loss = 1.9377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:41:45,841 - INFO - Fold 1 Epoch 4 Batch 100: Train Loss = 2.5174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 4 Batch 100: Train Loss = 2.5174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:42:19,402 - INFO - Fold 1 Epoch 4 Batch 150: Train Loss = 2.0816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 4 Batch 150: Train Loss = 2.0816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:42:54,948 - INFO - Fold 1 Epoch 4 Batch 200: Train Loss = 1.8225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 4 Batch 200: Train Loss = 1.8225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:43:29,336 - INFO - Fold 1 Epoch 4 Batch 250: Train Loss = 1.5477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 4 Batch 250: Train Loss = 1.5477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:44:04,635 - INFO - Fold 1 Epoch 4 Batch 300: Train Loss = 1.8619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 4 Batch 300: Train Loss = 1.8619\n",
      "{'mad': 4.047735189243631, 'mse': 32.86272415605501, 'mape': 506.8623923229874, 'kappa': 0.2398672794697425}\n",
      "------------ Save FOLD-BEST model - MSE: 32.8627 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:45:11,016 - INFO - ------------ Save FOLD-BEST model - MSE: 32.8627 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[21486  6835     0     0]\n",
      " [ 6467  5984     0     0]\n",
      " [   20   128     0     0]\n",
      " [    4    15     0     0]]\n",
      "Mean absolute deviation (MAD) = 4.047735189243631\n",
      "Mean squared error (MSE) = 32.86272415605501\n",
      "Mean absolute percentage error (MAPE) = 506.8623923229874\n",
      "Cohen kappa score = 0.2398672794697425\n",
      "------------ Save best model - MSE: 32.8627 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:45:11,739 - INFO - ------------ Save best model - MSE: 32.8627 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ auroc-BEST model - auroc: 0.8934 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:45:11,741 - INFO - ------------ auroc-BEST model - auroc: 0.8934 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 32.8627, mad = 4.0477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:45:11,742 - INFO - Fold 1, mse = 32.8627, mad = 4.0477\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 15:45:12,437 - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 2.0642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 5 Batch 0: Train Loss = 2.0642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:45:46,906 - INFO - Fold 1 Epoch 5 Batch 50: Train Loss = 1.6918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 5 Batch 50: Train Loss = 1.6918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:46:23,794 - INFO - Fold 1 Epoch 5 Batch 100: Train Loss = 2.0811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 5 Batch 100: Train Loss = 2.0811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:46:59,555 - INFO - Fold 1 Epoch 5 Batch 150: Train Loss = 1.9649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 5 Batch 150: Train Loss = 1.9649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:47:34,946 - INFO - Fold 1 Epoch 5 Batch 200: Train Loss = 2.4046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 5 Batch 200: Train Loss = 2.4046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:48:09,207 - INFO - Fold 1 Epoch 5 Batch 250: Train Loss = 2.2570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 5 Batch 250: Train Loss = 2.2570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:48:43,526 - INFO - Fold 1 Epoch 5 Batch 300: Train Loss = 1.6933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 5 Batch 300: Train Loss = 1.6933\n",
      "{'mad': 3.937007420655504, 'mse': 32.993707554237474, 'mape': 451.03299406760027, 'kappa': 0.21336907860480325}\n",
      "Fold 1, mse = 32.9937, mad = 3.9370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:49:50,810 - INFO - Fold 1, mse = 32.9937, mad = 3.9370\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 15:49:51,628 - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 2.0574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 6 Batch 0: Train Loss = 2.0574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:50:25,901 - INFO - Fold 1 Epoch 6 Batch 50: Train Loss = 1.8595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 6 Batch 50: Train Loss = 1.8595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:51:02,628 - INFO - Fold 1 Epoch 6 Batch 100: Train Loss = 2.2123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 6 Batch 100: Train Loss = 2.2123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:51:38,544 - INFO - Fold 1 Epoch 6 Batch 150: Train Loss = 2.0014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 6 Batch 150: Train Loss = 2.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:52:14,552 - INFO - Fold 1 Epoch 6 Batch 200: Train Loss = 2.3401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 6 Batch 200: Train Loss = 2.3401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:52:48,697 - INFO - Fold 1 Epoch 6 Batch 250: Train Loss = 1.9460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 6 Batch 250: Train Loss = 1.9460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:53:23,540 - INFO - Fold 1 Epoch 6 Batch 300: Train Loss = 1.8688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 6 Batch 300: Train Loss = 1.8688\n",
      "{'mad': 4.015845053320882, 'mse': 32.85355656160994, 'mape': 496.8214636594963, 'kappa': 0.23614220423970067}\n",
      "------------ Save FOLD-BEST model - MSE: 32.8536 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:54:30,091 - INFO - ------------ Save FOLD-BEST model - MSE: 32.8536 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[22208  6113     0     0]\n",
      " [ 6909  5542     0     0]\n",
      " [   24   124     0     0]\n",
      " [    4    15     0     0]]\n",
      "Mean absolute deviation (MAD) = 4.015845053320882\n",
      "Mean squared error (MSE) = 32.85355656160994\n",
      "Mean absolute percentage error (MAPE) = 496.8214636594963\n",
      "Cohen kappa score = 0.23614220423970067\n",
      "------------ Save best model - MSE: 32.8536 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:54:30,702 - INFO - ------------ Save best model - MSE: 32.8536 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ auroc-BEST model - auroc: 0.8939 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:54:30,704 - INFO - ------------ auroc-BEST model - auroc: 0.8939 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 32.8536, mad = 4.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:54:30,706 - INFO - Fold 1, mse = 32.8536, mad = 4.0158\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 15:54:31,493 - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 1.6944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 7 Batch 0: Train Loss = 1.6944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:55:05,622 - INFO - Fold 1 Epoch 7 Batch 50: Train Loss = 1.9571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 7 Batch 50: Train Loss = 1.9571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:55:41,679 - INFO - Fold 1 Epoch 7 Batch 100: Train Loss = 2.2342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 7 Batch 100: Train Loss = 2.2342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:56:18,322 - INFO - Fold 1 Epoch 7 Batch 150: Train Loss = 1.7951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 7 Batch 150: Train Loss = 1.7951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:56:53,646 - INFO - Fold 1 Epoch 7 Batch 200: Train Loss = 2.1941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 7 Batch 200: Train Loss = 2.1941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:57:27,897 - INFO - Fold 1 Epoch 7 Batch 250: Train Loss = 1.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 7 Batch 250: Train Loss = 1.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:58:02,102 - INFO - Fold 1 Epoch 7 Batch 300: Train Loss = 1.8761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 7 Batch 300: Train Loss = 1.8761\n",
      "{'mad': 3.9180147542955446, 'mse': 33.449096292461554, 'mape': 396.5127611729633, 'kappa': 0.2195902352009863}\n",
      "------------ auroc-BEST model - auroc: 0.9010 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:59:10,604 - INFO - ------------ auroc-BEST model - auroc: 0.9010 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 33.4491, mad = 3.9180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:59:10,606 - INFO - Fold 1, mse = 33.4491, mad = 3.9180\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 15:59:11,411 - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 1.4885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 8 Batch 0: Train Loss = 1.4885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 15:59:46,523 - INFO - Fold 1 Epoch 8 Batch 50: Train Loss = 1.9196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 8 Batch 50: Train Loss = 1.9196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:00:20,244 - INFO - Fold 1 Epoch 8 Batch 100: Train Loss = 1.3026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 8 Batch 100: Train Loss = 1.3026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:00:58,229 - INFO - Fold 1 Epoch 8 Batch 150: Train Loss = 1.9507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 8 Batch 150: Train Loss = 1.9507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:01:33,089 - INFO - Fold 1 Epoch 8 Batch 200: Train Loss = 1.8795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 8 Batch 200: Train Loss = 1.8795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:02:07,637 - INFO - Fold 1 Epoch 8 Batch 250: Train Loss = 1.9294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 8 Batch 250: Train Loss = 1.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:02:41,826 - INFO - Fold 1 Epoch 8 Batch 300: Train Loss = 1.8042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 8 Batch 300: Train Loss = 1.8042\n",
      "{'mad': 3.9111106367681017, 'mse': 32.972109092143704, 'mape': 429.8559701067821, 'kappa': 0.21850339530392104}\n",
      "Fold 1, mse = 32.9721, mad = 3.9111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:03:49,085 - INFO - Fold 1, mse = 32.9721, mad = 3.9111\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 16:03:49,861 - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 1.7606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 9 Batch 0: Train Loss = 1.7606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:04:23,701 - INFO - Fold 1 Epoch 9 Batch 50: Train Loss = 1.9525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 9 Batch 50: Train Loss = 1.9525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:04:58,345 - INFO - Fold 1 Epoch 9 Batch 100: Train Loss = 1.8067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 9 Batch 100: Train Loss = 1.8067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:05:34,146 - INFO - Fold 1 Epoch 9 Batch 150: Train Loss = 2.0553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 9 Batch 150: Train Loss = 2.0553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:06:11,785 - INFO - Fold 1 Epoch 9 Batch 200: Train Loss = 2.1818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 9 Batch 200: Train Loss = 2.1818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:06:45,764 - INFO - Fold 1 Epoch 9 Batch 250: Train Loss = 2.3851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 9 Batch 250: Train Loss = 2.3851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:07:19,960 - INFO - Fold 1 Epoch 9 Batch 300: Train Loss = 2.1292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 9 Batch 300: Train Loss = 2.1292\n",
      "{'mad': 3.9329709604940892, 'mse': 32.9301935506127, 'mape': 433.5329448830448, 'kappa': 0.22578771495677585}\n",
      "Fold 1, mse = 32.9302, mad = 3.9330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:08:27,823 - INFO - Fold 1, mse = 32.9302, mad = 3.9330\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 16:08:28,493 - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 2.0851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 10 Batch 0: Train Loss = 2.0851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:09:03,547 - INFO - Fold 1 Epoch 10 Batch 50: Train Loss = 1.9993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 10 Batch 50: Train Loss = 1.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:09:37,934 - INFO - Fold 1 Epoch 10 Batch 100: Train Loss = 2.5670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 10 Batch 100: Train Loss = 2.5670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:10:12,921 - INFO - Fold 1 Epoch 10 Batch 150: Train Loss = 2.0849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 10 Batch 150: Train Loss = 2.0849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:10:50,826 - INFO - Fold 1 Epoch 10 Batch 200: Train Loss = 1.8692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 10 Batch 200: Train Loss = 1.8692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:11:25,468 - INFO - Fold 1 Epoch 10 Batch 250: Train Loss = 1.6096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 10 Batch 250: Train Loss = 1.6096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:12:00,632 - INFO - Fold 1 Epoch 10 Batch 300: Train Loss = 1.7122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 10 Batch 300: Train Loss = 1.7122\n",
      "{'mad': 3.9713910546090965, 'mse': 32.70810439154092, 'mape': 457.66284445025656, 'kappa': 0.23021654700820193}\n",
      "Fold 1, epoch 10: Loss = 1.9558 Valid loss = 2.1919 MSE = 32.7081 AUROC = 0.8990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:13:07,531 - INFO - Fold 1, epoch 10: Loss = 1.9558 Valid loss = 2.1919 MSE = 32.7081 AUROC = 0.8990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save FOLD-BEST model - MSE: 32.7081 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:13:07,533 - INFO - ------------ Save FOLD-BEST model - MSE: 32.7081 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[22871  5450     0     0]\n",
      " [ 7347  5104     0     0]\n",
      " [   20   128     0     0]\n",
      " [   10     9     0     0]]\n",
      "Mean absolute deviation (MAD) = 3.9713910546090965\n",
      "Mean squared error (MSE) = 32.70810439154092\n",
      "Mean absolute percentage error (MAPE) = 457.66284445025656\n",
      "Cohen kappa score = 0.23021654700820193\n",
      "------------ Save best model - MSE: 32.7081 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:13:08,162 - INFO - ------------ Save best model - MSE: 32.7081 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 32.7081, mad = 3.9714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:13:08,164 - INFO - Fold 1, mse = 32.7081, mad = 3.9714\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 16:13:08,813 - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 1.6669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 11 Batch 0: Train Loss = 1.6669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:13:42,923 - INFO - Fold 1 Epoch 11 Batch 50: Train Loss = 1.7001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 11 Batch 50: Train Loss = 1.7001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:14:16,720 - INFO - Fold 1 Epoch 11 Batch 100: Train Loss = 1.8949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 11 Batch 100: Train Loss = 1.8949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:14:52,182 - INFO - Fold 1 Epoch 11 Batch 150: Train Loss = 2.0311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 11 Batch 150: Train Loss = 2.0311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:15:28,194 - INFO - Fold 1 Epoch 11 Batch 200: Train Loss = 1.8629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 11 Batch 200: Train Loss = 1.8629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:16:05,365 - INFO - Fold 1 Epoch 11 Batch 250: Train Loss = 1.8718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 11 Batch 250: Train Loss = 1.8718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:16:38,486 - INFO - Fold 1 Epoch 11 Batch 300: Train Loss = 1.8175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 11 Batch 300: Train Loss = 1.8175\n",
      "{'mad': 3.9698232292631634, 'mse': 33.00536763731596, 'mape': 448.21959185496354, 'kappa': 0.21877240304002088}\n",
      "Fold 1, mse = 33.0054, mad = 3.9698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:17:43,269 - INFO - Fold 1, mse = 33.0054, mad = 3.9698\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 16:17:43,963 - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 1.8640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 12 Batch 0: Train Loss = 1.8640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:18:16,312 - INFO - Fold 1 Epoch 12 Batch 50: Train Loss = 1.5726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 12 Batch 50: Train Loss = 1.5726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:18:48,199 - INFO - Fold 1 Epoch 12 Batch 100: Train Loss = 1.9754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 12 Batch 100: Train Loss = 1.9754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:19:20,909 - INFO - Fold 1 Epoch 12 Batch 150: Train Loss = 1.5225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 12 Batch 150: Train Loss = 1.5225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:19:53,348 - INFO - Fold 1 Epoch 12 Batch 200: Train Loss = 2.0881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 12 Batch 200: Train Loss = 2.0881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:20:26,118 - INFO - Fold 1 Epoch 12 Batch 250: Train Loss = 2.1917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 12 Batch 250: Train Loss = 2.1917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:20:58,775 - INFO - Fold 1 Epoch 12 Batch 300: Train Loss = 2.3423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 12 Batch 300: Train Loss = 2.3423\n",
      "{'mad': 3.9874868531051813, 'mse': 33.034227148854015, 'mape': 467.6794775354362, 'kappa': 0.21365425881475542}\n",
      "Fold 1, mse = 33.0342, mad = 3.9875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:22:01,437 - INFO - Fold 1, mse = 33.0342, mad = 3.9875\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 16:22:02,102 - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 2.4940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 13 Batch 0: Train Loss = 2.4940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:22:35,061 - INFO - Fold 1 Epoch 13 Batch 50: Train Loss = 1.8512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 13 Batch 50: Train Loss = 1.8512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:23:06,733 - INFO - Fold 1 Epoch 13 Batch 100: Train Loss = 1.5586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 13 Batch 100: Train Loss = 1.5586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:23:37,955 - INFO - Fold 1 Epoch 13 Batch 150: Train Loss = 1.4928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 13 Batch 150: Train Loss = 1.4928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:24:09,874 - INFO - Fold 1 Epoch 13 Batch 200: Train Loss = 1.7501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 13 Batch 200: Train Loss = 1.7501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:24:41,803 - INFO - Fold 1 Epoch 13 Batch 250: Train Loss = 2.0556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 13 Batch 250: Train Loss = 2.0556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:25:13,034 - INFO - Fold 1 Epoch 13 Batch 300: Train Loss = 2.1148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 13 Batch 300: Train Loss = 2.1148\n",
      "{'mad': 4.050331361307888, 'mse': 32.492277112918735, 'mape': 506.5065694609695, 'kappa': 0.26413752444218175}\n",
      "------------ Save FOLD-BEST model - MSE: 32.4923 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:26:14,728 - INFO - ------------ Save FOLD-BEST model - MSE: 32.4923 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[20933  7388     0     0]\n",
      " [ 5827  6624     0     0]\n",
      " [   14   134     0     0]\n",
      " [    4    15     0     0]]\n",
      "Mean absolute deviation (MAD) = 4.050331361307888\n",
      "Mean squared error (MSE) = 32.492277112918735\n",
      "Mean absolute percentage error (MAPE) = 506.5065694609695\n",
      "Cohen kappa score = 0.26413752444218175\n",
      "------------ Save best model - MSE: 32.4923 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:26:15,240 - INFO - ------------ Save best model - MSE: 32.4923 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 32.4923, mad = 4.0503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:26:15,242 - INFO - Fold 1, mse = 32.4923, mad = 4.0503\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 16:26:15,835 - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 2.0021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 14 Batch 0: Train Loss = 2.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:26:47,637 - INFO - Fold 1 Epoch 14 Batch 50: Train Loss = 1.8387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 14 Batch 50: Train Loss = 1.8387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:27:19,014 - INFO - Fold 1 Epoch 14 Batch 100: Train Loss = 2.0838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 14 Batch 100: Train Loss = 2.0838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:27:50,077 - INFO - Fold 1 Epoch 14 Batch 150: Train Loss = 1.7038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 14 Batch 150: Train Loss = 1.7038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:28:22,764 - INFO - Fold 1 Epoch 14 Batch 200: Train Loss = 1.9284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 14 Batch 200: Train Loss = 1.9284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:28:53,635 - INFO - Fold 1 Epoch 14 Batch 250: Train Loss = 2.3570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 14 Batch 250: Train Loss = 2.3570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:29:24,993 - INFO - Fold 1 Epoch 14 Batch 300: Train Loss = 1.8578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 14 Batch 300: Train Loss = 1.8578\n",
      "{'mad': 4.066344344256096, 'mse': 32.77347478815828, 'mape': 509.73045673161926, 'kappa': 0.2534088896888923}\n",
      "Fold 1, mse = 32.7735, mad = 4.0663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:30:27,803 - INFO - Fold 1, mse = 32.7735, mad = 4.0663\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 16:30:28,451 - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 2.2109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 15 Batch 0: Train Loss = 2.2109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:30:59,822 - INFO - Fold 1 Epoch 15 Batch 50: Train Loss = 1.5622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 15 Batch 50: Train Loss = 1.5622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:31:30,841 - INFO - Fold 1 Epoch 15 Batch 100: Train Loss = 2.1202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 15 Batch 100: Train Loss = 2.1202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:32:01,320 - INFO - Fold 1 Epoch 15 Batch 150: Train Loss = 1.6915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 15 Batch 150: Train Loss = 1.6915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:32:33,488 - INFO - Fold 1 Epoch 15 Batch 200: Train Loss = 1.5115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 15 Batch 200: Train Loss = 1.5115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:33:04,789 - INFO - Fold 1 Epoch 15 Batch 250: Train Loss = 2.2461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 15 Batch 250: Train Loss = 2.2461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:33:35,977 - INFO - Fold 1 Epoch 15 Batch 300: Train Loss = 1.8076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 15 Batch 300: Train Loss = 1.8076\n",
      "{'mad': 3.947240544651155, 'mse': 32.77530830045723, 'mape': 455.65162488846687, 'kappa': 0.2400044013767142}\n",
      "------------ auroc-BEST model - auroc: 0.9013 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:34:38,040 - INFO - ------------ auroc-BEST model - auroc: 0.9013 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 32.7753, mad = 3.9472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:34:38,043 - INFO - Fold 1, mse = 32.7753, mad = 3.9472\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 16:34:38,654 - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 2.6926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 16 Batch 0: Train Loss = 2.6926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:35:09,188 - INFO - Fold 1 Epoch 16 Batch 50: Train Loss = 1.2861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 16 Batch 50: Train Loss = 1.2861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:35:41,274 - INFO - Fold 1 Epoch 16 Batch 100: Train Loss = 2.8845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 16 Batch 100: Train Loss = 2.8845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:36:12,650 - INFO - Fold 1 Epoch 16 Batch 150: Train Loss = 2.4809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 16 Batch 150: Train Loss = 2.4809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:36:44,316 - INFO - Fold 1 Epoch 16 Batch 200: Train Loss = 1.7183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 16 Batch 200: Train Loss = 1.7183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:37:16,387 - INFO - Fold 1 Epoch 16 Batch 250: Train Loss = 1.7107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 16 Batch 250: Train Loss = 1.7107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:37:47,878 - INFO - Fold 1 Epoch 16 Batch 300: Train Loss = 1.3520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 16 Batch 300: Train Loss = 1.3520\n",
      "{'mad': 3.951003386044204, 'mse': 32.86817251083676, 'mape': 449.0595207915896, 'kappa': 0.22221896078488845}\n",
      "Fold 1, mse = 32.8682, mad = 3.9510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:38:52,555 - INFO - Fold 1, mse = 32.8682, mad = 3.9510\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 16:38:53,118 - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 2.0129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 17 Batch 0: Train Loss = 2.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:39:24,536 - INFO - Fold 1 Epoch 17 Batch 50: Train Loss = 1.3997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 17 Batch 50: Train Loss = 1.3997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:39:55,699 - INFO - Fold 1 Epoch 17 Batch 100: Train Loss = 1.9749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 17 Batch 100: Train Loss = 1.9749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:40:28,388 - INFO - Fold 1 Epoch 17 Batch 150: Train Loss = 1.5644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 17 Batch 150: Train Loss = 1.5644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:41:00,422 - INFO - Fold 1 Epoch 17 Batch 200: Train Loss = 1.6970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 17 Batch 200: Train Loss = 1.6970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:41:32,502 - INFO - Fold 1 Epoch 17 Batch 250: Train Loss = 1.7180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 17 Batch 250: Train Loss = 1.7180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:42:04,337 - INFO - Fold 1 Epoch 17 Batch 300: Train Loss = 1.9679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 17 Batch 300: Train Loss = 1.9679\n",
      "{'mad': 4.007962402742449, 'mse': 32.58767988503764, 'mape': 485.05371922898337, 'kappa': 0.24838662968996617}\n",
      "------------ auroc-BEST model - auroc: 0.9017 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:43:05,840 - INFO - ------------ auroc-BEST model - auroc: 0.9017 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 32.5877, mad = 4.0080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:43:05,841 - INFO - Fold 1, mse = 32.5877, mad = 4.0080\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 16:43:06,519 - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 1.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 18 Batch 0: Train Loss = 1.8836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:43:38,946 - INFO - Fold 1 Epoch 18 Batch 50: Train Loss = 2.0516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 18 Batch 50: Train Loss = 2.0516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:44:09,881 - INFO - Fold 1 Epoch 18 Batch 100: Train Loss = 1.9191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 18 Batch 100: Train Loss = 1.9191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:44:40,916 - INFO - Fold 1 Epoch 18 Batch 150: Train Loss = 1.8872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 18 Batch 150: Train Loss = 1.8872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:45:12,732 - INFO - Fold 1 Epoch 18 Batch 200: Train Loss = 1.5745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 18 Batch 200: Train Loss = 1.5745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:45:45,020 - INFO - Fold 1 Epoch 18 Batch 250: Train Loss = 1.7026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 18 Batch 250: Train Loss = 1.7026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:46:16,571 - INFO - Fold 1 Epoch 18 Batch 300: Train Loss = 2.2710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 18 Batch 300: Train Loss = 2.2710\n",
      "{'mad': 3.960779348157656, 'mse': 32.911178466788805, 'mape': 456.48619804841945, 'kappa': 0.22709603740170758}\n",
      "Fold 1, mse = 32.9112, mad = 3.9608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:47:19,217 - INFO - Fold 1, mse = 32.9112, mad = 3.9608\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 16:47:19,867 - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 1.7687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 19 Batch 0: Train Loss = 1.7687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:47:52,299 - INFO - Fold 1 Epoch 19 Batch 50: Train Loss = 1.7185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 19 Batch 50: Train Loss = 1.7185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:48:24,548 - INFO - Fold 1 Epoch 19 Batch 100: Train Loss = 2.2331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 19 Batch 100: Train Loss = 2.2331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:48:56,868 - INFO - Fold 1 Epoch 19 Batch 150: Train Loss = 1.3804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 19 Batch 150: Train Loss = 1.3804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:49:29,786 - INFO - Fold 1 Epoch 19 Batch 200: Train Loss = 2.0842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 19 Batch 200: Train Loss = 2.0842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:50:01,772 - INFO - Fold 1 Epoch 19 Batch 250: Train Loss = 1.8930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 19 Batch 250: Train Loss = 1.8930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:50:34,180 - INFO - Fold 1 Epoch 19 Batch 300: Train Loss = 2.2290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 19 Batch 300: Train Loss = 2.2290\n",
      "{'mad': 3.9359390440063233, 'mse': 32.776496231951775, 'mape': 452.6503627281211, 'kappa': 0.2229358802783209}\n",
      "Fold 1, mse = 32.7765, mad = 3.9359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:51:36,419 - INFO - Fold 1, mse = 32.7765, mad = 3.9359\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 16:51:37,654 - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 3.3757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 0 Batch 0: Train Loss = 3.3757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:52:08,839 - INFO - Fold 2 Epoch 0 Batch 50: Train Loss = 1.8438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 0 Batch 50: Train Loss = 1.8438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:52:40,427 - INFO - Fold 2 Epoch 0 Batch 100: Train Loss = 2.1019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 0 Batch 100: Train Loss = 2.1019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:53:12,185 - INFO - Fold 2 Epoch 0 Batch 150: Train Loss = 2.2961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 0 Batch 150: Train Loss = 2.2961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:53:43,313 - INFO - Fold 2 Epoch 0 Batch 200: Train Loss = 1.7127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 0 Batch 200: Train Loss = 1.7127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:54:15,659 - INFO - Fold 2 Epoch 0 Batch 250: Train Loss = 2.6488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 0 Batch 250: Train Loss = 2.6488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:54:46,764 - INFO - Fold 2 Epoch 0 Batch 300: Train Loss = 2.0684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 0 Batch 300: Train Loss = 2.0684\n",
      "{'mad': 3.9306480254513123, 'mse': 29.695315822533278, 'mape': 483.87272207521835, 'kappa': 0.20837466982842312}\n",
      "Fold 2, epoch 0: Loss = 2.3224 Valid loss = 2.0414 MSE = 29.6953 AUROC = 0.8370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:55:51,055 - INFO - Fold 2, epoch 0: Loss = 2.3224 Valid loss = 2.0414 MSE = 29.6953 AUROC = 0.8370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save FOLD-BEST model - MSE: 29.6953 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:55:51,057 - INFO - ------------ Save FOLD-BEST model - MSE: 29.6953 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[23116  5325     0]\n",
      " [ 8009  4958     0]\n",
      " [   49   100     0]]\n",
      "Mean absolute deviation (MAD) = 3.9306480254513123\n",
      "Mean squared error (MSE) = 29.695315822533278\n",
      "Mean absolute percentage error (MAPE) = 483.87272207521835\n",
      "Cohen kappa score = 0.20837466982842312\n",
      "------------ Save best model - MSE: 29.6953 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:55:51,618 - INFO - ------------ Save best model - MSE: 29.6953 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ auroc-BEST model - auroc: 0.8370 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:55:51,622 - INFO - ------------ auroc-BEST model - auroc: 0.8370 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 29.6953, mad = 3.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:55:51,624 - INFO - Fold 2, mse = 29.6953, mad = 3.9306\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 16:55:52,205 - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 2.2251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 1 Batch 0: Train Loss = 2.2251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:56:23,681 - INFO - Fold 2 Epoch 1 Batch 50: Train Loss = 3.0948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 1 Batch 50: Train Loss = 3.0948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:56:54,899 - INFO - Fold 2 Epoch 1 Batch 100: Train Loss = 1.7043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 1 Batch 100: Train Loss = 1.7043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:57:27,098 - INFO - Fold 2 Epoch 1 Batch 150: Train Loss = 1.9784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 1 Batch 150: Train Loss = 1.9784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:57:58,134 - INFO - Fold 2 Epoch 1 Batch 200: Train Loss = 2.2807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 1 Batch 200: Train Loss = 2.2807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:58:29,819 - INFO - Fold 2 Epoch 1 Batch 250: Train Loss = 3.1452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 1 Batch 250: Train Loss = 3.1452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:59:01,660 - INFO - Fold 2 Epoch 1 Batch 300: Train Loss = 2.1593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 1 Batch 300: Train Loss = 2.1593\n",
      "{'mad': 3.9253607774495247, 'mse': 29.26123542247495, 'mape': 475.43143686047574, 'kappa': 0.22441785671616354}\n",
      "------------ Save FOLD-BEST model - MSE: 29.2612 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:00:04,470 - INFO - ------------ Save FOLD-BEST model - MSE: 29.2612 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[22423  6018     0]\n",
      " [ 7425  5542     0]\n",
      " [   35   114     0]]\n",
      "Mean absolute deviation (MAD) = 3.9253607774495247\n",
      "Mean squared error (MSE) = 29.26123542247495\n",
      "Mean absolute percentage error (MAPE) = 475.43143686047574\n",
      "Cohen kappa score = 0.22441785671616354\n",
      "------------ Save best model - MSE: 29.2612 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:00:05,048 - INFO - ------------ Save best model - MSE: 29.2612 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ auroc-BEST model - auroc: 0.8638 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:00:05,050 - INFO - ------------ auroc-BEST model - auroc: 0.8638 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 29.2612, mad = 3.9254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:00:05,051 - INFO - Fold 2, mse = 29.2612, mad = 3.9254\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:00:05,789 - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 1.7665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 2 Batch 0: Train Loss = 1.7665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:00:37,709 - INFO - Fold 2 Epoch 2 Batch 50: Train Loss = 2.3541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 2 Batch 50: Train Loss = 2.3541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:01:09,854 - INFO - Fold 2 Epoch 2 Batch 100: Train Loss = 2.4012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 2 Batch 100: Train Loss = 2.4012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:01:42,315 - INFO - Fold 2 Epoch 2 Batch 150: Train Loss = 1.8889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 2 Batch 150: Train Loss = 1.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:02:14,303 - INFO - Fold 2 Epoch 2 Batch 200: Train Loss = 1.6458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 2 Batch 200: Train Loss = 1.6458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:02:45,303 - INFO - Fold 2 Epoch 2 Batch 250: Train Loss = 1.4539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 2 Batch 250: Train Loss = 1.4539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:03:17,413 - INFO - Fold 2 Epoch 2 Batch 300: Train Loss = 1.8976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 2 Batch 300: Train Loss = 1.8976\n",
      "{'mad': 3.885147881581057, 'mse': 29.152128375467544, 'mape': 463.84010066925805, 'kappa': 0.22330725078764402}\n",
      "------------ Save FOLD-BEST model - MSE: 29.1521 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:04:20,134 - INFO - ------------ Save FOLD-BEST model - MSE: 29.1521 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[23270  5171     0]\n",
      " [ 7909  5058     0]\n",
      " [   36   113     0]]\n",
      "Mean absolute deviation (MAD) = 3.885147881581057\n",
      "Mean squared error (MSE) = 29.152128375467544\n",
      "Mean absolute percentage error (MAPE) = 463.84010066925805\n",
      "Cohen kappa score = 0.22330725078764402\n",
      "------------ Save best model - MSE: 29.1521 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:04:20,585 - INFO - ------------ Save best model - MSE: 29.1521 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ auroc-BEST model - auroc: 0.8666 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:04:20,587 - INFO - ------------ auroc-BEST model - auroc: 0.8666 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 29.1521, mad = 3.8851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:04:20,588 - INFO - Fold 2, mse = 29.1521, mad = 3.8851\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:04:21,205 - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 1.6258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 3 Batch 0: Train Loss = 1.6258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:04:53,700 - INFO - Fold 2 Epoch 3 Batch 50: Train Loss = 2.0785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 3 Batch 50: Train Loss = 2.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:05:26,483 - INFO - Fold 2 Epoch 3 Batch 100: Train Loss = 2.6059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 3 Batch 100: Train Loss = 2.6059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:05:58,413 - INFO - Fold 2 Epoch 3 Batch 150: Train Loss = 2.0895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 3 Batch 150: Train Loss = 2.0895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:06:31,424 - INFO - Fold 2 Epoch 3 Batch 200: Train Loss = 2.4166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 3 Batch 200: Train Loss = 2.4166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:07:04,046 - INFO - Fold 2 Epoch 3 Batch 250: Train Loss = 1.9904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 3 Batch 250: Train Loss = 1.9904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:07:37,290 - INFO - Fold 2 Epoch 3 Batch 300: Train Loss = 1.7383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 3 Batch 300: Train Loss = 1.7383\n",
      "{'mad': 3.8708828078202697, 'mse': 28.813869503280834, 'mape': 449.95099765943263, 'kappa': 0.22535405360348115}\n",
      "------------ Save FOLD-BEST model - MSE: 28.8139 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:08:40,426 - INFO - ------------ Save FOLD-BEST model - MSE: 28.8139 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[22707  5734     0]\n",
      " [ 7577  5390     0]\n",
      " [   28   121     0]]\n",
      "Mean absolute deviation (MAD) = 3.8708828078202697\n",
      "Mean squared error (MSE) = 28.813869503280834\n",
      "Mean absolute percentage error (MAPE) = 449.95099765943263\n",
      "Cohen kappa score = 0.22535405360348115\n",
      "------------ Save best model - MSE: 28.8139 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:08:41,079 - INFO - ------------ Save best model - MSE: 28.8139 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ auroc-BEST model - auroc: 0.8696 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:08:41,087 - INFO - ------------ auroc-BEST model - auroc: 0.8696 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 28.8139, mad = 3.8709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:08:41,091 - INFO - Fold 2, mse = 28.8139, mad = 3.8709\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:08:41,771 - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 2.0555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 4 Batch 0: Train Loss = 2.0555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:09:14,772 - INFO - Fold 2 Epoch 4 Batch 50: Train Loss = 2.5624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 4 Batch 50: Train Loss = 2.5624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:09:46,866 - INFO - Fold 2 Epoch 4 Batch 100: Train Loss = 1.6597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 4 Batch 100: Train Loss = 1.6597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:10:18,879 - INFO - Fold 2 Epoch 4 Batch 150: Train Loss = 2.6463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 4 Batch 150: Train Loss = 2.6463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:10:51,110 - INFO - Fold 2 Epoch 4 Batch 200: Train Loss = 2.5107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 4 Batch 200: Train Loss = 2.5107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:11:21,759 - INFO - Fold 2 Epoch 4 Batch 250: Train Loss = 1.9911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 4 Batch 250: Train Loss = 1.9911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:11:53,834 - INFO - Fold 2 Epoch 4 Batch 300: Train Loss = 1.7517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 4 Batch 300: Train Loss = 1.7517\n",
      "{'mad': 3.903653541738839, 'mse': 28.835993947944687, 'mape': 470.88154717759767, 'kappa': 0.2389820078022834}\n",
      "------------ auroc-BEST model - auroc: 0.8717 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:12:56,892 - INFO - ------------ auroc-BEST model - auroc: 0.8717 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 28.8360, mad = 3.9037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:12:56,894 - INFO - Fold 2, mse = 28.8360, mad = 3.9037\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:12:57,605 - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 2.6032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 5 Batch 0: Train Loss = 2.6032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:13:29,300 - INFO - Fold 2 Epoch 5 Batch 50: Train Loss = 1.9003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 5 Batch 50: Train Loss = 1.9003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:14:00,857 - INFO - Fold 2 Epoch 5 Batch 100: Train Loss = 1.9505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 5 Batch 100: Train Loss = 1.9505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:14:33,319 - INFO - Fold 2 Epoch 5 Batch 150: Train Loss = 2.1688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 5 Batch 150: Train Loss = 2.1688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:15:05,660 - INFO - Fold 2 Epoch 5 Batch 200: Train Loss = 2.3022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 5 Batch 200: Train Loss = 2.3022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:15:37,926 - INFO - Fold 2 Epoch 5 Batch 250: Train Loss = 1.7047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 5 Batch 250: Train Loss = 1.7047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:16:09,654 - INFO - Fold 2 Epoch 5 Batch 300: Train Loss = 1.6288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 5 Batch 300: Train Loss = 1.6288\n",
      "{'mad': 3.9460713455279715, 'mse': 28.787805491048708, 'mape': 498.6851475372917, 'kappa': 0.25722061753197556}\n",
      "------------ Save FOLD-BEST model - MSE: 28.7878 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:17:11,865 - INFO - ------------ Save FOLD-BEST model - MSE: 28.7878 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[22136  6305     0]\n",
      " [ 6817  6150     0]\n",
      " [   27   122     0]]\n",
      "Mean absolute deviation (MAD) = 3.9460713455279715\n",
      "Mean squared error (MSE) = 28.787805491048708\n",
      "Mean absolute percentage error (MAPE) = 498.6851475372917\n",
      "Cohen kappa score = 0.25722061753197556\n",
      "------------ Save best model - MSE: 28.7878 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:17:12,308 - INFO - ------------ Save best model - MSE: 28.7878 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 28.7878, mad = 3.9461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:17:12,310 - INFO - Fold 2, mse = 28.7878, mad = 3.9461\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:17:12,917 - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 1.7524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 6 Batch 0: Train Loss = 1.7524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:17:44,594 - INFO - Fold 2 Epoch 6 Batch 50: Train Loss = 1.9163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 6 Batch 50: Train Loss = 1.9163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:18:15,519 - INFO - Fold 2 Epoch 6 Batch 100: Train Loss = 1.3592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 6 Batch 100: Train Loss = 1.3592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:18:47,111 - INFO - Fold 2 Epoch 6 Batch 150: Train Loss = 1.7663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 6 Batch 150: Train Loss = 1.7663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:19:18,869 - INFO - Fold 2 Epoch 6 Batch 200: Train Loss = 2.3393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 6 Batch 200: Train Loss = 2.3393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:19:51,797 - INFO - Fold 2 Epoch 6 Batch 250: Train Loss = 1.8482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 6 Batch 250: Train Loss = 1.8482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:20:24,626 - INFO - Fold 2 Epoch 6 Batch 300: Train Loss = 2.0682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 6 Batch 300: Train Loss = 2.0682\n",
      "{'mad': 3.863567261511235, 'mse': 28.82341393657903, 'mape': 454.7923998387113, 'kappa': 0.22386835109890513}\n",
      "------------ auroc-BEST model - auroc: 0.8739 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:21:27,659 - INFO - ------------ auroc-BEST model - auroc: 0.8739 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 28.8234, mad = 3.8636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:21:27,661 - INFO - Fold 2, mse = 28.8234, mad = 3.8636\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:21:28,210 - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 1.3490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 7 Batch 0: Train Loss = 1.3490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:21:59,554 - INFO - Fold 2 Epoch 7 Batch 50: Train Loss = 2.0192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 7 Batch 50: Train Loss = 2.0192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:22:31,559 - INFO - Fold 2 Epoch 7 Batch 100: Train Loss = 2.0379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 7 Batch 100: Train Loss = 2.0379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:23:03,235 - INFO - Fold 2 Epoch 7 Batch 150: Train Loss = 2.4252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 7 Batch 150: Train Loss = 2.4252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:23:34,586 - INFO - Fold 2 Epoch 7 Batch 200: Train Loss = 2.7790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 7 Batch 200: Train Loss = 2.7790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:24:06,993 - INFO - Fold 2 Epoch 7 Batch 250: Train Loss = 1.6444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 7 Batch 250: Train Loss = 1.6444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:24:39,038 - INFO - Fold 2 Epoch 7 Batch 300: Train Loss = 2.0177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 7 Batch 300: Train Loss = 2.0177\n",
      "{'mad': 3.868827265928087, 'mse': 28.90177458302101, 'mape': 455.43739901562634, 'kappa': 0.22487066939697542}\n",
      "------------ auroc-BEST model - auroc: 0.8786 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:25:40,985 - INFO - ------------ auroc-BEST model - auroc: 0.8786 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 28.9018, mad = 3.8688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:25:40,993 - INFO - Fold 2, mse = 28.9018, mad = 3.8688\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:25:41,717 - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 1.8654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 8 Batch 0: Train Loss = 1.8654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:26:12,904 - INFO - Fold 2 Epoch 8 Batch 50: Train Loss = 1.7984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 8 Batch 50: Train Loss = 1.7984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:26:45,155 - INFO - Fold 2 Epoch 8 Batch 100: Train Loss = 2.2124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 8 Batch 100: Train Loss = 2.2124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:27:16,419 - INFO - Fold 2 Epoch 8 Batch 150: Train Loss = 2.0683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 8 Batch 150: Train Loss = 2.0683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:27:49,807 - INFO - Fold 2 Epoch 8 Batch 200: Train Loss = 1.8025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 8 Batch 200: Train Loss = 1.8025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:28:22,473 - INFO - Fold 2 Epoch 8 Batch 250: Train Loss = 2.0569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 8 Batch 250: Train Loss = 2.0569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:28:55,289 - INFO - Fold 2 Epoch 8 Batch 300: Train Loss = 1.9698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 8 Batch 300: Train Loss = 1.9698\n",
      "{'mad': 3.8609033953537693, 'mse': 29.018019379630516, 'mape': 438.74982417389134, 'kappa': 0.22382000772330635}\n",
      "Fold 2, mse = 29.0180, mad = 3.8609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:29:57,912 - INFO - Fold 2, mse = 29.0180, mad = 3.8609\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:29:58,608 - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 2.2997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 9 Batch 0: Train Loss = 2.2997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:30:30,747 - INFO - Fold 2 Epoch 9 Batch 50: Train Loss = 1.9729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 9 Batch 50: Train Loss = 1.9729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:31:03,621 - INFO - Fold 2 Epoch 9 Batch 100: Train Loss = 2.1190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 9 Batch 100: Train Loss = 2.1190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:31:36,074 - INFO - Fold 2 Epoch 9 Batch 150: Train Loss = 2.0345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 9 Batch 150: Train Loss = 2.0345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:32:08,342 - INFO - Fold 2 Epoch 9 Batch 200: Train Loss = 2.6099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 9 Batch 200: Train Loss = 2.6099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:32:40,837 - INFO - Fold 2 Epoch 9 Batch 250: Train Loss = 3.0199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 9 Batch 250: Train Loss = 3.0199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:33:12,207 - INFO - Fold 2 Epoch 9 Batch 300: Train Loss = 2.3661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 9 Batch 300: Train Loss = 2.3661\n",
      "{'mad': 3.858200461846268, 'mse': 28.936465838262126, 'mape': 449.91197469724494, 'kappa': 0.21504544364183298}\n",
      "Fold 2, mse = 28.9365, mad = 3.8582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:34:09,869 - INFO - Fold 2, mse = 28.9365, mad = 3.8582\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:34:10,433 - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 2.0632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 10 Batch 0: Train Loss = 2.0632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:34:38,882 - INFO - Fold 2 Epoch 10 Batch 50: Train Loss = 2.2587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 10 Batch 50: Train Loss = 2.2587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:35:06,612 - INFO - Fold 2 Epoch 10 Batch 100: Train Loss = 1.9296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 10 Batch 100: Train Loss = 1.9296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:35:33,621 - INFO - Fold 2 Epoch 10 Batch 150: Train Loss = 1.5737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 10 Batch 150: Train Loss = 1.5737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:36:01,205 - INFO - Fold 2 Epoch 10 Batch 200: Train Loss = 2.1628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 10 Batch 200: Train Loss = 2.1628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:36:27,514 - INFO - Fold 2 Epoch 10 Batch 250: Train Loss = 1.7514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 10 Batch 250: Train Loss = 1.7514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:36:53,404 - INFO - Fold 2 Epoch 10 Batch 300: Train Loss = 2.9792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 10 Batch 300: Train Loss = 2.9792\n",
      "{'mad': 3.9194223269950283, 'mse': 28.95714028819497, 'mape': 472.61886301056865, 'kappa': 0.24580742814104395}\n",
      "Fold 2, epoch 10: Loss = 2.0616 Valid loss = 1.9824 MSE = 28.9571 AUROC = 0.8662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:37:47,852 - INFO - Fold 2, epoch 10: Loss = 2.0616 Valid loss = 1.9824 MSE = 28.9571 AUROC = 0.8662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 28.9571, mad = 3.9194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:37:47,854 - INFO - Fold 2, mse = 28.9571, mad = 3.9194\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:37:48,498 - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 2.0266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 11 Batch 0: Train Loss = 2.0266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:38:15,873 - INFO - Fold 2 Epoch 11 Batch 50: Train Loss = 1.8176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 11 Batch 50: Train Loss = 1.8176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:38:42,929 - INFO - Fold 2 Epoch 11 Batch 100: Train Loss = 1.4288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 11 Batch 100: Train Loss = 1.4288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:39:09,461 - INFO - Fold 2 Epoch 11 Batch 150: Train Loss = 1.6575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 11 Batch 150: Train Loss = 1.6575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:39:35,340 - INFO - Fold 2 Epoch 11 Batch 200: Train Loss = 1.8613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 11 Batch 200: Train Loss = 1.8613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:40:01,419 - INFO - Fold 2 Epoch 11 Batch 250: Train Loss = 1.6692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 11 Batch 250: Train Loss = 1.6692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:40:27,533 - INFO - Fold 2 Epoch 11 Batch 300: Train Loss = 1.9709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 11 Batch 300: Train Loss = 1.9709\n",
      "{'mad': 3.8270370305417236, 'mse': 28.832304972106197, 'mape': 433.8915221918227, 'kappa': 0.21505351825315144}\n",
      "Fold 2, mse = 28.8323, mad = 3.8270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:41:22,383 - INFO - Fold 2, mse = 28.8323, mad = 3.8270\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:41:22,970 - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 2.0685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 12 Batch 0: Train Loss = 2.0685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:41:49,626 - INFO - Fold 2 Epoch 12 Batch 50: Train Loss = 1.8602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 12 Batch 50: Train Loss = 1.8602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:42:16,452 - INFO - Fold 2 Epoch 12 Batch 100: Train Loss = 1.9442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 12 Batch 100: Train Loss = 1.9442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:42:42,939 - INFO - Fold 2 Epoch 12 Batch 150: Train Loss = 2.1301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 12 Batch 150: Train Loss = 2.1301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:43:09,169 - INFO - Fold 2 Epoch 12 Batch 200: Train Loss = 2.0985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 12 Batch 200: Train Loss = 2.0985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:43:35,266 - INFO - Fold 2 Epoch 12 Batch 250: Train Loss = 1.8140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 12 Batch 250: Train Loss = 1.8140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:44:01,353 - INFO - Fold 2 Epoch 12 Batch 300: Train Loss = 1.5608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 12 Batch 300: Train Loss = 1.5608\n",
      "{'mad': 3.886500690412708, 'mse': 28.538562005851077, 'mape': 469.95959957419177, 'kappa': 0.2462164615045983}\n",
      "------------ Save FOLD-BEST model - MSE: 28.5386 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:44:55,464 - INFO - ------------ Save FOLD-BEST model - MSE: 28.5386 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[22426  6015     0]\n",
      " [ 7128  5839     0]\n",
      " [   34   115     0]]\n",
      "Mean absolute deviation (MAD) = 3.886500690412708\n",
      "Mean squared error (MSE) = 28.538562005851077\n",
      "Mean absolute percentage error (MAPE) = 469.95959957419177\n",
      "Cohen kappa score = 0.2462164615045983\n",
      "------------ Save best model - MSE: 28.5386 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:44:55,866 - INFO - ------------ Save best model - MSE: 28.5386 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 28.5386, mad = 3.8865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:44:55,867 - INFO - Fold 2, mse = 28.5386, mad = 3.8865\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:44:56,406 - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 2.0819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 13 Batch 0: Train Loss = 2.0819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:45:22,208 - INFO - Fold 2 Epoch 13 Batch 50: Train Loss = 2.5676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 13 Batch 50: Train Loss = 2.5676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:45:47,796 - INFO - Fold 2 Epoch 13 Batch 100: Train Loss = 1.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 13 Batch 100: Train Loss = 1.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:46:14,236 - INFO - Fold 2 Epoch 13 Batch 150: Train Loss = 3.8137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 13 Batch 150: Train Loss = 3.8137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:46:40,847 - INFO - Fold 2 Epoch 13 Batch 200: Train Loss = 2.2900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 13 Batch 200: Train Loss = 2.2900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:47:06,873 - INFO - Fold 2 Epoch 13 Batch 250: Train Loss = 1.8037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 13 Batch 250: Train Loss = 1.8037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:47:33,234 - INFO - Fold 2 Epoch 13 Batch 300: Train Loss = 1.8214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 13 Batch 300: Train Loss = 1.8214\n",
      "{'mad': 3.8934578035919256, 'mse': 28.758525912252043, 'mape': 466.3366770245962, 'kappa': 0.23416206471149925}\n",
      "Fold 2, mse = 28.7585, mad = 3.8935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:48:27,900 - INFO - Fold 2, mse = 28.7585, mad = 3.8935\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:48:28,454 - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 1.7764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 14 Batch 0: Train Loss = 1.7764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:48:55,080 - INFO - Fold 2 Epoch 14 Batch 50: Train Loss = 3.8724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 14 Batch 50: Train Loss = 3.8724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:49:21,947 - INFO - Fold 2 Epoch 14 Batch 100: Train Loss = 2.4633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 14 Batch 100: Train Loss = 2.4633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:49:48,716 - INFO - Fold 2 Epoch 14 Batch 150: Train Loss = 1.5120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 14 Batch 150: Train Loss = 1.5120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:50:15,442 - INFO - Fold 2 Epoch 14 Batch 200: Train Loss = 1.6444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 14 Batch 200: Train Loss = 1.6444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:50:41,820 - INFO - Fold 2 Epoch 14 Batch 250: Train Loss = 2.2635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 14 Batch 250: Train Loss = 2.2635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:51:07,844 - INFO - Fold 2 Epoch 14 Batch 300: Train Loss = 2.0735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 14 Batch 300: Train Loss = 2.0735\n",
      "{'mad': 3.892148867788829, 'mse': 28.78038155682617, 'mape': 461.34908980317624, 'kappa': 0.2451296250199836}\n",
      "Fold 2, mse = 28.7804, mad = 3.8921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:52:02,074 - INFO - Fold 2, mse = 28.7804, mad = 3.8921\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:52:02,636 - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 1.7583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 15 Batch 0: Train Loss = 1.7583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:52:29,205 - INFO - Fold 2 Epoch 15 Batch 50: Train Loss = 1.9399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 15 Batch 50: Train Loss = 1.9399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:52:55,836 - INFO - Fold 2 Epoch 15 Batch 100: Train Loss = 1.8224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 15 Batch 100: Train Loss = 1.8224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:53:22,339 - INFO - Fold 2 Epoch 15 Batch 150: Train Loss = 2.1843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 15 Batch 150: Train Loss = 2.1843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:53:48,654 - INFO - Fold 2 Epoch 15 Batch 200: Train Loss = 1.5986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 15 Batch 200: Train Loss = 1.5986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:54:15,279 - INFO - Fold 2 Epoch 15 Batch 250: Train Loss = 1.7275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 15 Batch 250: Train Loss = 1.7275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:54:41,360 - INFO - Fold 2 Epoch 15 Batch 300: Train Loss = 2.2412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 15 Batch 300: Train Loss = 2.2412\n",
      "{'mad': 3.879548147757464, 'mse': 28.984809649668357, 'mape': 449.8762708775224, 'kappa': 0.23191971583047566}\n",
      "Fold 2, mse = 28.9848, mad = 3.8795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:55:38,044 - INFO - Fold 2, mse = 28.9848, mad = 3.8795\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:55:38,597 - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 2.1595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 16 Batch 0: Train Loss = 2.1595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:56:04,965 - INFO - Fold 2 Epoch 16 Batch 50: Train Loss = 2.2333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 16 Batch 50: Train Loss = 2.2333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:56:31,265 - INFO - Fold 2 Epoch 16 Batch 100: Train Loss = 1.9827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 16 Batch 100: Train Loss = 1.9827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:56:57,531 - INFO - Fold 2 Epoch 16 Batch 150: Train Loss = 2.5081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 16 Batch 150: Train Loss = 2.5081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:57:24,014 - INFO - Fold 2 Epoch 16 Batch 200: Train Loss = 2.6508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 16 Batch 200: Train Loss = 2.6508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:57:51,224 - INFO - Fold 2 Epoch 16 Batch 250: Train Loss = 1.7018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 16 Batch 250: Train Loss = 1.7018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:58:17,323 - INFO - Fold 2 Epoch 16 Batch 300: Train Loss = 1.8804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 16 Batch 300: Train Loss = 1.8804\n",
      "{'mad': 3.9932559412906126, 'mse': 28.70724425398088, 'mape': 517.0803420409273, 'kappa': 0.2565339112389441}\n",
      "Fold 2, mse = 28.7072, mad = 3.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:59:12,016 - INFO - Fold 2, mse = 28.7072, mad = 3.9933\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 17:59:12,556 - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 2.5259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 17 Batch 0: Train Loss = 2.5259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 17:59:39,164 - INFO - Fold 2 Epoch 17 Batch 50: Train Loss = 2.1098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 17 Batch 50: Train Loss = 2.1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:00:05,542 - INFO - Fold 2 Epoch 17 Batch 100: Train Loss = 1.5889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 17 Batch 100: Train Loss = 1.5889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:00:32,211 - INFO - Fold 2 Epoch 17 Batch 150: Train Loss = 1.7587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 17 Batch 150: Train Loss = 1.7587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:00:59,047 - INFO - Fold 2 Epoch 17 Batch 200: Train Loss = 1.5606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 17 Batch 200: Train Loss = 1.5606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:01:26,382 - INFO - Fold 2 Epoch 17 Batch 250: Train Loss = 1.6681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 17 Batch 250: Train Loss = 1.6681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:01:52,743 - INFO - Fold 2 Epoch 17 Batch 300: Train Loss = 2.6056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 17 Batch 300: Train Loss = 2.6056\n",
      "{'mad': 3.9386336345712976, 'mse': 28.935770697718596, 'mape': 487.9205187391343, 'kappa': 0.24378374525188218}\n",
      "Fold 2, mse = 28.9358, mad = 3.9386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:02:49,549 - INFO - Fold 2, mse = 28.9358, mad = 3.9386\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 18:02:50,100 - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 2.1967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 18 Batch 0: Train Loss = 2.1967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:03:16,444 - INFO - Fold 2 Epoch 18 Batch 50: Train Loss = 1.5011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 18 Batch 50: Train Loss = 1.5011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:03:42,610 - INFO - Fold 2 Epoch 18 Batch 100: Train Loss = 3.1797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 18 Batch 100: Train Loss = 3.1797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:04:08,709 - INFO - Fold 2 Epoch 18 Batch 150: Train Loss = 1.8683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 18 Batch 150: Train Loss = 1.8683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:04:34,955 - INFO - Fold 2 Epoch 18 Batch 200: Train Loss = 1.9059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 18 Batch 200: Train Loss = 1.9059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:05:01,186 - INFO - Fold 2 Epoch 18 Batch 250: Train Loss = 2.1808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 18 Batch 250: Train Loss = 2.1808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:05:27,480 - INFO - Fold 2 Epoch 18 Batch 300: Train Loss = 2.2133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 18 Batch 300: Train Loss = 2.2133\n",
      "{'mad': 3.839879485567237, 'mse': 28.9176472909918, 'mape': 439.4072585139189, 'kappa': 0.22423268582557876}\n",
      "Fold 2, mse = 28.9176, mad = 3.8399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:06:22,184 - INFO - Fold 2, mse = 28.9176, mad = 3.8399\n",
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "2023-11-08 18:06:22,775 - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 1.6087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 19 Batch 0: Train Loss = 1.6087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:06:49,646 - INFO - Fold 2 Epoch 19 Batch 50: Train Loss = 1.6084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 19 Batch 50: Train Loss = 1.6084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:07:15,862 - INFO - Fold 2 Epoch 19 Batch 100: Train Loss = 2.0418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 19 Batch 100: Train Loss = 2.0418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:07:42,026 - INFO - Fold 2 Epoch 19 Batch 150: Train Loss = 1.8572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 19 Batch 150: Train Loss = 1.8572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:08:08,632 - INFO - Fold 2 Epoch 19 Batch 200: Train Loss = 2.1665\n"
     ]
    }
   ],
   "source": [
    "if target_dataset == 'TJ':\n",
    "    n_splits = 5\n",
    "    epochs = 150\n",
    "elif target_dataset == 'HM':\n",
    "    n_splits = 3\n",
    "    epochs = 20\n",
    "\n",
    "transfer_flag = True\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "if target_dataset == 'TJ':    \n",
    "    data_str = 'covid'\n",
    "elif target_dataset == 'HM':\n",
    "    data_str = 'spain'\n",
    "\n",
    "if teacher_flag:\n",
    "    file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-rnn'\n",
    "else: \n",
    "    file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher' + '-rnn'\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "fold_count = 0\n",
    "total_train_loss = []\n",
    "total_valid_loss = []\n",
    "\n",
    "global_best = 10000\n",
    "mse = []\n",
    "mad = []\n",
    "mape = []\n",
    "auroc = []\n",
    "auprc = []\n",
    "kappa = []\n",
    "history = []\n",
    "\n",
    "pad_token = np.zeros(input_dim)\n",
    "# begin_time = time.time()\n",
    "\n",
    "for train, test in kfold.split(long_x, long_y_kfold):\n",
    "    train_x = [long_x[i] for i in train]\n",
    "    train_y = [long_time[i] for i in train]\n",
    "    train_outcome = [long_y[i] for i in train]\n",
    "    train_x_len = [all_x_len[i] for i in train]\n",
    "    #train_static = [long_static[i] for i in train]\n",
    "    \n",
    "    train_x, train_y, train_x_len, train_outcome = get_n2n_data(train_x, train_y, train_x_len, outcome=train_outcome)\n",
    "    if len(train_x) % 256 == 1:\n",
    "        print(len(train_x))\n",
    "        print('wrong squeeze!')\n",
    "        \n",
    "for train, test in kfold.split(long_x, long_y_kfold):\n",
    "# for test, train in kfold.split(long_x, long_y_kfold):\n",
    "    \n",
    "    model = distcare_target(input_dim = input_dim,output_dim=output_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, hidden_dim=hidden_dim).to(device)\n",
    "    \n",
    "    if transfer_flag:\n",
    "        checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:3\" if torch.cuda.is_available() == True else 'cpu'))\n",
    "        pretrain_dict = checkpoint['net']\n",
    "        model_dict = model.state_dict()\n",
    "        pretrain_dict = transfer_rnn_dict(pretrain_dict, model_dict)\n",
    "        model_dict.update(pretrain_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    fold_count += 1\n",
    "#     print(train)\n",
    "\n",
    "    \n",
    "    train_x = [long_x[i] for i in train]\n",
    "    train_y = [long_time[i] for i in train]\n",
    "    train_outcome = [long_y[i] for i in train]\n",
    "    train_x_len = [all_x_len[i] for i in train]\n",
    "    #train_static = [long_static[i] for i in train]\n",
    "    \n",
    "    train_x, train_y, train_x_len, train_outcome = get_n2n_data(train_x, train_y, train_x_len, outcome=train_outcome)\n",
    "    \n",
    "    test_x = [long_x[i] for i in test]\n",
    "    test_y = [long_time[i] for i in test]\n",
    "    test_outcome = [long_y[i] for i in test]\n",
    "    test_x_len = [all_x_len[i] for i in test]\n",
    "    #test_static = [long_static[i] for i in test]\n",
    "    \n",
    "    test_x, test_y, test_x_len, test_outcome = get_n2n_data(test_x, test_y, test_x_len, outcome=test_outcome)\n",
    "    \n",
    "    if not os.path.exists('./model/'+data_str):\n",
    "        os.mkdir('./model/'+data_str)\n",
    "        \n",
    "    if transfer_flag:\n",
    "        target_file_name = './model/'+data_str+'/distcare-trans-'+str(n_splits)+'-fold-LOS-regression' + str(fold_count)#4114\n",
    "    else:\n",
    "        target_file_name = './model/'+data_str+'/distcare-no-trans-'+str(n_splits)+'-fold-LOS-regression' + str(fold_count)#4114\n",
    "    \n",
    "    fold_train_loss = []\n",
    "    fold_valid_loss = []\n",
    "    best_mse = 10000\n",
    "    best_mad = 0\n",
    "    best_auroc = 0\n",
    "    beat_auprc = 0\n",
    "    best_mape = 0\n",
    "    best_kappa = 0\n",
    "    \n",
    "    for each_epoch in range(epochs):\n",
    "       \n",
    "        \n",
    "        epoch_loss = []\n",
    "        counter_batch = 0\n",
    "        model.train()  \n",
    "        \n",
    "        for step, (batch_x, batch_y, batch_lens, batch_outcome) in enumerate(ckd_batch_iter(train_x, train_y, train_x_len, batch_size, shuffle=True, outcome=train_outcome)):  \n",
    "            optimizer.zero_grad()\n",
    "            batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "            batch_outcome = torch.tensor(batch_outcome, dtype=torch.float32).to(device)\n",
    "\n",
    "            masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "            opt, decov_loss, emb, outcome = model(batch_x, batch_lens)\n",
    "\n",
    "#             MSE_Loss = get_re_loss(opt, batch_y.unsqueeze(-1))\n",
    "            pred_loss = get_target_multitask_loss(opt, batch_y.unsqueeze(-1), outcome, batch_outcome.unsqueeze(-1))\n",
    "\n",
    "#             model_loss = pred_loss + 1e7*decov_loss\n",
    "            model_loss = pred_loss\n",
    "\n",
    "            loss = model_loss\n",
    "\n",
    "            epoch_loss.append(pred_loss.cpu().detach().numpy())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % 50 == 0:\n",
    "                print('Fold %d Epoch %d Batch %d: Train Loss = %.4f'%(fold_count,each_epoch, step, loss.cpu().detach().numpy()))\n",
    "                logger.info('Fold %d Epoch %d Batch %d: Train Loss = %.4f'%(fold_count,each_epoch, step, loss.cpu().detach().numpy()))\n",
    "            \n",
    "        epoch_loss = np.mean(epoch_loss)\n",
    "        fold_train_loss.append(epoch_loss)\n",
    "\n",
    "        #Validation\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_pred_flatten = []\n",
    "        y_true_flatten = []\n",
    "        outcome_pred_flatten = []\n",
    "        outcome_true_flatten = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            valid_true = []\n",
    "            valid_pred = []\n",
    "            for batch_x, batch_y, batch_lens, batch_outcome in ckd_batch_iter(test_x, test_y, test_x_len, batch_size, outcome=test_outcome):\n",
    "                batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "                batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "                batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "                batch_outcome = torch.tensor(batch_outcome, dtype=torch.float32).to(device)\n",
    "                masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "               \n",
    "                opt, decov_loss, emb, outcome = model(batch_x, batch_lens)\n",
    "                \n",
    "#                 MSE_Loss = get_re_loss(opt, batch_y.unsqueeze(-1))\n",
    "                pred_loss = get_target_multitask_loss(opt, batch_y.unsqueeze(-1), outcome, batch_outcome.unsqueeze(-1))\n",
    "                \n",
    "                valid_loss.append(pred_loss.cpu().detach().numpy())\n",
    "\n",
    "                y_pred_flatten += [reverse_los(x, los_info) for x in list(opt.cpu().detach().numpy().flatten())]\n",
    "                y_true_flatten += [reverse_los(x, los_info) for x in list(batch_y.cpu().numpy().flatten())]\n",
    "                outcome_pred_flatten += list(outcome.cpu().detach().numpy().flatten())\n",
    "                outcome_true_flatten += list(batch_outcome.cpu().numpy().flatten())\n",
    "            \n",
    "\n",
    "            valid_loss = np.mean(valid_loss)\n",
    "            fold_valid_loss.append(valid_loss)\n",
    "            ret = metrics.print_metrics_regression(y_true_flatten, y_pred_flatten, verbose=0)\n",
    "            print(ret)\n",
    "            ret_outcome = metrics.print_metrics_binary(outcome_true_flatten, outcome_pred_flatten, verbose=0)\n",
    "            history.append((ret, ret_outcome))\n",
    "            #print()\n",
    "\n",
    "            if each_epoch % 10 == 0:\n",
    "                print('Fold %d, epoch %d: Loss = %.4f Valid loss = %.4f MSE = %.4f AUROC = %.4f' % (\n",
    "                    fold_count, each_epoch, fold_train_loss[-1], fold_valid_loss[-1], ret['mse'], ret_outcome['auroc']), flush=True)\n",
    "                logger.info('Fold %d, epoch %d: Loss = %.4f Valid loss = %.4f MSE = %.4f AUROC = %.4f' % (\n",
    "                    fold_count, each_epoch, fold_train_loss[-1], fold_valid_loss[-1], ret['mse'], ret_outcome['auroc']))\n",
    "                # metrics.print_metrics_regression(y_true_flatten, y_pred_flatten)\n",
    "                \n",
    "            cur_mse = ret['mse']\n",
    "            cur_auroc = ret_outcome['auroc']\n",
    "            if cur_mse < best_mse:\n",
    "                print('------------ Save FOLD-BEST model - MSE: %.4f ------------' % cur_mse, flush=True)\n",
    "                logger.info('------------ Save FOLD-BEST model - MSE: %.4f ------------' % cur_mse)\n",
    "                metrics.print_metrics_regression(y_true_flatten, y_pred_flatten)\n",
    "                best_mse = cur_mse\n",
    "                best_mad = ret['mad']\n",
    "                state = {\n",
    "                    'net': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': each_epoch\n",
    "                }\n",
    "                torch.save(state, target_file_name + '_' + str(fold_count))\n",
    "\n",
    "                if cur_mse < global_best:\n",
    "                    global_best = cur_mse\n",
    "                    state = {\n",
    "                        'net': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'epoch': each_epoch\n",
    "                    }\n",
    "                    torch.save(state, target_file_name)\n",
    "                    print('------------ Save best model - MSE: %.4f ------------' % cur_mse, flush=True)\n",
    "                    logger.info('------------ Save best model - MSE: %.4f ------------' % cur_mse)\n",
    "            if cur_auroc > best_auroc:\n",
    "                print('------------ auroc-BEST model - auroc: %.4f ------------' % cur_auroc, flush=True)\n",
    "                logger.info('------------ auroc-BEST model - auroc: %.4f ------------' % cur_auroc)\n",
    "                best_auroc = ret_outcome['auroc']\n",
    "                best_auprc = ret_outcome['auprc']\n",
    "\n",
    "        print('Fold %d, mse = %.4f, mad = %.4f' % (fold_count, ret['mse'], ret['mad']), flush=True)\n",
    "        logger.info('Fold %d, mse = %.4f, mad = %.4f' % (fold_count, ret['mse'], ret['mad']))\n",
    "\n",
    "    mse.append(best_mse)\n",
    "    mad.append(best_mad)\n",
    "    auroc.append(best_auroc)\n",
    "    auprc.append(best_auprc)\n",
    "    total_train_loss.append(fold_train_loss)\n",
    "    total_valid_loss.append(fold_valid_loss)\n",
    "\n",
    "\n",
    "print('mse %.4f(%.4f)' % (np.mean(mse), np.std(mse)))\n",
    "print('mad %.4f(%.4f)' % (np.mean(mad), np.std(mad)))\n",
    "print('auroc %.4f(%.4f)' % (np.mean(auroc), np.std(auroc)))\n",
    "print('auprc %.4f(%.4f)' % (np.mean(auprc), np.std(auprc)))\n",
    "logger.info('mse %.4f(%.4f)' % (np.mean(mse), np.std(mse)))\n",
    "logger.info('mad %.4f(%.4f)' % (np.mean(mad), np.std(mad)))\n",
    "logger.info('auroc %.4f(%.4f)' % (np.mean(auroc), np.std(auroc)))\n",
    "logger.info('auprc %.4f(%.4f)' % (np.mean(auprc), np.std(auprc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mse %.4f(%.4f)' % (np.mean(mse), np.std(mse)))\n",
    "print('mad %.4f(%.4f)' % (np.mean(mad), np.std(mad)))\n",
    "print('auroc %.4f(%.4f)' % (np.mean(auroc), np.std(auroc)))\n",
    "print('auprc %.4f(%.4f)' % (np.mean(auprc), np.std(auprc)))\n",
    "logger.info('mse %.4f(%.4f)' % (np.mean(mse), np.std(mse)))\n",
    "logger.info('mad %.4f(%.4f)' % (np.mean(mad), np.std(mad)))\n",
    "logger.info('auroc %.4f(%.4f)' % (np.mean(auroc), np.std(auroc)))\n",
    "logger.info('auprc %.4f(%.4f)' % (np.mean(auprc), np.std(auprc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
