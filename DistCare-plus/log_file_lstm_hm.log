2023-11-08 00:35:11,904 - __main__ - INFO - 这是希望输出的info内容
2023-11-08 00:35:11,905 - __main__ - WARNING - 这是希望输出的warning内容
2023-11-08 00:35:55,327 - __main__ - INFO - 这是希望输出的info内容
2023-11-08 00:35:55,328 - __main__ - WARNING - 这是希望输出的warning内容
2023-11-08 00:36:20,643 - __main__ - INFO - [[-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 1.2605692444279462, 0.585371321489137, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.9546043520029015, 0.2987563180527675, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 1.0769903089729194, 0.4420638197709522, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.566534136852991, 1.803485086093707, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, 0.46506052412282983, -0.05951243624269434, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, 0.281481588667803, -0.2744736888199714, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.8934113735178925, 0.9436400757845987, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592]]
2023-11-08 00:36:20,660 - __main__ - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
2023-11-08 00:36:20,661 - __main__ - INFO - 110609
2023-11-08 00:36:20,662 - __main__ - INFO - [[-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8962118618028821, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8617355345389544, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8272592072750267, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7927828800110989, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7583065527471711, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7238302254832434, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6893538982193156, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6548775709553878, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.62040124369146, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5859249164275323, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5514485891636045, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5169722618996767, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.48249593463574897, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.4480196073718212, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.41354328010789343, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3790669528439657, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3445906255800379, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.31011429831611015, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.27563797105218235, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.2411616437882546, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.20668531652432684, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.17220898926039907, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.1377326619964713, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.10325633473254354, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.06878000746861578, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.034303680204688006, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.0001726470592397616, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.034648974323167527, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.06912530158709529, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.10360162885102306, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.13807795611495083, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.1725542833788786, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.20703061064280637, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.24150693790673414, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.2759832651706619, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.31045959243458965, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.34493591969851745, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.3794122469624452, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.413888574226373, '0']]
2023-11-08 00:36:41,436 - __main__ - INFO - [[-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.1311661871017867, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, -0.14686926072725967, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 0.585371321489137, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 1.2605692444279462, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.2987563180527675, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.9546043520029015, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 0.4420638197709522, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 1.0769903089729194, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.803485086093707, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 1.566534136852991, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.281481588667803, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.9436400757845987, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.8934113735178925, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, -0.05951243624269434, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.46506052412282983, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.22028861018279403, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.22028861018279403, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.22028861018279403, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, -0.2744736888199714, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 0.22028861018279403, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.5610886922563408, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, -0.26925521769727756, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.5610886922563408, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, -0.26925521769727756, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 0.585371321489137, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797, 1.0157973304879104, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 0.585371321489137, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.0157973304879104, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.08379506547549039, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.6486394595778567, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.08379506547549039, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.6486394595778567, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.08379506547549039, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.6486394595778567, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.08379506547549039, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.6486394595778567, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.373562580939153, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.627727115338, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.373562580939153, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.627727115338, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.373562580939153, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.627727115338, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.373562580939153, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.627727115338, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.2271025671936751, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.8934113735178925, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.2271025671936751, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.8934113735178925, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.2271025671936751, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.8934113735178925, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.2271025671936751, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 0.8934113735178925, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.6601775843755224, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.1993762659429372, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.6601775843755224, -0.01724690479013476, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592, 1.1993762659429372, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844]]
2023-11-08 00:36:41,442 - __main__ - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
2023-11-08 00:36:41,445 - __main__ - INFO - 34
2023-11-08 00:36:43,672 - __main__ - INFO - 32269
2023-11-08 00:36:43,674 - __main__ - INFO - 4034
2023-11-08 00:36:43,676 - __main__ - INFO - 4033
2023-11-08 00:36:51,080 - __main__ - INFO - Training Teacher
2023-11-08 00:36:52,303 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 0.6813
2023-11-08 00:37:04,754 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 0.4996
2023-11-08 00:37:16,828 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 0.2391
2023-11-08 00:37:29,012 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 0.2436
2023-11-08 00:37:41,647 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 0.2711
2023-11-08 00:37:53,295 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 0.3009
2023-11-08 00:38:04,450 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 0.2886
2023-11-08 00:38:13,764 - __main__ - INFO - Epoch 0: Loss = 0.3316 Valid loss = 0.2416 roc = 0.7973
2023-11-08 00:38:14,630 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 0.1587
2023-11-08 00:38:27,109 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 0.2500
2023-11-08 00:38:39,402 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 0.2628
2023-11-08 00:38:51,481 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 0.2361
2023-11-08 00:39:03,444 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 0.2415
2023-11-08 00:39:16,300 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 0.2698
2023-11-08 00:39:29,389 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 0.2120
2023-11-08 00:39:40,192 - __main__ - INFO - Epoch 1: Loss = 0.2466 Valid loss = 0.2295 roc = 0.8193
2023-11-08 00:39:41,015 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 0.1787
2023-11-08 00:39:54,573 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 0.2387
2023-11-08 00:40:08,090 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 0.3349
2023-11-08 00:40:21,977 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 0.2058
2023-11-08 00:40:37,327 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 0.2697
2023-11-08 00:40:53,287 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 0.2649
2023-11-08 00:41:08,407 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 0.2103
2023-11-08 00:41:19,172 - __main__ - INFO - Epoch 2: Loss = 0.2369 Valid loss = 0.2223 roc = 0.8333
2023-11-08 00:41:19,970 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 0.2598
2023-11-08 00:41:34,010 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 0.3022
2023-11-08 00:41:48,388 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 0.2363
2023-11-08 00:42:01,614 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 0.2516
2023-11-08 00:42:17,684 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 0.2431
2023-11-08 00:42:32,583 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 0.2539
2023-11-08 00:42:47,141 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 0.1724
2023-11-08 00:42:59,415 - __main__ - INFO - Epoch 3: Loss = 0.2368 Valid loss = 0.2139 roc = 0.8309
2023-11-08 00:43:00,301 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 0.2104
2023-11-08 00:43:15,401 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 0.2593
2023-11-08 00:43:29,813 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 0.2162
2023-11-08 00:43:45,308 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 0.2661
2023-11-08 00:44:00,588 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 0.2421
2023-11-08 00:44:14,985 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 0.2270
2023-11-08 00:44:28,661 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 0.2610
2023-11-08 00:44:39,236 - __main__ - INFO - Epoch 4: Loss = 0.2307 Valid loss = 0.2081 roc = 0.8434
2023-11-08 00:44:40,247 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 0.2602
2023-11-08 00:44:54,888 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 0.2203
2023-11-08 00:45:09,804 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 0.2629
2023-11-08 00:45:24,064 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 0.2454
2023-11-08 00:45:38,946 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 0.2088
2023-11-08 00:45:53,484 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 0.2333
2023-11-08 00:46:08,185 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 0.2057
2023-11-08 00:46:20,190 - __main__ - INFO - Epoch 5: Loss = 0.2239 Valid loss = 0.2018 roc = 0.8560
2023-11-08 00:46:21,032 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 0.2620
2023-11-08 00:46:36,385 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 0.2416
2023-11-08 00:46:50,882 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 0.2620
2023-11-08 00:47:06,249 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 0.2191
2023-11-08 00:47:21,605 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 0.3565
2023-11-08 00:47:36,544 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 0.2302
2023-11-08 00:47:52,192 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 0.2092
2023-11-08 00:48:03,712 - __main__ - INFO - Epoch 6: Loss = 0.2228 Valid loss = 0.1958 roc = 0.8663
2023-11-08 00:48:04,524 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 0.2529
2023-11-08 00:48:19,114 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 0.2137
2023-11-08 00:48:34,239 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 0.2317
2023-11-08 00:48:48,842 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 0.2209
2023-11-08 00:49:03,719 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 0.1781
2023-11-08 00:49:19,458 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 0.2377
2023-11-08 00:49:34,025 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 0.2419
2023-11-08 00:49:46,026 - __main__ - INFO - Epoch 7: Loss = 0.2164 Valid loss = 0.1961 roc = 0.8730
2023-11-08 00:49:46,786 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 0.2213
2023-11-08 00:50:02,721 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 0.1645
2023-11-08 00:50:17,767 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 0.1742
2023-11-08 00:50:32,725 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 0.1668
2023-11-08 00:50:46,377 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 0.1919
2023-11-08 00:51:01,276 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 0.2367
2023-11-08 00:51:15,759 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 0.2395
2023-11-08 00:51:26,868 - __main__ - INFO - Epoch 8: Loss = 0.2164 Valid loss = 0.1935 roc = 0.8717
2023-11-08 00:51:27,479 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 0.2314
2023-11-08 00:51:42,417 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 0.2536
2023-11-08 00:51:56,000 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 0.2456
2023-11-08 00:52:11,463 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 0.2265
2023-11-08 00:52:26,814 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 0.1959
2023-11-08 00:52:40,988 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 0.1794
2023-11-08 00:52:55,928 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 0.1425
2023-11-08 00:53:07,961 - __main__ - INFO - Epoch 9: Loss = 0.2126 Valid loss = 0.1956 roc = 0.8780
2023-11-08 00:53:08,953 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 0.1662
2023-11-08 00:53:24,456 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 0.2807
2023-11-08 00:53:39,762 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 0.1428
2023-11-08 00:53:55,384 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 0.2732
2023-11-08 00:54:09,639 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 0.2626
2023-11-08 00:54:23,248 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 0.1620
2023-11-08 00:54:38,102 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 0.1900
2023-11-08 00:54:48,736 - __main__ - INFO - Epoch 10: Loss = 0.2076 Valid loss = 0.1880 roc = 0.8886
2023-11-08 00:54:49,456 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 0.1515
2023-11-08 00:55:03,649 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 0.2466
2023-11-08 00:55:18,098 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 0.2474
2023-11-08 00:55:33,689 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 0.1776
2023-11-08 00:55:48,668 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 0.2323
2023-11-08 00:56:04,553 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 0.2416
2023-11-08 00:56:20,043 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 0.2024
2023-11-08 00:56:30,844 - __main__ - INFO - Epoch 11: Loss = 0.2044 Valid loss = 0.1822 roc = 0.8864
2023-11-08 00:56:31,616 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 0.3055
2023-11-08 00:56:46,311 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 0.1874
2023-11-08 00:57:01,043 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 0.2034
2023-11-08 00:57:15,020 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 0.2310
2023-11-08 00:57:29,137 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 0.1541
2023-11-08 00:57:43,751 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 0.2428
2023-11-08 00:57:57,702 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 0.2037
2023-11-08 00:58:09,182 - __main__ - INFO - Epoch 12: Loss = 0.2038 Valid loss = 0.1772 roc = 0.8961
2023-11-08 00:58:09,838 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 0.1963
2023-11-08 00:58:24,026 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 0.2227
2023-11-08 00:58:38,149 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 0.1862
2023-11-08 00:58:53,201 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 0.1864
2023-11-08 00:59:08,193 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 0.2047
2023-11-08 00:59:23,082 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 0.2308
2023-11-08 00:59:37,190 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 0.2057
2023-11-08 00:59:48,774 - __main__ - INFO - Epoch 13: Loss = 0.2003 Valid loss = 0.1701 roc = 0.9045
2023-11-08 00:59:49,603 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 0.1775
2023-11-08 01:00:03,992 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 0.2620
2023-11-08 01:00:18,398 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 0.1781
2023-11-08 01:00:34,125 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 0.2123
2023-11-08 01:00:48,996 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 0.1761
2023-11-08 01:01:04,475 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 0.1648
2023-11-08 01:01:18,078 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 0.1437
2023-11-08 01:01:29,390 - __main__ - INFO - Epoch 14: Loss = 0.1919 Valid loss = 0.1669 roc = 0.9021
2023-11-08 01:01:29,948 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 0.2699
2023-11-08 01:01:45,324 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 0.2714
2023-11-08 01:01:59,778 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 0.1896
2023-11-08 01:02:14,617 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 0.1912
2023-11-08 01:02:30,034 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 0.1477
2023-11-08 01:02:44,567 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 0.2392
2023-11-08 01:02:59,104 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 0.1414
2023-11-08 01:03:09,215 - __main__ - INFO - Epoch 15: Loss = 0.1911 Valid loss = 0.1694 roc = 0.9029
2023-11-08 01:03:09,967 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 0.2132
2023-11-08 01:03:24,854 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 0.1817
2023-11-08 01:03:39,608 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 0.1536
2023-11-08 01:03:53,920 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 0.1933
2023-11-08 01:04:08,934 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 0.2024
2023-11-08 01:04:23,392 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 0.1443
2023-11-08 01:04:38,588 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 0.1841
2023-11-08 01:04:50,139 - __main__ - INFO - Epoch 16: Loss = 0.1905 Valid loss = 0.1621 roc = 0.8941
2023-11-08 01:04:50,772 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 0.1681
2023-11-08 01:05:05,735 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 0.1667
2023-11-08 01:05:19,919 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 0.1766
2023-11-08 01:05:35,165 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 0.1330
2023-11-08 01:05:47,971 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 0.1201
2023-11-08 01:06:02,860 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 0.2131
2023-11-08 01:06:17,515 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 0.2042
2023-11-08 01:06:28,842 - __main__ - INFO - Epoch 17: Loss = 0.1858 Valid loss = 0.1607 roc = 0.8988
2023-11-08 01:06:29,419 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 0.2146
2023-11-08 01:06:42,725 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 0.2633
2023-11-08 01:06:57,660 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 0.1391
2023-11-08 01:07:12,188 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 0.1997
2023-11-08 01:07:26,376 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 0.1372
2023-11-08 01:07:41,919 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 0.1602
2023-11-08 01:07:55,844 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 0.1417
2023-11-08 01:08:07,282 - __main__ - INFO - Epoch 18: Loss = 0.1826 Valid loss = 0.1627 roc = 0.9037
2023-11-08 01:08:07,865 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 0.1628
2023-11-08 01:08:22,517 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 0.2008
2023-11-08 01:08:36,727 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 0.2523
2023-11-08 01:08:51,281 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 0.1959
2023-11-08 01:09:06,532 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 0.1751
2023-11-08 01:09:20,826 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 0.1933
2023-11-08 01:09:36,165 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 0.1926
2023-11-08 01:09:46,964 - __main__ - INFO - Epoch 19: Loss = 0.1784 Valid loss = 0.1560 roc = 0.9009
2023-11-08 01:09:47,970 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 0.1764
2023-11-08 01:10:02,084 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 0.1961
2023-11-08 01:10:15,925 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 0.1254
2023-11-08 01:10:31,188 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 0.2290
2023-11-08 01:10:46,403 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 0.1908
2023-11-08 01:11:01,011 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 0.1621
2023-11-08 01:11:15,588 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 0.1220
2023-11-08 01:11:26,647 - __main__ - INFO - Epoch 20: Loss = 0.1782 Valid loss = 0.1574 roc = 0.9060
2023-11-08 01:11:27,400 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 0.1446
2023-11-08 01:11:42,171 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 0.2081
2023-11-08 01:11:57,069 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 0.1321
2023-11-08 01:12:13,208 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 0.1672
2023-11-08 01:12:28,765 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 0.1932
2023-11-08 01:12:45,087 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 0.1433
2023-11-08 01:13:00,801 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 0.2495
2023-11-08 01:13:13,003 - __main__ - INFO - Epoch 21: Loss = 0.1733 Valid loss = 0.1516 roc = 0.9060
2023-11-08 01:13:13,707 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 0.2973
2023-11-08 01:13:29,768 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 0.1746
2023-11-08 01:13:46,929 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 0.1295
2023-11-08 01:14:03,133 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 0.1629
2023-11-08 01:14:19,242 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 0.1651
2023-11-08 01:14:34,035 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 0.1430
2023-11-08 01:14:48,137 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 0.1971
2023-11-08 01:15:01,297 - __main__ - INFO - Epoch 22: Loss = 0.1722 Valid loss = 0.1507 roc = 0.9083
2023-11-08 01:15:02,181 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 0.2005
2023-11-08 01:15:20,363 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 0.1812
2023-11-08 01:15:37,568 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 0.1536
2023-11-08 01:15:53,615 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 0.1578
2023-11-08 01:16:09,264 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 0.2268
2023-11-08 01:16:23,073 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 0.2172
2023-11-08 01:16:37,711 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 0.1465
2023-11-08 01:16:49,544 - __main__ - INFO - Epoch 23: Loss = 0.1751 Valid loss = 0.1543 roc = 0.9096
2023-11-08 01:16:50,339 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 0.1726
2023-11-08 01:17:04,748 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 0.1488
2023-11-08 01:17:18,301 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 0.1246
2023-11-08 01:17:33,722 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 0.2234
2023-11-08 01:17:48,949 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 0.2004
2023-11-08 01:18:05,105 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 0.2142
2023-11-08 01:18:21,281 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 0.1746
2023-11-08 01:18:34,158 - __main__ - INFO - Epoch 24: Loss = 0.1677 Valid loss = 0.1498 roc = 0.9091
2023-11-08 01:18:35,167 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 0.1723
2023-11-08 01:18:51,721 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 0.1619
2023-11-08 01:19:06,994 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 0.1890
2023-11-08 01:19:23,827 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 0.1560
2023-11-08 01:19:41,491 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 0.1800
2023-11-08 01:19:57,782 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 0.2060
2023-11-08 01:20:13,523 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 0.1290
2023-11-08 01:20:26,014 - __main__ - INFO - Epoch 25: Loss = 0.1699 Valid loss = 0.1506 roc = 0.9096
2023-11-08 01:20:26,928 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 0.1538
2023-11-08 01:20:43,667 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 0.1420
2023-11-08 01:20:59,868 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 0.1490
2023-11-08 01:21:16,304 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 0.1683
2023-11-08 01:21:32,660 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 0.1916
2023-11-08 01:21:47,465 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 0.1890
2023-11-08 01:22:03,800 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 0.2118
2023-11-08 01:22:15,043 - __main__ - INFO - Epoch 26: Loss = 0.1711 Valid loss = 0.1501 roc = 0.9042
2023-11-08 01:22:15,710 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 0.1639
2023-11-08 01:22:29,886 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 0.1176
2023-11-08 01:22:44,304 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 0.1679
2023-11-08 01:22:59,191 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 0.1301
2023-11-08 01:23:13,575 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 0.1835
2023-11-08 01:23:28,696 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 0.2067
2023-11-08 01:23:42,400 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 0.1640
2023-11-08 01:23:52,713 - __main__ - INFO - Epoch 27: Loss = 0.1728 Valid loss = 0.1528 roc = 0.9131
2023-11-08 01:23:53,731 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 0.2114
2023-11-08 01:24:08,306 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 0.1053
2023-11-08 01:24:21,842 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 0.2104
2023-11-08 01:24:35,781 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 0.1736
2023-11-08 01:24:51,491 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 0.2639
2023-11-08 01:25:06,953 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 0.1976
2023-11-08 01:25:23,690 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 0.1467
2023-11-08 01:25:35,866 - __main__ - INFO - Epoch 28: Loss = 0.1713 Valid loss = 0.1483 roc = 0.9108
2023-11-08 01:25:36,584 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 0.1605
2023-11-08 01:25:50,938 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 0.2235
2023-11-08 01:26:06,613 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 0.2482
2023-11-08 01:26:21,389 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 0.1782
2023-11-08 01:26:37,202 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 0.1941
2023-11-08 01:26:51,211 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 0.1270
2023-11-08 01:27:05,491 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 0.1514
2023-11-08 01:27:16,179 - __main__ - INFO - Epoch 29: Loss = 0.1713 Valid loss = 0.1486 roc = 0.9112
2023-11-08 01:27:16,897 - __main__ - INFO - Epoch 30 Batch 0: Train Loss = 0.1852
2023-11-08 01:27:31,557 - __main__ - INFO - Epoch 30 Batch 20: Train Loss = 0.1837
2023-11-08 01:27:45,783 - __main__ - INFO - Epoch 30 Batch 40: Train Loss = 0.1579
2023-11-08 01:28:00,076 - __main__ - INFO - Epoch 30 Batch 60: Train Loss = 0.1164
2023-11-08 01:28:15,353 - __main__ - INFO - Epoch 30 Batch 80: Train Loss = 0.1406
2023-11-08 01:28:30,139 - __main__ - INFO - Epoch 30 Batch 100: Train Loss = 0.1961
2023-11-08 01:28:45,755 - __main__ - INFO - Epoch 30 Batch 120: Train Loss = 0.1750
2023-11-08 01:28:56,633 - __main__ - INFO - Epoch 30: Loss = 0.1662 Valid loss = 0.1484 roc = 0.9109
2023-11-08 01:28:57,143 - __main__ - INFO - Epoch 31 Batch 0: Train Loss = 0.2005
2023-11-08 01:29:11,602 - __main__ - INFO - Epoch 31 Batch 20: Train Loss = 0.1663
2023-11-08 01:29:26,881 - __main__ - INFO - Epoch 31 Batch 40: Train Loss = 0.1964
2023-11-08 01:29:42,015 - __main__ - INFO - Epoch 31 Batch 60: Train Loss = 0.1726
2023-11-08 01:29:57,629 - __main__ - INFO - Epoch 31 Batch 80: Train Loss = 0.1363
2023-11-08 01:30:13,606 - __main__ - INFO - Epoch 31 Batch 100: Train Loss = 0.2059
2023-11-08 01:30:30,549 - __main__ - INFO - Epoch 31 Batch 120: Train Loss = 0.0971
2023-11-08 01:30:42,598 - __main__ - INFO - Epoch 31: Loss = 0.1686 Valid loss = 0.1477 roc = 0.9137
2023-11-08 01:30:43,340 - __main__ - INFO - Epoch 32 Batch 0: Train Loss = 0.1185
2023-11-08 01:30:59,084 - __main__ - INFO - Epoch 32 Batch 20: Train Loss = 0.1523
2023-11-08 01:31:15,907 - __main__ - INFO - Epoch 32 Batch 40: Train Loss = 0.1501
2023-11-08 01:31:32,043 - __main__ - INFO - Epoch 32 Batch 60: Train Loss = 0.2018
2023-11-08 01:31:49,376 - __main__ - INFO - Epoch 32 Batch 80: Train Loss = 0.1945
2023-11-08 01:32:07,343 - __main__ - INFO - Epoch 32 Batch 100: Train Loss = 0.1453
2023-11-08 01:32:25,057 - __main__ - INFO - Epoch 32 Batch 120: Train Loss = 0.1558
2023-11-08 01:32:37,987 - __main__ - INFO - Epoch 32: Loss = 0.1632 Valid loss = 0.1468 roc = 0.9081
2023-11-08 01:32:38,741 - __main__ - INFO - Epoch 33 Batch 0: Train Loss = 0.0974
2023-11-08 01:32:57,612 - __main__ - INFO - Epoch 33 Batch 20: Train Loss = 0.1649
2023-11-08 01:33:14,925 - __main__ - INFO - Epoch 33 Batch 40: Train Loss = 0.1341
2023-11-08 01:33:33,465 - __main__ - INFO - Epoch 33 Batch 60: Train Loss = 0.2202
2023-11-08 01:33:51,202 - __main__ - INFO - Epoch 33 Batch 80: Train Loss = 0.1558
2023-11-08 01:34:08,937 - __main__ - INFO - Epoch 33 Batch 100: Train Loss = 0.1204
2023-11-08 01:34:27,148 - __main__ - INFO - Epoch 33 Batch 120: Train Loss = 0.1495
2023-11-08 01:34:39,891 - __main__ - INFO - Epoch 33: Loss = 0.1706 Valid loss = 0.1472 roc = 0.9105
2023-11-08 01:34:40,870 - __main__ - INFO - Epoch 34 Batch 0: Train Loss = 0.1231
2023-11-08 01:34:58,676 - __main__ - INFO - Epoch 34 Batch 20: Train Loss = 0.2652
2023-11-08 01:35:16,332 - __main__ - INFO - Epoch 34 Batch 40: Train Loss = 0.0971
2023-11-08 01:35:34,959 - __main__ - INFO - Epoch 34 Batch 60: Train Loss = 0.1580
2023-11-08 01:35:52,723 - __main__ - INFO - Epoch 34 Batch 80: Train Loss = 0.1974
2023-11-08 01:36:09,756 - __main__ - INFO - Epoch 34 Batch 100: Train Loss = 0.1402
2023-11-08 01:36:27,430 - __main__ - INFO - Epoch 34 Batch 120: Train Loss = 0.1439
2023-11-08 01:36:40,499 - __main__ - INFO - Epoch 34: Loss = 0.1643 Valid loss = 0.1472 roc = 0.9077
2023-11-08 01:36:41,686 - __main__ - INFO - Epoch 35 Batch 0: Train Loss = 0.1633
2023-11-08 01:36:57,522 - __main__ - INFO - Epoch 35 Batch 20: Train Loss = 0.2089
2023-11-08 01:37:15,027 - __main__ - INFO - Epoch 35 Batch 40: Train Loss = 0.2192
2023-11-08 01:37:32,580 - __main__ - INFO - Epoch 35 Batch 60: Train Loss = 0.1490
2023-11-08 01:37:49,972 - __main__ - INFO - Epoch 35 Batch 80: Train Loss = 0.1298
2023-11-08 01:38:06,542 - __main__ - INFO - Epoch 35 Batch 100: Train Loss = 0.1625
2023-11-08 01:38:24,810 - __main__ - INFO - Epoch 35 Batch 120: Train Loss = 0.1729
2023-11-08 01:38:38,426 - __main__ - INFO - Epoch 35: Loss = 0.1648 Valid loss = 0.1458 roc = 0.9085
2023-11-08 01:38:39,447 - __main__ - INFO - Epoch 36 Batch 0: Train Loss = 0.1027
2023-11-08 01:38:56,505 - __main__ - INFO - Epoch 36 Batch 20: Train Loss = 0.2449
2023-11-08 01:39:14,134 - __main__ - INFO - Epoch 36 Batch 40: Train Loss = 0.1547
2023-11-08 01:39:30,258 - __main__ - INFO - Epoch 36 Batch 60: Train Loss = 0.2031
2023-11-08 01:39:47,935 - __main__ - INFO - Epoch 36 Batch 80: Train Loss = 0.2643
2023-11-08 01:40:05,871 - __main__ - INFO - Epoch 36 Batch 100: Train Loss = 0.1516
2023-11-08 01:40:23,300 - __main__ - INFO - Epoch 36 Batch 120: Train Loss = 0.2164
2023-11-08 01:40:36,767 - __main__ - INFO - Epoch 36: Loss = 0.1645 Valid loss = 0.1453 roc = 0.9109
2023-11-08 01:40:37,577 - __main__ - INFO - Epoch 37 Batch 0: Train Loss = 0.1498
2023-11-08 01:40:55,532 - __main__ - INFO - Epoch 37 Batch 20: Train Loss = 0.1587
2023-11-08 01:41:11,627 - __main__ - INFO - Epoch 37 Batch 40: Train Loss = 0.1278
2023-11-08 01:41:28,927 - __main__ - INFO - Epoch 37 Batch 60: Train Loss = 0.1980
2023-11-08 01:41:46,304 - __main__ - INFO - Epoch 37 Batch 80: Train Loss = 0.2509
2023-11-08 01:42:04,143 - __main__ - INFO - Epoch 37 Batch 100: Train Loss = 0.1127
2023-11-08 01:42:23,259 - __main__ - INFO - Epoch 37 Batch 120: Train Loss = 0.1701
2023-11-08 01:42:36,527 - __main__ - INFO - Epoch 37: Loss = 0.1634 Valid loss = 0.1466 roc = 0.9098
2023-11-08 01:42:37,434 - __main__ - INFO - Epoch 38 Batch 0: Train Loss = 0.1994
2023-11-08 01:42:53,756 - __main__ - INFO - Epoch 38 Batch 20: Train Loss = 0.2214
2023-11-08 01:43:11,021 - __main__ - INFO - Epoch 38 Batch 40: Train Loss = 0.1265
2023-11-08 01:43:28,124 - __main__ - INFO - Epoch 38 Batch 60: Train Loss = 0.1721
2023-11-08 01:43:45,692 - __main__ - INFO - Epoch 38 Batch 80: Train Loss = 0.1433
2023-11-08 01:44:02,169 - __main__ - INFO - Epoch 38 Batch 100: Train Loss = 0.1781
2023-11-08 01:44:20,714 - __main__ - INFO - Epoch 38 Batch 120: Train Loss = 0.1509
2023-11-08 01:44:34,974 - __main__ - INFO - Epoch 38: Loss = 0.1713 Valid loss = 0.1455 roc = 0.9106
2023-11-08 01:44:35,836 - __main__ - INFO - Epoch 39 Batch 0: Train Loss = 0.1894
2023-11-08 01:44:53,087 - __main__ - INFO - Epoch 39 Batch 20: Train Loss = 0.1567
2023-11-08 01:45:10,724 - __main__ - INFO - Epoch 39 Batch 40: Train Loss = 0.1710
2023-11-08 01:45:28,374 - __main__ - INFO - Epoch 39 Batch 60: Train Loss = 0.1413
2023-11-08 01:45:46,142 - __main__ - INFO - Epoch 39 Batch 80: Train Loss = 0.1605
2023-11-08 01:46:03,192 - __main__ - INFO - Epoch 39 Batch 100: Train Loss = 0.1680
2023-11-08 01:46:21,858 - __main__ - INFO - Epoch 39 Batch 120: Train Loss = 0.1868
2023-11-08 01:46:35,063 - __main__ - INFO - Epoch 39: Loss = 0.1640 Valid loss = 0.1491 roc = 0.9047
2023-11-08 01:46:36,027 - __main__ - INFO - Epoch 40 Batch 0: Train Loss = 0.1629
2023-11-08 01:46:52,232 - __main__ - INFO - Epoch 40 Batch 20: Train Loss = 0.1158
2023-11-08 01:47:09,478 - __main__ - INFO - Epoch 40 Batch 40: Train Loss = 0.1287
2023-11-08 01:47:26,614 - __main__ - INFO - Epoch 40 Batch 60: Train Loss = 0.1822
2023-11-08 01:47:43,838 - __main__ - INFO - Epoch 40 Batch 80: Train Loss = 0.1190
2023-11-08 01:48:00,384 - __main__ - INFO - Epoch 40 Batch 100: Train Loss = 0.1351
2023-11-08 01:48:18,837 - __main__ - INFO - Epoch 40 Batch 120: Train Loss = 0.1574
2023-11-08 01:48:33,217 - __main__ - INFO - Epoch 40: Loss = 0.1668 Valid loss = 0.1467 roc = 0.9126
2023-11-08 01:48:34,041 - __main__ - INFO - Epoch 41 Batch 0: Train Loss = 0.1874
2023-11-08 01:48:51,952 - __main__ - INFO - Epoch 41 Batch 20: Train Loss = 0.1921
2023-11-08 01:49:09,900 - __main__ - INFO - Epoch 41 Batch 40: Train Loss = 0.1004
2023-11-08 01:49:27,590 - __main__ - INFO - Epoch 41 Batch 60: Train Loss = 0.1823
2023-11-08 01:49:44,669 - __main__ - INFO - Epoch 41 Batch 80: Train Loss = 0.1875
2023-11-08 01:50:00,260 - __main__ - INFO - Epoch 41 Batch 100: Train Loss = 0.1285
2023-11-08 01:50:18,622 - __main__ - INFO - Epoch 41 Batch 120: Train Loss = 0.1157
2023-11-08 01:50:31,402 - __main__ - INFO - Epoch 41: Loss = 0.1634 Valid loss = 0.1438 roc = 0.9164
2023-11-08 01:50:32,608 - __main__ - INFO - Epoch 42 Batch 0: Train Loss = 0.1929
2023-11-08 01:50:50,496 - __main__ - INFO - Epoch 42 Batch 20: Train Loss = 0.1917
2023-11-08 01:51:09,360 - __main__ - INFO - Epoch 42 Batch 40: Train Loss = 0.1379
2023-11-08 01:51:27,135 - __main__ - INFO - Epoch 42 Batch 60: Train Loss = 0.2048
2023-11-08 01:51:44,757 - __main__ - INFO - Epoch 42 Batch 80: Train Loss = 0.1837
2023-11-08 01:52:02,007 - __main__ - INFO - Epoch 42 Batch 100: Train Loss = 0.1511
2023-11-08 01:52:19,945 - __main__ - INFO - Epoch 42 Batch 120: Train Loss = 0.2094
2023-11-08 01:52:32,818 - __main__ - INFO - Epoch 42: Loss = 0.1623 Valid loss = 0.1451 roc = 0.9111
2023-11-08 01:52:33,737 - __main__ - INFO - Epoch 43 Batch 0: Train Loss = 0.1603
2023-11-08 01:52:50,541 - __main__ - INFO - Epoch 43 Batch 20: Train Loss = 0.1926
2023-11-08 01:53:07,222 - __main__ - INFO - Epoch 43 Batch 40: Train Loss = 0.1551
2023-11-08 01:53:25,106 - __main__ - INFO - Epoch 43 Batch 60: Train Loss = 0.1577
2023-11-08 01:53:42,729 - __main__ - INFO - Epoch 43 Batch 80: Train Loss = 0.1148
2023-11-08 01:54:00,338 - __main__ - INFO - Epoch 43 Batch 100: Train Loss = 0.1246
2023-11-08 01:54:19,535 - __main__ - INFO - Epoch 43 Batch 120: Train Loss = 0.1830
2023-11-08 01:54:31,732 - __main__ - INFO - Epoch 43: Loss = 0.1647 Valid loss = 0.1423 roc = 0.9173
2023-11-08 01:54:32,714 - __main__ - INFO - Epoch 44 Batch 0: Train Loss = 0.1991
2023-11-08 01:54:49,760 - __main__ - INFO - Epoch 44 Batch 20: Train Loss = 0.1669
2023-11-08 01:55:09,122 - __main__ - INFO - Epoch 44 Batch 40: Train Loss = 0.1345
2023-11-08 01:55:26,807 - __main__ - INFO - Epoch 44 Batch 60: Train Loss = 0.2167
2023-11-08 01:55:42,757 - __main__ - INFO - Epoch 44 Batch 80: Train Loss = 0.1161
2023-11-08 01:55:59,840 - __main__ - INFO - Epoch 44 Batch 100: Train Loss = 0.1243
2023-11-08 01:56:17,944 - __main__ - INFO - Epoch 44 Batch 120: Train Loss = 0.1564
2023-11-08 01:56:31,184 - __main__ - INFO - Epoch 44: Loss = 0.1605 Valid loss = 0.1421 roc = 0.9189
2023-11-08 01:56:31,930 - __main__ - INFO - Epoch 45 Batch 0: Train Loss = 0.1258
2023-11-08 01:56:50,475 - __main__ - INFO - Epoch 45 Batch 20: Train Loss = 0.1568
2023-11-08 01:57:08,292 - __main__ - INFO - Epoch 45 Batch 40: Train Loss = 0.1455
2023-11-08 01:57:25,424 - __main__ - INFO - Epoch 45 Batch 60: Train Loss = 0.1464
2023-11-08 01:57:42,586 - __main__ - INFO - Epoch 45 Batch 80: Train Loss = 0.1610
2023-11-08 01:57:59,603 - __main__ - INFO - Epoch 45 Batch 100: Train Loss = 0.1643
2023-11-08 01:58:17,623 - __main__ - INFO - Epoch 45 Batch 120: Train Loss = 0.1352
2023-11-08 01:58:30,858 - __main__ - INFO - Epoch 45: Loss = 0.1615 Valid loss = 0.1421 roc = 0.9189
2023-11-08 01:58:31,890 - __main__ - INFO - Epoch 46 Batch 0: Train Loss = 0.2124
2023-11-08 01:58:49,315 - __main__ - INFO - Epoch 46 Batch 20: Train Loss = 0.1053
2023-11-08 01:59:07,817 - __main__ - INFO - Epoch 46 Batch 40: Train Loss = 0.1950
2023-11-08 01:59:25,164 - __main__ - INFO - Epoch 46 Batch 60: Train Loss = 0.1404
2023-11-08 01:59:42,987 - __main__ - INFO - Epoch 46 Batch 80: Train Loss = 0.1391
2023-11-08 01:59:58,797 - __main__ - INFO - Epoch 46 Batch 100: Train Loss = 0.1601
2023-11-08 02:00:18,048 - __main__ - INFO - Epoch 46 Batch 120: Train Loss = 0.1719
2023-11-08 02:00:32,290 - __main__ - INFO - Epoch 46: Loss = 0.1623 Valid loss = 0.1424 roc = 0.9201
2023-11-08 02:00:33,250 - __main__ - INFO - Epoch 47 Batch 0: Train Loss = 0.1986
2023-11-08 02:00:49,704 - __main__ - INFO - Epoch 47 Batch 20: Train Loss = 0.1142
2023-11-08 02:01:06,996 - __main__ - INFO - Epoch 47 Batch 40: Train Loss = 0.2030
2023-11-08 02:01:23,608 - __main__ - INFO - Epoch 47 Batch 60: Train Loss = 0.1172
2023-11-08 02:01:40,835 - __main__ - INFO - Epoch 47 Batch 80: Train Loss = 0.2051
2023-11-08 02:01:57,431 - __main__ - INFO - Epoch 47 Batch 100: Train Loss = 0.1100
2023-11-08 02:02:15,700 - __main__ - INFO - Epoch 47 Batch 120: Train Loss = 0.1904
2023-11-08 02:02:28,178 - __main__ - INFO - Epoch 47: Loss = 0.1622 Valid loss = 0.1434 roc = 0.9145
2023-11-08 02:02:29,030 - __main__ - INFO - Epoch 48 Batch 0: Train Loss = 0.1333
2023-11-08 02:02:47,596 - __main__ - INFO - Epoch 48 Batch 20: Train Loss = 0.1644
2023-11-08 02:03:05,775 - __main__ - INFO - Epoch 48 Batch 40: Train Loss = 0.1562
2023-11-08 02:03:24,402 - __main__ - INFO - Epoch 48 Batch 60: Train Loss = 0.1479
2023-11-08 02:03:41,646 - __main__ - INFO - Epoch 48 Batch 80: Train Loss = 0.2025
2023-11-08 02:03:59,982 - __main__ - INFO - Epoch 48 Batch 100: Train Loss = 0.1910
2023-11-08 02:04:17,265 - __main__ - INFO - Epoch 48 Batch 120: Train Loss = 0.1045
2023-11-08 02:04:30,080 - __main__ - INFO - Epoch 48: Loss = 0.1627 Valid loss = 0.1413 roc = 0.9167
2023-11-08 02:04:31,161 - __main__ - INFO - Epoch 49 Batch 0: Train Loss = 0.1277
2023-11-08 02:04:50,386 - __main__ - INFO - Epoch 49 Batch 20: Train Loss = 0.2202
2023-11-08 02:05:07,749 - __main__ - INFO - Epoch 49 Batch 40: Train Loss = 0.1421
2023-11-08 02:05:25,599 - __main__ - INFO - Epoch 49 Batch 60: Train Loss = 0.1069
2023-11-08 02:05:42,310 - __main__ - INFO - Epoch 49 Batch 80: Train Loss = 0.1378
2023-11-08 02:05:59,856 - __main__ - INFO - Epoch 49 Batch 100: Train Loss = 0.1208
2023-11-08 02:06:17,020 - __main__ - INFO - Epoch 49 Batch 120: Train Loss = 0.1555
2023-11-08 02:06:29,155 - __main__ - INFO - Epoch 49: Loss = 0.1642 Valid loss = 0.1407 roc = 0.9165
2023-11-08 02:06:30,330 - __main__ - INFO - Epoch 50 Batch 0: Train Loss = 0.1863
2023-11-08 02:06:48,908 - __main__ - INFO - Epoch 50 Batch 20: Train Loss = 0.1543
2023-11-08 02:07:05,864 - __main__ - INFO - Epoch 50 Batch 40: Train Loss = 0.1575
2023-11-08 02:07:23,638 - __main__ - INFO - Epoch 50 Batch 60: Train Loss = 0.2060
2023-11-08 02:07:41,767 - __main__ - INFO - Epoch 50 Batch 80: Train Loss = 0.1204
2023-11-08 02:07:58,634 - __main__ - INFO - Epoch 50 Batch 100: Train Loss = 0.1675
2023-11-08 02:08:16,439 - __main__ - INFO - Epoch 50 Batch 120: Train Loss = 0.1807
2023-11-08 02:08:29,070 - __main__ - INFO - Epoch 50: Loss = 0.1594 Valid loss = 0.1419 roc = 0.9183
2023-11-08 02:08:29,762 - __main__ - INFO - Epoch 51 Batch 0: Train Loss = 0.1437
2023-11-08 02:08:48,330 - __main__ - INFO - Epoch 51 Batch 20: Train Loss = 0.1851
2023-11-08 02:09:06,507 - __main__ - INFO - Epoch 51 Batch 40: Train Loss = 0.1634
2023-11-08 02:09:24,824 - __main__ - INFO - Epoch 51 Batch 60: Train Loss = 0.1508
2023-11-08 02:09:42,759 - __main__ - INFO - Epoch 51 Batch 80: Train Loss = 0.1006
2023-11-08 02:10:00,511 - __main__ - INFO - Epoch 51 Batch 100: Train Loss = 0.1633
2023-11-08 02:10:19,027 - __main__ - INFO - Epoch 51 Batch 120: Train Loss = 0.0800
2023-11-08 02:10:32,636 - __main__ - INFO - Epoch 51: Loss = 0.1564 Valid loss = 0.1403 roc = 0.9164
2023-11-08 02:10:33,443 - __main__ - INFO - Epoch 52 Batch 0: Train Loss = 0.1339
2023-11-08 02:10:51,554 - __main__ - INFO - Epoch 52 Batch 20: Train Loss = 0.1797
2023-11-08 02:11:11,644 - __main__ - INFO - Epoch 52 Batch 40: Train Loss = 0.1664
2023-11-08 02:11:28,493 - __main__ - INFO - Epoch 52 Batch 60: Train Loss = 0.1755
2023-11-08 02:11:45,275 - __main__ - INFO - Epoch 52 Batch 80: Train Loss = 0.0911
2023-11-08 02:12:02,516 - __main__ - INFO - Epoch 52 Batch 100: Train Loss = 0.1357
2023-11-08 02:12:20,415 - __main__ - INFO - Epoch 52 Batch 120: Train Loss = 0.1240
2023-11-08 02:12:33,432 - __main__ - INFO - Epoch 52: Loss = 0.1594 Valid loss = 0.1400 roc = 0.9197
2023-11-08 02:12:34,079 - __main__ - INFO - Epoch 53 Batch 0: Train Loss = 0.1409
2023-11-08 02:12:51,530 - __main__ - INFO - Epoch 53 Batch 20: Train Loss = 0.2135
2023-11-08 02:13:09,852 - __main__ - INFO - Epoch 53 Batch 40: Train Loss = 0.1262
2023-11-08 02:13:27,329 - __main__ - INFO - Epoch 53 Batch 60: Train Loss = 0.1453
2023-11-08 02:13:45,048 - __main__ - INFO - Epoch 53 Batch 80: Train Loss = 0.1975
2023-11-08 02:14:02,423 - __main__ - INFO - Epoch 53 Batch 100: Train Loss = 0.1744
2023-11-08 02:14:21,092 - __main__ - INFO - Epoch 53 Batch 120: Train Loss = 0.1386
2023-11-08 02:14:34,614 - __main__ - INFO - Epoch 53: Loss = 0.1598 Valid loss = 0.1402 roc = 0.9182
2023-11-08 02:14:35,571 - __main__ - INFO - Epoch 54 Batch 0: Train Loss = 0.1299
2023-11-08 02:14:51,226 - __main__ - INFO - Epoch 54 Batch 20: Train Loss = 0.1916
2023-11-08 02:15:06,990 - __main__ - INFO - Epoch 54 Batch 40: Train Loss = 0.1931
2023-11-08 02:15:24,639 - __main__ - INFO - Epoch 54 Batch 60: Train Loss = 0.1292
2023-11-08 02:15:42,057 - __main__ - INFO - Epoch 54 Batch 80: Train Loss = 0.1507
2023-11-08 02:16:00,134 - __main__ - INFO - Epoch 54 Batch 100: Train Loss = 0.1745
2023-11-08 02:16:18,034 - __main__ - INFO - Epoch 54 Batch 120: Train Loss = 0.1166
2023-11-08 02:16:32,113 - __main__ - INFO - Epoch 54: Loss = 0.1582 Valid loss = 0.1421 roc = 0.9155
2023-11-08 02:16:33,056 - __main__ - INFO - Epoch 55 Batch 0: Train Loss = 0.1511
2023-11-08 02:16:51,459 - __main__ - INFO - Epoch 55 Batch 20: Train Loss = 0.1591
2023-11-08 02:17:08,547 - __main__ - INFO - Epoch 55 Batch 40: Train Loss = 0.1770
2023-11-08 02:17:25,283 - __main__ - INFO - Epoch 55 Batch 60: Train Loss = 0.1712
2023-11-08 02:17:42,757 - __main__ - INFO - Epoch 55 Batch 80: Train Loss = 0.1397
2023-11-08 02:18:00,806 - __main__ - INFO - Epoch 55 Batch 100: Train Loss = 0.1467
2023-11-08 02:18:18,274 - __main__ - INFO - Epoch 55 Batch 120: Train Loss = 0.1486
2023-11-08 02:18:31,591 - __main__ - INFO - Epoch 55: Loss = 0.1574 Valid loss = 0.1410 roc = 0.9103
2023-11-08 02:18:32,473 - __main__ - INFO - Epoch 56 Batch 0: Train Loss = 0.1487
2023-11-08 02:18:50,450 - __main__ - INFO - Epoch 56 Batch 20: Train Loss = 0.1710
2023-11-08 02:19:09,661 - __main__ - INFO - Epoch 56 Batch 40: Train Loss = 0.1631
2023-11-08 02:19:27,281 - __main__ - INFO - Epoch 56 Batch 60: Train Loss = 0.1541
2023-11-08 02:19:45,367 - __main__ - INFO - Epoch 56 Batch 80: Train Loss = 0.0772
2023-11-08 02:20:03,023 - __main__ - INFO - Epoch 56 Batch 100: Train Loss = 0.1279
2023-11-08 02:20:20,559 - __main__ - INFO - Epoch 56 Batch 120: Train Loss = 0.1151
2023-11-08 02:20:34,416 - __main__ - INFO - Epoch 56: Loss = 0.1568 Valid loss = 0.1431 roc = 0.9180
2023-11-08 02:20:35,132 - __main__ - INFO - Epoch 57 Batch 0: Train Loss = 0.1637
2023-11-08 02:20:53,408 - __main__ - INFO - Epoch 57 Batch 20: Train Loss = 0.1269
2023-11-08 02:21:09,914 - __main__ - INFO - Epoch 57 Batch 40: Train Loss = 0.1508
2023-11-08 02:21:26,603 - __main__ - INFO - Epoch 57 Batch 60: Train Loss = 0.1301
2023-11-08 02:21:43,777 - __main__ - INFO - Epoch 57 Batch 80: Train Loss = 0.1697
2023-11-08 02:22:01,525 - __main__ - INFO - Epoch 57 Batch 100: Train Loss = 0.1573
2023-11-08 02:22:19,915 - __main__ - INFO - Epoch 57 Batch 120: Train Loss = 0.1592
2023-11-08 02:22:32,331 - __main__ - INFO - Epoch 57: Loss = 0.1580 Valid loss = 0.1383 roc = 0.9208
2023-11-08 02:22:33,176 - __main__ - INFO - Epoch 58 Batch 0: Train Loss = 0.1786
2023-11-08 02:22:52,250 - __main__ - INFO - Epoch 58 Batch 20: Train Loss = 0.1444
2023-11-08 02:23:10,177 - __main__ - INFO - Epoch 58 Batch 40: Train Loss = 0.1970
2023-11-08 02:23:28,935 - __main__ - INFO - Epoch 58 Batch 60: Train Loss = 0.1784
2023-11-08 02:23:45,896 - __main__ - INFO - Epoch 58 Batch 80: Train Loss = 0.1597
2023-11-08 02:24:02,814 - __main__ - INFO - Epoch 58 Batch 100: Train Loss = 0.0730
2023-11-08 02:24:20,392 - __main__ - INFO - Epoch 58 Batch 120: Train Loss = 0.1941
2023-11-08 02:24:33,642 - __main__ - INFO - Epoch 58: Loss = 0.1555 Valid loss = 0.1399 roc = 0.9170
2023-11-08 02:24:34,552 - __main__ - INFO - Epoch 59 Batch 0: Train Loss = 0.1170
2023-11-08 02:24:52,881 - __main__ - INFO - Epoch 59 Batch 20: Train Loss = 0.1191
2023-11-08 02:25:10,336 - __main__ - INFO - Epoch 59 Batch 40: Train Loss = 0.1796
2023-11-08 02:25:27,676 - __main__ - INFO - Epoch 59 Batch 60: Train Loss = 0.1040
2023-11-08 02:25:46,017 - __main__ - INFO - Epoch 59 Batch 80: Train Loss = 0.1365
2023-11-08 02:26:03,719 - __main__ - INFO - Epoch 59 Batch 100: Train Loss = 0.1656
2023-11-08 02:26:20,881 - __main__ - INFO - Epoch 59 Batch 120: Train Loss = 0.1244
2023-11-08 02:26:33,731 - __main__ - INFO - Epoch 59: Loss = 0.1563 Valid loss = 0.1389 roc = 0.9195
2023-11-08 02:26:34,796 - __main__ - INFO - Epoch 60 Batch 0: Train Loss = 0.1280
2023-11-08 02:26:51,336 - __main__ - INFO - Epoch 60 Batch 20: Train Loss = 0.1447
2023-11-08 02:27:07,599 - __main__ - INFO - Epoch 60 Batch 40: Train Loss = 0.1237
2023-11-08 02:27:25,036 - __main__ - INFO - Epoch 60 Batch 60: Train Loss = 0.1647
2023-11-08 02:27:43,318 - __main__ - INFO - Epoch 60 Batch 80: Train Loss = 0.1157
2023-11-08 02:28:00,361 - __main__ - INFO - Epoch 60 Batch 100: Train Loss = 0.1744
2023-11-08 02:28:18,380 - __main__ - INFO - Epoch 60 Batch 120: Train Loss = 0.1749
2023-11-08 02:28:31,455 - __main__ - INFO - Epoch 60: Loss = 0.1541 Valid loss = 0.1403 roc = 0.9188
2023-11-08 02:28:32,438 - __main__ - INFO - Epoch 61 Batch 0: Train Loss = 0.1600
2023-11-08 02:28:49,888 - __main__ - INFO - Epoch 61 Batch 20: Train Loss = 0.1122
2023-11-08 02:29:08,207 - __main__ - INFO - Epoch 61 Batch 40: Train Loss = 0.1443
2023-11-08 02:29:25,183 - __main__ - INFO - Epoch 61 Batch 60: Train Loss = 0.1370
2023-11-08 02:29:43,621 - __main__ - INFO - Epoch 61 Batch 80: Train Loss = 0.1175
2023-11-08 02:30:02,083 - __main__ - INFO - Epoch 61 Batch 100: Train Loss = 0.1570
2023-11-08 02:30:19,402 - __main__ - INFO - Epoch 61 Batch 120: Train Loss = 0.1912
2023-11-08 02:30:33,670 - __main__ - INFO - Epoch 61: Loss = 0.1553 Valid loss = 0.1411 roc = 0.9161
2023-11-08 02:30:34,279 - __main__ - INFO - Epoch 62 Batch 0: Train Loss = 0.1268
2023-11-08 02:30:51,228 - __main__ - INFO - Epoch 62 Batch 20: Train Loss = 0.1487
2023-11-08 02:31:08,870 - __main__ - INFO - Epoch 62 Batch 40: Train Loss = 0.1823
2023-11-08 02:31:26,822 - __main__ - INFO - Epoch 62 Batch 60: Train Loss = 0.1816
2023-11-08 02:31:45,066 - __main__ - INFO - Epoch 62 Batch 80: Train Loss = 0.1532
2023-11-08 02:32:02,783 - __main__ - INFO - Epoch 62 Batch 100: Train Loss = 0.1544
2023-11-08 02:32:21,426 - __main__ - INFO - Epoch 62 Batch 120: Train Loss = 0.0954
2023-11-08 02:32:33,050 - __main__ - INFO - Epoch 62: Loss = 0.1555 Valid loss = 0.1383 roc = 0.9227
2023-11-08 02:32:34,150 - __main__ - INFO - Epoch 63 Batch 0: Train Loss = 0.1565
2023-11-08 02:32:51,138 - __main__ - INFO - Epoch 63 Batch 20: Train Loss = 0.2273
2023-11-08 02:33:09,402 - __main__ - INFO - Epoch 63 Batch 40: Train Loss = 0.1579
2023-11-08 02:33:27,349 - __main__ - INFO - Epoch 63 Batch 60: Train Loss = 0.1506
2023-11-08 02:33:43,384 - __main__ - INFO - Epoch 63 Batch 80: Train Loss = 0.2050
2023-11-08 02:34:02,632 - __main__ - INFO - Epoch 63 Batch 100: Train Loss = 0.2006
2023-11-08 02:34:20,224 - __main__ - INFO - Epoch 63 Batch 120: Train Loss = 0.0962
2023-11-08 02:34:34,121 - __main__ - INFO - Epoch 63: Loss = 0.1636 Valid loss = 0.1404 roc = 0.9231
2023-11-08 02:34:34,823 - __main__ - INFO - Epoch 64 Batch 0: Train Loss = 0.2186
2023-11-08 02:34:52,270 - __main__ - INFO - Epoch 64 Batch 20: Train Loss = 0.1441
2023-11-08 02:35:09,452 - __main__ - INFO - Epoch 64 Batch 40: Train Loss = 0.1148
2023-11-08 02:35:27,387 - __main__ - INFO - Epoch 64 Batch 60: Train Loss = 0.1504
2023-11-08 02:35:45,572 - __main__ - INFO - Epoch 64 Batch 80: Train Loss = 0.1903
2023-11-08 02:36:01,739 - __main__ - INFO - Epoch 64 Batch 100: Train Loss = 0.1318
2023-11-08 02:36:20,273 - __main__ - INFO - Epoch 64 Batch 120: Train Loss = 0.1565
2023-11-08 02:36:33,382 - __main__ - INFO - Epoch 64: Loss = 0.1587 Valid loss = 0.1408 roc = 0.9150
2023-11-08 02:36:34,232 - __main__ - INFO - Epoch 65 Batch 0: Train Loss = 0.2006
2023-11-08 02:36:53,813 - __main__ - INFO - Epoch 65 Batch 20: Train Loss = 0.1398
2023-11-08 02:37:10,098 - __main__ - INFO - Epoch 65 Batch 40: Train Loss = 0.1315
2023-11-08 02:37:27,491 - __main__ - INFO - Epoch 65 Batch 60: Train Loss = 0.1109
2023-11-08 02:37:46,261 - __main__ - INFO - Epoch 65 Batch 80: Train Loss = 0.1482
2023-11-08 02:38:04,144 - __main__ - INFO - Epoch 65 Batch 100: Train Loss = 0.1581
2023-11-08 02:38:21,709 - __main__ - INFO - Epoch 65 Batch 120: Train Loss = 0.1445
2023-11-08 02:38:34,857 - __main__ - INFO - Epoch 65: Loss = 0.1568 Valid loss = 0.1406 roc = 0.9172
2023-11-08 02:38:35,930 - __main__ - INFO - Epoch 66 Batch 0: Train Loss = 0.1508
2023-11-08 02:38:54,133 - __main__ - INFO - Epoch 66 Batch 20: Train Loss = 0.2172
2023-11-08 02:39:12,003 - __main__ - INFO - Epoch 66 Batch 40: Train Loss = 0.1560
2023-11-08 02:39:29,311 - __main__ - INFO - Epoch 66 Batch 60: Train Loss = 0.1554
2023-11-08 02:39:46,843 - __main__ - INFO - Epoch 66 Batch 80: Train Loss = 0.1597
2023-11-08 02:40:05,064 - __main__ - INFO - Epoch 66 Batch 100: Train Loss = 0.1523
2023-11-08 02:40:23,408 - __main__ - INFO - Epoch 66 Batch 120: Train Loss = 0.1131
2023-11-08 02:40:37,726 - __main__ - INFO - Epoch 66: Loss = 0.1556 Valid loss = 0.1407 roc = 0.9168
2023-11-08 02:40:38,575 - __main__ - INFO - Epoch 67 Batch 0: Train Loss = 0.1643
2023-11-08 02:40:56,149 - __main__ - INFO - Epoch 67 Batch 20: Train Loss = 0.1148
2023-11-08 02:41:13,275 - __main__ - INFO - Epoch 67 Batch 40: Train Loss = 0.1349
2023-11-08 02:41:30,745 - __main__ - INFO - Epoch 67 Batch 60: Train Loss = 0.1706
2023-11-08 02:41:49,931 - __main__ - INFO - Epoch 67 Batch 80: Train Loss = 0.1315
2023-11-08 02:42:08,852 - __main__ - INFO - Epoch 67 Batch 100: Train Loss = 0.1540
2023-11-08 02:42:24,705 - __main__ - INFO - Epoch 67 Batch 120: Train Loss = 0.1557
2023-11-08 02:42:39,121 - __main__ - INFO - Epoch 67: Loss = 0.1539 Valid loss = 0.1389 roc = 0.9180
2023-11-08 02:42:39,820 - __main__ - INFO - Epoch 68 Batch 0: Train Loss = 0.1380
2023-11-08 02:42:56,456 - __main__ - INFO - Epoch 68 Batch 20: Train Loss = 0.1919
2023-11-08 02:43:13,697 - __main__ - INFO - Epoch 68 Batch 40: Train Loss = 0.1417
2023-11-08 02:43:30,855 - __main__ - INFO - Epoch 68 Batch 60: Train Loss = 0.1679
2023-11-08 02:43:48,886 - __main__ - INFO - Epoch 68 Batch 80: Train Loss = 0.2228
2023-11-08 02:44:06,604 - __main__ - INFO - Epoch 68 Batch 100: Train Loss = 0.1928
2023-11-08 02:44:24,543 - __main__ - INFO - Epoch 68 Batch 120: Train Loss = 0.2168
2023-11-08 02:44:37,886 - __main__ - INFO - Epoch 68: Loss = 0.1514 Valid loss = 0.1379 roc = 0.9177
2023-11-08 02:44:38,933 - __main__ - INFO - Epoch 69 Batch 0: Train Loss = 0.2038
2023-11-08 02:44:54,900 - __main__ - INFO - Epoch 69 Batch 20: Train Loss = 0.1690
2023-11-08 02:45:12,636 - __main__ - INFO - Epoch 69 Batch 40: Train Loss = 0.1976
2023-11-08 02:45:29,813 - __main__ - INFO - Epoch 69 Batch 60: Train Loss = 0.1625
2023-11-08 02:45:47,048 - __main__ - INFO - Epoch 69 Batch 80: Train Loss = 0.1987
2023-11-08 02:46:05,618 - __main__ - INFO - Epoch 69 Batch 100: Train Loss = 0.1970
2023-11-08 02:46:21,862 - __main__ - INFO - Epoch 69 Batch 120: Train Loss = 0.1644
2023-11-08 02:46:34,406 - __main__ - INFO - Epoch 69: Loss = 0.1561 Valid loss = 0.1390 roc = 0.9202
2023-11-08 02:46:35,307 - __main__ - INFO - Epoch 70 Batch 0: Train Loss = 0.2005
2023-11-08 02:46:51,624 - __main__ - INFO - Epoch 70 Batch 20: Train Loss = 0.1946
2023-11-08 02:47:09,155 - __main__ - INFO - Epoch 70 Batch 40: Train Loss = 0.1661
2023-11-08 02:47:26,129 - __main__ - INFO - Epoch 70 Batch 60: Train Loss = 0.1917
2023-11-08 02:47:42,440 - __main__ - INFO - Epoch 70 Batch 80: Train Loss = 0.1713
2023-11-08 02:47:59,997 - __main__ - INFO - Epoch 70 Batch 100: Train Loss = 0.1489
2023-11-08 02:48:17,492 - __main__ - INFO - Epoch 70 Batch 120: Train Loss = 0.1164
2023-11-08 02:48:29,373 - __main__ - INFO - Epoch 70: Loss = 0.1550 Valid loss = 0.1407 roc = 0.9165
2023-11-08 02:48:30,206 - __main__ - INFO - Epoch 71 Batch 0: Train Loss = 0.1415
2023-11-08 02:48:48,266 - __main__ - INFO - Epoch 71 Batch 20: Train Loss = 0.1178
2023-11-08 02:49:06,445 - __main__ - INFO - Epoch 71 Batch 40: Train Loss = 0.1508
2023-11-08 02:49:23,494 - __main__ - INFO - Epoch 71 Batch 60: Train Loss = 0.1819
2023-11-08 02:49:41,447 - __main__ - INFO - Epoch 71 Batch 80: Train Loss = 0.1338
2023-11-08 02:49:58,636 - __main__ - INFO - Epoch 71 Batch 100: Train Loss = 0.1815
2023-11-08 02:50:15,108 - __main__ - INFO - Epoch 71 Batch 120: Train Loss = 0.1763
2023-11-08 02:50:28,660 - __main__ - INFO - Epoch 71: Loss = 0.1553 Valid loss = 0.1398 roc = 0.9166
2023-11-08 02:50:29,943 - __main__ - INFO - Epoch 72 Batch 0: Train Loss = 0.1585
2023-11-08 02:50:47,503 - __main__ - INFO - Epoch 72 Batch 20: Train Loss = 0.2150
2023-11-08 02:51:05,223 - __main__ - INFO - Epoch 72 Batch 40: Train Loss = 0.1623
2023-11-08 02:51:22,095 - __main__ - INFO - Epoch 72 Batch 60: Train Loss = 0.1406
2023-11-08 02:51:39,608 - __main__ - INFO - Epoch 72 Batch 80: Train Loss = 0.1093
2023-11-08 02:51:57,947 - __main__ - INFO - Epoch 72 Batch 100: Train Loss = 0.1960
2023-11-08 02:52:15,490 - __main__ - INFO - Epoch 72 Batch 120: Train Loss = 0.1802
2023-11-08 02:52:29,199 - __main__ - INFO - Epoch 72: Loss = 0.1567 Valid loss = 0.1392 roc = 0.9189
2023-11-08 02:52:30,191 - __main__ - INFO - Epoch 73 Batch 0: Train Loss = 0.1524
2023-11-08 02:52:47,800 - __main__ - INFO - Epoch 73 Batch 20: Train Loss = 0.1827
2023-11-08 02:53:05,239 - __main__ - INFO - Epoch 73 Batch 40: Train Loss = 0.1653
2023-11-08 02:53:24,572 - __main__ - INFO - Epoch 73 Batch 60: Train Loss = 0.1230
2023-11-08 02:53:41,565 - __main__ - INFO - Epoch 73 Batch 80: Train Loss = 0.1917
2023-11-08 02:53:58,302 - __main__ - INFO - Epoch 73 Batch 100: Train Loss = 0.0821
2023-11-08 02:54:15,634 - __main__ - INFO - Epoch 73 Batch 120: Train Loss = 0.1483
2023-11-08 02:54:28,947 - __main__ - INFO - Epoch 73: Loss = 0.1536 Valid loss = 0.1377 roc = 0.9182
2023-11-08 02:54:29,864 - __main__ - INFO - Epoch 74 Batch 0: Train Loss = 0.1556
2023-11-08 02:54:47,043 - __main__ - INFO - Epoch 74 Batch 20: Train Loss = 0.2104
2023-11-08 02:55:05,572 - __main__ - INFO - Epoch 74 Batch 40: Train Loss = 0.2126
2023-11-08 02:55:22,877 - __main__ - INFO - Epoch 74 Batch 60: Train Loss = 0.2046
2023-11-08 02:55:39,248 - __main__ - INFO - Epoch 74 Batch 80: Train Loss = 0.1650
2023-11-08 02:55:56,526 - __main__ - INFO - Epoch 74 Batch 100: Train Loss = 0.1173
2023-11-08 02:56:14,026 - __main__ - INFO - Epoch 74 Batch 120: Train Loss = 0.2394
2023-11-08 02:56:25,880 - __main__ - INFO - Epoch 74: Loss = 0.1527 Valid loss = 0.1364 roc = 0.9191
2023-11-08 02:56:26,626 - __main__ - INFO - Epoch 75 Batch 0: Train Loss = 0.2111
2023-11-08 02:56:44,286 - __main__ - INFO - Epoch 75 Batch 20: Train Loss = 0.1597
2023-11-08 02:57:02,729 - __main__ - INFO - Epoch 75 Batch 40: Train Loss = 0.1954
2023-11-08 02:57:19,538 - __main__ - INFO - Epoch 75 Batch 60: Train Loss = 0.1314
2023-11-08 02:57:37,419 - __main__ - INFO - Epoch 75 Batch 80: Train Loss = 0.1123
2023-11-08 02:57:53,807 - __main__ - INFO - Epoch 75 Batch 100: Train Loss = 0.1365
2023-11-08 02:58:11,070 - __main__ - INFO - Epoch 75 Batch 120: Train Loss = 0.1175
2023-11-08 02:58:24,052 - __main__ - INFO - Epoch 75: Loss = 0.1536 Valid loss = 0.1381 roc = 0.9184
2023-11-08 02:58:24,819 - __main__ - INFO - Epoch 76 Batch 0: Train Loss = 0.1508
2023-11-08 02:58:42,529 - __main__ - INFO - Epoch 76 Batch 20: Train Loss = 0.1530
2023-11-08 02:59:00,777 - __main__ - INFO - Epoch 76 Batch 40: Train Loss = 0.1804
2023-11-08 02:59:18,735 - __main__ - INFO - Epoch 76 Batch 60: Train Loss = 0.1357
2023-11-08 02:59:36,279 - __main__ - INFO - Epoch 76 Batch 80: Train Loss = 0.1200
2023-11-08 02:59:55,414 - __main__ - INFO - Epoch 76 Batch 100: Train Loss = 0.1324
2023-11-08 03:00:13,418 - __main__ - INFO - Epoch 76 Batch 120: Train Loss = 0.1711
2023-11-08 03:00:26,864 - __main__ - INFO - Epoch 76: Loss = 0.1558 Valid loss = 0.1379 roc = 0.9186
2023-11-08 03:00:27,793 - __main__ - INFO - Epoch 77 Batch 0: Train Loss = 0.1339
2023-11-08 03:00:45,125 - __main__ - INFO - Epoch 77 Batch 20: Train Loss = 0.2227
2023-11-08 03:01:03,979 - __main__ - INFO - Epoch 77 Batch 40: Train Loss = 0.2211
2023-11-08 03:01:22,583 - __main__ - INFO - Epoch 77 Batch 60: Train Loss = 0.2238
2023-11-08 03:01:39,811 - __main__ - INFO - Epoch 77 Batch 80: Train Loss = 0.1139
2023-11-08 03:01:58,219 - __main__ - INFO - Epoch 77 Batch 100: Train Loss = 0.1266
2023-11-08 03:02:16,141 - __main__ - INFO - Epoch 77 Batch 120: Train Loss = 0.0845
2023-11-08 03:02:28,493 - __main__ - INFO - Epoch 77: Loss = 0.1567 Valid loss = 0.1389 roc = 0.9209
2023-11-08 03:02:29,480 - __main__ - INFO - Epoch 78 Batch 0: Train Loss = 0.0856
2023-11-08 03:02:46,360 - __main__ - INFO - Epoch 78 Batch 20: Train Loss = 0.1407
2023-11-08 03:03:05,960 - __main__ - INFO - Epoch 78 Batch 40: Train Loss = 0.2134
2023-11-08 03:03:23,997 - __main__ - INFO - Epoch 78 Batch 60: Train Loss = 0.1188
2023-11-08 03:03:41,479 - __main__ - INFO - Epoch 78 Batch 80: Train Loss = 0.1737
2023-11-08 03:04:00,263 - __main__ - INFO - Epoch 78 Batch 100: Train Loss = 0.0971
2023-11-08 03:04:17,547 - __main__ - INFO - Epoch 78 Batch 120: Train Loss = 0.1547
2023-11-08 03:04:31,538 - __main__ - INFO - Epoch 78: Loss = 0.1540 Valid loss = 0.1385 roc = 0.9189
2023-11-08 03:04:32,500 - __main__ - INFO - Epoch 79 Batch 0: Train Loss = 0.1302
2023-11-08 03:04:49,197 - __main__ - INFO - Epoch 79 Batch 20: Train Loss = 0.0903
2023-11-08 03:05:07,884 - __main__ - INFO - Epoch 79 Batch 40: Train Loss = 0.1386
2023-11-08 03:05:23,130 - __main__ - INFO - Epoch 79 Batch 60: Train Loss = 0.0732
2023-11-08 03:05:39,497 - __main__ - INFO - Epoch 79 Batch 80: Train Loss = 0.1397
2023-11-08 03:05:57,308 - __main__ - INFO - Epoch 79 Batch 100: Train Loss = 0.1794
2023-11-08 03:06:16,340 - __main__ - INFO - Epoch 79 Batch 120: Train Loss = 0.1148
2023-11-08 03:06:29,610 - __main__ - INFO - Epoch 79: Loss = 0.1538 Valid loss = 0.1395 roc = 0.9209
2023-11-08 03:06:30,343 - __main__ - INFO - Epoch 80 Batch 0: Train Loss = 0.1238
2023-11-08 03:06:48,093 - __main__ - INFO - Epoch 80 Batch 20: Train Loss = 0.1846
2023-11-08 03:07:06,225 - __main__ - INFO - Epoch 80 Batch 40: Train Loss = 0.1050
2023-11-08 03:07:24,767 - __main__ - INFO - Epoch 80 Batch 60: Train Loss = 0.1921
2023-11-08 03:07:42,057 - __main__ - INFO - Epoch 80 Batch 80: Train Loss = 0.1397
2023-11-08 03:07:59,620 - __main__ - INFO - Epoch 80 Batch 100: Train Loss = 0.1047
2023-11-08 03:08:18,133 - __main__ - INFO - Epoch 80 Batch 120: Train Loss = 0.1248
2023-11-08 03:08:31,513 - __main__ - INFO - Epoch 80: Loss = 0.1507 Valid loss = 0.1387 roc = 0.9179
2023-11-08 03:08:32,409 - __main__ - INFO - Epoch 81 Batch 0: Train Loss = 0.1241
2023-11-08 03:08:49,506 - __main__ - INFO - Epoch 81 Batch 20: Train Loss = 0.1499
2023-11-08 03:09:07,921 - __main__ - INFO - Epoch 81 Batch 40: Train Loss = 0.1700
2023-11-08 03:09:25,731 - __main__ - INFO - Epoch 81 Batch 60: Train Loss = 0.1337
2023-11-08 03:09:43,400 - __main__ - INFO - Epoch 81 Batch 80: Train Loss = 0.1545
2023-11-08 03:09:59,341 - __main__ - INFO - Epoch 81 Batch 100: Train Loss = 0.1355
2023-11-08 03:10:17,123 - __main__ - INFO - Epoch 81 Batch 120: Train Loss = 0.1778
2023-11-08 03:10:30,103 - __main__ - INFO - Epoch 81: Loss = 0.1537 Valid loss = 0.1372 roc = 0.9219
2023-11-08 03:10:30,750 - __main__ - INFO - Epoch 82 Batch 0: Train Loss = 0.1299
2023-11-08 03:10:48,522 - __main__ - INFO - Epoch 82 Batch 20: Train Loss = 0.1450
2023-11-08 03:11:05,511 - __main__ - INFO - Epoch 82 Batch 40: Train Loss = 0.1128
2023-11-08 03:11:22,224 - __main__ - INFO - Epoch 82 Batch 60: Train Loss = 0.1876
2023-11-08 03:11:40,762 - __main__ - INFO - Epoch 82 Batch 80: Train Loss = 0.1639
2023-11-08 03:11:58,324 - __main__ - INFO - Epoch 82 Batch 100: Train Loss = 0.1144
2023-11-08 03:12:16,201 - __main__ - INFO - Epoch 82 Batch 120: Train Loss = 0.1630
2023-11-08 03:12:28,432 - __main__ - INFO - Epoch 82: Loss = 0.1505 Valid loss = 0.1372 roc = 0.9208
2023-11-08 03:12:29,511 - __main__ - INFO - Epoch 83 Batch 0: Train Loss = 0.1244
2023-11-08 03:12:46,874 - __main__ - INFO - Epoch 83 Batch 20: Train Loss = 0.1092
2023-11-08 03:13:02,158 - __main__ - INFO - Epoch 83 Batch 40: Train Loss = 0.1522
2023-11-08 03:13:19,175 - __main__ - INFO - Epoch 83 Batch 60: Train Loss = 0.1291
2023-11-08 03:13:35,050 - __main__ - INFO - Epoch 83 Batch 80: Train Loss = 0.1992
2023-11-08 03:13:53,233 - __main__ - INFO - Epoch 83 Batch 100: Train Loss = 0.1393
2023-11-08 03:14:11,900 - __main__ - INFO - Epoch 83 Batch 120: Train Loss = 0.1759
2023-11-08 03:14:25,217 - __main__ - INFO - Epoch 83: Loss = 0.1556 Valid loss = 0.1382 roc = 0.9193
2023-11-08 03:14:26,106 - __main__ - INFO - Epoch 84 Batch 0: Train Loss = 0.1306
2023-11-08 03:14:44,083 - __main__ - INFO - Epoch 84 Batch 20: Train Loss = 0.1643
2023-11-08 03:15:03,016 - __main__ - INFO - Epoch 84 Batch 40: Train Loss = 0.1754
2023-11-08 03:15:21,472 - __main__ - INFO - Epoch 84 Batch 60: Train Loss = 0.0849
2023-11-08 03:15:38,743 - __main__ - INFO - Epoch 84 Batch 80: Train Loss = 0.1412
2023-11-08 03:15:56,470 - __main__ - INFO - Epoch 84 Batch 100: Train Loss = 0.1871
2023-11-08 03:16:14,335 - __main__ - INFO - Epoch 84 Batch 120: Train Loss = 0.2315
2023-11-08 03:16:28,575 - __main__ - INFO - Epoch 84: Loss = 0.1566 Valid loss = 0.1387 roc = 0.9179
2023-11-08 03:16:29,115 - __main__ - INFO - Epoch 85 Batch 0: Train Loss = 0.1690
2023-11-08 03:16:46,827 - __main__ - INFO - Epoch 85 Batch 20: Train Loss = 0.1568
2023-11-08 03:17:05,787 - __main__ - INFO - Epoch 85 Batch 40: Train Loss = 0.1540
2023-11-08 03:17:22,596 - __main__ - INFO - Epoch 85 Batch 60: Train Loss = 0.1676
2023-11-08 03:17:41,284 - __main__ - INFO - Epoch 85 Batch 80: Train Loss = 0.1429
2023-11-08 03:18:00,018 - __main__ - INFO - Epoch 85 Batch 100: Train Loss = 0.1596
2023-11-08 03:18:18,350 - __main__ - INFO - Epoch 85 Batch 120: Train Loss = 0.1600
2023-11-08 03:18:31,844 - __main__ - INFO - Epoch 85: Loss = 0.1535 Valid loss = 0.1393 roc = 0.9210
2023-11-08 03:18:32,743 - __main__ - INFO - Epoch 86 Batch 0: Train Loss = 0.1419
2023-11-08 03:18:51,014 - __main__ - INFO - Epoch 86 Batch 20: Train Loss = 0.1579
2023-11-08 03:19:08,059 - __main__ - INFO - Epoch 86 Batch 40: Train Loss = 0.1706
2023-11-08 03:19:26,547 - __main__ - INFO - Epoch 86 Batch 60: Train Loss = 0.1943
2023-11-08 03:19:44,827 - __main__ - INFO - Epoch 86 Batch 80: Train Loss = 0.1088
2023-11-08 03:20:02,780 - __main__ - INFO - Epoch 86 Batch 100: Train Loss = 0.1946
2023-11-08 03:20:19,527 - __main__ - INFO - Epoch 86 Batch 120: Train Loss = 0.1691
2023-11-08 03:20:32,608 - __main__ - INFO - Epoch 86: Loss = 0.1558 Valid loss = 0.1414 roc = 0.9212
2023-11-08 03:20:33,254 - __main__ - INFO - Epoch 87 Batch 0: Train Loss = 0.1087
2023-11-08 03:20:51,007 - __main__ - INFO - Epoch 87 Batch 20: Train Loss = 0.1581
2023-11-08 03:21:08,662 - __main__ - INFO - Epoch 87 Batch 40: Train Loss = 0.1465
2023-11-08 03:21:25,021 - __main__ - INFO - Epoch 87 Batch 60: Train Loss = 0.1104
2023-11-08 03:21:41,636 - __main__ - INFO - Epoch 87 Batch 80: Train Loss = 0.1528
2023-11-08 03:21:59,328 - __main__ - INFO - Epoch 87 Batch 100: Train Loss = 0.1614
2023-11-08 03:22:16,844 - __main__ - INFO - Epoch 87 Batch 120: Train Loss = 0.1597
2023-11-08 03:22:29,538 - __main__ - INFO - Epoch 87: Loss = 0.1550 Valid loss = 0.1380 roc = 0.9231
2023-11-08 03:22:30,555 - __main__ - INFO - Epoch 88 Batch 0: Train Loss = 0.1496
2023-11-08 03:22:48,711 - __main__ - INFO - Epoch 88 Batch 20: Train Loss = 0.1837
2023-11-08 03:23:07,231 - __main__ - INFO - Epoch 88 Batch 40: Train Loss = 0.1719
2023-11-08 03:23:24,419 - __main__ - INFO - Epoch 88 Batch 60: Train Loss = 0.1244
2023-11-08 03:23:42,241 - __main__ - INFO - Epoch 88 Batch 80: Train Loss = 0.1602
2023-11-08 03:24:00,287 - __main__ - INFO - Epoch 88 Batch 100: Train Loss = 0.0974
2023-11-08 03:24:17,877 - __main__ - INFO - Epoch 88 Batch 120: Train Loss = 0.1442
2023-11-08 03:24:31,682 - __main__ - INFO - Epoch 88: Loss = 0.1551 Valid loss = 0.1403 roc = 0.9208
2023-11-08 03:24:32,739 - __main__ - INFO - Epoch 89 Batch 0: Train Loss = 0.1222
2023-11-08 03:24:51,777 - __main__ - INFO - Epoch 89 Batch 20: Train Loss = 0.1808
2023-11-08 03:25:08,459 - __main__ - INFO - Epoch 89 Batch 40: Train Loss = 0.1561
2023-11-08 03:25:25,937 - __main__ - INFO - Epoch 89 Batch 60: Train Loss = 0.1233
2023-11-08 03:25:42,783 - __main__ - INFO - Epoch 89 Batch 80: Train Loss = 0.1436
2023-11-08 03:25:59,684 - __main__ - INFO - Epoch 89 Batch 100: Train Loss = 0.1388
2023-11-08 03:26:16,826 - __main__ - INFO - Epoch 89 Batch 120: Train Loss = 0.2311
2023-11-08 03:26:30,782 - __main__ - INFO - Epoch 89: Loss = 0.1563 Valid loss = 0.1381 roc = 0.9193
2023-11-08 03:26:31,408 - __main__ - INFO - Epoch 90 Batch 0: Train Loss = 0.1437
2023-11-08 03:26:49,667 - __main__ - INFO - Epoch 90 Batch 20: Train Loss = 0.1120
2023-11-08 03:27:07,963 - __main__ - INFO - Epoch 90 Batch 40: Train Loss = 0.1528
2023-11-08 03:27:24,892 - __main__ - INFO - Epoch 90 Batch 60: Train Loss = 0.1468
2023-11-08 03:27:42,953 - __main__ - INFO - Epoch 90 Batch 80: Train Loss = 0.1828
2023-11-08 03:28:00,914 - __main__ - INFO - Epoch 90 Batch 100: Train Loss = 0.1362
2023-11-08 03:28:18,127 - __main__ - INFO - Epoch 90 Batch 120: Train Loss = 0.1562
2023-11-08 03:28:31,367 - __main__ - INFO - Epoch 90: Loss = 0.1514 Valid loss = 0.1374 roc = 0.9197
2023-11-08 03:28:32,510 - __main__ - INFO - Epoch 91 Batch 0: Train Loss = 0.1952
2023-11-08 03:28:51,317 - __main__ - INFO - Epoch 91 Batch 20: Train Loss = 0.1346
2023-11-08 03:29:09,562 - __main__ - INFO - Epoch 91 Batch 40: Train Loss = 0.2089
2023-11-08 03:29:27,219 - __main__ - INFO - Epoch 91 Batch 60: Train Loss = 0.1208
2023-11-08 03:29:45,323 - __main__ - INFO - Epoch 91 Batch 80: Train Loss = 0.1459
2023-11-08 03:30:02,686 - __main__ - INFO - Epoch 91 Batch 100: Train Loss = 0.2345
2023-11-08 03:30:19,846 - __main__ - INFO - Epoch 91 Batch 120: Train Loss = 0.0911
2023-11-08 03:30:32,901 - __main__ - INFO - Epoch 91: Loss = 0.1538 Valid loss = 0.1382 roc = 0.9191
2023-11-08 03:30:33,795 - __main__ - INFO - Epoch 92 Batch 0: Train Loss = 0.1324
2023-11-08 03:30:52,124 - __main__ - INFO - Epoch 92 Batch 20: Train Loss = 0.1587
2023-11-08 03:31:10,264 - __main__ - INFO - Epoch 92 Batch 40: Train Loss = 0.1273
2023-11-08 03:31:27,948 - __main__ - INFO - Epoch 92 Batch 60: Train Loss = 0.1095
2023-11-08 03:31:45,611 - __main__ - INFO - Epoch 92 Batch 80: Train Loss = 0.1133
2023-11-08 03:32:03,376 - __main__ - INFO - Epoch 92 Batch 100: Train Loss = 0.2136
2023-11-08 03:32:20,558 - __main__ - INFO - Epoch 92 Batch 120: Train Loss = 0.1656
2023-11-08 03:32:33,581 - __main__ - INFO - Epoch 92: Loss = 0.1517 Valid loss = 0.1379 roc = 0.9194
2023-11-08 03:32:34,528 - __main__ - INFO - Epoch 93 Batch 0: Train Loss = 0.1713
2023-11-08 03:32:53,799 - __main__ - INFO - Epoch 93 Batch 20: Train Loss = 0.1211
2023-11-08 03:33:12,317 - __main__ - INFO - Epoch 93 Batch 40: Train Loss = 0.1367
2023-11-08 03:33:29,259 - __main__ - INFO - Epoch 93 Batch 60: Train Loss = 0.0884
2023-11-08 03:33:45,622 - __main__ - INFO - Epoch 93 Batch 80: Train Loss = 0.1479
2023-11-08 03:34:03,649 - __main__ - INFO - Epoch 93 Batch 100: Train Loss = 0.1458
2023-11-08 03:34:20,627 - __main__ - INFO - Epoch 93 Batch 120: Train Loss = 0.1680
2023-11-08 03:34:33,802 - __main__ - INFO - Epoch 93: Loss = 0.1509 Valid loss = 0.1362 roc = 0.9231
2023-11-08 03:34:35,016 - __main__ - INFO - Epoch 94 Batch 0: Train Loss = 0.1364
2023-11-08 03:34:52,617 - __main__ - INFO - Epoch 94 Batch 20: Train Loss = 0.1315
2023-11-08 03:35:09,639 - __main__ - INFO - Epoch 94 Batch 40: Train Loss = 0.1542
2023-11-08 03:35:27,975 - __main__ - INFO - Epoch 94 Batch 60: Train Loss = 0.1437
2023-11-08 03:35:44,095 - __main__ - INFO - Epoch 94 Batch 80: Train Loss = 0.1858
2023-11-08 03:36:01,079 - __main__ - INFO - Epoch 94 Batch 100: Train Loss = 0.1356
2023-11-08 03:36:18,021 - __main__ - INFO - Epoch 94 Batch 120: Train Loss = 0.1157
2023-11-08 03:36:32,329 - __main__ - INFO - Epoch 94: Loss = 0.1489 Valid loss = 0.1372 roc = 0.9178
2023-11-08 03:36:33,451 - __main__ - INFO - Epoch 95 Batch 0: Train Loss = 0.1235
2023-11-08 03:36:51,483 - __main__ - INFO - Epoch 95 Batch 20: Train Loss = 0.1379
2023-11-08 03:37:07,767 - __main__ - INFO - Epoch 95 Batch 40: Train Loss = 0.1904
2023-11-08 03:37:25,451 - __main__ - INFO - Epoch 95 Batch 60: Train Loss = 0.1553
2023-11-08 03:37:42,368 - __main__ - INFO - Epoch 95 Batch 80: Train Loss = 0.2152
2023-11-08 03:37:59,767 - __main__ - INFO - Epoch 95 Batch 100: Train Loss = 0.1139
2023-11-08 03:38:17,203 - __main__ - INFO - Epoch 95 Batch 120: Train Loss = 0.1296
2023-11-08 03:38:30,789 - __main__ - INFO - Epoch 95: Loss = 0.1474 Valid loss = 0.1341 roc = 0.9259
2023-11-08 03:38:31,631 - __main__ - INFO - Epoch 96 Batch 0: Train Loss = 0.1981
2023-11-08 03:38:48,685 - __main__ - INFO - Epoch 96 Batch 20: Train Loss = 0.2050
2023-11-08 03:39:07,541 - __main__ - INFO - Epoch 96 Batch 40: Train Loss = 0.1840
2023-11-08 03:39:25,871 - __main__ - INFO - Epoch 96 Batch 60: Train Loss = 0.1722
2023-11-08 03:39:43,282 - __main__ - INFO - Epoch 96 Batch 80: Train Loss = 0.1488
2023-11-08 03:40:01,242 - __main__ - INFO - Epoch 96 Batch 100: Train Loss = 0.1521
2023-11-08 03:40:18,912 - __main__ - INFO - Epoch 96 Batch 120: Train Loss = 0.1829
2023-11-08 03:40:31,813 - __main__ - INFO - Epoch 96: Loss = 0.1535 Valid loss = 0.1362 roc = 0.9224
2023-11-08 03:40:32,687 - __main__ - INFO - Epoch 97 Batch 0: Train Loss = 0.1262
2023-11-08 03:40:50,444 - __main__ - INFO - Epoch 97 Batch 20: Train Loss = 0.1375
2023-11-08 03:41:07,589 - __main__ - INFO - Epoch 97 Batch 40: Train Loss = 0.1224
2023-11-08 03:41:24,814 - __main__ - INFO - Epoch 97 Batch 60: Train Loss = 0.1464
2023-11-08 03:41:41,756 - __main__ - INFO - Epoch 97 Batch 80: Train Loss = 0.1505
2023-11-08 03:41:59,150 - __main__ - INFO - Epoch 97 Batch 100: Train Loss = 0.1206
2023-11-08 03:42:17,507 - __main__ - INFO - Epoch 97 Batch 120: Train Loss = 0.1264
2023-11-08 03:42:31,174 - __main__ - INFO - Epoch 97: Loss = 0.1500 Valid loss = 0.1395 roc = 0.9151
2023-11-08 03:42:31,888 - __main__ - INFO - Epoch 98 Batch 0: Train Loss = 0.1262
2023-11-08 03:42:48,843 - __main__ - INFO - Epoch 98 Batch 20: Train Loss = 0.1582
2023-11-08 03:43:07,117 - __main__ - INFO - Epoch 98 Batch 40: Train Loss = 0.1772
2023-11-08 03:43:24,277 - __main__ - INFO - Epoch 98 Batch 60: Train Loss = 0.1602
2023-11-08 03:43:41,435 - __main__ - INFO - Epoch 98 Batch 80: Train Loss = 0.1318
2023-11-08 03:43:59,114 - __main__ - INFO - Epoch 98 Batch 100: Train Loss = 0.1198
2023-11-08 03:44:16,295 - __main__ - INFO - Epoch 98 Batch 120: Train Loss = 0.1653
2023-11-08 03:44:29,142 - __main__ - INFO - Epoch 98: Loss = 0.1509 Valid loss = 0.1395 roc = 0.9174
2023-11-08 03:44:29,900 - __main__ - INFO - Epoch 99 Batch 0: Train Loss = 0.1446
2023-11-08 03:44:47,558 - __main__ - INFO - Epoch 99 Batch 20: Train Loss = 0.1629
2023-11-08 03:45:06,845 - __main__ - INFO - Epoch 99 Batch 40: Train Loss = 0.1537
2023-11-08 03:45:23,876 - __main__ - INFO - Epoch 99 Batch 60: Train Loss = 0.1469
2023-11-08 03:45:40,572 - __main__ - INFO - Epoch 99 Batch 80: Train Loss = 0.1087
2023-11-08 03:45:59,335 - __main__ - INFO - Epoch 99 Batch 100: Train Loss = 0.1857
2023-11-08 03:46:15,617 - __main__ - INFO - Epoch 99 Batch 120: Train Loss = 0.1543
2023-11-08 03:46:28,669 - __main__ - INFO - Epoch 99: Loss = 0.1515 Valid loss = 0.1370 roc = 0.9222
2023-11-08 03:46:29,717 - __main__ - INFO - Epoch 100 Batch 0: Train Loss = 0.1663
2023-11-08 03:46:48,739 - __main__ - INFO - Epoch 100 Batch 20: Train Loss = 0.1198
2023-11-08 03:47:06,227 - __main__ - INFO - Epoch 100 Batch 40: Train Loss = 0.1864
2023-11-08 03:47:23,901 - __main__ - INFO - Epoch 100 Batch 60: Train Loss = 0.1242
2023-11-08 03:47:41,163 - __main__ - INFO - Epoch 100 Batch 80: Train Loss = 0.1316
2023-11-08 03:47:58,406 - __main__ - INFO - Epoch 100 Batch 100: Train Loss = 0.1496
2023-11-08 03:48:15,424 - __main__ - INFO - Epoch 100 Batch 120: Train Loss = 0.1605
2023-11-08 03:48:28,548 - __main__ - INFO - Epoch 100: Loss = 0.1497 Valid loss = 0.1382 roc = 0.9209
2023-11-08 03:48:29,265 - __main__ - INFO - Epoch 101 Batch 0: Train Loss = 0.1637
2023-11-08 03:48:47,394 - __main__ - INFO - Epoch 101 Batch 20: Train Loss = 0.1382
2023-11-08 03:49:06,121 - __main__ - INFO - Epoch 101 Batch 40: Train Loss = 0.1672
2023-11-08 03:49:23,106 - __main__ - INFO - Epoch 101 Batch 60: Train Loss = 0.1620
2023-11-08 03:49:40,753 - __main__ - INFO - Epoch 101 Batch 80: Train Loss = 0.1598
2023-11-08 03:49:57,050 - __main__ - INFO - Epoch 101 Batch 100: Train Loss = 0.1108
2023-11-08 03:50:14,537 - __main__ - INFO - Epoch 101 Batch 120: Train Loss = 0.1224
2023-11-08 03:50:28,365 - __main__ - INFO - Epoch 101: Loss = 0.1497 Valid loss = 0.1337 roc = 0.9242
2023-11-08 03:50:29,419 - __main__ - INFO - Epoch 102 Batch 0: Train Loss = 0.1612
2023-11-08 03:50:46,553 - __main__ - INFO - Epoch 102 Batch 20: Train Loss = 0.1431
2023-11-08 03:51:04,420 - __main__ - INFO - Epoch 102 Batch 40: Train Loss = 0.1254
2023-11-08 03:51:22,602 - __main__ - INFO - Epoch 102 Batch 60: Train Loss = 0.1160
2023-11-08 03:51:41,473 - __main__ - INFO - Epoch 102 Batch 80: Train Loss = 0.1497
2023-11-08 03:51:58,888 - __main__ - INFO - Epoch 102 Batch 100: Train Loss = 0.2260
2023-11-08 03:52:16,843 - __main__ - INFO - Epoch 102 Batch 120: Train Loss = 0.1320
2023-11-08 03:52:30,056 - __main__ - INFO - Epoch 102: Loss = 0.1477 Valid loss = 0.1367 roc = 0.9216
2023-11-08 03:52:31,012 - __main__ - INFO - Epoch 103 Batch 0: Train Loss = 0.1674
2023-11-08 03:52:48,358 - __main__ - INFO - Epoch 103 Batch 20: Train Loss = 0.1664
2023-11-08 03:53:06,453 - __main__ - INFO - Epoch 103 Batch 40: Train Loss = 0.1888
2023-11-08 03:53:24,677 - __main__ - INFO - Epoch 103 Batch 60: Train Loss = 0.1279
2023-11-08 03:53:42,931 - __main__ - INFO - Epoch 103 Batch 80: Train Loss = 0.0802
2023-11-08 03:54:00,174 - __main__ - INFO - Epoch 103 Batch 100: Train Loss = 0.1499
2023-11-08 03:54:16,905 - __main__ - INFO - Epoch 103 Batch 120: Train Loss = 0.0999
2023-11-08 03:54:30,708 - __main__ - INFO - Epoch 103: Loss = 0.1508 Valid loss = 0.1444 roc = 0.9174
2023-11-08 03:54:31,599 - __main__ - INFO - Epoch 104 Batch 0: Train Loss = 0.1327
2023-11-08 03:54:49,040 - __main__ - INFO - Epoch 104 Batch 20: Train Loss = 0.1861
2023-11-08 03:55:05,934 - __main__ - INFO - Epoch 104 Batch 40: Train Loss = 0.1650
2023-11-08 03:55:23,372 - __main__ - INFO - Epoch 104 Batch 60: Train Loss = 0.1110
2023-11-08 03:55:41,197 - __main__ - INFO - Epoch 104 Batch 80: Train Loss = 0.1361
2023-11-08 03:55:59,566 - __main__ - INFO - Epoch 104 Batch 100: Train Loss = 0.1602
2023-11-08 03:56:17,169 - __main__ - INFO - Epoch 104 Batch 120: Train Loss = 0.2347
2023-11-08 03:56:31,037 - __main__ - INFO - Epoch 104: Loss = 0.1551 Valid loss = 0.1378 roc = 0.9198
2023-11-08 03:56:31,812 - __main__ - INFO - Epoch 105 Batch 0: Train Loss = 0.1857
2023-11-08 03:56:49,996 - __main__ - INFO - Epoch 105 Batch 20: Train Loss = 0.1516
2023-11-08 03:57:08,851 - __main__ - INFO - Epoch 105 Batch 40: Train Loss = 0.1499
2023-11-08 03:57:27,224 - __main__ - INFO - Epoch 105 Batch 60: Train Loss = 0.1660
2023-11-08 03:57:44,102 - __main__ - INFO - Epoch 105 Batch 80: Train Loss = 0.1319
2023-11-08 03:58:00,939 - __main__ - INFO - Epoch 105 Batch 100: Train Loss = 0.1233
2023-11-08 03:58:17,211 - __main__ - INFO - Epoch 105 Batch 120: Train Loss = 0.1607
2023-11-08 03:58:29,678 - __main__ - INFO - Epoch 105: Loss = 0.1509 Valid loss = 0.1358 roc = 0.9247
2023-11-08 03:58:30,565 - __main__ - INFO - Epoch 106 Batch 0: Train Loss = 0.2114
2023-11-08 03:58:49,876 - __main__ - INFO - Epoch 106 Batch 20: Train Loss = 0.2185
2023-11-08 03:59:07,911 - __main__ - INFO - Epoch 106 Batch 40: Train Loss = 0.1232
2023-11-08 03:59:25,416 - __main__ - INFO - Epoch 106 Batch 60: Train Loss = 0.1825
2023-11-08 03:59:41,995 - __main__ - INFO - Epoch 106 Batch 80: Train Loss = 0.1492
2023-11-08 03:59:59,774 - __main__ - INFO - Epoch 106 Batch 100: Train Loss = 0.1094
2023-11-08 04:00:17,615 - __main__ - INFO - Epoch 106 Batch 120: Train Loss = 0.1027
2023-11-08 04:00:30,611 - __main__ - INFO - Epoch 106: Loss = 0.1513 Valid loss = 0.1374 roc = 0.9193
2023-11-08 04:00:31,705 - __main__ - INFO - Epoch 107 Batch 0: Train Loss = 0.1397
2023-11-08 04:00:50,351 - __main__ - INFO - Epoch 107 Batch 20: Train Loss = 0.1607
2023-11-08 04:01:07,186 - __main__ - INFO - Epoch 107 Batch 40: Train Loss = 0.1883
2023-11-08 04:01:24,905 - __main__ - INFO - Epoch 107 Batch 60: Train Loss = 0.2099
2023-11-08 04:01:40,716 - __main__ - INFO - Epoch 107 Batch 80: Train Loss = 0.1119
2023-11-08 04:01:58,391 - __main__ - INFO - Epoch 107 Batch 100: Train Loss = 0.1833
2023-11-08 04:02:13,273 - __main__ - INFO - Epoch 107 Batch 120: Train Loss = 0.0908
2023-11-08 04:02:27,181 - __main__ - INFO - Epoch 107: Loss = 0.1531 Valid loss = 0.1386 roc = 0.9194
2023-11-08 04:02:28,339 - __main__ - INFO - Epoch 108 Batch 0: Train Loss = 0.2251
2023-11-08 04:02:45,772 - __main__ - INFO - Epoch 108 Batch 20: Train Loss = 0.1568
2023-11-08 04:03:02,553 - __main__ - INFO - Epoch 108 Batch 40: Train Loss = 0.1981
2023-11-08 04:03:19,922 - __main__ - INFO - Epoch 108 Batch 60: Train Loss = 0.1424
2023-11-08 04:03:38,419 - __main__ - INFO - Epoch 108 Batch 80: Train Loss = 0.2044
2023-11-08 04:03:55,952 - __main__ - INFO - Epoch 108 Batch 100: Train Loss = 0.1061
2023-11-08 04:04:13,887 - __main__ - INFO - Epoch 108 Batch 120: Train Loss = 0.2172
2023-11-08 04:04:26,545 - __main__ - INFO - Epoch 108: Loss = 0.1504 Valid loss = 0.1361 roc = 0.9226
2023-11-08 04:04:27,560 - __main__ - INFO - Epoch 109 Batch 0: Train Loss = 0.1058
2023-11-08 04:04:46,922 - __main__ - INFO - Epoch 109 Batch 20: Train Loss = 0.1199
2023-11-08 04:05:04,401 - __main__ - INFO - Epoch 109 Batch 40: Train Loss = 0.2120
2023-11-08 04:05:22,248 - __main__ - INFO - Epoch 109 Batch 60: Train Loss = 0.1757
2023-11-08 04:05:39,617 - __main__ - INFO - Epoch 109 Batch 80: Train Loss = 0.1634
2023-11-08 04:05:57,906 - __main__ - INFO - Epoch 109 Batch 100: Train Loss = 0.1078
2023-11-08 04:06:14,522 - __main__ - INFO - Epoch 109 Batch 120: Train Loss = 0.1359
2023-11-08 04:06:28,040 - __main__ - INFO - Epoch 109: Loss = 0.1476 Valid loss = 0.1374 roc = 0.9186
2023-11-08 04:06:28,845 - __main__ - INFO - Epoch 110 Batch 0: Train Loss = 0.1429
2023-11-08 04:06:46,458 - __main__ - INFO - Epoch 110 Batch 20: Train Loss = 0.1722
2023-11-08 04:07:05,314 - __main__ - INFO - Epoch 110 Batch 40: Train Loss = 0.1539
2023-11-08 04:07:21,811 - __main__ - INFO - Epoch 110 Batch 60: Train Loss = 0.0903
2023-11-08 04:07:38,454 - __main__ - INFO - Epoch 110 Batch 80: Train Loss = 0.1647
2023-11-08 04:07:54,951 - __main__ - INFO - Epoch 110 Batch 100: Train Loss = 0.1422
2023-11-08 04:08:13,198 - __main__ - INFO - Epoch 110 Batch 120: Train Loss = 0.1440
2023-11-08 04:08:26,244 - __main__ - INFO - Epoch 110: Loss = 0.1457 Valid loss = 0.1377 roc = 0.9246
2023-11-08 04:08:27,404 - __main__ - INFO - Epoch 111 Batch 0: Train Loss = 0.1175
2023-11-08 04:08:46,116 - __main__ - INFO - Epoch 111 Batch 20: Train Loss = 0.1652
2023-11-08 04:09:04,048 - __main__ - INFO - Epoch 111 Batch 40: Train Loss = 0.1601
2023-11-08 04:09:20,655 - __main__ - INFO - Epoch 111 Batch 60: Train Loss = 0.1275
2023-11-08 04:09:38,827 - __main__ - INFO - Epoch 111 Batch 80: Train Loss = 0.2026
2023-11-08 04:09:55,991 - __main__ - INFO - Epoch 111 Batch 100: Train Loss = 0.1129
2023-11-08 04:10:13,482 - __main__ - INFO - Epoch 111 Batch 120: Train Loss = 0.1543
2023-11-08 04:10:26,818 - __main__ - INFO - Epoch 111: Loss = 0.1487 Valid loss = 0.1345 roc = 0.9246
2023-11-08 04:10:27,867 - __main__ - INFO - Epoch 112 Batch 0: Train Loss = 0.1399
2023-11-08 04:10:45,695 - __main__ - INFO - Epoch 112 Batch 20: Train Loss = 0.2313
2023-11-08 04:11:03,791 - __main__ - INFO - Epoch 112 Batch 40: Train Loss = 0.1912
2023-11-08 04:11:20,395 - __main__ - INFO - Epoch 112 Batch 60: Train Loss = 0.1047
2023-11-08 04:11:38,923 - __main__ - INFO - Epoch 112 Batch 80: Train Loss = 0.0664
2023-11-08 04:11:57,362 - __main__ - INFO - Epoch 112 Batch 100: Train Loss = 0.1334
2023-11-08 04:12:14,715 - __main__ - INFO - Epoch 112 Batch 120: Train Loss = 0.1112
2023-11-08 04:12:28,253 - __main__ - INFO - Epoch 112: Loss = 0.1530 Valid loss = 0.1343 roc = 0.9254
2023-11-08 04:12:29,131 - __main__ - INFO - Epoch 113 Batch 0: Train Loss = 0.1612
2023-11-08 04:12:47,544 - __main__ - INFO - Epoch 113 Batch 20: Train Loss = 0.1823
2023-11-08 04:13:04,464 - __main__ - INFO - Epoch 113 Batch 40: Train Loss = 0.1182
2023-11-08 04:13:21,884 - __main__ - INFO - Epoch 113 Batch 60: Train Loss = 0.0923
2023-11-08 04:13:38,883 - __main__ - INFO - Epoch 113 Batch 80: Train Loss = 0.1279
2023-11-08 04:13:56,220 - __main__ - INFO - Epoch 113 Batch 100: Train Loss = 0.1333
2023-11-08 04:14:14,216 - __main__ - INFO - Epoch 113 Batch 120: Train Loss = 0.1472
2023-11-08 04:14:27,947 - __main__ - INFO - Epoch 113: Loss = 0.1482 Valid loss = 0.1333 roc = 0.9261
2023-11-08 04:14:29,314 - __main__ - INFO - Epoch 114 Batch 0: Train Loss = 0.1312
2023-11-08 04:14:48,291 - __main__ - INFO - Epoch 114 Batch 20: Train Loss = 0.1472
2023-11-08 04:15:06,017 - __main__ - INFO - Epoch 114 Batch 40: Train Loss = 0.1477
2023-11-08 04:15:23,264 - __main__ - INFO - Epoch 114 Batch 60: Train Loss = 0.1602
2023-11-08 04:15:40,634 - __main__ - INFO - Epoch 114 Batch 80: Train Loss = 0.1582
2023-11-08 04:15:57,807 - __main__ - INFO - Epoch 114 Batch 100: Train Loss = 0.1177
2023-11-08 04:16:15,138 - __main__ - INFO - Epoch 114 Batch 120: Train Loss = 0.2395
2023-11-08 04:16:28,982 - __main__ - INFO - Epoch 114: Loss = 0.1521 Valid loss = 0.1348 roc = 0.9217
2023-11-08 04:16:30,137 - __main__ - INFO - Epoch 115 Batch 0: Train Loss = 0.1344
2023-11-08 04:16:47,722 - __main__ - INFO - Epoch 115 Batch 20: Train Loss = 0.1216
2023-11-08 04:17:05,958 - __main__ - INFO - Epoch 115 Batch 40: Train Loss = 0.1584
2023-11-08 04:17:23,597 - __main__ - INFO - Epoch 115 Batch 60: Train Loss = 0.1275
2023-11-08 04:17:41,299 - __main__ - INFO - Epoch 115 Batch 80: Train Loss = 0.1335
2023-11-08 04:17:58,487 - __main__ - INFO - Epoch 115 Batch 100: Train Loss = 0.1284
2023-11-08 04:18:15,499 - __main__ - INFO - Epoch 115 Batch 120: Train Loss = 0.2259
2023-11-08 04:18:29,752 - __main__ - INFO - Epoch 115: Loss = 0.1539 Valid loss = 0.1347 roc = 0.9221
2023-11-08 04:18:30,660 - __main__ - INFO - Epoch 116 Batch 0: Train Loss = 0.1430
2023-11-08 04:18:48,273 - __main__ - INFO - Epoch 116 Batch 20: Train Loss = 0.1248
2023-11-08 04:19:05,105 - __main__ - INFO - Epoch 116 Batch 40: Train Loss = 0.1423
2023-11-08 04:19:23,223 - __main__ - INFO - Epoch 116 Batch 60: Train Loss = 0.1184
2023-11-08 04:19:40,342 - __main__ - INFO - Epoch 116 Batch 80: Train Loss = 0.1316
2023-11-08 04:19:58,341 - __main__ - INFO - Epoch 116 Batch 100: Train Loss = 0.1291
2023-11-08 04:20:16,447 - __main__ - INFO - Epoch 116 Batch 120: Train Loss = 0.1277
2023-11-08 04:20:31,243 - __main__ - INFO - Epoch 116: Loss = 0.1444 Valid loss = 0.1344 roc = 0.9220
2023-11-08 04:20:32,172 - __main__ - INFO - Epoch 117 Batch 0: Train Loss = 0.1309
2023-11-08 04:20:49,672 - __main__ - INFO - Epoch 117 Batch 20: Train Loss = 0.1727
2023-11-08 04:21:08,296 - __main__ - INFO - Epoch 117 Batch 40: Train Loss = 0.2253
2023-11-08 04:21:25,247 - __main__ - INFO - Epoch 117 Batch 60: Train Loss = 0.1286
2023-11-08 04:21:43,237 - __main__ - INFO - Epoch 117 Batch 80: Train Loss = 0.1527
2023-11-08 04:22:00,350 - __main__ - INFO - Epoch 117 Batch 100: Train Loss = 0.1492
2023-11-08 04:22:18,315 - __main__ - INFO - Epoch 117 Batch 120: Train Loss = 0.1439
2023-11-08 04:22:32,988 - __main__ - INFO - Epoch 117: Loss = 0.1457 Valid loss = 0.1362 roc = 0.9215
2023-11-08 04:22:33,992 - __main__ - INFO - Epoch 118 Batch 0: Train Loss = 0.1241
2023-11-08 04:22:51,057 - __main__ - INFO - Epoch 118 Batch 20: Train Loss = 0.1511
2023-11-08 04:23:08,739 - __main__ - INFO - Epoch 118 Batch 40: Train Loss = 0.1002
2023-11-08 04:23:25,147 - __main__ - INFO - Epoch 118 Batch 60: Train Loss = 0.1850
2023-11-08 04:23:43,203 - __main__ - INFO - Epoch 118 Batch 80: Train Loss = 0.1850
2023-11-08 04:24:00,241 - __main__ - INFO - Epoch 118 Batch 100: Train Loss = 0.1310
2023-11-08 04:24:17,710 - __main__ - INFO - Epoch 118 Batch 120: Train Loss = 0.0963
2023-11-08 04:24:32,330 - __main__ - INFO - Epoch 118: Loss = 0.1443 Valid loss = 0.1362 roc = 0.9185
2023-11-08 04:24:33,468 - __main__ - INFO - Epoch 119 Batch 0: Train Loss = 0.1441
2023-11-08 04:24:50,656 - __main__ - INFO - Epoch 119 Batch 20: Train Loss = 0.1318
2023-11-08 04:25:06,103 - __main__ - INFO - Epoch 119 Batch 40: Train Loss = 0.1442
2023-11-08 04:25:24,043 - __main__ - INFO - Epoch 119 Batch 60: Train Loss = 0.1429
2023-11-08 04:25:40,576 - __main__ - INFO - Epoch 119 Batch 80: Train Loss = 0.1107
2023-11-08 04:25:58,011 - __main__ - INFO - Epoch 119 Batch 100: Train Loss = 0.1156
2023-11-08 04:26:15,214 - __main__ - INFO - Epoch 119 Batch 120: Train Loss = 0.1392
2023-11-08 04:26:29,284 - __main__ - INFO - Epoch 119: Loss = 0.1459 Valid loss = 0.1369 roc = 0.9184
2023-11-08 04:26:30,101 - __main__ - INFO - Epoch 120 Batch 0: Train Loss = 0.1331
2023-11-08 04:26:47,615 - __main__ - INFO - Epoch 120 Batch 20: Train Loss = 0.1365
2023-11-08 04:27:04,398 - __main__ - INFO - Epoch 120 Batch 40: Train Loss = 0.1156
2023-11-08 04:27:22,118 - __main__ - INFO - Epoch 120 Batch 60: Train Loss = 0.1133
2023-11-08 04:27:39,121 - __main__ - INFO - Epoch 120 Batch 80: Train Loss = 0.1609
2023-11-08 04:27:58,075 - __main__ - INFO - Epoch 120 Batch 100: Train Loss = 0.1767
2023-11-08 04:28:16,153 - __main__ - INFO - Epoch 120 Batch 120: Train Loss = 0.1146
2023-11-08 04:28:28,925 - __main__ - INFO - Epoch 120: Loss = 0.1453 Valid loss = 0.1341 roc = 0.9245
2023-11-08 04:28:30,246 - __main__ - INFO - Epoch 121 Batch 0: Train Loss = 0.1439
2023-11-08 04:28:47,834 - __main__ - INFO - Epoch 121 Batch 20: Train Loss = 0.1397
2023-11-08 04:29:05,293 - __main__ - INFO - Epoch 121 Batch 40: Train Loss = 0.1971
2023-11-08 04:29:23,227 - __main__ - INFO - Epoch 121 Batch 60: Train Loss = 0.1067
2023-11-08 04:29:40,006 - __main__ - INFO - Epoch 121 Batch 80: Train Loss = 0.1519
2023-11-08 04:29:56,899 - __main__ - INFO - Epoch 121 Batch 100: Train Loss = 0.1607
2023-11-08 04:30:15,049 - __main__ - INFO - Epoch 121 Batch 120: Train Loss = 0.1285
2023-11-08 04:30:29,627 - __main__ - INFO - Epoch 121: Loss = 0.1453 Valid loss = 0.1354 roc = 0.9224
2023-11-08 04:30:30,844 - __main__ - INFO - Epoch 122 Batch 0: Train Loss = 0.1297
2023-11-08 04:30:49,074 - __main__ - INFO - Epoch 122 Batch 20: Train Loss = 0.0687
2023-11-08 04:31:06,397 - __main__ - INFO - Epoch 122 Batch 40: Train Loss = 0.1518
2023-11-08 04:31:24,194 - __main__ - INFO - Epoch 122 Batch 60: Train Loss = 0.1629
2023-11-08 04:31:41,392 - __main__ - INFO - Epoch 122 Batch 80: Train Loss = 0.2225
2023-11-08 04:31:59,051 - __main__ - INFO - Epoch 122 Batch 100: Train Loss = 0.1909
2023-11-08 04:32:16,437 - __main__ - INFO - Epoch 122 Batch 120: Train Loss = 0.1473
2023-11-08 04:32:29,884 - __main__ - INFO - Epoch 122: Loss = 0.1420 Valid loss = 0.1354 roc = 0.9216
2023-11-08 04:32:30,833 - __main__ - INFO - Epoch 123 Batch 0: Train Loss = 0.1254
2023-11-08 04:32:48,694 - __main__ - INFO - Epoch 123 Batch 20: Train Loss = 0.1015
2023-11-08 04:33:07,039 - __main__ - INFO - Epoch 123 Batch 40: Train Loss = 0.1687
2023-11-08 04:33:25,940 - __main__ - INFO - Epoch 123 Batch 60: Train Loss = 0.1187
2023-11-08 04:33:42,991 - __main__ - INFO - Epoch 123 Batch 80: Train Loss = 0.1223
2023-11-08 04:34:00,134 - __main__ - INFO - Epoch 123 Batch 100: Train Loss = 0.1326
2023-11-08 04:34:18,881 - __main__ - INFO - Epoch 123 Batch 120: Train Loss = 0.1344
2023-11-08 04:34:33,838 - __main__ - INFO - Epoch 123: Loss = 0.1487 Valid loss = 0.1334 roc = 0.9234
2023-11-08 04:34:34,781 - __main__ - INFO - Epoch 124 Batch 0: Train Loss = 0.1945
2023-11-08 04:34:50,746 - __main__ - INFO - Epoch 124 Batch 20: Train Loss = 0.1574
2023-11-08 04:35:09,883 - __main__ - INFO - Epoch 124 Batch 40: Train Loss = 0.1795
2023-11-08 04:35:25,229 - __main__ - INFO - Epoch 124 Batch 60: Train Loss = 0.1062
2023-11-08 04:35:42,704 - __main__ - INFO - Epoch 124 Batch 80: Train Loss = 0.1527
2023-11-08 04:35:59,418 - __main__ - INFO - Epoch 124 Batch 100: Train Loss = 0.0857
2023-11-08 04:36:18,048 - __main__ - INFO - Epoch 124 Batch 120: Train Loss = 0.1461
2023-11-08 04:36:32,113 - __main__ - INFO - Epoch 124: Loss = 0.1423 Valid loss = 0.1322 roc = 0.9262
2023-11-08 04:36:33,384 - __main__ - INFO - Epoch 125 Batch 0: Train Loss = 0.1296
2023-11-08 04:36:51,307 - __main__ - INFO - Epoch 125 Batch 20: Train Loss = 0.1822
2023-11-08 04:37:08,123 - __main__ - INFO - Epoch 125 Batch 40: Train Loss = 0.1252
2023-11-08 04:37:25,478 - __main__ - INFO - Epoch 125 Batch 60: Train Loss = 0.1355
2023-11-08 04:37:43,857 - __main__ - INFO - Epoch 125 Batch 80: Train Loss = 0.1174
2023-11-08 04:38:01,258 - __main__ - INFO - Epoch 125 Batch 100: Train Loss = 0.1982
2023-11-08 04:38:19,153 - __main__ - INFO - Epoch 125 Batch 120: Train Loss = 0.1641
2023-11-08 04:38:34,140 - __main__ - INFO - Epoch 125: Loss = 0.1454 Valid loss = 0.1328 roc = 0.9230
2023-11-08 04:38:35,085 - __main__ - INFO - Epoch 126 Batch 0: Train Loss = 0.1070
2023-11-08 04:38:52,034 - __main__ - INFO - Epoch 126 Batch 20: Train Loss = 0.1444
2023-11-08 04:39:10,070 - __main__ - INFO - Epoch 126 Batch 40: Train Loss = 0.1198
2023-11-08 04:39:26,586 - __main__ - INFO - Epoch 126 Batch 60: Train Loss = 0.1294
2023-11-08 04:39:44,389 - __main__ - INFO - Epoch 126 Batch 80: Train Loss = 0.1240
2023-11-08 04:40:01,582 - __main__ - INFO - Epoch 126 Batch 100: Train Loss = 0.1424
2023-11-08 04:40:19,519 - __main__ - INFO - Epoch 126 Batch 120: Train Loss = 0.1570
2023-11-08 04:40:33,925 - __main__ - INFO - Epoch 126: Loss = 0.1481 Valid loss = 0.1330 roc = 0.9244
2023-11-08 04:40:34,821 - __main__ - INFO - Epoch 127 Batch 0: Train Loss = 0.1250
2023-11-08 04:40:52,089 - __main__ - INFO - Epoch 127 Batch 20: Train Loss = 0.1468
2023-11-08 04:41:09,293 - __main__ - INFO - Epoch 127 Batch 40: Train Loss = 0.1105
2023-11-08 04:41:26,383 - __main__ - INFO - Epoch 127 Batch 60: Train Loss = 0.1310
2023-11-08 04:41:44,089 - __main__ - INFO - Epoch 127 Batch 80: Train Loss = 0.0894
2023-11-08 04:42:00,870 - __main__ - INFO - Epoch 127 Batch 100: Train Loss = 0.1347
2023-11-08 04:42:19,865 - __main__ - INFO - Epoch 127 Batch 120: Train Loss = 0.1795
2023-11-08 04:42:34,173 - __main__ - INFO - Epoch 127: Loss = 0.1447 Valid loss = 0.1337 roc = 0.9231
2023-11-08 04:42:35,500 - __main__ - INFO - Epoch 128 Batch 0: Train Loss = 0.2351
2023-11-08 04:42:53,096 - __main__ - INFO - Epoch 128 Batch 20: Train Loss = 0.1426
2023-11-08 04:43:10,611 - __main__ - INFO - Epoch 128 Batch 40: Train Loss = 0.1200
2023-11-08 04:43:28,394 - __main__ - INFO - Epoch 128 Batch 60: Train Loss = 0.1721
2023-11-08 04:43:45,627 - __main__ - INFO - Epoch 128 Batch 80: Train Loss = 0.2078
2023-11-08 04:44:03,892 - __main__ - INFO - Epoch 128 Batch 100: Train Loss = 0.1604
2023-11-08 04:44:21,890 - __main__ - INFO - Epoch 128 Batch 120: Train Loss = 0.1396
2023-11-08 04:44:36,009 - __main__ - INFO - Epoch 128: Loss = 0.1440 Valid loss = 0.1340 roc = 0.9186
2023-11-08 04:44:36,779 - __main__ - INFO - Epoch 129 Batch 0: Train Loss = 0.1683
2023-11-08 04:44:54,171 - __main__ - INFO - Epoch 129 Batch 20: Train Loss = 0.1789
2023-11-08 04:45:12,678 - __main__ - INFO - Epoch 129 Batch 40: Train Loss = 0.1732
2023-11-08 04:45:30,805 - __main__ - INFO - Epoch 129 Batch 60: Train Loss = 0.1564
2023-11-08 04:45:48,925 - __main__ - INFO - Epoch 129 Batch 80: Train Loss = 0.0960
2023-11-08 04:46:06,337 - __main__ - INFO - Epoch 129 Batch 100: Train Loss = 0.0944
2023-11-08 04:46:22,969 - __main__ - INFO - Epoch 129 Batch 120: Train Loss = 0.1501
2023-11-08 04:46:35,691 - __main__ - INFO - Epoch 129: Loss = 0.1415 Valid loss = 0.1382 roc = 0.9150
2023-11-08 04:46:36,302 - __main__ - INFO - Epoch 130 Batch 0: Train Loss = 0.1434
2023-11-08 04:46:55,131 - __main__ - INFO - Epoch 130 Batch 20: Train Loss = 0.1184
2023-11-08 04:47:11,857 - __main__ - INFO - Epoch 130 Batch 40: Train Loss = 0.1128
2023-11-08 04:47:28,899 - __main__ - INFO - Epoch 130 Batch 60: Train Loss = 0.1047
2023-11-08 04:47:46,165 - __main__ - INFO - Epoch 130 Batch 80: Train Loss = 0.1072
2023-11-08 04:48:05,061 - __main__ - INFO - Epoch 130 Batch 100: Train Loss = 0.1543
2023-11-08 04:48:23,098 - __main__ - INFO - Epoch 130 Batch 120: Train Loss = 0.1315
2023-11-08 04:48:38,008 - __main__ - INFO - Epoch 130: Loss = 0.1463 Valid loss = 0.1359 roc = 0.9212
2023-11-08 04:48:38,964 - __main__ - INFO - Epoch 131 Batch 0: Train Loss = 0.1382
2023-11-08 04:48:56,294 - __main__ - INFO - Epoch 131 Batch 20: Train Loss = 0.1591
2023-11-08 04:49:13,097 - __main__ - INFO - Epoch 131 Batch 40: Train Loss = 0.1572
2023-11-08 04:49:30,769 - __main__ - INFO - Epoch 131 Batch 60: Train Loss = 0.1865
2023-11-08 04:49:49,046 - __main__ - INFO - Epoch 131 Batch 80: Train Loss = 0.1315
2023-11-08 04:50:06,253 - __main__ - INFO - Epoch 131 Batch 100: Train Loss = 0.1630
2023-11-08 04:50:23,001 - __main__ - INFO - Epoch 131 Batch 120: Train Loss = 0.1176
2023-11-08 04:50:37,187 - __main__ - INFO - Epoch 131: Loss = 0.1447 Valid loss = 0.1342 roc = 0.9219
2023-11-08 04:50:38,050 - __main__ - INFO - Epoch 132 Batch 0: Train Loss = 0.2044
2023-11-08 04:50:55,357 - __main__ - INFO - Epoch 132 Batch 20: Train Loss = 0.1649
2023-11-08 04:51:12,773 - __main__ - INFO - Epoch 132 Batch 40: Train Loss = 0.1499
2023-11-08 04:51:29,983 - __main__ - INFO - Epoch 132 Batch 60: Train Loss = 0.1128
2023-11-08 04:51:47,305 - __main__ - INFO - Epoch 132 Batch 80: Train Loss = 0.1648
2023-11-08 04:52:04,782 - __main__ - INFO - Epoch 132 Batch 100: Train Loss = 0.1229
2023-11-08 04:52:22,394 - __main__ - INFO - Epoch 132 Batch 120: Train Loss = 0.1193
2023-11-08 04:52:36,632 - __main__ - INFO - Epoch 132: Loss = 0.1468 Valid loss = 0.1364 roc = 0.9193
2023-11-08 04:52:37,460 - __main__ - INFO - Epoch 133 Batch 0: Train Loss = 0.1313
2023-11-08 04:52:54,002 - __main__ - INFO - Epoch 133 Batch 20: Train Loss = 0.1678
2023-11-08 04:53:11,122 - __main__ - INFO - Epoch 133 Batch 40: Train Loss = 0.1575
2023-11-08 04:53:29,131 - __main__ - INFO - Epoch 133 Batch 60: Train Loss = 0.1174
2023-11-08 04:53:47,847 - __main__ - INFO - Epoch 133 Batch 80: Train Loss = 0.1363
2023-11-08 04:54:05,437 - __main__ - INFO - Epoch 133 Batch 100: Train Loss = 0.1036
2023-11-08 04:54:23,657 - __main__ - INFO - Epoch 133 Batch 120: Train Loss = 0.1626
2023-11-08 04:54:37,340 - __main__ - INFO - Epoch 133: Loss = 0.1435 Valid loss = 0.1333 roc = 0.9252
2023-11-08 04:54:38,235 - __main__ - INFO - Epoch 134 Batch 0: Train Loss = 0.0881
2023-11-08 04:54:55,337 - __main__ - INFO - Epoch 134 Batch 20: Train Loss = 0.1688
2023-11-08 04:55:13,361 - __main__ - INFO - Epoch 134 Batch 40: Train Loss = 0.1356
2023-11-08 04:55:31,028 - __main__ - INFO - Epoch 134 Batch 60: Train Loss = 0.1157
2023-11-08 04:55:49,224 - __main__ - INFO - Epoch 134 Batch 80: Train Loss = 0.1629
2023-11-08 04:56:07,302 - __main__ - INFO - Epoch 134 Batch 100: Train Loss = 0.1536
2023-11-08 04:56:26,096 - __main__ - INFO - Epoch 134 Batch 120: Train Loss = 0.1234
2023-11-08 04:56:39,993 - __main__ - INFO - Epoch 134: Loss = 0.1480 Valid loss = 0.1388 roc = 0.9216
2023-11-08 04:56:41,064 - __main__ - INFO - Epoch 135 Batch 0: Train Loss = 0.1751
2023-11-08 04:56:59,645 - __main__ - INFO - Epoch 135 Batch 20: Train Loss = 0.1380
2023-11-08 04:57:16,793 - __main__ - INFO - Epoch 135 Batch 40: Train Loss = 0.1029
2023-11-08 04:57:33,682 - __main__ - INFO - Epoch 135 Batch 60: Train Loss = 0.1210
2023-11-08 04:57:51,442 - __main__ - INFO - Epoch 135 Batch 80: Train Loss = 0.1472
2023-11-08 04:58:10,050 - __main__ - INFO - Epoch 135 Batch 100: Train Loss = 0.1280
2023-11-08 04:58:28,087 - __main__ - INFO - Epoch 135 Batch 120: Train Loss = 0.1281
2023-11-08 04:58:41,386 - __main__ - INFO - Epoch 135: Loss = 0.1490 Valid loss = 0.1366 roc = 0.9238
2023-11-08 04:58:41,952 - __main__ - INFO - Epoch 136 Batch 0: Train Loss = 0.2082
2023-11-08 04:58:59,929 - __main__ - INFO - Epoch 136 Batch 20: Train Loss = 0.1057
2023-11-08 04:59:17,457 - __main__ - INFO - Epoch 136 Batch 40: Train Loss = 0.1812
2023-11-08 04:59:34,129 - __main__ - INFO - Epoch 136 Batch 60: Train Loss = 0.1338
2023-11-08 04:59:51,459 - __main__ - INFO - Epoch 136 Batch 80: Train Loss = 0.1715
2023-11-08 05:00:07,984 - __main__ - INFO - Epoch 136 Batch 100: Train Loss = 0.1325
2023-11-08 05:00:24,922 - __main__ - INFO - Epoch 136 Batch 120: Train Loss = 0.1038
2023-11-08 05:00:38,745 - __main__ - INFO - Epoch 136: Loss = 0.1471 Valid loss = 0.1362 roc = 0.9222
2023-11-08 05:00:39,362 - __main__ - INFO - Epoch 137 Batch 0: Train Loss = 0.1002
2023-11-08 05:00:55,747 - __main__ - INFO - Epoch 137 Batch 20: Train Loss = 0.1675
2023-11-08 05:01:13,303 - __main__ - INFO - Epoch 137 Batch 40: Train Loss = 0.1862
2023-11-08 05:01:32,215 - __main__ - INFO - Epoch 137 Batch 60: Train Loss = 0.0988
2023-11-08 05:01:51,296 - __main__ - INFO - Epoch 137 Batch 80: Train Loss = 0.1129
2023-11-08 05:02:09,083 - __main__ - INFO - Epoch 137 Batch 100: Train Loss = 0.1177
2023-11-08 05:02:25,716 - __main__ - INFO - Epoch 137 Batch 120: Train Loss = 0.1726
2023-11-08 05:02:40,411 - __main__ - INFO - Epoch 137: Loss = 0.1422 Valid loss = 0.1343 roc = 0.9237
2023-11-08 05:02:41,528 - __main__ - INFO - Epoch 138 Batch 0: Train Loss = 0.1565
2023-11-08 05:02:58,129 - __main__ - INFO - Epoch 138 Batch 20: Train Loss = 0.0961
2023-11-08 05:03:18,227 - __main__ - INFO - Epoch 138 Batch 40: Train Loss = 0.1785
2023-11-08 05:03:34,952 - __main__ - INFO - Epoch 138 Batch 60: Train Loss = 0.1388
2023-11-08 05:03:53,302 - __main__ - INFO - Epoch 138 Batch 80: Train Loss = 0.1356
2023-11-08 05:04:10,740 - __main__ - INFO - Epoch 138 Batch 100: Train Loss = 0.1266
2023-11-08 05:04:30,031 - __main__ - INFO - Epoch 138 Batch 120: Train Loss = 0.1099
2023-11-08 05:04:43,322 - __main__ - INFO - Epoch 138: Loss = 0.1425 Valid loss = 0.1378 roc = 0.9232
2023-11-08 05:04:44,033 - __main__ - INFO - Epoch 139 Batch 0: Train Loss = 0.1029
2023-11-08 05:05:01,331 - __main__ - INFO - Epoch 139 Batch 20: Train Loss = 0.1840
2023-11-08 05:05:18,839 - __main__ - INFO - Epoch 139 Batch 40: Train Loss = 0.1609
2023-11-08 05:05:36,338 - __main__ - INFO - Epoch 139 Batch 60: Train Loss = 0.1193
2023-11-08 05:05:53,042 - __main__ - INFO - Epoch 139 Batch 80: Train Loss = 0.1333
2023-11-08 05:06:11,411 - __main__ - INFO - Epoch 139 Batch 100: Train Loss = 0.1381
2023-11-08 05:06:27,802 - __main__ - INFO - Epoch 139 Batch 120: Train Loss = 0.1559
2023-11-08 05:06:41,740 - __main__ - INFO - Epoch 139: Loss = 0.1462 Valid loss = 0.1404 roc = 0.9182
2023-11-08 05:06:42,559 - __main__ - INFO - Epoch 140 Batch 0: Train Loss = 0.1013
2023-11-08 05:06:59,061 - __main__ - INFO - Epoch 140 Batch 20: Train Loss = 0.0743
2023-11-08 05:07:15,832 - __main__ - INFO - Epoch 140 Batch 40: Train Loss = 0.2030
2023-11-08 05:07:34,754 - __main__ - INFO - Epoch 140 Batch 60: Train Loss = 0.1458
2023-11-08 05:07:52,859 - __main__ - INFO - Epoch 140 Batch 80: Train Loss = 0.1906
2023-11-08 05:08:10,734 - __main__ - INFO - Epoch 140 Batch 100: Train Loss = 0.1791
2023-11-08 05:08:28,603 - __main__ - INFO - Epoch 140 Batch 120: Train Loss = 0.0897
2023-11-08 05:08:42,167 - __main__ - INFO - Epoch 140: Loss = 0.1424 Valid loss = 0.1374 roc = 0.9196
2023-11-08 05:08:43,083 - __main__ - INFO - Epoch 141 Batch 0: Train Loss = 0.1405
2023-11-08 05:08:58,694 - __main__ - INFO - Epoch 141 Batch 20: Train Loss = 0.1202
2023-11-08 05:09:17,292 - __main__ - INFO - Epoch 141 Batch 40: Train Loss = 0.1163
2023-11-08 05:09:33,959 - __main__ - INFO - Epoch 141 Batch 60: Train Loss = 0.1137
2023-11-08 05:09:52,061 - __main__ - INFO - Epoch 141 Batch 80: Train Loss = 0.1696
2023-11-08 05:10:08,151 - __main__ - INFO - Epoch 141 Batch 100: Train Loss = 0.1094
2023-11-08 05:10:25,782 - __main__ - INFO - Epoch 141 Batch 120: Train Loss = 0.1278
2023-11-08 05:10:39,114 - __main__ - INFO - Epoch 141: Loss = 0.1417 Valid loss = 0.1364 roc = 0.9191
2023-11-08 05:10:40,064 - __main__ - INFO - Epoch 142 Batch 0: Train Loss = 0.1746
2023-11-08 05:10:58,209 - __main__ - INFO - Epoch 142 Batch 20: Train Loss = 0.1460
2023-11-08 05:11:16,645 - __main__ - INFO - Epoch 142 Batch 40: Train Loss = 0.1031
2023-11-08 05:11:33,899 - __main__ - INFO - Epoch 142 Batch 60: Train Loss = 0.1135
2023-11-08 05:11:51,543 - __main__ - INFO - Epoch 142 Batch 80: Train Loss = 0.1271
2023-11-08 05:12:09,575 - __main__ - INFO - Epoch 142 Batch 100: Train Loss = 0.2128
2023-11-08 05:12:28,718 - __main__ - INFO - Epoch 142 Batch 120: Train Loss = 0.0972
2023-11-08 05:12:42,493 - __main__ - INFO - Epoch 142: Loss = 0.1437 Valid loss = 0.1352 roc = 0.9256
2023-11-08 05:12:43,404 - __main__ - INFO - Epoch 143 Batch 0: Train Loss = 0.1194
2023-11-08 05:13:01,159 - __main__ - INFO - Epoch 143 Batch 20: Train Loss = 0.1090
2023-11-08 05:13:19,835 - __main__ - INFO - Epoch 143 Batch 40: Train Loss = 0.1195
2023-11-08 05:13:36,289 - __main__ - INFO - Epoch 143 Batch 60: Train Loss = 0.1438
2023-11-08 05:13:54,184 - __main__ - INFO - Epoch 143 Batch 80: Train Loss = 0.1697
2023-11-08 05:14:10,527 - __main__ - INFO - Epoch 143 Batch 100: Train Loss = 0.1267
2023-11-08 05:14:26,998 - __main__ - INFO - Epoch 143 Batch 120: Train Loss = 0.1513
2023-11-08 05:14:39,839 - __main__ - INFO - Epoch 143: Loss = 0.1432 Valid loss = 0.1323 roc = 0.9250
2023-11-08 05:14:40,771 - __main__ - INFO - Epoch 144 Batch 0: Train Loss = 0.1736
2023-11-08 05:14:57,943 - __main__ - INFO - Epoch 144 Batch 20: Train Loss = 0.1694
2023-11-08 05:15:17,196 - __main__ - INFO - Epoch 144 Batch 40: Train Loss = 0.1091
2023-11-08 05:15:33,322 - __main__ - INFO - Epoch 144 Batch 60: Train Loss = 0.1472
2023-11-08 05:15:52,707 - __main__ - INFO - Epoch 144 Batch 80: Train Loss = 0.1207
2023-11-08 05:16:08,870 - __main__ - INFO - Epoch 144 Batch 100: Train Loss = 0.1612
2023-11-08 05:16:26,957 - __main__ - INFO - Epoch 144 Batch 120: Train Loss = 0.1727
2023-11-08 05:16:40,076 - __main__ - INFO - Epoch 144: Loss = 0.1450 Valid loss = 0.1343 roc = 0.9236
2023-11-08 05:16:40,911 - __main__ - INFO - Epoch 145 Batch 0: Train Loss = 0.1780
2023-11-08 05:16:58,507 - __main__ - INFO - Epoch 145 Batch 20: Train Loss = 0.1133
2023-11-08 05:17:16,485 - __main__ - INFO - Epoch 145 Batch 40: Train Loss = 0.1157
2023-11-08 05:17:33,213 - __main__ - INFO - Epoch 145 Batch 60: Train Loss = 0.2143
2023-11-08 05:17:50,343 - __main__ - INFO - Epoch 145 Batch 80: Train Loss = 0.1447
2023-11-08 05:18:08,091 - __main__ - INFO - Epoch 145 Batch 100: Train Loss = 0.1571
2023-11-08 05:18:26,370 - __main__ - INFO - Epoch 145 Batch 120: Train Loss = 0.1599
2023-11-08 05:18:39,491 - __main__ - INFO - Epoch 145: Loss = 0.1410 Valid loss = 0.1393 roc = 0.9159
2023-11-08 05:18:40,101 - __main__ - INFO - Epoch 146 Batch 0: Train Loss = 0.0875
2023-11-08 05:18:57,002 - __main__ - INFO - Epoch 146 Batch 20: Train Loss = 0.1659
2023-11-08 05:19:15,120 - __main__ - INFO - Epoch 146 Batch 40: Train Loss = 0.1797
2023-11-08 05:19:32,576 - __main__ - INFO - Epoch 146 Batch 60: Train Loss = 0.1172
2023-11-08 05:19:49,602 - __main__ - INFO - Epoch 146 Batch 80: Train Loss = 0.1742
2023-11-08 05:20:08,416 - __main__ - INFO - Epoch 146 Batch 100: Train Loss = 0.0905
2023-11-08 05:20:25,075 - __main__ - INFO - Epoch 146 Batch 120: Train Loss = 0.1416
2023-11-08 05:20:38,180 - __main__ - INFO - Epoch 146: Loss = 0.1459 Valid loss = 0.1349 roc = 0.9204
2023-11-08 05:20:39,073 - __main__ - INFO - Epoch 147 Batch 0: Train Loss = 0.1509
2023-11-08 05:20:57,330 - __main__ - INFO - Epoch 147 Batch 20: Train Loss = 0.1514
2023-11-08 05:21:15,060 - __main__ - INFO - Epoch 147 Batch 40: Train Loss = 0.1490
2023-11-08 05:21:32,411 - __main__ - INFO - Epoch 147 Batch 60: Train Loss = 0.1334
2023-11-08 05:21:49,851 - __main__ - INFO - Epoch 147 Batch 80: Train Loss = 0.1124
2023-11-08 05:22:07,580 - __main__ - INFO - Epoch 147 Batch 100: Train Loss = 0.1563
2023-11-08 05:22:24,441 - __main__ - INFO - Epoch 147 Batch 120: Train Loss = 0.1466
2023-11-08 05:22:38,904 - __main__ - INFO - Epoch 147: Loss = 0.1454 Valid loss = 0.1337 roc = 0.9219
2023-11-08 05:22:39,989 - __main__ - INFO - Epoch 148 Batch 0: Train Loss = 0.1222
2023-11-08 05:22:56,324 - __main__ - INFO - Epoch 148 Batch 20: Train Loss = 0.1894
2023-11-08 05:23:13,262 - __main__ - INFO - Epoch 148 Batch 40: Train Loss = 0.1289
2023-11-08 05:23:32,135 - __main__ - INFO - Epoch 148 Batch 60: Train Loss = 0.1769
2023-11-08 05:23:49,869 - __main__ - INFO - Epoch 148 Batch 80: Train Loss = 0.1377
2023-11-08 05:24:07,872 - __main__ - INFO - Epoch 148 Batch 100: Train Loss = 0.2440
2023-11-08 05:24:25,679 - __main__ - INFO - Epoch 148 Batch 120: Train Loss = 0.1260
2023-11-08 05:24:39,134 - __main__ - INFO - Epoch 148: Loss = 0.1406 Valid loss = 0.1346 roc = 0.9212
2023-11-08 05:24:40,006 - __main__ - INFO - Epoch 149 Batch 0: Train Loss = 0.1319
2023-11-08 05:24:57,100 - __main__ - INFO - Epoch 149 Batch 20: Train Loss = 0.0996
2023-11-08 05:25:15,112 - __main__ - INFO - Epoch 149 Batch 40: Train Loss = 0.0955
2023-11-08 05:25:32,712 - __main__ - INFO - Epoch 149 Batch 60: Train Loss = 0.2209
2023-11-08 05:25:51,218 - __main__ - INFO - Epoch 149 Batch 80: Train Loss = 0.1000
2023-11-08 05:26:09,443 - __main__ - INFO - Epoch 149 Batch 100: Train Loss = 0.1313
2023-11-08 05:26:26,796 - __main__ - INFO - Epoch 149 Batch 120: Train Loss = 0.1405
2023-11-08 05:26:39,972 - __main__ - INFO - Epoch 149: Loss = 0.1400 Valid loss = 0.1345 roc = 0.9239
2023-11-08 05:26:40,017 - __main__ - INFO - auroc 0.9262
2023-11-08 05:26:40,021 - __main__ - INFO - auprc 0.7242
2023-11-08 05:26:40,022 - __main__ - INFO - minpse 0.6541
2023-11-08 05:26:40,226 - __main__ - INFO - last saved model is in epoch 124
2023-11-08 05:26:40,963 - __main__ - INFO - Batch 0: Test Loss = 0.1390
2023-11-08 05:26:48,906 - __main__ - INFO - 
==>Predicting on test
2023-11-08 05:26:48,910 - __main__ - INFO - Test Loss = 0.1237
2023-11-08 05:26:48,992 - __main__ - INFO - load target data
2023-11-08 05:26:52,556 - __main__ - INFO - [[ 0.35672857  0.36219378 -0.04155072 -0.39134566  0.76306464 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [ 2.89139623 -0.16505646 -0.7197426  -0.5274138   0.76306464 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-0.72955757  0.88944403  1.54089701 -1.13972045  0.01202093 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-0.72955757  0.36219378  1.54089701 -1.13972045  0.01202093 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-0.09589066  1.41669427  0.63664116 -0.93561823 -0.09527102 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [ 2.07668162  1.41669427  0.63664116 -1.75202709 -0.09527102 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [ 0.08515703  0.88944403  0.18451324 -0.32331159 -0.09527102 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [ 1.08091933  1.41669427  0.4105772   0.9013017   1.19223247 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-0.54850988  1.94394452  0.18451324  1.98984685  1.08494051 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-0.54850988  1.41669427 -1.62399845 -1.0036523  -1.1681906  -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-1.72531987  1.41669427  0.4105772   0.28899506  1.51410834 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [ 0.17568088  1.41669427  0.18451324  0.56113134  2.05056813 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-0.63903373  1.41669427 -1.17187053 -1.0036523   0.54848072 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-1.27270064  1.41669427 -2.52825429  0.83326763 -0.30985494 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-1.27270064  0.88944403 -2.52825429  0.83326763 -0.30985494 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-1.1821768   1.94394452 -0.04155072  0.35702913  1.51410834 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-0.09589066  1.41669427 -0.49367864  0.4250632   0.11931289 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-1.27270064  0.88944403 -0.04155072  0.83326763  0.11931289 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [ 0.17568088  1.94394452  0.86270513 -0.79955009  0.97764855 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-0.91060526  1.94394452 -1.17187053  0.15292691  1.19223247 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-0.27693835  0.88944403 -0.94580656 -2.09219745 -1.38277452 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-1.54427218  0.88944403  0.18451324  1.58164242 -1.1681906  -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-0.27693835  1.41669427 -0.94580656  0.22096099  0.44118876 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-3.62632062  0.36219378 -1.39793449 -1.88809523  0.01202093 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-0.63903373  1.41669427 -1.17187053  0.01685877 -0.84631473 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-0.91060526  1.41669427 -0.26761468  0.49309727  0.01202093 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-1.09165295  0.88944403  0.4105772   1.51360835  0.22660485 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]
 [-2.35898678  1.41669427  0.4105772   1.3775402   1.51410834 -0.05394446
   0.23950892 -0.09317341  0.27449751 -0.08003117 -0.11276048 -0.19232824
  -0.32689733 -0.30210297 -0.1920395  -0.0936725  -0.05866173 -0.02507609
  -0.25397049 -0.38426823  0.1113217   0.10224702 -0.17622869 -0.22210885
  -0.04331355 -0.14866188]]
2023-11-08 05:26:52,597 - __main__ - INFO - 26
2023-11-08 05:26:52,615 - __main__ - INFO - 4255
2023-11-08 05:26:53,493 - __main__ - INFO - Batch 0: Test Loss = 0.1324
2023-11-08 05:27:05,405 - __main__ - INFO - Batch 20: Test Loss = 0.1524
2023-11-08 05:27:16,470 - __main__ - INFO - Batch 40: Test Loss = 0.1532
2023-11-08 05:27:26,623 - __main__ - INFO - Batch 60: Test Loss = 0.1168
2023-11-08 05:27:37,995 - __main__ - INFO - Batch 80: Test Loss = 0.0813
2023-11-08 05:27:48,779 - __main__ - INFO - Batch 100: Test Loss = 0.0730
2023-11-08 05:28:00,391 - __main__ - INFO - Batch 120: Test Loss = 0.1101
2023-11-08 05:28:03,787 - __main__ - INFO - Training Student
2023-11-08 05:28:05,029 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 0.7165
2023-11-08 05:28:30,399 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 0.5330
2023-11-08 05:28:53,573 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 0.3833
2023-11-08 05:29:17,755 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 0.3689
2023-11-08 05:29:42,034 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 0.2508
2023-11-08 05:30:04,440 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 0.2475
2023-11-08 05:30:28,575 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 0.2564
2023-11-08 05:30:43,767 - __main__ - INFO - ------------ Save best model - AUROC: 0.7610 ------------
2023-11-08 05:30:44,767 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 0.3054
2023-11-08 05:31:10,464 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 0.3109
2023-11-08 05:31:34,881 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 0.3227
2023-11-08 05:31:59,326 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 0.3487
2023-11-08 05:32:23,502 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 0.2560
2023-11-08 05:32:47,958 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 0.2296
2023-11-08 05:33:12,065 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 0.2761
2023-11-08 05:33:29,811 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 0.3008
2023-11-08 05:33:54,634 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 0.2899
2023-11-08 05:34:18,075 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 0.2995
2023-11-08 05:34:41,235 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 0.3475
2023-11-08 05:35:05,202 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 0.2296
2023-11-08 05:35:29,651 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 0.2301
2023-11-08 05:35:52,216 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 0.2693
2023-11-08 05:36:07,896 - __main__ - INFO - ------------ Save best model - AUROC: 0.7771 ------------
2023-11-08 05:36:09,198 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 0.2955
2023-11-08 05:36:33,805 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 0.2889
2023-11-08 05:36:56,043 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 0.3037
2023-11-08 05:37:19,598 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 0.2994
2023-11-08 05:37:42,214 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 0.1974
2023-11-08 05:38:05,359 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 0.2044
2023-11-08 05:38:28,083 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 0.2267
2023-11-08 05:38:42,419 - __main__ - INFO - ------------ Save best model - AUROC: 0.8112 ------------
2023-11-08 05:38:43,583 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 0.2630
2023-11-08 05:39:07,509 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 0.2602
2023-11-08 05:39:30,688 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 0.2355
2023-11-08 05:39:52,391 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 0.2340
2023-11-08 05:40:15,074 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 0.1610
2023-11-08 05:40:38,063 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 0.1441
2023-11-08 05:41:00,831 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 0.2086
2023-11-08 05:41:16,030 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 0.2406
2023-11-08 05:41:38,703 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 0.2509
2023-11-08 05:41:59,968 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 0.2441
2023-11-08 05:42:22,676 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 0.2449
2023-11-08 05:42:45,679 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 0.1597
2023-11-08 05:43:08,055 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 0.1281
2023-11-08 05:43:30,538 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 0.1806
2023-11-08 05:43:45,415 - __main__ - INFO - ------------ Save best model - AUROC: 0.8266 ------------
2023-11-08 05:43:46,468 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 0.2287
2023-11-08 05:44:09,815 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 0.2323
2023-11-08 05:44:32,632 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 0.2345
2023-11-08 05:44:55,563 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 0.2389
2023-11-08 05:45:17,647 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 0.1520
2023-11-08 05:45:40,045 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 0.1179
2023-11-08 05:46:02,837 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 0.1820
2023-11-08 05:46:17,651 - __main__ - INFO - ------------ Save best model - AUROC: 0.8388 ------------
2023-11-08 05:46:18,831 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 0.2264
2023-11-08 05:46:42,671 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 0.2372
2023-11-08 05:47:05,736 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 0.2350
2023-11-08 05:47:28,830 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 0.2134
2023-11-08 05:47:51,352 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 0.1576
2023-11-08 05:48:13,997 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 0.1494
2023-11-08 05:48:36,495 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 0.1711
2023-11-08 05:48:50,791 - __main__ - INFO - ------------ Save best model - AUROC: 0.8429 ------------
2023-11-08 05:48:51,755 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 0.2145
2023-11-08 05:49:15,072 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 0.1895
2023-11-08 05:49:37,822 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 0.2230
2023-11-08 05:50:01,075 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 0.2254
2023-11-08 05:50:23,799 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 0.1378
2023-11-08 05:50:48,040 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 0.1501
2023-11-08 05:51:10,075 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 0.1624
2023-11-08 05:51:24,684 - __main__ - INFO - ------------ Save best model - AUROC: 0.8556 ------------
2023-11-08 05:51:25,788 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 0.2220
2023-11-08 05:51:49,056 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 0.1968
2023-11-08 05:52:11,120 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 0.2008
2023-11-08 05:52:32,777 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 0.2199
2023-11-08 05:52:55,475 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 0.1551
2023-11-08 05:53:18,016 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 0.1227
2023-11-08 05:53:39,886 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 0.1623
2023-11-08 05:53:55,339 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 0.2116
2023-11-08 05:54:18,033 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 0.1984
2023-11-08 05:54:41,419 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 0.2177
2023-11-08 05:55:03,039 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 0.2004
2023-11-08 05:55:26,875 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 0.1566
2023-11-08 05:55:50,019 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 0.1271
2023-11-08 05:56:12,458 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 0.1719
2023-11-08 05:56:27,213 - __main__ - INFO - ------------ Save best model - AUROC: 0.8585 ------------
2023-11-08 05:56:28,309 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 0.1970
2023-11-08 05:56:51,825 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 0.2156
2023-11-08 05:57:14,709 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 0.2349
2023-11-08 05:57:38,073 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 0.2220
2023-11-08 05:58:00,176 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 0.1511
2023-11-08 05:58:23,954 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 0.1344
2023-11-08 05:58:47,480 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 0.1516
2023-11-08 05:59:03,775 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 0.1859
2023-11-08 05:59:27,094 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 0.2204
2023-11-08 05:59:49,067 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 0.2054
2023-11-08 06:00:10,605 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 0.2151
2023-11-08 06:00:33,488 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 0.1501
2023-11-08 06:00:56,765 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 0.1018
2023-11-08 06:01:19,716 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 0.1489
2023-11-08 06:01:34,012 - __main__ - INFO - ------------ Save best model - AUROC: 0.8616 ------------
2023-11-08 06:01:35,036 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 0.2017
2023-11-08 06:01:59,435 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 0.2179
2023-11-08 06:02:21,666 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 0.2276
2023-11-08 06:02:43,642 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 0.1919
2023-11-08 06:03:06,611 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 0.1432
2023-11-08 06:03:29,595 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 0.1349
2023-11-08 06:03:52,698 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 0.1656
2023-11-08 06:04:07,982 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 0.2008
2023-11-08 06:04:29,967 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 0.2073
2023-11-08 06:04:52,576 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 0.2032
2023-11-08 06:05:14,705 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 0.1961
2023-11-08 06:05:37,758 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 0.1457
2023-11-08 06:06:01,323 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 0.1277
2023-11-08 06:06:23,442 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 0.1555
2023-11-08 06:06:37,959 - __main__ - INFO - ------------ Save best model - AUROC: 0.8694 ------------
2023-11-08 06:06:38,780 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 0.1818
2023-11-08 06:07:02,073 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 0.2110
2023-11-08 06:07:24,804 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 0.2166
2023-11-08 06:07:47,842 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 0.2007
2023-11-08 06:08:10,283 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 0.1516
2023-11-08 06:08:33,386 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 0.1266
2023-11-08 06:08:56,527 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 0.1561
2023-11-08 06:09:12,709 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 0.1906
2023-11-08 06:09:35,200 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 0.2252
2023-11-08 06:09:58,206 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 0.2195
2023-11-08 06:10:20,454 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 0.2056
2023-11-08 06:10:43,498 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 0.1403
2023-11-08 06:11:07,216 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 0.1243
2023-11-08 06:11:30,015 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 0.1443
2023-11-08 06:11:44,918 - __main__ - INFO - ------------ Save best model - AUROC: 0.8708 ------------
2023-11-08 06:11:45,980 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 0.1883
2023-11-08 06:12:08,974 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 0.2291
2023-11-08 06:12:29,571 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 0.2122
2023-11-08 06:12:52,539 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 0.2023
2023-11-08 06:13:14,819 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 0.1489
2023-11-08 06:13:38,431 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 0.1274
2023-11-08 06:14:00,694 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 0.1582
2023-11-08 06:14:15,386 - __main__ - INFO - ------------ Save best model - AUROC: 0.8710 ------------
2023-11-08 06:14:16,489 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 0.2046
2023-11-08 06:14:39,365 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 0.2118
2023-11-08 06:15:01,156 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 0.2109
2023-11-08 06:15:23,404 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 0.2246
2023-11-08 06:15:46,244 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 0.1472
2023-11-08 06:16:09,214 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 0.1215
2023-11-08 06:16:32,535 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 0.1747
2023-11-08 06:16:47,551 - __main__ - INFO - ------------ Save best model - AUROC: 0.8740 ------------
2023-11-08 06:16:48,511 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 0.2147
2023-11-08 06:17:12,082 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 0.2081
2023-11-08 06:17:34,148 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 0.2034
2023-11-08 06:17:57,452 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 0.2179
2023-11-08 06:18:20,406 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 0.1427
2023-11-08 06:18:43,903 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 0.1288
2023-11-08 06:19:06,969 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 0.1410
2023-11-08 06:19:20,722 - __main__ - INFO - ------------ Save best model - AUROC: 0.8782 ------------
2023-11-08 06:19:21,877 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 0.1943
2023-11-08 06:19:44,602 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 0.1959
2023-11-08 06:20:07,246 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 0.2075
2023-11-08 06:20:28,655 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 0.2141
2023-11-08 06:20:51,541 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 0.1461
2023-11-08 06:21:14,385 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 0.1268
2023-11-08 06:21:35,821 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 0.1416
2023-11-08 06:21:51,932 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 0.1904
2023-11-08 06:22:14,772 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 0.2132
2023-11-08 06:22:37,846 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 0.2128
2023-11-08 06:23:01,198 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 0.2046
2023-11-08 06:23:23,627 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 0.1551
2023-11-08 06:23:45,139 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 0.1179
2023-11-08 06:24:05,340 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 0.1504
2023-11-08 06:24:19,165 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 0.1931
2023-11-08 06:24:40,552 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 0.2079
2023-11-08 06:25:01,799 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 0.2256
2023-11-08 06:25:22,402 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 0.1999
2023-11-08 06:25:43,637 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 0.1373
2023-11-08 06:26:05,524 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 0.1442
2023-11-08 06:26:26,009 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 0.1682
2023-11-08 06:26:38,728 - __main__ - INFO - ------------ Save best model - AUROC: 0.8791 ------------
2023-11-08 06:26:39,798 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 0.2302
2023-11-08 06:27:01,602 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 0.2058
2023-11-08 06:27:21,731 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 0.2142
2023-11-08 06:27:42,384 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 0.2218
2023-11-08 06:28:02,717 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 0.1374
2023-11-08 06:28:23,341 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 0.1206
2023-11-08 06:28:42,819 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 0.1333
2023-11-08 06:28:56,742 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 0.1849
2023-11-08 06:29:17,636 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 0.2106
2023-11-08 06:29:38,170 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 0.2180
2023-11-08 06:29:58,463 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 0.2107
2023-11-08 06:30:19,188 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 0.1350
2023-11-08 06:30:39,693 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 0.1264
2023-11-08 06:31:01,281 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 0.1408
2023-11-08 06:31:15,153 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 0.2028
2023-11-08 06:31:37,440 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 0.2099
2023-11-08 06:31:58,292 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 0.1846
2023-11-08 06:32:19,275 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 0.2066
2023-11-08 06:32:40,196 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 0.1455
2023-11-08 06:33:01,726 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 0.1180
2023-11-08 06:33:22,445 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 0.1469
2023-11-08 06:33:35,570 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 0.2083
2023-11-08 06:33:55,960 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 0.2336
2023-11-08 06:34:16,678 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 0.2291
2023-11-08 06:34:37,628 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 0.2091
2023-11-08 06:34:58,861 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 0.1494
2023-11-08 06:35:20,135 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 0.1065
2023-11-08 06:35:40,633 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 0.1362
2023-11-08 06:35:55,294 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 0.2150
2023-11-08 06:36:15,698 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 0.2037
2023-11-08 06:36:36,398 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 0.2454
2023-11-08 06:36:56,632 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 0.2017
2023-11-08 06:37:17,322 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 0.1534
2023-11-08 06:37:38,160 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 0.1330
2023-11-08 06:37:57,923 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 0.1458
2023-11-08 06:38:11,380 - __main__ - INFO - ------------ Save best model - AUROC: 0.8819 ------------
2023-11-08 06:38:12,539 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 0.1986
2023-11-08 06:38:33,469 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 0.2165
2023-11-08 06:38:54,406 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 0.2196
2023-11-08 06:39:15,147 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 0.2028
2023-11-08 06:39:36,071 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 0.1601
2023-11-08 06:39:58,019 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 0.1392
2023-11-08 06:40:17,956 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 0.1578
2023-11-08 06:40:30,315 - __main__ - INFO - ------------ Save best model - AUROC: 0.8859 ------------
2023-11-08 06:40:31,370 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 0.2110
2023-11-08 06:40:52,983 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 0.2207
2023-11-08 06:41:13,537 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 0.2267
2023-11-08 06:41:34,894 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 0.2000
2023-11-08 06:41:55,604 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 0.1396
2023-11-08 06:42:16,395 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 0.1192
2023-11-08 06:42:36,425 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 0.1180
2023-11-08 06:42:49,332 - __main__ - INFO - ------------ Save best model - AUROC: 0.8876 ------------
2023-11-08 06:42:50,182 - __main__ - INFO - Epoch 30 Batch 0: Train Loss = 0.2120
2023-11-08 06:43:10,624 - __main__ - INFO - Epoch 30 Batch 20: Train Loss = 0.2232
2023-11-08 06:43:30,974 - __main__ - INFO - Epoch 30 Batch 40: Train Loss = 0.2399
2023-11-08 06:43:50,717 - __main__ - INFO - Epoch 30 Batch 60: Train Loss = 0.2234
2023-11-08 06:44:11,724 - __main__ - INFO - Epoch 30 Batch 80: Train Loss = 0.1301
2023-11-08 06:44:32,113 - __main__ - INFO - Epoch 30 Batch 100: Train Loss = 0.1223
2023-11-08 06:44:52,103 - __main__ - INFO - Epoch 30 Batch 120: Train Loss = 0.1459
2023-11-08 06:45:06,624 - __main__ - INFO - Epoch 31 Batch 0: Train Loss = 0.1876
2023-11-08 06:45:28,201 - __main__ - INFO - Epoch 31 Batch 20: Train Loss = 0.2174
2023-11-08 06:45:48,379 - __main__ - INFO - Epoch 31 Batch 40: Train Loss = 0.2201
2023-11-08 06:46:08,291 - __main__ - INFO - Epoch 31 Batch 60: Train Loss = 0.2108
2023-11-08 06:46:28,495 - __main__ - INFO - Epoch 31 Batch 80: Train Loss = 0.1741
2023-11-08 06:46:50,002 - __main__ - INFO - Epoch 31 Batch 100: Train Loss = 0.1062
2023-11-08 06:47:10,933 - __main__ - INFO - Epoch 31 Batch 120: Train Loss = 0.1155
2023-11-08 06:47:23,811 - __main__ - INFO - ------------ Save best model - AUROC: 0.8883 ------------
2023-11-08 06:47:24,767 - __main__ - INFO - Epoch 32 Batch 0: Train Loss = 0.1996
2023-11-08 06:47:45,316 - __main__ - INFO - Epoch 32 Batch 20: Train Loss = 0.2190
2023-11-08 06:48:05,345 - __main__ - INFO - Epoch 32 Batch 40: Train Loss = 0.1956
2023-11-08 06:48:26,243 - __main__ - INFO - Epoch 32 Batch 60: Train Loss = 0.2023
2023-11-08 06:48:47,481 - __main__ - INFO - Epoch 32 Batch 80: Train Loss = 0.1454
2023-11-08 06:49:09,211 - __main__ - INFO - Epoch 32 Batch 100: Train Loss = 0.1219
2023-11-08 06:49:30,031 - __main__ - INFO - Epoch 32 Batch 120: Train Loss = 0.1704
2023-11-08 06:49:43,293 - __main__ - INFO - Epoch 33 Batch 0: Train Loss = 0.1855
2023-11-08 06:50:04,177 - __main__ - INFO - Epoch 33 Batch 20: Train Loss = 0.2180
2023-11-08 06:50:24,501 - __main__ - INFO - Epoch 33 Batch 40: Train Loss = 0.2348
2023-11-08 06:50:45,808 - __main__ - INFO - Epoch 33 Batch 60: Train Loss = 0.2074
2023-11-08 06:51:07,022 - __main__ - INFO - Epoch 33 Batch 80: Train Loss = 0.1377
2023-11-08 06:51:28,502 - __main__ - INFO - Epoch 33 Batch 100: Train Loss = 0.1288
2023-11-08 06:51:48,733 - __main__ - INFO - Epoch 33 Batch 120: Train Loss = 0.1772
2023-11-08 06:52:02,687 - __main__ - INFO - Epoch 34 Batch 0: Train Loss = 0.1970
2023-11-08 06:52:24,315 - __main__ - INFO - Epoch 34 Batch 20: Train Loss = 0.2153
2023-11-08 06:52:44,224 - __main__ - INFO - Epoch 34 Batch 40: Train Loss = 0.2152
2023-11-08 06:53:04,517 - __main__ - INFO - Epoch 34 Batch 60: Train Loss = 0.2124
2023-11-08 06:53:25,217 - __main__ - INFO - Epoch 34 Batch 80: Train Loss = 0.1388
2023-11-08 06:53:45,907 - __main__ - INFO - Epoch 34 Batch 100: Train Loss = 0.1345
2023-11-08 06:54:06,777 - __main__ - INFO - Epoch 34 Batch 120: Train Loss = 0.1531
2023-11-08 06:54:20,920 - __main__ - INFO - Epoch 35 Batch 0: Train Loss = 0.2051
2023-11-08 06:54:42,282 - __main__ - INFO - Epoch 35 Batch 20: Train Loss = 0.2193
2023-11-08 06:55:02,961 - __main__ - INFO - Epoch 35 Batch 40: Train Loss = 0.2354
2023-11-08 06:55:23,809 - __main__ - INFO - Epoch 35 Batch 60: Train Loss = 0.2141
2023-11-08 06:55:44,265 - __main__ - INFO - Epoch 35 Batch 80: Train Loss = 0.1307
2023-11-08 06:56:06,683 - __main__ - INFO - Epoch 35 Batch 100: Train Loss = 0.1205
2023-11-08 06:56:26,994 - __main__ - INFO - Epoch 35 Batch 120: Train Loss = 0.1350
2023-11-08 06:56:41,231 - __main__ - INFO - Epoch 36 Batch 0: Train Loss = 0.1818
2023-11-08 06:57:02,671 - __main__ - INFO - Epoch 36 Batch 20: Train Loss = 0.2136
2023-11-08 06:57:23,363 - __main__ - INFO - Epoch 36 Batch 40: Train Loss = 0.2277
2023-11-08 06:57:43,487 - __main__ - INFO - Epoch 36 Batch 60: Train Loss = 0.1924
2023-11-08 06:58:04,103 - __main__ - INFO - Epoch 36 Batch 80: Train Loss = 0.1244
2023-11-08 06:58:25,218 - __main__ - INFO - Epoch 36 Batch 100: Train Loss = 0.1289
2023-11-08 06:58:46,424 - __main__ - INFO - Epoch 36 Batch 120: Train Loss = 0.1462
2023-11-08 06:58:58,991 - __main__ - INFO - ------------ Save best model - AUROC: 0.8895 ------------
2023-11-08 06:59:00,077 - __main__ - INFO - Epoch 37 Batch 0: Train Loss = 0.2067
2023-11-08 06:59:20,153 - __main__ - INFO - Epoch 37 Batch 20: Train Loss = 0.2105
2023-11-08 06:59:42,130 - __main__ - INFO - Epoch 37 Batch 40: Train Loss = 0.1921
2023-11-08 07:00:03,185 - __main__ - INFO - Epoch 37 Batch 60: Train Loss = 0.2033
2023-11-08 07:00:23,710 - __main__ - INFO - Epoch 37 Batch 80: Train Loss = 0.1431
2023-11-08 07:00:44,827 - __main__ - INFO - Epoch 37 Batch 100: Train Loss = 0.1439
2023-11-08 07:01:05,413 - __main__ - INFO - Epoch 37 Batch 120: Train Loss = 0.1581
2023-11-08 07:01:19,378 - __main__ - INFO - Epoch 38 Batch 0: Train Loss = 0.1897
2023-11-08 07:01:40,897 - __main__ - INFO - Epoch 38 Batch 20: Train Loss = 0.2230
2023-11-08 07:02:00,818 - __main__ - INFO - Epoch 38 Batch 40: Train Loss = 0.2326
2023-11-08 07:02:20,156 - __main__ - INFO - Epoch 38 Batch 60: Train Loss = 0.2098
2023-11-08 07:02:40,964 - __main__ - INFO - Epoch 38 Batch 80: Train Loss = 0.1393
2023-11-08 07:03:00,978 - __main__ - INFO - Epoch 38 Batch 100: Train Loss = 0.1086
2023-11-08 07:03:20,535 - __main__ - INFO - Epoch 38 Batch 120: Train Loss = 0.1641
2023-11-08 07:03:35,530 - __main__ - INFO - Epoch 39 Batch 0: Train Loss = 0.2002
2023-11-08 07:03:57,077 - __main__ - INFO - Epoch 39 Batch 20: Train Loss = 0.2204
2023-11-08 07:04:17,326 - __main__ - INFO - Epoch 39 Batch 40: Train Loss = 0.2125
2023-11-08 07:04:37,847 - __main__ - INFO - Epoch 39 Batch 60: Train Loss = 0.2009
2023-11-08 07:04:59,092 - __main__ - INFO - Epoch 39 Batch 80: Train Loss = 0.1396
2023-11-08 07:05:20,498 - __main__ - INFO - Epoch 39 Batch 100: Train Loss = 0.1146
2023-11-08 07:05:41,835 - __main__ - INFO - Epoch 39 Batch 120: Train Loss = 0.1765
2023-11-08 07:05:55,363 - __main__ - INFO - Epoch 40 Batch 0: Train Loss = 0.2032
2023-11-08 07:06:16,921 - __main__ - INFO - Epoch 40 Batch 20: Train Loss = 0.2334
2023-11-08 07:06:37,536 - __main__ - INFO - Epoch 40 Batch 40: Train Loss = 0.2272
2023-11-08 07:06:58,511 - __main__ - INFO - Epoch 40 Batch 60: Train Loss = 0.1993
2023-11-08 07:07:19,250 - __main__ - INFO - Epoch 40 Batch 80: Train Loss = 0.1617
2023-11-08 07:07:39,859 - __main__ - INFO - Epoch 40 Batch 100: Train Loss = 0.1313
2023-11-08 07:08:00,225 - __main__ - INFO - Epoch 40 Batch 120: Train Loss = 0.1440
2023-11-08 07:08:13,773 - __main__ - INFO - ------------ Save best model - AUROC: 0.8897 ------------
2023-11-08 07:08:14,641 - __main__ - INFO - Epoch 41 Batch 0: Train Loss = 0.1954
2023-11-08 07:08:34,819 - __main__ - INFO - Epoch 41 Batch 20: Train Loss = 0.1959
2023-11-08 07:08:54,476 - __main__ - INFO - Epoch 41 Batch 40: Train Loss = 0.2280
2023-11-08 07:09:14,515 - __main__ - INFO - Epoch 41 Batch 60: Train Loss = 0.1812
2023-11-08 07:09:35,271 - __main__ - INFO - Epoch 41 Batch 80: Train Loss = 0.1333
2023-11-08 07:09:55,968 - __main__ - INFO - Epoch 41 Batch 100: Train Loss = 0.1209
2023-11-08 07:10:16,547 - __main__ - INFO - Epoch 41 Batch 120: Train Loss = 0.1555
2023-11-08 07:10:30,810 - __main__ - INFO - Epoch 42 Batch 0: Train Loss = 0.1865
2023-11-08 07:10:52,162 - __main__ - INFO - Epoch 42 Batch 20: Train Loss = 0.2117
2023-11-08 07:11:12,827 - __main__ - INFO - Epoch 42 Batch 40: Train Loss = 0.2249
2023-11-08 07:11:33,811 - __main__ - INFO - Epoch 42 Batch 60: Train Loss = 0.1989
2023-11-08 07:11:54,747 - __main__ - INFO - Epoch 42 Batch 80: Train Loss = 0.1516
2023-11-08 07:12:15,856 - __main__ - INFO - Epoch 42 Batch 100: Train Loss = 0.1287
2023-11-08 07:12:36,621 - __main__ - INFO - Epoch 42 Batch 120: Train Loss = 0.1659
2023-11-08 07:12:50,501 - __main__ - INFO - Epoch 43 Batch 0: Train Loss = 0.2040
2023-11-08 07:13:11,985 - __main__ - INFO - Epoch 43 Batch 20: Train Loss = 0.2230
2023-11-08 07:13:33,255 - __main__ - INFO - Epoch 43 Batch 40: Train Loss = 0.2185
2023-11-08 07:13:52,756 - __main__ - INFO - Epoch 43 Batch 60: Train Loss = 0.1804
2023-11-08 07:14:13,083 - __main__ - INFO - Epoch 43 Batch 80: Train Loss = 0.1336
2023-11-08 07:14:34,493 - __main__ - INFO - Epoch 43 Batch 100: Train Loss = 0.1312
2023-11-08 07:14:55,871 - __main__ - INFO - Epoch 43 Batch 120: Train Loss = 0.1621
2023-11-08 07:15:08,209 - __main__ - INFO - Epoch 44 Batch 0: Train Loss = 0.2114
2023-11-08 07:15:28,846 - __main__ - INFO - Epoch 44 Batch 20: Train Loss = 0.2073
2023-11-08 07:15:48,713 - __main__ - INFO - Epoch 44 Batch 40: Train Loss = 0.1991
2023-11-08 07:16:08,406 - __main__ - INFO - Epoch 44 Batch 60: Train Loss = 0.1716
2023-11-08 07:16:28,715 - __main__ - INFO - Epoch 44 Batch 80: Train Loss = 0.1477
2023-11-08 07:16:50,055 - __main__ - INFO - Epoch 44 Batch 100: Train Loss = 0.1441
2023-11-08 07:17:10,523 - __main__ - INFO - Epoch 44 Batch 120: Train Loss = 0.1505
2023-11-08 07:17:24,592 - __main__ - INFO - Epoch 45 Batch 0: Train Loss = 0.2137
2023-11-08 07:17:46,241 - __main__ - INFO - Epoch 45 Batch 20: Train Loss = 0.2022
2023-11-08 07:18:06,330 - __main__ - INFO - Epoch 45 Batch 40: Train Loss = 0.2020
2023-11-08 07:18:27,070 - __main__ - INFO - Epoch 45 Batch 60: Train Loss = 0.1904
2023-11-08 07:18:48,318 - __main__ - INFO - Epoch 45 Batch 80: Train Loss = 0.1311
2023-11-08 07:19:08,698 - __main__ - INFO - Epoch 45 Batch 100: Train Loss = 0.1230
2023-11-08 07:19:29,298 - __main__ - INFO - Epoch 45 Batch 120: Train Loss = 0.1739
2023-11-08 07:19:42,798 - __main__ - INFO - Epoch 46 Batch 0: Train Loss = 0.2247
2023-11-08 07:20:04,126 - __main__ - INFO - Epoch 46 Batch 20: Train Loss = 0.2326
2023-11-08 07:20:24,490 - __main__ - INFO - Epoch 46 Batch 40: Train Loss = 0.2053
2023-11-08 07:20:44,994 - __main__ - INFO - Epoch 46 Batch 60: Train Loss = 0.1930
2023-11-08 07:21:05,161 - __main__ - INFO - Epoch 46 Batch 80: Train Loss = 0.1392
2023-11-08 07:21:25,852 - __main__ - INFO - Epoch 46 Batch 100: Train Loss = 0.1276
2023-11-08 07:21:47,035 - __main__ - INFO - Epoch 46 Batch 120: Train Loss = 0.1669
2023-11-08 07:22:01,383 - __main__ - INFO - Epoch 47 Batch 0: Train Loss = 0.1808
2023-11-08 07:22:22,529 - __main__ - INFO - Epoch 47 Batch 20: Train Loss = 0.2085
2023-11-08 07:22:43,360 - __main__ - INFO - Epoch 47 Batch 40: Train Loss = 0.2461
2023-11-08 07:23:03,616 - __main__ - INFO - Epoch 47 Batch 60: Train Loss = 0.2124
2023-11-08 07:23:24,202 - __main__ - INFO - Epoch 47 Batch 80: Train Loss = 0.1319
2023-11-08 07:23:44,577 - __main__ - INFO - Epoch 47 Batch 100: Train Loss = 0.1148
2023-11-08 07:24:05,456 - __main__ - INFO - Epoch 47 Batch 120: Train Loss = 0.1464
2023-11-08 07:24:18,775 - __main__ - INFO - Epoch 48 Batch 0: Train Loss = 0.1895
2023-11-08 07:24:39,269 - __main__ - INFO - Epoch 48 Batch 20: Train Loss = 0.2158
2023-11-08 07:24:59,794 - __main__ - INFO - Epoch 48 Batch 40: Train Loss = 0.2409
2023-11-08 07:25:20,025 - __main__ - INFO - Epoch 48 Batch 60: Train Loss = 0.2089
2023-11-08 07:25:42,131 - __main__ - INFO - Epoch 48 Batch 80: Train Loss = 0.1276
2023-11-08 07:26:02,084 - __main__ - INFO - Epoch 48 Batch 100: Train Loss = 0.1400
2023-11-08 07:26:21,988 - __main__ - INFO - Epoch 48 Batch 120: Train Loss = 0.1639
2023-11-08 07:26:35,824 - __main__ - INFO - Epoch 49 Batch 0: Train Loss = 0.2228
2023-11-08 07:26:57,457 - __main__ - INFO - Epoch 49 Batch 20: Train Loss = 0.1943
2023-11-08 07:27:17,727 - __main__ - INFO - Epoch 49 Batch 40: Train Loss = 0.2106
2023-11-08 07:27:38,852 - __main__ - INFO - Epoch 49 Batch 60: Train Loss = 0.2328
2023-11-08 07:27:59,463 - __main__ - INFO - Epoch 49 Batch 80: Train Loss = 0.1336
2023-11-08 07:28:19,976 - __main__ - INFO - Epoch 49 Batch 100: Train Loss = 0.1257
2023-11-08 07:28:40,329 - __main__ - INFO - Epoch 49 Batch 120: Train Loss = 0.1603
2023-11-08 07:28:53,775 - __main__ - INFO - Epoch 50 Batch 0: Train Loss = 0.2020
2023-11-08 07:29:14,923 - __main__ - INFO - Epoch 50 Batch 20: Train Loss = 0.2202
2023-11-08 07:29:34,912 - __main__ - INFO - Epoch 50 Batch 40: Train Loss = 0.2212
2023-11-08 07:29:54,337 - __main__ - INFO - Epoch 50 Batch 60: Train Loss = 0.1923
2023-11-08 07:30:15,187 - __main__ - INFO - Epoch 50 Batch 80: Train Loss = 0.1376
2023-11-08 07:30:37,490 - __main__ - INFO - Epoch 50 Batch 100: Train Loss = 0.1179
2023-11-08 07:30:59,024 - __main__ - INFO - Epoch 50 Batch 120: Train Loss = 0.1638
2023-11-08 07:31:12,766 - __main__ - INFO - Epoch 51 Batch 0: Train Loss = 0.1996
2023-11-08 07:31:32,387 - __main__ - INFO - Epoch 51 Batch 20: Train Loss = 0.2173
2023-11-08 07:31:52,535 - __main__ - INFO - Epoch 51 Batch 40: Train Loss = 0.2168
2023-11-08 07:32:12,735 - __main__ - INFO - Epoch 51 Batch 60: Train Loss = 0.2007
2023-11-08 07:32:32,973 - __main__ - INFO - Epoch 51 Batch 80: Train Loss = 0.1591
2023-11-08 07:32:54,337 - __main__ - INFO - Epoch 51 Batch 100: Train Loss = 0.1476
2023-11-08 07:33:14,987 - __main__ - INFO - Epoch 51 Batch 120: Train Loss = 0.1499
2023-11-08 07:33:28,161 - __main__ - INFO - Epoch 52 Batch 0: Train Loss = 0.2057
2023-11-08 07:33:49,439 - __main__ - INFO - Epoch 52 Batch 20: Train Loss = 0.2227
2023-11-08 07:34:08,986 - __main__ - INFO - Epoch 52 Batch 40: Train Loss = 0.1954
2023-11-08 07:34:29,830 - __main__ - INFO - Epoch 52 Batch 60: Train Loss = 0.1893
2023-11-08 07:34:50,307 - __main__ - INFO - Epoch 52 Batch 80: Train Loss = 0.1618
2023-11-08 07:35:11,326 - __main__ - INFO - Epoch 52 Batch 100: Train Loss = 0.1430
2023-11-08 07:35:32,395 - __main__ - INFO - Epoch 52 Batch 120: Train Loss = 0.1161
2023-11-08 07:35:45,311 - __main__ - INFO - Epoch 53 Batch 0: Train Loss = 0.1963
2023-11-08 07:36:06,610 - __main__ - INFO - Epoch 53 Batch 20: Train Loss = 0.2266
2023-11-08 07:36:27,437 - __main__ - INFO - Epoch 53 Batch 40: Train Loss = 0.1898
2023-11-08 07:36:48,315 - __main__ - INFO - Epoch 53 Batch 60: Train Loss = 0.1955
2023-11-08 07:37:09,580 - __main__ - INFO - Epoch 53 Batch 80: Train Loss = 0.1287
2023-11-08 07:37:30,416 - __main__ - INFO - Epoch 53 Batch 100: Train Loss = 0.1167
2023-11-08 07:37:50,479 - __main__ - INFO - Epoch 53 Batch 120: Train Loss = 0.1561
2023-11-08 07:38:04,255 - __main__ - INFO - Epoch 54 Batch 0: Train Loss = 0.2052
2023-11-08 07:38:25,534 - __main__ - INFO - Epoch 54 Batch 20: Train Loss = 0.2212
2023-11-08 07:38:46,835 - __main__ - INFO - Epoch 54 Batch 40: Train Loss = 0.2035
2023-11-08 07:39:07,090 - __main__ - INFO - Epoch 54 Batch 60: Train Loss = 0.2014
2023-11-08 07:39:26,957 - __main__ - INFO - Epoch 54 Batch 80: Train Loss = 0.1516
2023-11-08 07:39:47,818 - __main__ - INFO - Epoch 54 Batch 100: Train Loss = 0.1392
2023-11-08 07:40:08,914 - __main__ - INFO - Epoch 54 Batch 120: Train Loss = 0.1477
2023-11-08 07:40:24,226 - __main__ - INFO - Epoch 55 Batch 0: Train Loss = 0.2225
2023-11-08 07:40:44,923 - __main__ - INFO - Epoch 55 Batch 20: Train Loss = 0.2170
2023-11-08 07:41:05,846 - __main__ - INFO - Epoch 55 Batch 40: Train Loss = 0.2232
2023-11-08 07:41:25,726 - __main__ - INFO - Epoch 55 Batch 60: Train Loss = 0.2180
2023-11-08 07:41:46,085 - __main__ - INFO - Epoch 55 Batch 80: Train Loss = 0.1601
2023-11-08 07:42:07,227 - __main__ - INFO - Epoch 55 Batch 100: Train Loss = 0.1225
2023-11-08 07:42:27,615 - __main__ - INFO - Epoch 55 Batch 120: Train Loss = 0.1669
2023-11-08 07:42:42,079 - __main__ - INFO - Epoch 56 Batch 0: Train Loss = 0.1874
2023-11-08 07:43:03,316 - __main__ - INFO - Epoch 56 Batch 20: Train Loss = 0.2313
2023-11-08 07:43:23,880 - __main__ - INFO - Epoch 56 Batch 40: Train Loss = 0.2142
2023-11-08 07:43:45,496 - __main__ - INFO - Epoch 56 Batch 60: Train Loss = 0.2056
2023-11-08 07:44:05,971 - __main__ - INFO - Epoch 56 Batch 80: Train Loss = 0.1421
2023-11-08 07:44:26,590 - __main__ - INFO - Epoch 56 Batch 100: Train Loss = 0.1457
2023-11-08 07:44:48,278 - __main__ - INFO - Epoch 56 Batch 120: Train Loss = 0.1836
2023-11-08 07:45:01,627 - __main__ - INFO - Epoch 57 Batch 0: Train Loss = 0.1897
2023-11-08 07:45:22,301 - __main__ - INFO - Epoch 57 Batch 20: Train Loss = 0.2211
2023-11-08 07:45:41,634 - __main__ - INFO - Epoch 57 Batch 40: Train Loss = 0.2121
2023-11-08 07:46:02,451 - __main__ - INFO - Epoch 57 Batch 60: Train Loss = 0.2050
2023-11-08 07:46:21,811 - __main__ - INFO - Epoch 57 Batch 80: Train Loss = 0.1468
2023-11-08 07:46:42,366 - __main__ - INFO - Epoch 57 Batch 100: Train Loss = 0.1270
2023-11-08 07:47:02,808 - __main__ - INFO - Epoch 57 Batch 120: Train Loss = 0.1588
2023-11-08 07:47:16,257 - __main__ - INFO - Epoch 58 Batch 0: Train Loss = 0.2070
2023-11-08 07:47:37,933 - __main__ - INFO - Epoch 58 Batch 20: Train Loss = 0.1942
2023-11-08 07:47:58,806 - __main__ - INFO - Epoch 58 Batch 40: Train Loss = 0.2339
2023-11-08 07:48:18,523 - __main__ - INFO - Epoch 58 Batch 60: Train Loss = 0.2173
2023-11-08 07:48:39,563 - __main__ - INFO - Epoch 58 Batch 80: Train Loss = 0.1604
2023-11-08 07:48:59,543 - __main__ - INFO - Epoch 58 Batch 100: Train Loss = 0.1350
2023-11-08 07:49:20,853 - __main__ - INFO - Epoch 58 Batch 120: Train Loss = 0.1749
2023-11-08 07:49:35,971 - __main__ - INFO - Epoch 59 Batch 0: Train Loss = 0.2074
2023-11-08 07:49:57,463 - __main__ - INFO - Epoch 59 Batch 20: Train Loss = 0.2148
2023-11-08 07:50:16,987 - __main__ - INFO - Epoch 59 Batch 40: Train Loss = 0.2123
2023-11-08 07:50:37,419 - __main__ - INFO - Epoch 59 Batch 60: Train Loss = 0.2275
2023-11-08 07:50:57,506 - __main__ - INFO - Epoch 59 Batch 80: Train Loss = 0.1266
2023-11-08 07:51:18,839 - __main__ - INFO - Epoch 59 Batch 100: Train Loss = 0.1402
2023-11-08 07:51:39,983 - __main__ - INFO - Epoch 59 Batch 120: Train Loss = 0.2040
2023-11-08 07:51:54,114 - __main__ - INFO - Epoch 60 Batch 0: Train Loss = 0.1964
2023-11-08 07:52:15,461 - __main__ - INFO - Epoch 60 Batch 20: Train Loss = 0.1937
2023-11-08 07:52:36,315 - __main__ - INFO - Epoch 60 Batch 40: Train Loss = 0.2111
2023-11-08 07:52:58,138 - __main__ - INFO - Epoch 60 Batch 60: Train Loss = 0.2014
2023-11-08 07:53:18,905 - __main__ - INFO - Epoch 60 Batch 80: Train Loss = 0.1407
2023-11-08 07:53:39,321 - __main__ - INFO - Epoch 60 Batch 100: Train Loss = 0.1472
2023-11-08 07:54:00,258 - __main__ - INFO - Epoch 60 Batch 120: Train Loss = 0.1638
2023-11-08 07:54:13,804 - __main__ - INFO - Epoch 61 Batch 0: Train Loss = 0.2037
2023-11-08 07:54:34,022 - __main__ - INFO - Epoch 61 Batch 20: Train Loss = 0.2158
2023-11-08 07:54:54,110 - __main__ - INFO - Epoch 61 Batch 40: Train Loss = 0.2231
2023-11-08 07:55:14,654 - __main__ - INFO - Epoch 61 Batch 60: Train Loss = 0.1839
2023-11-08 07:55:34,973 - __main__ - INFO - Epoch 61 Batch 80: Train Loss = 0.1224
2023-11-08 07:55:55,374 - __main__ - INFO - Epoch 61 Batch 100: Train Loss = 0.1075
2023-11-08 07:56:16,591 - __main__ - INFO - Epoch 61 Batch 120: Train Loss = 0.1613
2023-11-08 07:56:29,576 - __main__ - INFO - Epoch 62 Batch 0: Train Loss = 0.2001
2023-11-08 07:56:51,297 - __main__ - INFO - Epoch 62 Batch 20: Train Loss = 0.2296
2023-11-08 07:57:12,205 - __main__ - INFO - Epoch 62 Batch 40: Train Loss = 0.2127
2023-11-08 07:57:32,764 - __main__ - INFO - Epoch 62 Batch 60: Train Loss = 0.2011
2023-11-08 07:57:53,270 - __main__ - INFO - Epoch 62 Batch 80: Train Loss = 0.1367
2023-11-08 07:58:14,081 - __main__ - INFO - Epoch 62 Batch 100: Train Loss = 0.1711
2023-11-08 07:58:34,516 - __main__ - INFO - Epoch 62 Batch 120: Train Loss = 0.1552
2023-11-08 07:58:47,399 - __main__ - INFO - Epoch 63 Batch 0: Train Loss = 0.1944
2023-11-08 07:59:07,865 - __main__ - INFO - Epoch 63 Batch 20: Train Loss = 0.2118
2023-11-08 07:59:28,227 - __main__ - INFO - Epoch 63 Batch 40: Train Loss = 0.2276
2023-11-08 07:59:47,998 - __main__ - INFO - Epoch 63 Batch 60: Train Loss = 0.2008
2023-11-08 08:00:08,280 - __main__ - INFO - Epoch 63 Batch 80: Train Loss = 0.1370
2023-11-08 08:00:29,703 - __main__ - INFO - Epoch 63 Batch 100: Train Loss = 0.1472
2023-11-08 08:00:49,223 - __main__ - INFO - Epoch 63 Batch 120: Train Loss = 0.1484
2023-11-08 08:01:03,261 - __main__ - INFO - Epoch 64 Batch 0: Train Loss = 0.2075
2023-11-08 08:01:24,657 - __main__ - INFO - Epoch 64 Batch 20: Train Loss = 0.2288
2023-11-08 08:01:45,265 - __main__ - INFO - Epoch 64 Batch 40: Train Loss = 0.2007
2023-11-08 08:02:04,721 - __main__ - INFO - Epoch 64 Batch 60: Train Loss = 0.2063
2023-11-08 08:02:25,796 - __main__ - INFO - Epoch 64 Batch 80: Train Loss = 0.1353
2023-11-08 08:02:47,017 - __main__ - INFO - Epoch 64 Batch 100: Train Loss = 0.1119
2023-11-08 08:03:08,568 - __main__ - INFO - Epoch 64 Batch 120: Train Loss = 0.1684
2023-11-08 08:03:22,806 - __main__ - INFO - Epoch 65 Batch 0: Train Loss = 0.1797
2023-11-08 08:03:43,781 - __main__ - INFO - Epoch 65 Batch 20: Train Loss = 0.2018
2023-11-08 08:04:05,119 - __main__ - INFO - Epoch 65 Batch 40: Train Loss = 0.2287
2023-11-08 08:04:25,146 - __main__ - INFO - Epoch 65 Batch 60: Train Loss = 0.2020
2023-11-08 08:04:45,630 - __main__ - INFO - Epoch 65 Batch 80: Train Loss = 0.1395
2023-11-08 08:05:06,675 - __main__ - INFO - Epoch 65 Batch 100: Train Loss = 0.1264
2023-11-08 08:05:26,827 - __main__ - INFO - Epoch 65 Batch 120: Train Loss = 0.1627
2023-11-08 08:05:40,686 - __main__ - INFO - Epoch 66 Batch 0: Train Loss = 0.1953
2023-11-08 08:06:02,255 - __main__ - INFO - Epoch 66 Batch 20: Train Loss = 0.2099
2023-11-08 08:06:22,095 - __main__ - INFO - Epoch 66 Batch 40: Train Loss = 0.2188
2023-11-08 08:06:43,067 - __main__ - INFO - Epoch 66 Batch 60: Train Loss = 0.2008
2023-11-08 08:07:03,235 - __main__ - INFO - Epoch 66 Batch 80: Train Loss = 0.1405
2023-11-08 08:07:24,715 - __main__ - INFO - Epoch 66 Batch 100: Train Loss = 0.1365
2023-11-08 08:07:45,003 - __main__ - INFO - Epoch 66 Batch 120: Train Loss = 0.1645
2023-11-08 08:07:57,920 - __main__ - INFO - Epoch 67 Batch 0: Train Loss = 0.1990
2023-11-08 08:08:18,483 - __main__ - INFO - Epoch 67 Batch 20: Train Loss = 0.2062
2023-11-08 08:08:38,717 - __main__ - INFO - Epoch 67 Batch 40: Train Loss = 0.2323
2023-11-08 08:08:59,373 - __main__ - INFO - Epoch 67 Batch 60: Train Loss = 0.1815
2023-11-08 08:09:20,843 - __main__ - INFO - Epoch 67 Batch 80: Train Loss = 0.1289
2023-11-08 08:09:40,939 - __main__ - INFO - Epoch 67 Batch 100: Train Loss = 0.1424
2023-11-08 08:10:00,835 - __main__ - INFO - Epoch 67 Batch 120: Train Loss = 0.1547
2023-11-08 08:10:14,208 - __main__ - INFO - Epoch 68 Batch 0: Train Loss = 0.2009
2023-11-08 08:10:35,367 - __main__ - INFO - Epoch 68 Batch 20: Train Loss = 0.2060
2023-11-08 08:10:55,911 - __main__ - INFO - Epoch 68 Batch 40: Train Loss = 0.2305
2023-11-08 08:11:15,729 - __main__ - INFO - Epoch 68 Batch 60: Train Loss = 0.1859
2023-11-08 08:11:36,592 - __main__ - INFO - Epoch 68 Batch 80: Train Loss = 0.1411
2023-11-08 08:11:57,878 - __main__ - INFO - Epoch 68 Batch 100: Train Loss = 0.1149
2023-11-08 08:12:18,639 - __main__ - INFO - Epoch 68 Batch 120: Train Loss = 0.1392
2023-11-08 08:12:32,741 - __main__ - INFO - Epoch 69 Batch 0: Train Loss = 0.2194
2023-11-08 08:12:53,904 - __main__ - INFO - Epoch 69 Batch 20: Train Loss = 0.2005
2023-11-08 08:13:14,828 - __main__ - INFO - Epoch 69 Batch 40: Train Loss = 0.2197
2023-11-08 08:13:35,908 - __main__ - INFO - Epoch 69 Batch 60: Train Loss = 0.1833
2023-11-08 08:13:56,329 - __main__ - INFO - Epoch 69 Batch 80: Train Loss = 0.1352
2023-11-08 08:14:16,651 - __main__ - INFO - Epoch 69 Batch 100: Train Loss = 0.1327
2023-11-08 08:14:37,137 - __main__ - INFO - Epoch 69 Batch 120: Train Loss = 0.1590
2023-11-08 08:14:50,947 - __main__ - INFO - Epoch 70 Batch 0: Train Loss = 0.2129
2023-11-08 08:15:12,566 - __main__ - INFO - Epoch 70 Batch 20: Train Loss = 0.2041
2023-11-08 08:15:33,540 - __main__ - INFO - Epoch 70 Batch 40: Train Loss = 0.2139
2023-11-08 08:15:53,611 - __main__ - INFO - Epoch 70 Batch 60: Train Loss = 0.2043
2023-11-08 08:16:13,820 - __main__ - INFO - Epoch 70 Batch 80: Train Loss = 0.1439
2023-11-08 08:16:34,473 - __main__ - INFO - Epoch 70 Batch 100: Train Loss = 0.1288
2023-11-08 08:16:55,943 - __main__ - INFO - Epoch 70 Batch 120: Train Loss = 0.1711
2023-11-08 08:17:09,599 - __main__ - INFO - Epoch 71 Batch 0: Train Loss = 0.2074
2023-11-08 08:17:31,023 - __main__ - INFO - Epoch 71 Batch 20: Train Loss = 0.2025
2023-11-08 08:17:51,797 - __main__ - INFO - Epoch 71 Batch 40: Train Loss = 0.2197
2023-11-08 08:18:11,883 - __main__ - INFO - Epoch 71 Batch 60: Train Loss = 0.1909
2023-11-08 08:18:34,078 - __main__ - INFO - Epoch 71 Batch 80: Train Loss = 0.1177
2023-11-08 08:18:53,840 - __main__ - INFO - Epoch 71 Batch 100: Train Loss = 0.1325
2023-11-08 08:19:14,876 - __main__ - INFO - Epoch 71 Batch 120: Train Loss = 0.1490
2023-11-08 08:19:28,161 - __main__ - INFO - Epoch 72 Batch 0: Train Loss = 0.1923
2023-11-08 08:19:48,978 - __main__ - INFO - Epoch 72 Batch 20: Train Loss = 0.2486
2023-11-08 08:20:08,758 - __main__ - INFO - Epoch 72 Batch 40: Train Loss = 0.2178
2023-11-08 08:20:29,243 - __main__ - INFO - Epoch 72 Batch 60: Train Loss = 0.2207
2023-11-08 08:20:49,917 - __main__ - INFO - Epoch 72 Batch 80: Train Loss = 0.1465
2023-11-08 08:21:11,140 - __main__ - INFO - Epoch 72 Batch 100: Train Loss = 0.1194
2023-11-08 08:21:31,839 - __main__ - INFO - Epoch 72 Batch 120: Train Loss = 0.1462
2023-11-08 08:21:45,887 - __main__ - INFO - Epoch 73 Batch 0: Train Loss = 0.1913
2023-11-08 08:22:06,929 - __main__ - INFO - Epoch 73 Batch 20: Train Loss = 0.2177
2023-11-08 08:22:28,105 - __main__ - INFO - Epoch 73 Batch 40: Train Loss = 0.2310
2023-11-08 08:22:48,487 - __main__ - INFO - Epoch 73 Batch 60: Train Loss = 0.2022
2023-11-08 08:23:10,035 - __main__ - INFO - Epoch 73 Batch 80: Train Loss = 0.1390
2023-11-08 08:23:31,500 - __main__ - INFO - Epoch 73 Batch 100: Train Loss = 0.1172
2023-11-08 08:23:50,818 - __main__ - INFO - Epoch 73 Batch 120: Train Loss = 0.1727
2023-11-08 08:24:04,998 - __main__ - INFO - Epoch 74 Batch 0: Train Loss = 0.1819
2023-11-08 08:24:26,624 - __main__ - INFO - Epoch 74 Batch 20: Train Loss = 0.2238
2023-11-08 08:24:46,625 - __main__ - INFO - Epoch 74 Batch 40: Train Loss = 0.2243
2023-11-08 08:25:07,257 - __main__ - INFO - Epoch 74 Batch 60: Train Loss = 0.1990
2023-11-08 08:25:27,971 - __main__ - INFO - Epoch 74 Batch 80: Train Loss = 0.1417
2023-11-08 08:25:49,201 - __main__ - INFO - Epoch 74 Batch 100: Train Loss = 0.1130
2023-11-08 08:26:10,208 - __main__ - INFO - Epoch 74 Batch 120: Train Loss = 0.1766
2023-11-08 08:26:23,969 - __main__ - INFO - ------------ Save best model - AUROC: 0.8917 ------------
2023-11-08 08:26:24,935 - __main__ - INFO - Epoch 75 Batch 0: Train Loss = 0.2157
2023-11-08 08:26:45,943 - __main__ - INFO - Epoch 75 Batch 20: Train Loss = 0.1985
2023-11-08 08:27:05,200 - __main__ - INFO - Epoch 75 Batch 40: Train Loss = 0.2154
2023-11-08 08:27:25,659 - __main__ - INFO - Epoch 75 Batch 60: Train Loss = 0.2020
2023-11-08 08:27:47,123 - __main__ - INFO - Epoch 75 Batch 80: Train Loss = 0.1420
2023-11-08 08:28:08,676 - __main__ - INFO - Epoch 75 Batch 100: Train Loss = 0.1147
2023-11-08 08:28:29,920 - __main__ - INFO - Epoch 75 Batch 120: Train Loss = 0.1636
2023-11-08 08:28:42,627 - __main__ - INFO - Epoch 76 Batch 0: Train Loss = 0.1864
2023-11-08 08:29:03,607 - __main__ - INFO - Epoch 76 Batch 20: Train Loss = 0.1886
2023-11-08 08:29:23,995 - __main__ - INFO - Epoch 76 Batch 40: Train Loss = 0.2204
2023-11-08 08:29:45,057 - __main__ - INFO - Epoch 76 Batch 60: Train Loss = 0.2184
2023-11-08 08:30:06,068 - __main__ - INFO - Epoch 76 Batch 80: Train Loss = 0.1330
2023-11-08 08:30:26,493 - __main__ - INFO - Epoch 76 Batch 100: Train Loss = 0.1337
2023-11-08 08:30:46,434 - __main__ - INFO - Epoch 76 Batch 120: Train Loss = 0.1449
2023-11-08 08:30:59,708 - __main__ - INFO - Epoch 77 Batch 0: Train Loss = 0.1867
2023-11-08 08:31:20,927 - __main__ - INFO - Epoch 77 Batch 20: Train Loss = 0.1869
2023-11-08 08:31:40,347 - __main__ - INFO - Epoch 77 Batch 40: Train Loss = 0.1884
2023-11-08 08:32:00,894 - __main__ - INFO - Epoch 77 Batch 60: Train Loss = 0.1869
2023-11-08 08:32:22,261 - __main__ - INFO - Epoch 77 Batch 80: Train Loss = 0.1340
2023-11-08 08:32:42,825 - __main__ - INFO - Epoch 77 Batch 100: Train Loss = 0.1279
2023-11-08 08:33:03,715 - __main__ - INFO - Epoch 77 Batch 120: Train Loss = 0.1583
2023-11-08 08:33:17,624 - __main__ - INFO - ------------ Save best model - AUROC: 0.8922 ------------
2023-11-08 08:33:18,466 - __main__ - INFO - Epoch 78 Batch 0: Train Loss = 0.2002
2023-11-08 08:33:39,483 - __main__ - INFO - Epoch 78 Batch 20: Train Loss = 0.2359
2023-11-08 08:33:59,241 - __main__ - INFO - Epoch 78 Batch 40: Train Loss = 0.2210
2023-11-08 08:34:18,846 - __main__ - INFO - Epoch 78 Batch 60: Train Loss = 0.1882
2023-11-08 08:34:39,595 - __main__ - INFO - Epoch 78 Batch 80: Train Loss = 0.1353
2023-11-08 08:35:01,132 - __main__ - INFO - Epoch 78 Batch 100: Train Loss = 0.1160
2023-11-08 08:35:21,373 - __main__ - INFO - Epoch 78 Batch 120: Train Loss = 0.1629
2023-11-08 08:35:35,247 - __main__ - INFO - Epoch 79 Batch 0: Train Loss = 0.2282
2023-11-08 08:35:56,317 - __main__ - INFO - Epoch 79 Batch 20: Train Loss = 0.1924
2023-11-08 08:36:15,799 - __main__ - INFO - Epoch 79 Batch 40: Train Loss = 0.2387
2023-11-08 08:36:35,860 - __main__ - INFO - Epoch 79 Batch 60: Train Loss = 0.1858
2023-11-08 08:36:57,632 - __main__ - INFO - Epoch 79 Batch 80: Train Loss = 0.1368
2023-11-08 08:37:17,408 - __main__ - INFO - Epoch 79 Batch 100: Train Loss = 0.1108
2023-11-08 08:37:39,225 - __main__ - INFO - Epoch 79 Batch 120: Train Loss = 0.1418
2023-11-08 08:37:52,725 - __main__ - INFO - Epoch 80 Batch 0: Train Loss = 0.1701
2023-11-08 08:38:14,570 - __main__ - INFO - Epoch 80 Batch 20: Train Loss = 0.1990
2023-11-08 08:38:35,072 - __main__ - INFO - Epoch 80 Batch 40: Train Loss = 0.2227
2023-11-08 08:38:55,274 - __main__ - INFO - Epoch 80 Batch 60: Train Loss = 0.1934
2023-11-08 08:39:16,220 - __main__ - INFO - Epoch 80 Batch 80: Train Loss = 0.1404
2023-11-08 08:39:37,780 - __main__ - INFO - Epoch 80 Batch 100: Train Loss = 0.1227
2023-11-08 08:39:58,478 - __main__ - INFO - Epoch 80 Batch 120: Train Loss = 0.1435
2023-11-08 08:40:12,270 - __main__ - INFO - Epoch 81 Batch 0: Train Loss = 0.2047
2023-11-08 08:40:32,891 - __main__ - INFO - Epoch 81 Batch 20: Train Loss = 0.2018
2023-11-08 08:40:53,569 - __main__ - INFO - Epoch 81 Batch 40: Train Loss = 0.2032
2023-11-08 08:41:13,383 - __main__ - INFO - Epoch 81 Batch 60: Train Loss = 0.2162
2023-11-08 08:41:33,731 - __main__ - INFO - Epoch 81 Batch 80: Train Loss = 0.1345
2023-11-08 08:41:54,454 - __main__ - INFO - Epoch 81 Batch 100: Train Loss = 0.1164
2023-11-08 08:42:15,059 - __main__ - INFO - Epoch 81 Batch 120: Train Loss = 0.1550
2023-11-08 08:42:28,605 - __main__ - INFO - Epoch 82 Batch 0: Train Loss = 0.1998
2023-11-08 08:42:49,847 - __main__ - INFO - Epoch 82 Batch 20: Train Loss = 0.1931
2023-11-08 08:43:11,464 - __main__ - INFO - Epoch 82 Batch 40: Train Loss = 0.2011
2023-11-08 08:43:31,747 - __main__ - INFO - Epoch 82 Batch 60: Train Loss = 0.1985
2023-11-08 08:43:53,818 - __main__ - INFO - Epoch 82 Batch 80: Train Loss = 0.1315
2023-11-08 08:44:14,701 - __main__ - INFO - Epoch 82 Batch 100: Train Loss = 0.1379
2023-11-08 08:44:34,648 - __main__ - INFO - Epoch 82 Batch 120: Train Loss = 0.1374
2023-11-08 08:44:48,623 - __main__ - INFO - Epoch 83 Batch 0: Train Loss = 0.1861
2023-11-08 08:45:10,395 - __main__ - INFO - Epoch 83 Batch 20: Train Loss = 0.1997
2023-11-08 08:45:29,745 - __main__ - INFO - Epoch 83 Batch 40: Train Loss = 0.2076
2023-11-08 08:45:50,595 - __main__ - INFO - Epoch 83 Batch 60: Train Loss = 0.1744
2023-11-08 08:46:10,923 - __main__ - INFO - Epoch 83 Batch 80: Train Loss = 0.1390
2023-11-08 08:46:31,629 - __main__ - INFO - Epoch 83 Batch 100: Train Loss = 0.1264
2023-11-08 08:46:51,602 - __main__ - INFO - Epoch 83 Batch 120: Train Loss = 0.1623
2023-11-08 08:47:05,269 - __main__ - INFO - Epoch 84 Batch 0: Train Loss = 0.2211
2023-11-08 08:47:26,613 - __main__ - INFO - Epoch 84 Batch 20: Train Loss = 0.1818
2023-11-08 08:47:47,340 - __main__ - INFO - Epoch 84 Batch 40: Train Loss = 0.2304
2023-11-08 08:48:08,357 - __main__ - INFO - Epoch 84 Batch 60: Train Loss = 0.1753
2023-11-08 08:48:28,722 - __main__ - INFO - Epoch 84 Batch 80: Train Loss = 0.1401
2023-11-08 08:48:49,954 - __main__ - INFO - Epoch 84 Batch 100: Train Loss = 0.1017
2023-11-08 08:49:10,942 - __main__ - INFO - Epoch 84 Batch 120: Train Loss = 0.1562
2023-11-08 08:49:25,011 - __main__ - INFO - Epoch 85 Batch 0: Train Loss = 0.2114
2023-11-08 08:49:46,377 - __main__ - INFO - Epoch 85 Batch 20: Train Loss = 0.2114
2023-11-08 08:50:06,805 - __main__ - INFO - Epoch 85 Batch 40: Train Loss = 0.2108
2023-11-08 08:50:27,372 - __main__ - INFO - Epoch 85 Batch 60: Train Loss = 0.1997
2023-11-08 08:50:48,391 - __main__ - INFO - Epoch 85 Batch 80: Train Loss = 0.1287
2023-11-08 08:51:09,779 - __main__ - INFO - Epoch 85 Batch 100: Train Loss = 0.1283
2023-11-08 08:51:30,867 - __main__ - INFO - Epoch 85 Batch 120: Train Loss = 0.1407
2023-11-08 08:51:44,781 - __main__ - INFO - Epoch 86 Batch 0: Train Loss = 0.2020
2023-11-08 08:52:07,321 - __main__ - INFO - Epoch 86 Batch 20: Train Loss = 0.2128
2023-11-08 08:52:28,239 - __main__ - INFO - Epoch 86 Batch 40: Train Loss = 0.2120
2023-11-08 08:52:48,431 - __main__ - INFO - Epoch 86 Batch 60: Train Loss = 0.1781
2023-11-08 08:53:09,547 - __main__ - INFO - Epoch 86 Batch 80: Train Loss = 0.1260
2023-11-08 08:53:30,812 - __main__ - INFO - Epoch 86 Batch 100: Train Loss = 0.1071
2023-11-08 08:53:50,578 - __main__ - INFO - Epoch 86 Batch 120: Train Loss = 0.1684
2023-11-08 08:54:04,040 - __main__ - INFO - Epoch 87 Batch 0: Train Loss = 0.1923
2023-11-08 08:54:24,421 - __main__ - INFO - Epoch 87 Batch 20: Train Loss = 0.2087
2023-11-08 08:54:44,747 - __main__ - INFO - Epoch 87 Batch 40: Train Loss = 0.2426
2023-11-08 08:55:06,315 - __main__ - INFO - Epoch 87 Batch 60: Train Loss = 0.1956
2023-11-08 08:55:28,371 - __main__ - INFO - Epoch 87 Batch 80: Train Loss = 0.1308
2023-11-08 08:55:49,308 - __main__ - INFO - Epoch 87 Batch 100: Train Loss = 0.1106
2023-11-08 08:56:09,810 - __main__ - INFO - Epoch 87 Batch 120: Train Loss = 0.1414
2023-11-08 08:56:22,953 - __main__ - INFO - ------------ Save best model - AUROC: 0.8933 ------------
2023-11-08 08:56:23,983 - __main__ - INFO - Epoch 88 Batch 0: Train Loss = 0.1905
2023-11-08 08:56:46,272 - __main__ - INFO - Epoch 88 Batch 20: Train Loss = 0.1735
2023-11-08 08:57:07,382 - __main__ - INFO - Epoch 88 Batch 40: Train Loss = 0.2154
2023-11-08 08:57:27,384 - __main__ - INFO - Epoch 88 Batch 60: Train Loss = 0.1885
2023-11-08 08:57:48,224 - __main__ - INFO - Epoch 88 Batch 80: Train Loss = 0.1256
2023-11-08 08:58:08,965 - __main__ - INFO - Epoch 88 Batch 100: Train Loss = 0.1166
2023-11-08 08:58:29,231 - __main__ - INFO - Epoch 88 Batch 120: Train Loss = 0.1604
2023-11-08 08:58:42,804 - __main__ - INFO - ------------ Save best model - AUROC: 0.8935 ------------
2023-11-08 08:58:43,964 - __main__ - INFO - Epoch 89 Batch 0: Train Loss = 0.1888
2023-11-08 08:59:04,973 - __main__ - INFO - Epoch 89 Batch 20: Train Loss = 0.1877
2023-11-08 08:59:25,658 - __main__ - INFO - Epoch 89 Batch 40: Train Loss = 0.2182
2023-11-08 08:59:45,942 - __main__ - INFO - Epoch 89 Batch 60: Train Loss = 0.1774
2023-11-08 09:00:06,505 - __main__ - INFO - Epoch 89 Batch 80: Train Loss = 0.1278
2023-11-08 09:00:27,444 - __main__ - INFO - Epoch 89 Batch 100: Train Loss = 0.1298
2023-11-08 09:00:47,947 - __main__ - INFO - Epoch 89 Batch 120: Train Loss = 0.1742
2023-11-08 09:01:00,808 - __main__ - INFO - ------------ Save best model - AUROC: 0.8943 ------------
2023-11-08 09:01:01,798 - __main__ - INFO - Epoch 90 Batch 0: Train Loss = 0.1855
2023-11-08 09:01:22,653 - __main__ - INFO - Epoch 90 Batch 20: Train Loss = 0.2177
2023-11-08 09:01:43,212 - __main__ - INFO - Epoch 90 Batch 40: Train Loss = 0.2222
2023-11-08 09:02:03,995 - __main__ - INFO - Epoch 90 Batch 60: Train Loss = 0.1782
2023-11-08 09:02:24,976 - __main__ - INFO - Epoch 90 Batch 80: Train Loss = 0.1143
2023-11-08 09:02:45,921 - __main__ - INFO - Epoch 90 Batch 100: Train Loss = 0.1066
2023-11-08 09:03:07,222 - __main__ - INFO - Epoch 90 Batch 120: Train Loss = 0.1442
2023-11-08 09:03:21,138 - __main__ - INFO - Epoch 91 Batch 0: Train Loss = 0.1683
2023-11-08 09:03:43,065 - __main__ - INFO - Epoch 91 Batch 20: Train Loss = 0.2050
2023-11-08 09:04:03,433 - __main__ - INFO - Epoch 91 Batch 40: Train Loss = 0.2169
2023-11-08 09:04:23,489 - __main__ - INFO - Epoch 91 Batch 60: Train Loss = 0.1868
2023-11-08 09:04:44,247 - __main__ - INFO - Epoch 91 Batch 80: Train Loss = 0.1379
2023-11-08 09:05:05,617 - __main__ - INFO - Epoch 91 Batch 100: Train Loss = 0.1222
2023-11-08 09:05:25,438 - __main__ - INFO - Epoch 91 Batch 120: Train Loss = 0.1571
2023-11-08 09:05:38,663 - __main__ - INFO - ------------ Save best model - AUROC: 0.8951 ------------
2023-11-08 09:05:39,698 - __main__ - INFO - Epoch 92 Batch 0: Train Loss = 0.2050
2023-11-08 09:06:01,050 - __main__ - INFO - Epoch 92 Batch 20: Train Loss = 0.2083
2023-11-08 09:06:21,808 - __main__ - INFO - Epoch 92 Batch 40: Train Loss = 0.2159
2023-11-08 09:06:42,687 - __main__ - INFO - Epoch 92 Batch 60: Train Loss = 0.1925
2023-11-08 09:07:03,767 - __main__ - INFO - Epoch 92 Batch 80: Train Loss = 0.1420
2023-11-08 09:07:24,954 - __main__ - INFO - Epoch 92 Batch 100: Train Loss = 0.0933
2023-11-08 09:07:45,963 - __main__ - INFO - Epoch 92 Batch 120: Train Loss = 0.1469
2023-11-08 09:07:59,234 - __main__ - INFO - ------------ Save best model - AUROC: 0.8953 ------------
2023-11-08 09:08:00,056 - __main__ - INFO - Epoch 93 Batch 0: Train Loss = 0.1976
2023-11-08 09:08:20,740 - __main__ - INFO - Epoch 93 Batch 20: Train Loss = 0.2040
2023-11-08 09:08:41,912 - __main__ - INFO - Epoch 93 Batch 40: Train Loss = 0.2224
2023-11-08 09:09:01,940 - __main__ - INFO - Epoch 93 Batch 60: Train Loss = 0.2044
2023-11-08 09:09:23,420 - __main__ - INFO - Epoch 93 Batch 80: Train Loss = 0.1239
2023-11-08 09:09:45,419 - __main__ - INFO - Epoch 93 Batch 100: Train Loss = 0.1273
2023-11-08 09:10:05,416 - __main__ - INFO - Epoch 93 Batch 120: Train Loss = 0.1742
2023-11-08 09:10:19,439 - __main__ - INFO - Epoch 94 Batch 0: Train Loss = 0.1994
2023-11-08 09:10:40,942 - __main__ - INFO - Epoch 94 Batch 20: Train Loss = 0.1923
2023-11-08 09:11:00,729 - __main__ - INFO - Epoch 94 Batch 40: Train Loss = 0.2026
2023-11-08 09:11:21,848 - __main__ - INFO - Epoch 94 Batch 60: Train Loss = 0.2040
2023-11-08 09:11:42,280 - __main__ - INFO - Epoch 94 Batch 80: Train Loss = 0.1272
2023-11-08 09:12:02,943 - __main__ - INFO - Epoch 94 Batch 100: Train Loss = 0.1257
2023-11-08 09:12:24,140 - __main__ - INFO - Epoch 94 Batch 120: Train Loss = 0.1404
2023-11-08 09:12:38,513 - __main__ - INFO - Epoch 95 Batch 0: Train Loss = 0.1733
2023-11-08 09:12:59,241 - __main__ - INFO - Epoch 95 Batch 20: Train Loss = 0.1926
2023-11-08 09:13:19,486 - __main__ - INFO - Epoch 95 Batch 40: Train Loss = 0.2042
2023-11-08 09:13:39,471 - __main__ - INFO - Epoch 95 Batch 60: Train Loss = 0.1855
2023-11-08 09:14:00,924 - __main__ - INFO - Epoch 95 Batch 80: Train Loss = 0.1135
2023-11-08 09:14:22,590 - __main__ - INFO - Epoch 95 Batch 100: Train Loss = 0.0998
2023-11-08 09:14:43,146 - __main__ - INFO - Epoch 95 Batch 120: Train Loss = 0.1462
2023-11-08 09:14:56,484 - __main__ - INFO - Epoch 96 Batch 0: Train Loss = 0.1838
2023-11-08 09:15:17,714 - __main__ - INFO - Epoch 96 Batch 20: Train Loss = 0.2101
2023-11-08 09:15:38,340 - __main__ - INFO - Epoch 96 Batch 40: Train Loss = 0.1903
2023-11-08 09:15:59,876 - __main__ - INFO - Epoch 96 Batch 60: Train Loss = 0.1905
2023-11-08 09:16:20,441 - __main__ - INFO - Epoch 96 Batch 80: Train Loss = 0.1260
2023-11-08 09:16:41,907 - __main__ - INFO - Epoch 96 Batch 100: Train Loss = 0.1156
2023-11-08 09:17:02,460 - __main__ - INFO - Epoch 96 Batch 120: Train Loss = 0.1633
2023-11-08 09:17:16,203 - __main__ - INFO - Epoch 97 Batch 0: Train Loss = 0.1988
2023-11-08 09:17:37,713 - __main__ - INFO - Epoch 97 Batch 20: Train Loss = 0.2175
2023-11-08 09:17:57,153 - __main__ - INFO - Epoch 97 Batch 40: Train Loss = 0.2324
2023-11-08 09:18:16,888 - __main__ - INFO - Epoch 97 Batch 60: Train Loss = 0.2001
2023-11-08 09:18:37,284 - __main__ - INFO - Epoch 97 Batch 80: Train Loss = 0.1250
2023-11-08 09:18:58,801 - __main__ - INFO - Epoch 97 Batch 100: Train Loss = 0.1296
2023-11-08 09:19:19,068 - __main__ - INFO - Epoch 97 Batch 120: Train Loss = 0.1513
2023-11-08 09:19:33,198 - __main__ - INFO - Epoch 98 Batch 0: Train Loss = 0.1984
2023-11-08 09:19:55,034 - __main__ - INFO - Epoch 98 Batch 20: Train Loss = 0.1895
2023-11-08 09:20:15,354 - __main__ - INFO - Epoch 98 Batch 40: Train Loss = 0.2110
2023-11-08 09:20:35,860 - __main__ - INFO - Epoch 98 Batch 60: Train Loss = 0.1724
2023-11-08 09:20:56,574 - __main__ - INFO - Epoch 98 Batch 80: Train Loss = 0.1392
2023-11-08 09:21:17,648 - __main__ - INFO - Epoch 98 Batch 100: Train Loss = 0.0971
2023-11-08 09:21:37,999 - __main__ - INFO - Epoch 98 Batch 120: Train Loss = 0.1603
2023-11-08 09:21:51,383 - __main__ - INFO - Epoch 99 Batch 0: Train Loss = 0.2019
2023-11-08 09:22:13,188 - __main__ - INFO - Epoch 99 Batch 20: Train Loss = 0.2160
2023-11-08 09:22:32,939 - __main__ - INFO - Epoch 99 Batch 40: Train Loss = 0.2161
2023-11-08 09:22:52,838 - __main__ - INFO - Epoch 99 Batch 60: Train Loss = 0.2052
2023-11-08 09:23:13,819 - __main__ - INFO - Epoch 99 Batch 80: Train Loss = 0.1092
2023-11-08 09:23:36,011 - __main__ - INFO - Epoch 99 Batch 100: Train Loss = 0.1194
2023-11-08 09:23:56,497 - __main__ - INFO - Epoch 99 Batch 120: Train Loss = 0.1490
2023-11-08 09:24:10,210 - __main__ - INFO - Epoch 100 Batch 0: Train Loss = 0.1899
2023-11-08 09:24:31,179 - __main__ - INFO - Epoch 100 Batch 20: Train Loss = 0.1998
2023-11-08 09:24:51,272 - __main__ - INFO - Epoch 100 Batch 40: Train Loss = 0.2154
2023-11-08 09:25:10,931 - __main__ - INFO - Epoch 100 Batch 60: Train Loss = 0.1766
2023-11-08 09:25:32,212 - __main__ - INFO - Epoch 100 Batch 80: Train Loss = 0.1256
2023-11-08 09:25:53,404 - __main__ - INFO - Epoch 100 Batch 100: Train Loss = 0.1343
2023-11-08 09:26:14,741 - __main__ - INFO - Epoch 100 Batch 120: Train Loss = 0.1723
2023-11-08 09:26:28,627 - __main__ - INFO - Epoch 101 Batch 0: Train Loss = 0.1930
2023-11-08 09:26:49,466 - __main__ - INFO - Epoch 101 Batch 20: Train Loss = 0.2538
2023-11-08 09:27:09,990 - __main__ - INFO - Epoch 101 Batch 40: Train Loss = 0.2124
2023-11-08 09:27:30,347 - __main__ - INFO - Epoch 101 Batch 60: Train Loss = 0.1816
2023-11-08 09:27:51,975 - __main__ - INFO - Epoch 101 Batch 80: Train Loss = 0.1137
2023-11-08 09:28:12,737 - __main__ - INFO - Epoch 101 Batch 100: Train Loss = 0.1192
2023-11-08 09:28:33,256 - __main__ - INFO - Epoch 101 Batch 120: Train Loss = 0.1557
2023-11-08 09:28:47,021 - __main__ - INFO - Epoch 102 Batch 0: Train Loss = 0.1860
2023-11-08 09:29:08,047 - __main__ - INFO - Epoch 102 Batch 20: Train Loss = 0.2379
2023-11-08 09:29:29,173 - __main__ - INFO - Epoch 102 Batch 40: Train Loss = 0.2219
2023-11-08 09:29:49,315 - __main__ - INFO - Epoch 102 Batch 60: Train Loss = 0.1931
2023-11-08 09:30:10,603 - __main__ - INFO - Epoch 102 Batch 80: Train Loss = 0.1230
2023-11-08 09:30:31,760 - __main__ - INFO - Epoch 102 Batch 100: Train Loss = 0.1046
2023-11-08 09:30:52,688 - __main__ - INFO - Epoch 102 Batch 120: Train Loss = 0.1610
2023-11-08 09:31:05,966 - __main__ - INFO - Epoch 103 Batch 0: Train Loss = 0.2147
2023-11-08 09:31:26,253 - __main__ - INFO - Epoch 103 Batch 20: Train Loss = 0.1945
2023-11-08 09:31:46,996 - __main__ - INFO - Epoch 103 Batch 40: Train Loss = 0.2197
2023-11-08 09:32:07,365 - __main__ - INFO - Epoch 103 Batch 60: Train Loss = 0.1965
2023-11-08 09:32:28,847 - __main__ - INFO - Epoch 103 Batch 80: Train Loss = 0.1305
2023-11-08 09:32:49,921 - __main__ - INFO - Epoch 103 Batch 100: Train Loss = 0.1078
2023-11-08 09:33:10,238 - __main__ - INFO - Epoch 103 Batch 120: Train Loss = 0.1502
2023-11-08 09:33:24,395 - __main__ - INFO - Epoch 104 Batch 0: Train Loss = 0.1893
2023-11-08 09:33:46,827 - __main__ - INFO - Epoch 104 Batch 20: Train Loss = 0.1949
2023-11-08 09:34:08,089 - __main__ - INFO - Epoch 104 Batch 40: Train Loss = 0.2221
2023-11-08 09:34:28,916 - __main__ - INFO - Epoch 104 Batch 60: Train Loss = 0.1683
2023-11-08 09:34:49,732 - __main__ - INFO - Epoch 104 Batch 80: Train Loss = 0.1099
2023-11-08 09:35:10,757 - __main__ - INFO - Epoch 104 Batch 100: Train Loss = 0.1062
2023-11-08 09:35:31,757 - __main__ - INFO - Epoch 104 Batch 120: Train Loss = 0.1557
2023-11-08 09:35:45,371 - __main__ - INFO - Epoch 105 Batch 0: Train Loss = 0.2048
2023-11-08 09:36:06,682 - __main__ - INFO - Epoch 105 Batch 20: Train Loss = 0.2226
2023-11-08 09:36:26,778 - __main__ - INFO - Epoch 105 Batch 40: Train Loss = 0.2122
2023-11-08 09:36:47,139 - __main__ - INFO - Epoch 105 Batch 60: Train Loss = 0.1768
2023-11-08 09:37:08,913 - __main__ - INFO - Epoch 105 Batch 80: Train Loss = 0.1328
2023-11-08 09:37:29,937 - __main__ - INFO - Epoch 105 Batch 100: Train Loss = 0.1105
2023-11-08 09:37:50,754 - __main__ - INFO - Epoch 105 Batch 120: Train Loss = 0.1600
2023-11-08 09:38:05,309 - __main__ - INFO - Epoch 106 Batch 0: Train Loss = 0.2019
2023-11-08 09:38:27,437 - __main__ - INFO - Epoch 106 Batch 20: Train Loss = 0.1853
2023-11-08 09:38:47,393 - __main__ - INFO - Epoch 106 Batch 40: Train Loss = 0.1928
2023-11-08 09:39:08,616 - __main__ - INFO - Epoch 106 Batch 60: Train Loss = 0.1827
2023-11-08 09:39:29,354 - __main__ - INFO - Epoch 106 Batch 80: Train Loss = 0.1370
2023-11-08 09:39:51,007 - __main__ - INFO - Epoch 106 Batch 100: Train Loss = 0.1062
2023-11-08 09:40:11,935 - __main__ - INFO - Epoch 106 Batch 120: Train Loss = 0.1746
2023-11-08 09:40:27,155 - __main__ - INFO - Epoch 107 Batch 0: Train Loss = 0.1898
2023-11-08 09:40:48,889 - __main__ - INFO - Epoch 107 Batch 20: Train Loss = 0.2030
2023-11-08 09:41:10,105 - __main__ - INFO - Epoch 107 Batch 40: Train Loss = 0.2113
2023-11-08 09:41:30,811 - __main__ - INFO - Epoch 107 Batch 60: Train Loss = 0.1809
2023-11-08 09:41:51,763 - __main__ - INFO - Epoch 107 Batch 80: Train Loss = 0.1522
2023-11-08 09:42:13,404 - __main__ - INFO - Epoch 107 Batch 100: Train Loss = 0.1123
2023-11-08 09:42:34,076 - __main__ - INFO - Epoch 107 Batch 120: Train Loss = 0.1471
2023-11-08 09:42:49,117 - __main__ - INFO - Epoch 108 Batch 0: Train Loss = 0.1730
2023-11-08 09:43:11,130 - __main__ - INFO - Epoch 108 Batch 20: Train Loss = 0.2141
2023-11-08 09:43:31,785 - __main__ - INFO - Epoch 108 Batch 40: Train Loss = 0.1903
2023-11-08 09:43:53,011 - __main__ - INFO - Epoch 108 Batch 60: Train Loss = 0.1559
2023-11-08 09:44:14,135 - __main__ - INFO - Epoch 108 Batch 80: Train Loss = 0.1301
2023-11-08 09:44:34,782 - __main__ - INFO - Epoch 108 Batch 100: Train Loss = 0.1120
2023-11-08 09:44:57,046 - __main__ - INFO - Epoch 108 Batch 120: Train Loss = 0.1474
2023-11-08 09:45:11,800 - __main__ - INFO - Epoch 109 Batch 0: Train Loss = 0.1824
2023-11-08 09:45:34,661 - __main__ - INFO - Epoch 109 Batch 20: Train Loss = 0.2121
2023-11-08 09:45:56,290 - __main__ - INFO - Epoch 109 Batch 40: Train Loss = 0.2136
2023-11-08 09:46:18,371 - __main__ - INFO - Epoch 109 Batch 60: Train Loss = 0.1752
2023-11-08 09:46:40,653 - __main__ - INFO - Epoch 109 Batch 80: Train Loss = 0.1201
2023-11-08 09:47:02,900 - __main__ - INFO - Epoch 109 Batch 100: Train Loss = 0.1118
2023-11-08 09:47:23,964 - __main__ - INFO - Epoch 109 Batch 120: Train Loss = 0.1710
2023-11-08 09:47:38,448 - __main__ - INFO - Epoch 110 Batch 0: Train Loss = 0.1810
2023-11-08 09:48:00,389 - __main__ - INFO - Epoch 110 Batch 20: Train Loss = 0.2112
2023-11-08 09:48:21,404 - __main__ - INFO - Epoch 110 Batch 40: Train Loss = 0.1963
2023-11-08 09:48:42,122 - __main__ - INFO - Epoch 110 Batch 60: Train Loss = 0.1995
2023-11-08 09:49:04,411 - __main__ - INFO - Epoch 110 Batch 80: Train Loss = 0.1341
2023-11-08 09:49:26,262 - __main__ - INFO - Epoch 110 Batch 100: Train Loss = 0.1112
2023-11-08 09:49:47,719 - __main__ - INFO - Epoch 110 Batch 120: Train Loss = 0.1555
2023-11-08 09:50:03,190 - __main__ - INFO - Epoch 111 Batch 0: Train Loss = 0.1781
2023-11-08 09:50:24,834 - __main__ - INFO - Epoch 111 Batch 20: Train Loss = 0.2129
2023-11-08 09:50:45,486 - __main__ - INFO - Epoch 111 Batch 40: Train Loss = 0.2125
2023-11-08 09:51:07,833 - __main__ - INFO - Epoch 111 Batch 60: Train Loss = 0.2008
2023-11-08 09:51:30,354 - __main__ - INFO - Epoch 111 Batch 80: Train Loss = 0.1252
2023-11-08 09:51:52,123 - __main__ - INFO - Epoch 111 Batch 100: Train Loss = 0.1179
2023-11-08 09:52:13,705 - __main__ - INFO - Epoch 111 Batch 120: Train Loss = 0.1329
2023-11-08 09:52:28,824 - __main__ - INFO - Epoch 112 Batch 0: Train Loss = 0.2067
2023-11-08 09:52:50,669 - __main__ - INFO - Epoch 112 Batch 20: Train Loss = 0.2051
2023-11-08 09:53:11,947 - __main__ - INFO - Epoch 112 Batch 40: Train Loss = 0.1968
2023-11-08 09:53:33,190 - __main__ - INFO - Epoch 112 Batch 60: Train Loss = 0.1755
2023-11-08 09:53:54,164 - __main__ - INFO - Epoch 112 Batch 80: Train Loss = 0.1255
2023-11-08 09:54:16,086 - __main__ - INFO - Epoch 112 Batch 100: Train Loss = 0.1184
2023-11-08 09:54:37,995 - __main__ - INFO - Epoch 112 Batch 120: Train Loss = 0.1446
2023-11-08 09:54:52,974 - __main__ - INFO - Epoch 113 Batch 0: Train Loss = 0.1748
2023-11-08 09:55:15,399 - __main__ - INFO - Epoch 113 Batch 20: Train Loss = 0.1968
2023-11-08 09:55:37,041 - __main__ - INFO - Epoch 113 Batch 40: Train Loss = 0.2097
2023-11-08 09:55:58,531 - __main__ - INFO - Epoch 113 Batch 60: Train Loss = 0.1908
2023-11-08 09:56:20,676 - __main__ - INFO - Epoch 113 Batch 80: Train Loss = 0.1287
2023-11-08 09:56:42,823 - __main__ - INFO - Epoch 113 Batch 100: Train Loss = 0.1251
2023-11-08 09:57:04,542 - __main__ - INFO - Epoch 113 Batch 120: Train Loss = 0.1808
2023-11-08 09:57:20,146 - __main__ - INFO - Epoch 114 Batch 0: Train Loss = 0.1729
2023-11-08 09:57:43,105 - __main__ - INFO - Epoch 114 Batch 20: Train Loss = 0.1910
2023-11-08 09:58:04,758 - __main__ - INFO - Epoch 114 Batch 40: Train Loss = 0.2119
2023-11-08 09:58:25,670 - __main__ - INFO - Epoch 114 Batch 60: Train Loss = 0.1873
2023-11-08 09:58:47,673 - __main__ - INFO - Epoch 114 Batch 80: Train Loss = 0.1305
2023-11-08 09:59:08,986 - __main__ - INFO - Epoch 114 Batch 100: Train Loss = 0.1089
2023-11-08 09:59:28,769 - __main__ - INFO - Epoch 114 Batch 120: Train Loss = 0.1596
2023-11-08 09:59:41,933 - __main__ - INFO - ------------ Save best model - AUROC: 0.8978 ------------
2023-11-08 09:59:42,895 - __main__ - INFO - Epoch 115 Batch 0: Train Loss = 0.1877
2023-11-08 10:00:04,747 - __main__ - INFO - Epoch 115 Batch 20: Train Loss = 0.1914
2023-11-08 10:00:26,533 - __main__ - INFO - Epoch 115 Batch 40: Train Loss = 0.2158
2023-11-08 10:00:48,183 - __main__ - INFO - Epoch 115 Batch 60: Train Loss = 0.1986
2023-11-08 10:01:10,647 - __main__ - INFO - Epoch 115 Batch 80: Train Loss = 0.1438
2023-11-08 10:01:32,725 - __main__ - INFO - Epoch 115 Batch 100: Train Loss = 0.1248
2023-11-08 10:01:53,718 - __main__ - INFO - Epoch 115 Batch 120: Train Loss = 0.1388
2023-11-08 10:02:09,309 - __main__ - INFO - Epoch 116 Batch 0: Train Loss = 0.2038
2023-11-08 10:02:32,071 - __main__ - INFO - Epoch 116 Batch 20: Train Loss = 0.2222
2023-11-08 10:02:52,550 - __main__ - INFO - Epoch 116 Batch 40: Train Loss = 0.1920
2023-11-08 10:03:12,161 - __main__ - INFO - Epoch 116 Batch 60: Train Loss = 0.1754
2023-11-08 10:03:31,480 - __main__ - INFO - Epoch 116 Batch 80: Train Loss = 0.1333
2023-11-08 10:03:50,441 - __main__ - INFO - Epoch 116 Batch 100: Train Loss = 0.1260
2023-11-08 10:04:09,781 - __main__ - INFO - Epoch 116 Batch 120: Train Loss = 0.1420
2023-11-08 10:04:21,698 - __main__ - INFO - Epoch 117 Batch 0: Train Loss = 0.1803
2023-11-08 10:04:41,610 - __main__ - INFO - Epoch 117 Batch 20: Train Loss = 0.2175
2023-11-08 10:05:01,053 - __main__ - INFO - Epoch 117 Batch 40: Train Loss = 0.2043
2023-11-08 10:05:20,251 - __main__ - INFO - Epoch 117 Batch 60: Train Loss = 0.2035
2023-11-08 10:05:39,389 - __main__ - INFO - Epoch 117 Batch 80: Train Loss = 0.1257
2023-11-08 10:05:59,139 - __main__ - INFO - Epoch 117 Batch 100: Train Loss = 0.1307
2023-11-08 10:06:18,484 - __main__ - INFO - Epoch 117 Batch 120: Train Loss = 0.1482
2023-11-08 10:06:31,961 - __main__ - INFO - Epoch 118 Batch 0: Train Loss = 0.1844
2023-11-08 10:06:52,019 - __main__ - INFO - Epoch 118 Batch 20: Train Loss = 0.2109
2023-11-08 10:07:11,250 - __main__ - INFO - Epoch 118 Batch 40: Train Loss = 0.1887
2023-11-08 10:07:30,527 - __main__ - INFO - Epoch 118 Batch 60: Train Loss = 0.2059
2023-11-08 10:07:50,628 - __main__ - INFO - Epoch 118 Batch 80: Train Loss = 0.1260
2023-11-08 10:08:09,946 - __main__ - INFO - Epoch 118 Batch 100: Train Loss = 0.1293
2023-11-08 10:08:29,239 - __main__ - INFO - Epoch 118 Batch 120: Train Loss = 0.1538
2023-11-08 10:08:43,198 - __main__ - INFO - Epoch 119 Batch 0: Train Loss = 0.2020
2023-11-08 10:09:03,319 - __main__ - INFO - Epoch 119 Batch 20: Train Loss = 0.2081
2023-11-08 10:09:22,671 - __main__ - INFO - Epoch 119 Batch 40: Train Loss = 0.1901
2023-11-08 10:09:42,528 - __main__ - INFO - Epoch 119 Batch 60: Train Loss = 0.1926
2023-11-08 10:10:02,569 - __main__ - INFO - Epoch 119 Batch 80: Train Loss = 0.1336
2023-11-08 10:10:21,578 - __main__ - INFO - Epoch 119 Batch 100: Train Loss = 0.1091
2023-11-08 10:10:40,760 - __main__ - INFO - Epoch 119 Batch 120: Train Loss = 0.1454
2023-11-08 10:10:53,716 - __main__ - INFO - Epoch 120 Batch 0: Train Loss = 0.1797
2023-11-08 10:11:13,191 - __main__ - INFO - Epoch 120 Batch 20: Train Loss = 0.1846
2023-11-08 10:11:31,689 - __main__ - INFO - Epoch 120 Batch 40: Train Loss = 0.2041
2023-11-08 10:11:51,723 - __main__ - INFO - Epoch 120 Batch 60: Train Loss = 0.1835
2023-11-08 10:12:11,137 - __main__ - INFO - Epoch 120 Batch 80: Train Loss = 0.1212
2023-11-08 10:12:29,705 - __main__ - INFO - Epoch 120 Batch 100: Train Loss = 0.1105
2023-11-08 10:12:48,901 - __main__ - INFO - Epoch 120 Batch 120: Train Loss = 0.1527
2023-11-08 10:13:02,966 - __main__ - INFO - Epoch 121 Batch 0: Train Loss = 0.1715
2023-11-08 10:13:22,646 - __main__ - INFO - Epoch 121 Batch 20: Train Loss = 0.2042
2023-11-08 10:13:41,860 - __main__ - INFO - Epoch 121 Batch 40: Train Loss = 0.2184
2023-11-08 10:13:59,202 - __main__ - INFO - Epoch 121 Batch 60: Train Loss = 0.1671
2023-11-08 10:14:15,578 - __main__ - INFO - Epoch 121 Batch 80: Train Loss = 0.1234
2023-11-08 10:14:32,513 - __main__ - INFO - Epoch 121 Batch 100: Train Loss = 0.1059
2023-11-08 10:14:48,899 - __main__ - INFO - Epoch 121 Batch 120: Train Loss = 0.1581
2023-11-08 10:15:00,639 - __main__ - INFO - Epoch 122 Batch 0: Train Loss = 0.2036
2023-11-08 10:15:17,788 - __main__ - INFO - Epoch 122 Batch 20: Train Loss = 0.2010
2023-11-08 10:15:34,532 - __main__ - INFO - Epoch 122 Batch 40: Train Loss = 0.2144
2023-11-08 10:15:50,814 - __main__ - INFO - Epoch 122 Batch 60: Train Loss = 0.1938
2023-11-08 10:16:07,593 - __main__ - INFO - Epoch 122 Batch 80: Train Loss = 0.1195
2023-11-08 10:16:25,174 - __main__ - INFO - Epoch 122 Batch 100: Train Loss = 0.0980
2023-11-08 10:16:41,486 - __main__ - INFO - Epoch 122 Batch 120: Train Loss = 0.1553
2023-11-08 10:16:52,903 - __main__ - INFO - Epoch 123 Batch 0: Train Loss = 0.2031
2023-11-08 10:17:10,828 - __main__ - INFO - Epoch 123 Batch 20: Train Loss = 0.1973
2023-11-08 10:17:28,025 - __main__ - INFO - Epoch 123 Batch 40: Train Loss = 0.2114
2023-11-08 10:17:44,603 - __main__ - INFO - Epoch 123 Batch 60: Train Loss = 0.1694
2023-11-08 10:18:00,871 - __main__ - INFO - Epoch 123 Batch 80: Train Loss = 0.1313
2023-11-08 10:18:17,460 - __main__ - INFO - Epoch 123 Batch 100: Train Loss = 0.1266
2023-11-08 10:18:34,061 - __main__ - INFO - Epoch 123 Batch 120: Train Loss = 0.1343
2023-11-08 10:18:46,092 - __main__ - INFO - Epoch 124 Batch 0: Train Loss = 0.1922
2023-11-08 10:19:03,276 - __main__ - INFO - Epoch 124 Batch 20: Train Loss = 0.1947
2023-11-08 10:19:19,608 - __main__ - INFO - Epoch 124 Batch 40: Train Loss = 0.2212
2023-11-08 10:19:36,251 - __main__ - INFO - Epoch 124 Batch 60: Train Loss = 0.1821
2023-11-08 10:19:53,646 - __main__ - INFO - Epoch 124 Batch 80: Train Loss = 0.1591
2023-11-08 10:20:11,083 - __main__ - INFO - Epoch 124 Batch 100: Train Loss = 0.1202
2023-11-08 10:20:27,639 - __main__ - INFO - Epoch 124 Batch 120: Train Loss = 0.1557
2023-11-08 10:20:38,885 - __main__ - INFO - Epoch 125 Batch 0: Train Loss = 0.1511
2023-11-08 10:20:56,133 - __main__ - INFO - Epoch 125 Batch 20: Train Loss = 0.2030
2023-11-08 10:21:12,841 - __main__ - INFO - Epoch 125 Batch 40: Train Loss = 0.2004
2023-11-08 10:21:29,278 - __main__ - INFO - Epoch 125 Batch 60: Train Loss = 0.1929
2023-11-08 10:21:46,219 - __main__ - INFO - Epoch 125 Batch 80: Train Loss = 0.1249
2023-11-08 10:22:02,721 - __main__ - INFO - Epoch 125 Batch 100: Train Loss = 0.1274
2023-11-08 10:22:19,109 - __main__ - INFO - Epoch 125 Batch 120: Train Loss = 0.1450
2023-11-08 10:22:30,679 - __main__ - INFO - Epoch 126 Batch 0: Train Loss = 0.1843
2023-11-08 10:22:47,546 - __main__ - INFO - Epoch 126 Batch 20: Train Loss = 0.2271
2023-11-08 10:23:03,900 - __main__ - INFO - Epoch 126 Batch 40: Train Loss = 0.2288
2023-11-08 10:23:20,023 - __main__ - INFO - Epoch 126 Batch 60: Train Loss = 0.1992
2023-11-08 10:23:36,597 - __main__ - INFO - Epoch 126 Batch 80: Train Loss = 0.1195
2023-11-08 10:23:53,479 - __main__ - INFO - Epoch 126 Batch 100: Train Loss = 0.1047
2023-11-08 10:24:10,249 - __main__ - INFO - Epoch 126 Batch 120: Train Loss = 0.1524
2023-11-08 10:24:21,718 - __main__ - INFO - Epoch 127 Batch 0: Train Loss = 0.1825
2023-11-08 10:24:38,352 - __main__ - INFO - Epoch 127 Batch 20: Train Loss = 0.2163
2023-11-08 10:24:54,871 - __main__ - INFO - Epoch 127 Batch 40: Train Loss = 0.2178
2023-11-08 10:25:11,328 - __main__ - INFO - Epoch 127 Batch 60: Train Loss = 0.1794
2023-11-08 10:25:27,846 - __main__ - INFO - Epoch 127 Batch 80: Train Loss = 0.1250
2023-11-08 10:25:43,996 - __main__ - INFO - Epoch 127 Batch 100: Train Loss = 0.1104
2023-11-08 10:26:00,470 - __main__ - INFO - Epoch 127 Batch 120: Train Loss = 0.1751
2023-11-08 10:26:12,526 - __main__ - INFO - Epoch 128 Batch 0: Train Loss = 0.1940
2023-11-08 10:26:30,103 - __main__ - INFO - Epoch 128 Batch 20: Train Loss = 0.2371
2023-11-08 10:26:46,371 - __main__ - INFO - Epoch 128 Batch 40: Train Loss = 0.2148
2023-11-08 10:27:02,790 - __main__ - INFO - Epoch 128 Batch 60: Train Loss = 0.1982
2023-11-08 10:27:19,517 - __main__ - INFO - Epoch 128 Batch 80: Train Loss = 0.1394
2023-11-08 10:27:36,033 - __main__ - INFO - Epoch 128 Batch 100: Train Loss = 0.1170
2023-11-08 10:27:52,254 - __main__ - INFO - Epoch 128 Batch 120: Train Loss = 0.1589
2023-11-08 10:28:04,078 - __main__ - INFO - Epoch 129 Batch 0: Train Loss = 0.1821
2023-11-08 10:28:21,136 - __main__ - INFO - Epoch 129 Batch 20: Train Loss = 0.2341
2023-11-08 10:28:37,173 - __main__ - INFO - Epoch 129 Batch 40: Train Loss = 0.1894
2023-11-08 10:28:53,214 - __main__ - INFO - Epoch 129 Batch 60: Train Loss = 0.1817
2023-11-08 10:29:09,416 - __main__ - INFO - Epoch 129 Batch 80: Train Loss = 0.1241
2023-11-08 10:29:26,048 - __main__ - INFO - Epoch 129 Batch 100: Train Loss = 0.1070
2023-11-08 10:29:42,807 - __main__ - INFO - Epoch 129 Batch 120: Train Loss = 0.1537
2023-11-08 10:29:54,355 - __main__ - INFO - Epoch 130 Batch 0: Train Loss = 0.1974
2023-11-08 10:30:11,486 - __main__ - INFO - Epoch 130 Batch 20: Train Loss = 0.1984
2023-11-08 10:30:27,639 - __main__ - INFO - Epoch 130 Batch 40: Train Loss = 0.2091
2023-11-08 10:30:43,859 - __main__ - INFO - Epoch 130 Batch 60: Train Loss = 0.1990
2023-11-08 10:31:00,554 - __main__ - INFO - Epoch 130 Batch 80: Train Loss = 0.1168
2023-11-08 10:31:18,092 - __main__ - INFO - Epoch 130 Batch 100: Train Loss = 0.1132
2023-11-08 10:31:34,529 - __main__ - INFO - Epoch 130 Batch 120: Train Loss = 0.1545
2023-11-08 10:31:46,128 - __main__ - INFO - Epoch 131 Batch 0: Train Loss = 0.1810
2023-11-08 10:32:03,463 - __main__ - INFO - Epoch 131 Batch 20: Train Loss = 0.1914
2023-11-08 10:32:19,731 - __main__ - INFO - Epoch 131 Batch 40: Train Loss = 0.2015
2023-11-08 10:32:36,237 - __main__ - INFO - Epoch 131 Batch 60: Train Loss = 0.1780
2023-11-08 10:32:53,175 - __main__ - INFO - Epoch 131 Batch 80: Train Loss = 0.1220
2023-11-08 10:33:10,061 - __main__ - INFO - Epoch 131 Batch 100: Train Loss = 0.1091
2023-11-08 10:33:26,731 - __main__ - INFO - Epoch 131 Batch 120: Train Loss = 0.1481
2023-11-08 10:33:38,266 - __main__ - INFO - Epoch 132 Batch 0: Train Loss = 0.1762
2023-11-08 10:33:55,533 - __main__ - INFO - Epoch 132 Batch 20: Train Loss = 0.2161
2023-11-08 10:34:12,848 - __main__ - INFO - Epoch 132 Batch 40: Train Loss = 0.2237
2023-11-08 10:34:29,218 - __main__ - INFO - Epoch 132 Batch 60: Train Loss = 0.1804
2023-11-08 10:34:45,863 - __main__ - INFO - Epoch 132 Batch 80: Train Loss = 0.1182
2023-11-08 10:35:03,141 - __main__ - INFO - Epoch 132 Batch 100: Train Loss = 0.0971
2023-11-08 10:35:19,487 - __main__ - INFO - Epoch 132 Batch 120: Train Loss = 0.1337
2023-11-08 10:35:31,332 - __main__ - INFO - Epoch 133 Batch 0: Train Loss = 0.1873
2023-11-08 10:35:49,042 - __main__ - INFO - Epoch 133 Batch 20: Train Loss = 0.1946
2023-11-08 10:36:05,413 - __main__ - INFO - Epoch 133 Batch 40: Train Loss = 0.2275
2023-11-08 10:36:22,005 - __main__ - INFO - Epoch 133 Batch 60: Train Loss = 0.1885
2023-11-08 10:36:39,318 - __main__ - INFO - Epoch 133 Batch 80: Train Loss = 0.1288
2023-11-08 10:36:56,060 - __main__ - INFO - Epoch 133 Batch 100: Train Loss = 0.0999
2023-11-08 10:37:12,505 - __main__ - INFO - Epoch 133 Batch 120: Train Loss = 0.1336
2023-11-08 10:37:25,171 - __main__ - INFO - Epoch 134 Batch 0: Train Loss = 0.1960
2023-11-08 10:37:42,148 - __main__ - INFO - Epoch 134 Batch 20: Train Loss = 0.2060
2023-11-08 10:37:58,495 - __main__ - INFO - Epoch 134 Batch 40: Train Loss = 0.1956
2023-11-08 10:38:14,923 - __main__ - INFO - Epoch 134 Batch 60: Train Loss = 0.1834
2023-11-08 10:38:31,379 - __main__ - INFO - Epoch 134 Batch 80: Train Loss = 0.1336
2023-11-08 10:38:48,095 - __main__ - INFO - Epoch 134 Batch 100: Train Loss = 0.1202
2023-11-08 10:39:05,129 - __main__ - INFO - Epoch 134 Batch 120: Train Loss = 0.1399
2023-11-08 10:39:16,992 - __main__ - INFO - Epoch 135 Batch 0: Train Loss = 0.1952
2023-11-08 10:39:34,115 - __main__ - INFO - Epoch 135 Batch 20: Train Loss = 0.1872
2023-11-08 10:39:50,666 - __main__ - INFO - Epoch 135 Batch 40: Train Loss = 0.2179
2023-11-08 10:40:06,821 - __main__ - INFO - Epoch 135 Batch 60: Train Loss = 0.1764
2023-11-08 10:40:23,176 - __main__ - INFO - Epoch 135 Batch 80: Train Loss = 0.1277
2023-11-08 10:40:40,167 - __main__ - INFO - Epoch 135 Batch 100: Train Loss = 0.1303
2023-11-08 10:40:56,595 - __main__ - INFO - Epoch 135 Batch 120: Train Loss = 0.1487
2023-11-08 10:41:08,091 - __main__ - INFO - Epoch 136 Batch 0: Train Loss = 0.1841
2023-11-08 10:41:25,110 - __main__ - INFO - Epoch 136 Batch 20: Train Loss = 0.2095
2023-11-08 10:41:41,140 - __main__ - INFO - Epoch 136 Batch 40: Train Loss = 0.1970
2023-11-08 10:41:57,960 - __main__ - INFO - Epoch 136 Batch 60: Train Loss = 0.1838
2023-11-08 10:42:14,563 - __main__ - INFO - Epoch 136 Batch 80: Train Loss = 0.1165
2023-11-08 10:42:31,126 - __main__ - INFO - Epoch 136 Batch 100: Train Loss = 0.0983
2023-11-08 10:42:47,751 - __main__ - INFO - Epoch 136 Batch 120: Train Loss = 0.1499
2023-11-08 10:42:59,429 - __main__ - INFO - Epoch 137 Batch 0: Train Loss = 0.1914
2023-11-08 10:43:16,575 - __main__ - INFO - Epoch 137 Batch 20: Train Loss = 0.2085
2023-11-08 10:43:33,506 - __main__ - INFO - Epoch 137 Batch 40: Train Loss = 0.2214
2023-11-08 10:43:49,539 - __main__ - INFO - Epoch 137 Batch 60: Train Loss = 0.1758
2023-11-08 10:44:06,042 - __main__ - INFO - Epoch 137 Batch 80: Train Loss = 0.1431
2023-11-08 10:44:22,769 - __main__ - INFO - Epoch 137 Batch 100: Train Loss = 0.0959
2023-11-08 10:44:39,491 - __main__ - INFO - Epoch 137 Batch 120: Train Loss = 0.1266
2023-11-08 10:44:51,225 - __main__ - INFO - Epoch 138 Batch 0: Train Loss = 0.2018
2023-11-08 10:45:08,623 - __main__ - INFO - Epoch 138 Batch 20: Train Loss = 0.2008
2023-11-08 10:45:25,179 - __main__ - INFO - Epoch 138 Batch 40: Train Loss = 0.1950
2023-11-08 10:45:41,636 - __main__ - INFO - Epoch 138 Batch 60: Train Loss = 0.1789
2023-11-08 10:45:58,162 - __main__ - INFO - Epoch 138 Batch 80: Train Loss = 0.1047
2023-11-08 10:46:14,849 - __main__ - INFO - Epoch 138 Batch 100: Train Loss = 0.1000
2023-11-08 10:46:31,117 - __main__ - INFO - Epoch 138 Batch 120: Train Loss = 0.1576
2023-11-08 10:46:43,057 - __main__ - INFO - Epoch 139 Batch 0: Train Loss = 0.1868
2023-11-08 10:47:00,467 - __main__ - INFO - Epoch 139 Batch 20: Train Loss = 0.1970
2023-11-08 10:47:16,581 - __main__ - INFO - Epoch 139 Batch 40: Train Loss = 0.2017
2023-11-08 10:47:32,834 - __main__ - INFO - Epoch 139 Batch 60: Train Loss = 0.1884
2023-11-08 10:47:49,065 - __main__ - INFO - Epoch 139 Batch 80: Train Loss = 0.1240
2023-11-08 10:48:05,853 - __main__ - INFO - Epoch 139 Batch 100: Train Loss = 0.0984
2023-11-08 10:48:22,375 - __main__ - INFO - Epoch 139 Batch 120: Train Loss = 0.1399
2023-11-08 10:48:33,933 - __main__ - INFO - Epoch 140 Batch 0: Train Loss = 0.2086
2023-11-08 10:48:50,554 - __main__ - INFO - Epoch 140 Batch 20: Train Loss = 0.1947
2023-11-08 10:49:06,326 - __main__ - INFO - Epoch 140 Batch 40: Train Loss = 0.1870
2023-11-08 10:49:22,499 - __main__ - INFO - Epoch 140 Batch 60: Train Loss = 0.1979
2023-11-08 10:49:38,822 - __main__ - INFO - Epoch 140 Batch 80: Train Loss = 0.1344
2023-11-08 10:49:55,402 - __main__ - INFO - Epoch 140 Batch 100: Train Loss = 0.1296
2023-11-08 10:50:11,625 - __main__ - INFO - Epoch 140 Batch 120: Train Loss = 0.1415
2023-11-08 10:50:22,833 - __main__ - INFO - Epoch 141 Batch 0: Train Loss = 0.2002
2023-11-08 10:50:39,739 - __main__ - INFO - Epoch 141 Batch 20: Train Loss = 0.1901
2023-11-08 10:50:55,772 - __main__ - INFO - Epoch 141 Batch 40: Train Loss = 0.2161
2023-11-08 10:51:12,195 - __main__ - INFO - Epoch 141 Batch 60: Train Loss = 0.1745
2023-11-08 10:51:29,605 - __main__ - INFO - Epoch 141 Batch 80: Train Loss = 0.1351
2023-11-08 10:51:46,221 - __main__ - INFO - Epoch 141 Batch 100: Train Loss = 0.1240
2023-11-08 10:52:02,900 - __main__ - INFO - Epoch 141 Batch 120: Train Loss = 0.1461
2023-11-08 10:52:14,421 - __main__ - INFO - Epoch 142 Batch 0: Train Loss = 0.1996
2023-11-08 10:52:31,482 - __main__ - INFO - Epoch 142 Batch 20: Train Loss = 0.2139
2023-11-08 10:52:48,160 - __main__ - INFO - Epoch 142 Batch 40: Train Loss = 0.2025
2023-11-08 10:53:04,341 - __main__ - INFO - Epoch 142 Batch 60: Train Loss = 0.2003
2023-11-08 10:53:20,882 - __main__ - INFO - Epoch 142 Batch 80: Train Loss = 0.1208
2023-11-08 10:53:37,536 - __main__ - INFO - Epoch 142 Batch 100: Train Loss = 0.1081
2023-11-08 10:53:53,911 - __main__ - INFO - Epoch 142 Batch 120: Train Loss = 0.1408
2023-11-08 10:54:05,611 - __main__ - INFO - Epoch 143 Batch 0: Train Loss = 0.1738
2023-11-08 10:54:23,045 - __main__ - INFO - Epoch 143 Batch 20: Train Loss = 0.1961
2023-11-08 10:54:39,385 - __main__ - INFO - Epoch 143 Batch 40: Train Loss = 0.2197
2023-11-08 10:54:56,184 - __main__ - INFO - Epoch 143 Batch 60: Train Loss = 0.1766
2023-11-08 10:55:12,731 - __main__ - INFO - Epoch 143 Batch 80: Train Loss = 0.1458
2023-11-08 10:55:29,154 - __main__ - INFO - Epoch 143 Batch 100: Train Loss = 0.1311
2023-11-08 10:55:45,512 - __main__ - INFO - Epoch 143 Batch 120: Train Loss = 0.1368
2023-11-08 10:55:57,336 - __main__ - INFO - Epoch 144 Batch 0: Train Loss = 0.2001
2023-11-08 10:56:14,431 - __main__ - INFO - Epoch 144 Batch 20: Train Loss = 0.1974
2023-11-08 10:56:31,207 - __main__ - INFO - Epoch 144 Batch 40: Train Loss = 0.2272
2023-11-08 10:56:48,042 - __main__ - INFO - Epoch 144 Batch 60: Train Loss = 0.1946
2023-11-08 10:57:05,201 - __main__ - INFO - Epoch 144 Batch 80: Train Loss = 0.1321
2023-11-08 10:57:22,421 - __main__ - INFO - Epoch 144 Batch 100: Train Loss = 0.1193
2023-11-08 10:57:40,131 - __main__ - INFO - Epoch 144 Batch 120: Train Loss = 0.1403
2023-11-08 10:57:50,883 - __main__ - INFO - ------------ Save best model - AUROC: 0.8997 ------------
2023-11-08 10:57:51,674 - __main__ - INFO - Epoch 145 Batch 0: Train Loss = 0.1838
2023-11-08 10:58:08,723 - __main__ - INFO - Epoch 145 Batch 20: Train Loss = 0.1931
2023-11-08 10:58:24,684 - __main__ - INFO - Epoch 145 Batch 40: Train Loss = 0.2013
2023-11-08 10:58:40,947 - __main__ - INFO - Epoch 145 Batch 60: Train Loss = 0.1874
2023-11-08 10:58:57,867 - __main__ - INFO - Epoch 145 Batch 80: Train Loss = 0.1200
2023-11-08 10:59:15,254 - __main__ - INFO - Epoch 145 Batch 100: Train Loss = 0.0990
2023-11-08 10:59:31,715 - __main__ - INFO - Epoch 145 Batch 120: Train Loss = 0.1466
2023-11-08 10:59:43,097 - __main__ - INFO - Epoch 146 Batch 0: Train Loss = 0.2100
2023-11-08 10:59:59,615 - __main__ - INFO - Epoch 146 Batch 20: Train Loss = 0.2173
2023-11-08 11:00:16,365 - __main__ - INFO - Epoch 146 Batch 40: Train Loss = 0.2191
2023-11-08 11:00:33,185 - __main__ - INFO - Epoch 146 Batch 60: Train Loss = 0.1759
2023-11-08 11:00:50,036 - __main__ - INFO - Epoch 146 Batch 80: Train Loss = 0.1055
2023-11-08 11:01:06,619 - __main__ - INFO - Epoch 146 Batch 100: Train Loss = 0.0918
2023-11-08 11:01:22,982 - __main__ - INFO - Epoch 146 Batch 120: Train Loss = 0.1572
2023-11-08 11:01:34,234 - __main__ - INFO - Epoch 147 Batch 0: Train Loss = 0.1680
2023-11-08 11:01:51,042 - __main__ - INFO - Epoch 147 Batch 20: Train Loss = 0.2167
2023-11-08 11:02:07,970 - __main__ - INFO - Epoch 147 Batch 40: Train Loss = 0.2122
2023-11-08 11:02:24,428 - __main__ - INFO - Epoch 147 Batch 60: Train Loss = 0.1775
2023-11-08 11:02:40,884 - __main__ - INFO - Epoch 147 Batch 80: Train Loss = 0.1228
2023-11-08 11:02:57,891 - __main__ - INFO - Epoch 147 Batch 100: Train Loss = 0.1314
2023-11-08 11:03:14,745 - __main__ - INFO - Epoch 147 Batch 120: Train Loss = 0.1539
2023-11-08 11:03:26,691 - __main__ - INFO - Epoch 148 Batch 0: Train Loss = 0.2069
2023-11-08 11:03:44,486 - __main__ - INFO - Epoch 148 Batch 20: Train Loss = 0.2176
2023-11-08 11:04:00,928 - __main__ - INFO - Epoch 148 Batch 40: Train Loss = 0.1987
2023-11-08 11:04:17,208 - __main__ - INFO - Epoch 148 Batch 60: Train Loss = 0.2026
2023-11-08 11:04:33,731 - __main__ - INFO - Epoch 148 Batch 80: Train Loss = 0.1268
2023-11-08 11:04:50,353 - __main__ - INFO - Epoch 148 Batch 100: Train Loss = 0.1199
2023-11-08 11:05:06,466 - __main__ - INFO - Epoch 148 Batch 120: Train Loss = 0.1667
2023-11-08 11:05:17,905 - __main__ - INFO - Epoch 149 Batch 0: Train Loss = 0.1743
2023-11-08 11:05:34,760 - __main__ - INFO - Epoch 149 Batch 20: Train Loss = 0.2099
2023-11-08 11:05:51,260 - __main__ - INFO - Epoch 149 Batch 40: Train Loss = 0.2054
2023-11-08 11:06:07,824 - __main__ - INFO - Epoch 149 Batch 60: Train Loss = 0.1632
2023-11-08 11:06:24,322 - __main__ - INFO - Epoch 149 Batch 80: Train Loss = 0.1307
2023-11-08 11:06:40,869 - __main__ - INFO - Epoch 149 Batch 100: Train Loss = 0.1299
2023-11-08 11:06:56,955 - __main__ - INFO - Epoch 149 Batch 120: Train Loss = 0.1651
2023-11-08 11:07:07,501 - __main__ - INFO - auroc 0.8997
2023-11-08 11:07:07,502 - __main__ - INFO - auprc 0.6556
2023-11-08 11:07:07,503 - __main__ - INFO - minpse 0.6096
2023-11-08 11:07:07,618 - __main__ - INFO - last saved model is in epoch 144
2023-11-08 11:07:08,006 - __main__ - INFO - Batch 0: Test Loss = 0.1315
2023-11-08 11:07:14,010 - __main__ - INFO - 
==>Predicting on test
2023-11-08 11:07:14,012 - __main__ - INFO - Test Loss = 0.1424
2023-11-08 11:07:14,067 - __main__ - INFO - Transfer Target Dataset & Model
2023-11-08 11:07:16,636 - __main__ - INFO - [[0.35672856748565834, 0.362193782095267, -0.0415507190085758, -0.39134565918228126, 0.7630646387621486, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [2.891396229598206, -0.1650564631881193, -0.7197426028629784, -0.5274138023823791, 0.7630646387621486, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.7295575734197193, 0.8894440273786534, 1.5408970099849981, -1.1397204467828197, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.7295575734197193, 0.362193782095267, 1.5408970099849981, -1.1397204467828197, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.09589065789158233, 1.4166942726620397, 0.6366411648458108, -0.9356182319826729, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [2.076681623919173, 1.4166942726620397, 0.6366411648458108, -1.7520270911832603, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.08515703225931394, 0.8894440273786534, 0.18451324227620902, -0.32331158758223233, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [1.0809193280892433, 1.4166942726620397, 0.41057720356100985, 0.9013017012186487, 1.1922324705555387, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.548509883268823, 1.943944517945426, 0.18451324227620902, 1.989846846819432, 1.0849405126071912, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.548509883268823, 1.4166942726620397, -1.6239984480021659, -1.0036523035827218, -1.1681906043081072, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.7253198692496488, 1.4166942726620397, 0.41057720356100985, 0.28899505681820825, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.17568087733476206, 1.4166942726620397, 0.18451324227620902, 0.561131343218404, 2.050568134142319, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.6390337283442711, 1.4166942726620397, -1.171870525432564, -1.0036523035827218, 0.5484807228654535, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 1.4166942726620397, -2.5282542931413534, 0.8332676296185998, -0.30985494072132685, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 0.8894440273786534, -2.5282542931413534, 0.8332676296185998, -0.30985494072132685, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.18217679879696, 1.943944517945426, -0.0415507190085758, 0.3570291284182572, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.09589065789158233, 1.4166942726620397, -0.49367864157817754, 0.4250632000183061, 0.11931289107206332, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 0.8894440273786534, -0.0415507190085758, 0.8332676296185998, 0.11931289107206332, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.17568087733476206, 1.943944517945426, 0.8627051261306116, -0.7995500887825749, 0.9776485546588437, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.9106052635706156, 1.943944517945426, -1.171870525432564, 0.15292691361811034, 1.1922324705555387, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.2769383480424786, 0.8894440273786534, -0.9458065641477793, -2.092197449183505, -1.3827745202048023, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.5442721790987524, 0.8894440273786534, 0.18451324227620902, 1.5816424172191383, -1.1681906043081072, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.2769383480424786, 1.4166942726620397, -0.9458065641477793, 0.22096098521815927, 0.44118876491710596, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-3.6263206158340595, 0.362193782095267, -1.3979344867173649, -1.8880952343833581, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.6390337283442711, 1.4166942726620397, -1.171870525432564, 0.01685877041801243, -0.8463147304630646, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.9106052635706156, 1.4166942726620397, -0.26761468029337665, 0.4930972716183551, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.0916529537215118, 0.8894440273786534, 0.41057720356100985, 1.5136083456190894, 0.22660484902041086, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-2.3589867847777857, 1.4166942726620397, 0.41057720356100985, 1.3775402024189913, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185]]
2023-11-08 11:07:16,642 - __main__ - INFO - 99
2023-11-08 11:07:16,643 - __main__ - INFO - 4255
2023-11-08 11:07:16,983 - __main__ - INFO - {'los_mean': 5.363315937659429, 'los_std': 4.099244607743503, 'los_median': 4.333333333333333, 'large_los': 21.291666666666668, 'threshold': 3.7605509886094994}
2023-11-08 11:07:22,309 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 2.4994
2023-11-08 11:07:56,980 - __main__ - INFO - Fold 1 Epoch 0 Batch 50: Train Loss = 1.7867
2023-11-08 11:08:32,364 - __main__ - INFO - Fold 1 Epoch 0 Batch 100: Train Loss = 2.0775
2023-11-08 11:09:06,840 - __main__ - INFO - Fold 1 Epoch 0 Batch 150: Train Loss = 2.6137
2023-11-08 11:09:41,822 - __main__ - INFO - Fold 1 Epoch 0 Batch 200: Train Loss = 2.3083
2023-11-08 11:10:16,305 - __main__ - INFO - Fold 1 Epoch 0 Batch 250: Train Loss = 2.5347
2023-11-08 11:10:50,488 - __main__ - INFO - Fold 1 Epoch 0 Batch 300: Train Loss = 2.3444
2023-11-08 11:11:55,999 - __main__ - INFO - Fold 1, epoch 0: Loss = 2.2011 Valid loss = 2.2907 MSE = 33.0978 AUROC = 0.7749
2023-11-08 11:11:56,001 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.0978 ------------
2023-11-08 11:11:56,563 - __main__ - INFO - ------------ Save best model - MSE: 33.0978 ------------
2023-11-08 11:11:56,566 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.7749 ------------
2023-11-08 11:11:56,569 - __main__ - INFO - Fold 1, mse = 33.0978, mad = 3.9577
2023-11-08 11:11:57,189 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 2.3179
2023-11-08 11:12:31,109 - __main__ - INFO - Fold 1 Epoch 1 Batch 50: Train Loss = 2.1523
2023-11-08 11:13:05,763 - __main__ - INFO - Fold 1 Epoch 1 Batch 100: Train Loss = 1.4427
2023-11-08 11:13:40,197 - __main__ - INFO - Fold 1 Epoch 1 Batch 150: Train Loss = 1.9704
2023-11-08 11:14:15,341 - __main__ - INFO - Fold 1 Epoch 1 Batch 200: Train Loss = 2.2031
2023-11-08 11:14:49,226 - __main__ - INFO - Fold 1 Epoch 1 Batch 250: Train Loss = 2.0716
2023-11-08 11:15:24,026 - __main__ - INFO - Fold 1 Epoch 1 Batch 300: Train Loss = 2.3051
2023-11-08 11:16:29,884 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.1146 ------------
2023-11-08 11:16:30,472 - __main__ - INFO - ------------ Save best model - MSE: 32.1146 ------------
2023-11-08 11:16:30,474 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8218 ------------
2023-11-08 11:16:30,475 - __main__ - INFO - Fold 1, mse = 32.1146, mad = 3.9179
2023-11-08 11:16:31,208 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 1.8110
2023-11-08 11:17:06,204 - __main__ - INFO - Fold 1 Epoch 2 Batch 50: Train Loss = 2.1269
2023-11-08 11:17:41,353 - __main__ - INFO - Fold 1 Epoch 2 Batch 100: Train Loss = 1.6830
2023-11-08 11:18:15,485 - __main__ - INFO - Fold 1 Epoch 2 Batch 150: Train Loss = 2.0893
2023-11-08 11:18:50,394 - __main__ - INFO - Fold 1 Epoch 2 Batch 200: Train Loss = 2.3293
2023-11-08 11:19:24,947 - __main__ - INFO - Fold 1 Epoch 2 Batch 250: Train Loss = 1.8631
2023-11-08 11:19:58,798 - __main__ - INFO - Fold 1 Epoch 2 Batch 300: Train Loss = 2.7827
2023-11-08 11:21:03,758 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.7624 ------------
2023-11-08 11:21:04,314 - __main__ - INFO - ------------ Save best model - MSE: 31.7624 ------------
2023-11-08 11:21:04,316 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8438 ------------
2023-11-08 11:21:04,317 - __main__ - INFO - Fold 1, mse = 31.7624, mad = 3.8695
2023-11-08 11:21:04,974 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 2.2574
2023-11-08 11:21:39,166 - __main__ - INFO - Fold 1 Epoch 3 Batch 50: Train Loss = 1.5706
2023-11-08 11:22:13,817 - __main__ - INFO - Fold 1 Epoch 3 Batch 100: Train Loss = 2.3306
2023-11-08 11:22:46,961 - __main__ - INFO - Fold 1 Epoch 3 Batch 150: Train Loss = 2.2091
2023-11-08 11:23:21,110 - __main__ - INFO - Fold 1 Epoch 3 Batch 200: Train Loss = 1.7009
2023-11-08 11:23:55,569 - __main__ - INFO - Fold 1 Epoch 3 Batch 250: Train Loss = 1.5836
2023-11-08 11:24:29,484 - __main__ - INFO - Fold 1 Epoch 3 Batch 300: Train Loss = 1.4682
2023-11-08 11:25:33,208 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.5261 ------------
2023-11-08 11:25:33,811 - __main__ - INFO - ------------ Save best model - MSE: 31.5261 ------------
2023-11-08 11:25:33,813 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8590 ------------
2023-11-08 11:25:33,814 - __main__ - INFO - Fold 1, mse = 31.5261, mad = 3.9201
2023-11-08 11:25:34,443 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 1.9634
2023-11-08 11:26:08,150 - __main__ - INFO - Fold 1 Epoch 4 Batch 50: Train Loss = 2.1875
2023-11-08 11:26:42,596 - __main__ - INFO - Fold 1 Epoch 4 Batch 100: Train Loss = 1.6710
2023-11-08 11:27:20,839 - __main__ - INFO - Fold 1 Epoch 4 Batch 150: Train Loss = 1.5822
2023-11-08 11:27:59,250 - __main__ - INFO - Fold 1 Epoch 4 Batch 200: Train Loss = 2.2609
2023-11-08 11:28:37,271 - __main__ - INFO - Fold 1 Epoch 4 Batch 250: Train Loss = 1.9534
2023-11-08 11:29:13,927 - __main__ - INFO - Fold 1 Epoch 4 Batch 300: Train Loss = 2.1213
2023-11-08 11:30:25,673 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8670 ------------
2023-11-08 11:30:25,675 - __main__ - INFO - Fold 1, mse = 31.7346, mad = 3.8616
2023-11-08 11:30:26,343 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 1.6061
2023-11-08 11:31:02,970 - __main__ - INFO - Fold 1 Epoch 5 Batch 50: Train Loss = 1.8520
2023-11-08 11:31:41,852 - __main__ - INFO - Fold 1 Epoch 5 Batch 100: Train Loss = 1.5765
2023-11-08 11:32:20,741 - __main__ - INFO - Fold 1 Epoch 5 Batch 150: Train Loss = 1.6978
2023-11-08 11:32:59,234 - __main__ - INFO - Fold 1 Epoch 5 Batch 200: Train Loss = 1.7015
2023-11-08 11:33:35,088 - __main__ - INFO - Fold 1 Epoch 5 Batch 250: Train Loss = 1.8952
2023-11-08 11:34:13,639 - __main__ - INFO - Fold 1 Epoch 5 Batch 300: Train Loss = 1.8628
2023-11-08 11:35:18,688 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8783 ------------
2023-11-08 11:35:18,690 - __main__ - INFO - Fold 1, mse = 31.5455, mad = 3.8868
2023-11-08 11:35:19,407 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 1.7896
2023-11-08 11:35:54,649 - __main__ - INFO - Fold 1 Epoch 6 Batch 50: Train Loss = 1.8709
2023-11-08 11:36:29,402 - __main__ - INFO - Fold 1 Epoch 6 Batch 100: Train Loss = 2.6143
2023-11-08 11:37:04,645 - __main__ - INFO - Fold 1 Epoch 6 Batch 150: Train Loss = 1.5336
2023-11-08 11:37:38,509 - __main__ - INFO - Fold 1 Epoch 6 Batch 200: Train Loss = 1.9147
2023-11-08 11:38:12,680 - __main__ - INFO - Fold 1 Epoch 6 Batch 250: Train Loss = 1.8296
2023-11-08 11:38:47,582 - __main__ - INFO - Fold 1 Epoch 6 Batch 300: Train Loss = 1.8445
2023-11-08 11:39:53,325 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8800 ------------
2023-11-08 11:39:53,328 - __main__ - INFO - Fold 1, mse = 31.5608, mad = 3.8928
2023-11-08 11:39:54,081 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 1.4327
2023-11-08 11:40:31,120 - __main__ - INFO - Fold 1 Epoch 7 Batch 50: Train Loss = 2.3661
2023-11-08 11:41:10,171 - __main__ - INFO - Fold 1 Epoch 7 Batch 100: Train Loss = 1.7622
2023-11-08 11:41:51,509 - __main__ - INFO - Fold 1 Epoch 7 Batch 150: Train Loss = 1.3407
2023-11-08 11:42:32,285 - __main__ - INFO - Fold 1 Epoch 7 Batch 200: Train Loss = 2.5620
2023-11-08 11:43:11,902 - __main__ - INFO - Fold 1 Epoch 7 Batch 250: Train Loss = 1.2854
2023-11-08 11:43:52,614 - __main__ - INFO - Fold 1 Epoch 7 Batch 300: Train Loss = 1.6352
2023-11-08 11:45:06,764 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8827 ------------
2023-11-08 11:45:06,767 - __main__ - INFO - Fold 1, mse = 31.7501, mad = 3.8666
2023-11-08 11:45:07,608 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 1.7648
2023-11-08 11:45:51,798 - __main__ - INFO - Fold 1 Epoch 8 Batch 50: Train Loss = 2.5366
2023-11-08 11:46:39,996 - __main__ - INFO - Fold 1 Epoch 8 Batch 100: Train Loss = 1.7453
2023-11-08 11:47:24,669 - __main__ - INFO - Fold 1 Epoch 8 Batch 150: Train Loss = 1.8351
2023-11-08 11:48:08,500 - __main__ - INFO - Fold 1 Epoch 8 Batch 200: Train Loss = 1.7193
2023-11-08 11:48:53,371 - __main__ - INFO - Fold 1 Epoch 8 Batch 250: Train Loss = 1.7008
2023-11-08 11:49:33,883 - __main__ - INFO - Fold 1 Epoch 8 Batch 300: Train Loss = 1.7581
2023-11-08 11:50:51,358 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8886 ------------
2023-11-08 11:50:51,361 - __main__ - INFO - Fold 1, mse = 31.5872, mad = 3.8856
2023-11-08 11:50:51,996 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 1.7091
2023-11-08 11:51:32,218 - __main__ - INFO - Fold 1 Epoch 9 Batch 50: Train Loss = 1.5426
2023-11-08 11:52:10,124 - __main__ - INFO - Fold 1 Epoch 9 Batch 100: Train Loss = 1.6263
2023-11-08 11:52:49,656 - __main__ - INFO - Fold 1 Epoch 9 Batch 150: Train Loss = 1.6921
2023-11-08 11:53:28,772 - __main__ - INFO - Fold 1 Epoch 9 Batch 200: Train Loss = 1.7562
2023-11-08 11:54:09,732 - __main__ - INFO - Fold 1 Epoch 9 Batch 250: Train Loss = 1.7935
2023-11-08 11:54:54,044 - __main__ - INFO - Fold 1 Epoch 9 Batch 300: Train Loss = 1.9699
2023-11-08 11:56:09,528 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8894 ------------
2023-11-08 11:56:09,530 - __main__ - INFO - Fold 1, mse = 31.7389, mad = 3.8470
2023-11-08 11:56:10,336 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 1.8088
2023-11-08 11:56:49,686 - __main__ - INFO - Fold 1 Epoch 10 Batch 50: Train Loss = 1.5972
2023-11-08 11:57:30,254 - __main__ - INFO - Fold 1 Epoch 10 Batch 100: Train Loss = 1.8729
2023-11-08 11:58:11,223 - __main__ - INFO - Fold 1 Epoch 10 Batch 150: Train Loss = 1.7814
2023-11-08 11:58:51,340 - __main__ - INFO - Fold 1 Epoch 10 Batch 200: Train Loss = 1.4089
2023-11-08 11:59:31,102 - __main__ - INFO - Fold 1 Epoch 10 Batch 250: Train Loss = 1.3265
2023-11-08 12:00:11,201 - __main__ - INFO - Fold 1 Epoch 10 Batch 300: Train Loss = 1.3290
2023-11-08 12:01:24,262 - __main__ - INFO - Fold 1, epoch 10: Loss = 1.7854 Valid loss = 2.1453 MSE = 31.6044 AUROC = 0.8821
2023-11-08 12:01:24,265 - __main__ - INFO - Fold 1, mse = 31.6044, mad = 3.9478
2023-11-08 12:01:24,975 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 2.0115
2023-11-08 12:02:05,352 - __main__ - INFO - Fold 1 Epoch 11 Batch 50: Train Loss = 1.8695
2023-11-08 12:02:45,710 - __main__ - INFO - Fold 1 Epoch 11 Batch 100: Train Loss = 2.3299
2023-11-08 12:03:26,966 - __main__ - INFO - Fold 1 Epoch 11 Batch 150: Train Loss = 1.6827
2023-11-08 12:04:06,579 - __main__ - INFO - Fold 1 Epoch 11 Batch 200: Train Loss = 2.0172
2023-11-08 12:04:52,668 - __main__ - INFO - Fold 1 Epoch 11 Batch 250: Train Loss = 1.4159
2023-11-08 12:05:34,465 - __main__ - INFO - Fold 1 Epoch 11 Batch 300: Train Loss = 1.4916
2023-11-08 12:06:49,089 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.5183 ------------
2023-11-08 12:06:49,887 - __main__ - INFO - ------------ Save best model - MSE: 31.5183 ------------
2023-11-08 12:06:49,889 - __main__ - INFO - Fold 1, mse = 31.5183, mad = 3.9472
2023-11-08 12:06:50,490 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 1.9316
2023-11-08 12:07:31,091 - __main__ - INFO - Fold 1 Epoch 12 Batch 50: Train Loss = 1.6509
2023-11-08 12:08:11,720 - __main__ - INFO - Fold 1 Epoch 12 Batch 100: Train Loss = 1.8087
2023-11-08 12:08:51,883 - __main__ - INFO - Fold 1 Epoch 12 Batch 150: Train Loss = 2.4992
2023-11-08 12:09:32,638 - __main__ - INFO - Fold 1 Epoch 12 Batch 200: Train Loss = 1.5140
2023-11-08 12:10:16,153 - __main__ - INFO - Fold 1 Epoch 12 Batch 250: Train Loss = 1.4891
2023-11-08 12:11:04,088 - __main__ - INFO - Fold 1 Epoch 12 Batch 300: Train Loss = 1.8491
2023-11-08 12:12:25,277 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.4844 ------------
2023-11-08 12:12:26,266 - __main__ - INFO - ------------ Save best model - MSE: 31.4844 ------------
2023-11-08 12:12:26,269 - __main__ - INFO - Fold 1, mse = 31.4844, mad = 3.8607
2023-11-08 12:12:27,274 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 1.9375
2023-11-08 12:13:10,645 - __main__ - INFO - Fold 1 Epoch 13 Batch 50: Train Loss = 1.6736
2023-11-08 12:13:55,250 - __main__ - INFO - Fold 1 Epoch 13 Batch 100: Train Loss = 2.0853
2023-11-08 12:14:40,282 - __main__ - INFO - Fold 1 Epoch 13 Batch 150: Train Loss = 1.8808
2023-11-08 12:15:23,265 - __main__ - INFO - Fold 1 Epoch 13 Batch 200: Train Loss = 2.1941
2023-11-08 12:16:06,508 - __main__ - INFO - Fold 1 Epoch 13 Batch 250: Train Loss = 1.6554
2023-11-08 12:16:49,497 - __main__ - INFO - Fold 1 Epoch 13 Batch 300: Train Loss = 1.3339
2023-11-08 12:18:10,146 - __main__ - INFO - Fold 1, mse = 31.4871, mad = 3.9147
2023-11-08 12:18:10,968 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 1.5756
2023-11-08 12:18:54,269 - __main__ - INFO - Fold 1 Epoch 14 Batch 50: Train Loss = 1.2991
2023-11-08 12:19:38,934 - __main__ - INFO - Fold 1 Epoch 14 Batch 100: Train Loss = 1.4554
2023-11-08 12:20:23,197 - __main__ - INFO - Fold 1 Epoch 14 Batch 150: Train Loss = 1.5779
2023-11-08 12:21:07,145 - __main__ - INFO - Fold 1 Epoch 14 Batch 200: Train Loss = 1.5910
2023-11-08 12:21:52,388 - __main__ - INFO - Fold 1 Epoch 14 Batch 250: Train Loss = 1.5805
2023-11-08 12:22:35,776 - __main__ - INFO - Fold 1 Epoch 14 Batch 300: Train Loss = 1.9882
2023-11-08 12:23:57,300 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.3238 ------------
2023-11-08 12:23:58,114 - __main__ - INFO - ------------ Save best model - MSE: 31.3238 ------------
2023-11-08 12:23:58,115 - __main__ - INFO - Fold 1, mse = 31.3238, mad = 3.8786
2023-11-08 12:23:59,078 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 1.5376
2023-11-08 12:24:43,318 - __main__ - INFO - Fold 1 Epoch 15 Batch 50: Train Loss = 1.6494
2023-11-08 12:25:26,794 - __main__ - INFO - Fold 1 Epoch 15 Batch 100: Train Loss = 1.6142
2023-11-08 12:26:10,685 - __main__ - INFO - Fold 1 Epoch 15 Batch 150: Train Loss = 1.6325
2023-11-08 12:26:54,388 - __main__ - INFO - Fold 1 Epoch 15 Batch 200: Train Loss = 2.4654
2023-11-08 12:27:38,047 - __main__ - INFO - Fold 1 Epoch 15 Batch 250: Train Loss = 2.0401
2023-11-08 12:28:20,616 - __main__ - INFO - Fold 1 Epoch 15 Batch 300: Train Loss = 2.2335
2023-11-08 12:29:40,068 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.2421 ------------
2023-11-08 12:29:40,936 - __main__ - INFO - ------------ Save best model - MSE: 31.2421 ------------
2023-11-08 12:29:40,938 - __main__ - INFO - Fold 1, mse = 31.2421, mad = 3.8303
2023-11-08 12:29:41,977 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 2.1666
2023-11-08 12:30:24,906 - __main__ - INFO - Fold 1 Epoch 16 Batch 50: Train Loss = 2.0499
2023-11-08 12:31:08,060 - __main__ - INFO - Fold 1 Epoch 16 Batch 100: Train Loss = 1.8731
2023-11-08 12:31:52,131 - __main__ - INFO - Fold 1 Epoch 16 Batch 150: Train Loss = 1.6348
2023-11-08 12:32:35,143 - __main__ - INFO - Fold 1 Epoch 16 Batch 200: Train Loss = 1.6613
2023-11-08 12:33:18,707 - __main__ - INFO - Fold 1 Epoch 16 Batch 250: Train Loss = 1.9473
2023-11-08 12:34:02,281 - __main__ - INFO - Fold 1 Epoch 16 Batch 300: Train Loss = 1.4925
2023-11-08 12:35:22,670 - __main__ - INFO - Fold 1, mse = 31.4370, mad = 3.9008
2023-11-08 12:35:23,613 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 1.4595
2023-11-08 12:36:07,563 - __main__ - INFO - Fold 1 Epoch 17 Batch 50: Train Loss = 2.2000
2023-11-08 12:36:51,620 - __main__ - INFO - Fold 1 Epoch 17 Batch 100: Train Loss = 2.7002
2023-11-08 12:37:34,545 - __main__ - INFO - Fold 1 Epoch 17 Batch 150: Train Loss = 1.9544
2023-11-08 12:38:18,027 - __main__ - INFO - Fold 1 Epoch 17 Batch 200: Train Loss = 1.5596
2023-11-08 12:39:01,330 - __main__ - INFO - Fold 1 Epoch 17 Batch 250: Train Loss = 1.8071
2023-11-08 12:39:46,102 - __main__ - INFO - Fold 1 Epoch 17 Batch 300: Train Loss = 1.9761
2023-11-08 12:41:05,672 - __main__ - INFO - Fold 1, mse = 31.4986, mad = 3.9030
2023-11-08 12:41:06,545 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 1.7324
2023-11-08 12:41:49,967 - __main__ - INFO - Fold 1 Epoch 18 Batch 50: Train Loss = 1.6566
2023-11-08 12:42:33,798 - __main__ - INFO - Fold 1 Epoch 18 Batch 100: Train Loss = 1.5332
2023-11-08 12:43:17,197 - __main__ - INFO - Fold 1 Epoch 18 Batch 150: Train Loss = 1.8303
2023-11-08 12:44:00,429 - __main__ - INFO - Fold 1 Epoch 18 Batch 200: Train Loss = 2.2674
2023-11-08 12:44:43,651 - __main__ - INFO - Fold 1 Epoch 18 Batch 250: Train Loss = 1.3732
2023-11-08 12:45:25,962 - __main__ - INFO - Fold 1 Epoch 18 Batch 300: Train Loss = 1.6405
2023-11-08 12:46:45,403 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8927 ------------
2023-11-08 12:46:45,413 - __main__ - INFO - Fold 1, mse = 31.4217, mad = 3.9028
2023-11-08 12:46:46,331 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 1.4461
2023-11-08 12:47:29,002 - __main__ - INFO - Fold 1 Epoch 19 Batch 50: Train Loss = 1.1679
2023-11-08 12:48:12,675 - __main__ - INFO - Fold 1 Epoch 19 Batch 100: Train Loss = 1.9532
2023-11-08 12:48:56,952 - __main__ - INFO - Fold 1 Epoch 19 Batch 150: Train Loss = 1.8807
2023-11-08 12:49:39,560 - __main__ - INFO - Fold 1 Epoch 19 Batch 200: Train Loss = 2.3613
2023-11-08 12:50:22,198 - __main__ - INFO - Fold 1 Epoch 19 Batch 250: Train Loss = 1.4778
2023-11-08 12:51:05,423 - __main__ - INFO - Fold 1 Epoch 19 Batch 300: Train Loss = 1.5506
2023-11-08 12:52:24,662 - __main__ - INFO - Fold 1, mse = 31.6180, mad = 3.8938
2023-11-08 12:52:26,248 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 3.7121
2023-11-08 12:53:09,074 - __main__ - INFO - Fold 2 Epoch 0 Batch 50: Train Loss = 2.1251
2023-11-08 12:53:50,786 - __main__ - INFO - Fold 2 Epoch 0 Batch 100: Train Loss = 2.4100
2023-11-08 12:54:34,126 - __main__ - INFO - Fold 2 Epoch 0 Batch 150: Train Loss = 2.2670
2023-11-08 12:55:17,254 - __main__ - INFO - Fold 2 Epoch 0 Batch 200: Train Loss = 2.7184
2023-11-08 12:56:01,030 - __main__ - INFO - Fold 2 Epoch 0 Batch 250: Train Loss = 1.7988
2023-11-08 12:56:44,713 - __main__ - INFO - Fold 2 Epoch 0 Batch 300: Train Loss = 2.0336
2023-11-08 12:58:02,857 - __main__ - INFO - Fold 2, epoch 0: Loss = 2.3129 Valid loss = 2.0289 MSE = 28.8733 AUROC = 0.8002
2023-11-08 12:58:02,860 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 28.8733 ------------
2023-11-08 12:58:03,930 - __main__ - INFO - ------------ Save best model - MSE: 28.8733 ------------
2023-11-08 12:58:03,932 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8002 ------------
2023-11-08 12:58:03,935 - __main__ - INFO - Fold 2, mse = 28.8733, mad = 3.8772
2023-11-08 12:58:04,851 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 1.7929
2023-11-08 12:58:48,841 - __main__ - INFO - Fold 2 Epoch 1 Batch 50: Train Loss = 2.1808
2023-11-08 12:59:31,409 - __main__ - INFO - Fold 2 Epoch 1 Batch 100: Train Loss = 2.0569
2023-11-08 13:00:14,730 - __main__ - INFO - Fold 2 Epoch 1 Batch 150: Train Loss = 1.8671
2023-11-08 13:00:57,885 - __main__ - INFO - Fold 2 Epoch 1 Batch 200: Train Loss = 2.2742
2023-11-08 13:01:42,050 - __main__ - INFO - Fold 2 Epoch 1 Batch 250: Train Loss = 1.7029
2023-11-08 13:02:24,821 - __main__ - INFO - Fold 2 Epoch 1 Batch 300: Train Loss = 2.0077
2023-11-08 13:03:36,928 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 28.3253 ------------
2023-11-08 13:03:37,643 - __main__ - INFO - ------------ Save best model - MSE: 28.3253 ------------
2023-11-08 13:03:37,644 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8392 ------------
2023-11-08 13:03:37,646 - __main__ - INFO - Fold 2, mse = 28.3253, mad = 3.9066
2023-11-08 13:03:38,551 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 1.5920
2023-11-08 13:04:18,193 - __main__ - INFO - Fold 2 Epoch 2 Batch 50: Train Loss = 1.9452
2023-11-08 13:04:57,005 - __main__ - INFO - Fold 2 Epoch 2 Batch 100: Train Loss = 2.4664
2023-11-08 13:05:37,063 - __main__ - INFO - Fold 2 Epoch 2 Batch 150: Train Loss = 2.4206
2023-11-08 13:06:18,040 - __main__ - INFO - Fold 2 Epoch 2 Batch 200: Train Loss = 1.8509
2023-11-08 13:06:58,321 - __main__ - INFO - Fold 2 Epoch 2 Batch 250: Train Loss = 1.6190
2023-11-08 13:07:37,810 - __main__ - INFO - Fold 2 Epoch 2 Batch 300: Train Loss = 1.4073
2023-11-08 13:08:49,784 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 28.2498 ------------
2023-11-08 13:08:50,517 - __main__ - INFO - ------------ Save best model - MSE: 28.2498 ------------
2023-11-08 13:08:50,520 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8509 ------------
2023-11-08 13:08:50,522 - __main__ - INFO - Fold 2, mse = 28.2498, mad = 3.8484
2023-11-08 13:08:51,459 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 1.7306
2023-11-08 13:09:29,603 - __main__ - INFO - Fold 2 Epoch 3 Batch 50: Train Loss = 1.8365
2023-11-08 13:10:08,251 - __main__ - INFO - Fold 2 Epoch 3 Batch 100: Train Loss = 1.6833
2023-11-08 13:10:47,681 - __main__ - INFO - Fold 2 Epoch 3 Batch 150: Train Loss = 1.8227
2023-11-08 13:11:27,099 - __main__ - INFO - Fold 2 Epoch 3 Batch 200: Train Loss = 1.8633
2023-11-08 13:12:08,816 - __main__ - INFO - Fold 2 Epoch 3 Batch 250: Train Loss = 2.3635
2023-11-08 13:12:47,312 - __main__ - INFO - Fold 2 Epoch 3 Batch 300: Train Loss = 1.4934
2023-11-08 13:14:00,643 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.9510 ------------
2023-11-08 13:14:01,388 - __main__ - INFO - ------------ Save best model - MSE: 27.9510 ------------
2023-11-08 13:14:01,391 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8533 ------------
2023-11-08 13:14:01,394 - __main__ - INFO - Fold 2, mse = 27.9510, mad = 3.8355
2023-11-08 13:14:02,013 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 2.0231
2023-11-08 13:14:40,902 - __main__ - INFO - Fold 2 Epoch 4 Batch 50: Train Loss = 1.7503
2023-11-08 13:15:21,799 - __main__ - INFO - Fold 2 Epoch 4 Batch 100: Train Loss = 2.4259
2023-11-08 13:16:00,067 - __main__ - INFO - Fold 2 Epoch 4 Batch 150: Train Loss = 2.1325
2023-11-08 13:16:39,643 - __main__ - INFO - Fold 2 Epoch 4 Batch 200: Train Loss = 2.0097
2023-11-08 13:17:18,154 - __main__ - INFO - Fold 2 Epoch 4 Batch 250: Train Loss = 1.4494
2023-11-08 13:17:57,816 - __main__ - INFO - Fold 2 Epoch 4 Batch 300: Train Loss = 1.7621
2023-11-08 13:19:09,669 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8596 ------------
2023-11-08 13:19:09,671 - __main__ - INFO - Fold 2, mse = 28.1370, mad = 3.8918
2023-11-08 13:19:10,356 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 2.5917
2023-11-08 13:19:49,943 - __main__ - INFO - Fold 2 Epoch 5 Batch 50: Train Loss = 1.8150
2023-11-08 13:20:30,575 - __main__ - INFO - Fold 2 Epoch 5 Batch 100: Train Loss = 2.0259
2023-11-08 13:21:11,345 - __main__ - INFO - Fold 2 Epoch 5 Batch 150: Train Loss = 2.0096
2023-11-08 13:21:50,077 - __main__ - INFO - Fold 2 Epoch 5 Batch 200: Train Loss = 1.8122
2023-11-08 13:22:28,375 - __main__ - INFO - Fold 2 Epoch 5 Batch 250: Train Loss = 1.3765
2023-11-08 13:23:06,980 - __main__ - INFO - Fold 2 Epoch 5 Batch 300: Train Loss = 2.9487
2023-11-08 13:24:18,948 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8622 ------------
2023-11-08 13:24:18,950 - __main__ - INFO - Fold 2, mse = 28.0030, mad = 3.8333
2023-11-08 13:24:19,918 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 1.6129
2023-11-08 13:24:59,078 - __main__ - INFO - Fold 2 Epoch 6 Batch 50: Train Loss = 1.7288
2023-11-08 13:25:38,566 - __main__ - INFO - Fold 2 Epoch 6 Batch 100: Train Loss = 1.6288
2023-11-08 13:26:17,470 - __main__ - INFO - Fold 2 Epoch 6 Batch 150: Train Loss = 1.8721
2023-11-08 13:26:57,781 - __main__ - INFO - Fold 2 Epoch 6 Batch 200: Train Loss = 1.7774
2023-11-08 13:27:36,557 - __main__ - INFO - Fold 2 Epoch 6 Batch 250: Train Loss = 1.6668
2023-11-08 13:28:16,063 - __main__ - INFO - Fold 2 Epoch 6 Batch 300: Train Loss = 1.6218
2023-11-08 13:29:27,189 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8674 ------------
2023-11-08 13:29:27,191 - __main__ - INFO - Fold 2, mse = 28.4641, mad = 3.7725
2023-11-08 13:29:27,951 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 3.6094
2023-11-08 13:30:07,275 - __main__ - INFO - Fold 2 Epoch 7 Batch 50: Train Loss = 2.3449
2023-11-08 13:30:46,909 - __main__ - INFO - Fold 2 Epoch 7 Batch 100: Train Loss = 1.9641
2023-11-08 13:31:25,444 - __main__ - INFO - Fold 2 Epoch 7 Batch 150: Train Loss = 1.7686
2023-11-08 13:32:03,525 - __main__ - INFO - Fold 2 Epoch 7 Batch 200: Train Loss = 2.2641
2023-11-08 13:32:42,841 - __main__ - INFO - Fold 2 Epoch 7 Batch 250: Train Loss = 1.6139
2023-11-08 13:33:21,237 - __main__ - INFO - Fold 2 Epoch 7 Batch 300: Train Loss = 3.1466
2023-11-08 13:34:33,634 - __main__ - INFO - Fold 2, mse = 28.1774, mad = 3.8340
2023-11-08 13:34:34,386 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 1.6918
2023-11-08 13:35:12,364 - __main__ - INFO - Fold 2 Epoch 8 Batch 50: Train Loss = 1.4899
2023-11-08 13:35:52,537 - __main__ - INFO - Fold 2 Epoch 8 Batch 100: Train Loss = 1.6243
2023-11-08 13:36:32,151 - __main__ - INFO - Fold 2 Epoch 8 Batch 150: Train Loss = 2.3788
2023-11-08 13:37:10,772 - __main__ - INFO - Fold 2 Epoch 8 Batch 200: Train Loss = 1.6457
2023-11-08 13:37:50,065 - __main__ - INFO - Fold 2 Epoch 8 Batch 250: Train Loss = 1.7321
2023-11-08 13:38:29,610 - __main__ - INFO - Fold 2 Epoch 8 Batch 300: Train Loss = 2.5149
2023-11-08 13:39:42,016 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8707 ------------
2023-11-08 13:39:42,018 - __main__ - INFO - Fold 2, mse = 28.3697, mad = 3.7910
2023-11-08 13:39:42,698 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 2.0031
2023-11-08 13:40:21,813 - __main__ - INFO - Fold 2 Epoch 9 Batch 50: Train Loss = 1.4019
2023-11-08 13:41:01,614 - __main__ - INFO - Fold 2 Epoch 9 Batch 100: Train Loss = 1.5085
2023-11-08 13:41:40,274 - __main__ - INFO - Fold 2 Epoch 9 Batch 150: Train Loss = 1.7309
2023-11-08 13:42:20,041 - __main__ - INFO - Fold 2 Epoch 9 Batch 200: Train Loss = 1.4912
2023-11-08 13:42:59,517 - __main__ - INFO - Fold 2 Epoch 9 Batch 250: Train Loss = 2.1218
2023-11-08 13:43:39,229 - __main__ - INFO - Fold 2 Epoch 9 Batch 300: Train Loss = 1.5713
2023-11-08 13:44:49,779 - __main__ - INFO - Fold 2, mse = 28.3240, mad = 3.7870
2023-11-08 13:44:50,607 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 1.2679
2023-11-08 13:45:30,112 - __main__ - INFO - Fold 2 Epoch 10 Batch 50: Train Loss = 1.7264
2023-11-08 13:46:08,806 - __main__ - INFO - Fold 2 Epoch 10 Batch 100: Train Loss = 3.1994
2023-11-08 13:46:47,940 - __main__ - INFO - Fold 2 Epoch 10 Batch 150: Train Loss = 2.3162
2023-11-08 13:47:26,712 - __main__ - INFO - Fold 2 Epoch 10 Batch 200: Train Loss = 1.5007
2023-11-08 13:48:06,842 - __main__ - INFO - Fold 2 Epoch 10 Batch 250: Train Loss = 2.3573
2023-11-08 13:48:45,563 - __main__ - INFO - Fold 2 Epoch 10 Batch 300: Train Loss = 1.3970
2023-11-08 13:49:57,943 - __main__ - INFO - Fold 2, epoch 10: Loss = 1.8646 Valid loss = 1.9450 MSE = 28.3450 AUROC = 0.8698
2023-11-08 13:49:57,946 - __main__ - INFO - Fold 2, mse = 28.3450, mad = 3.8209
2023-11-08 13:49:58,734 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 1.8257
2023-11-08 13:50:37,867 - __main__ - INFO - Fold 2 Epoch 11 Batch 50: Train Loss = 1.6531
2023-11-08 13:51:17,837 - __main__ - INFO - Fold 2 Epoch 11 Batch 100: Train Loss = 1.8218
2023-11-08 13:51:55,977 - __main__ - INFO - Fold 2 Epoch 11 Batch 150: Train Loss = 1.7750
2023-11-08 13:52:35,641 - __main__ - INFO - Fold 2 Epoch 11 Batch 200: Train Loss = 1.9510
2023-11-08 13:53:15,649 - __main__ - INFO - Fold 2 Epoch 11 Batch 250: Train Loss = 1.7888
2023-11-08 13:53:54,015 - __main__ - INFO - Fold 2 Epoch 11 Batch 300: Train Loss = 1.6593
2023-11-08 13:55:05,304 - __main__ - INFO - Fold 2, mse = 28.5157, mad = 3.7977
2023-11-08 13:55:06,169 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 1.6357
2023-11-08 13:55:45,621 - __main__ - INFO - Fold 2 Epoch 12 Batch 50: Train Loss = 1.4374
2023-11-08 13:56:24,622 - __main__ - INFO - Fold 2 Epoch 12 Batch 100: Train Loss = 1.2007
2023-11-08 13:57:04,117 - __main__ - INFO - Fold 2 Epoch 12 Batch 150: Train Loss = 1.8731
2023-11-08 13:57:42,904 - __main__ - INFO - Fold 2 Epoch 12 Batch 200: Train Loss = 2.5482
2023-11-08 13:58:22,430 - __main__ - INFO - Fold 2 Epoch 12 Batch 250: Train Loss = 1.6919
2023-11-08 13:59:02,409 - __main__ - INFO - Fold 2 Epoch 12 Batch 300: Train Loss = 1.8959
2023-11-08 14:00:15,557 - __main__ - INFO - Fold 2, mse = 28.2880, mad = 3.8004
2023-11-08 14:00:16,256 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 1.4566
2023-11-08 14:00:56,005 - __main__ - INFO - Fold 2 Epoch 13 Batch 50: Train Loss = 1.6831
2023-11-08 14:01:34,990 - __main__ - INFO - Fold 2 Epoch 13 Batch 100: Train Loss = 1.8454
2023-11-08 14:02:13,451 - __main__ - INFO - Fold 2 Epoch 13 Batch 150: Train Loss = 1.4896
2023-11-08 14:02:53,872 - __main__ - INFO - Fold 2 Epoch 13 Batch 200: Train Loss = 1.3574
2023-11-08 14:03:34,877 - __main__ - INFO - Fold 2 Epoch 13 Batch 250: Train Loss = 1.6826
2023-11-08 14:04:13,866 - __main__ - INFO - Fold 2 Epoch 13 Batch 300: Train Loss = 1.8803
2023-11-08 14:05:24,993 - __main__ - INFO - Fold 2, mse = 28.3698, mad = 3.7525
2023-11-08 14:05:25,786 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 1.6761
2023-11-08 14:06:05,661 - __main__ - INFO - Fold 2 Epoch 14 Batch 50: Train Loss = 1.5478
2023-11-08 14:06:44,419 - __main__ - INFO - Fold 2 Epoch 14 Batch 100: Train Loss = 1.4471
2023-11-08 14:07:23,424 - __main__ - INFO - Fold 2 Epoch 14 Batch 150: Train Loss = 2.8837
2023-11-08 14:08:02,698 - __main__ - INFO - Fold 2 Epoch 14 Batch 200: Train Loss = 1.8076
2023-11-08 14:08:43,282 - __main__ - INFO - Fold 2 Epoch 14 Batch 250: Train Loss = 1.5224
2023-11-08 14:09:21,879 - __main__ - INFO - Fold 2 Epoch 14 Batch 300: Train Loss = 3.1199
2023-11-08 14:10:35,155 - __main__ - INFO - Fold 2, mse = 28.1955, mad = 3.7958
2023-11-08 14:10:35,958 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 1.4420
2023-11-08 14:11:15,760 - __main__ - INFO - Fold 2 Epoch 15 Batch 50: Train Loss = 1.6878
2023-11-08 14:11:58,011 - __main__ - INFO - Fold 2 Epoch 15 Batch 100: Train Loss = 1.6399
2023-11-08 14:12:37,479 - __main__ - INFO - Fold 2 Epoch 15 Batch 150: Train Loss = 1.3761
2023-11-08 14:13:16,325 - __main__ - INFO - Fold 2 Epoch 15 Batch 200: Train Loss = 1.9471
2023-11-08 14:13:54,699 - __main__ - INFO - Fold 2 Epoch 15 Batch 250: Train Loss = 1.6252
2023-11-08 14:14:33,535 - __main__ - INFO - Fold 2 Epoch 15 Batch 300: Train Loss = 1.4923
2023-11-08 14:15:43,702 - __main__ - INFO - Fold 2, mse = 28.2183, mad = 3.8346
2023-11-08 14:15:44,443 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 1.6131
2023-11-08 14:16:22,687 - __main__ - INFO - Fold 2 Epoch 16 Batch 50: Train Loss = 2.0918
2023-11-08 14:17:02,518 - __main__ - INFO - Fold 2 Epoch 16 Batch 100: Train Loss = 1.4305
2023-11-08 14:17:41,067 - __main__ - INFO - Fold 2 Epoch 16 Batch 150: Train Loss = 1.5934
2023-11-08 14:18:19,135 - __main__ - INFO - Fold 2 Epoch 16 Batch 200: Train Loss = 1.5624
2023-11-08 14:18:58,052 - __main__ - INFO - Fold 2 Epoch 16 Batch 250: Train Loss = 1.7029
2023-11-08 14:19:37,039 - __main__ - INFO - Fold 2 Epoch 16 Batch 300: Train Loss = 1.9856
2023-11-08 14:20:48,093 - __main__ - INFO - Fold 2, mse = 28.5546, mad = 3.8184
2023-11-08 14:20:48,920 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 1.3657
2023-11-08 14:21:26,361 - __main__ - INFO - Fold 2 Epoch 17 Batch 50: Train Loss = 4.0109
2023-11-08 14:22:06,001 - __main__ - INFO - Fold 2 Epoch 17 Batch 100: Train Loss = 1.5501
2023-11-08 14:22:46,154 - __main__ - INFO - Fold 2 Epoch 17 Batch 150: Train Loss = 1.7042
2023-11-08 14:23:24,837 - __main__ - INFO - Fold 2 Epoch 17 Batch 200: Train Loss = 1.4973
2023-11-08 14:24:04,058 - __main__ - INFO - Fold 2 Epoch 17 Batch 250: Train Loss = 1.6588
2023-11-08 14:24:43,544 - __main__ - INFO - Fold 2 Epoch 17 Batch 300: Train Loss = 1.5525
2023-11-08 14:25:57,239 - __main__ - INFO - Fold 2, mse = 28.4472, mad = 3.7667
2023-11-08 14:25:57,942 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 1.6562
2023-11-08 14:26:37,084 - __main__ - INFO - Fold 2 Epoch 18 Batch 50: Train Loss = 1.4246
2023-11-08 14:27:14,990 - __main__ - INFO - Fold 2 Epoch 18 Batch 100: Train Loss = 1.2913
2023-11-08 14:27:53,790 - __main__ - INFO - Fold 2 Epoch 18 Batch 150: Train Loss = 1.7678
2023-11-08 14:28:32,735 - __main__ - INFO - Fold 2 Epoch 18 Batch 200: Train Loss = 2.5254
2023-11-08 14:29:11,400 - __main__ - INFO - Fold 2 Epoch 18 Batch 250: Train Loss = 1.7616
2023-11-08 14:29:50,024 - __main__ - INFO - Fold 2 Epoch 18 Batch 300: Train Loss = 1.4720
2023-11-08 14:31:01,870 - __main__ - INFO - Fold 2, mse = 28.3809, mad = 3.7994
2023-11-08 14:31:02,609 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 2.9543
2023-11-08 14:31:41,166 - __main__ - INFO - Fold 2 Epoch 19 Batch 50: Train Loss = 1.4611
2023-11-08 14:32:20,545 - __main__ - INFO - Fold 2 Epoch 19 Batch 100: Train Loss = 2.0552
2023-11-08 14:32:59,439 - __main__ - INFO - Fold 2 Epoch 19 Batch 150: Train Loss = 2.2747
2023-11-08 14:33:37,672 - __main__ - INFO - Fold 2 Epoch 19 Batch 200: Train Loss = 1.7151
2023-11-08 14:34:17,648 - __main__ - INFO - Fold 2 Epoch 19 Batch 250: Train Loss = 1.4981
2023-11-08 14:34:56,330 - __main__ - INFO - Fold 2 Epoch 19 Batch 300: Train Loss = 2.7003
2023-11-08 14:36:07,123 - __main__ - INFO - Fold 2, mse = 28.4822, mad = 3.8059
2023-11-08 14:36:08,525 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 3.0248
2023-11-08 14:36:46,542 - __main__ - INFO - Fold 3 Epoch 0 Batch 50: Train Loss = 2.2308
2023-11-08 14:37:26,006 - __main__ - INFO - Fold 3 Epoch 0 Batch 100: Train Loss = 1.9581
2023-11-08 14:38:05,685 - __main__ - INFO - Fold 3 Epoch 0 Batch 150: Train Loss = 2.3557
2023-11-08 14:38:44,056 - __main__ - INFO - Fold 3 Epoch 0 Batch 200: Train Loss = 2.1004
2023-11-08 14:39:23,954 - __main__ - INFO - Fold 3 Epoch 0 Batch 250: Train Loss = 1.8773
2023-11-08 14:40:03,096 - __main__ - INFO - Fold 3 Epoch 0 Batch 300: Train Loss = 4.3468
2023-11-08 14:41:15,344 - __main__ - INFO - Fold 3, epoch 0: Loss = 2.3015 Valid loss = 2.0278 MSE = 29.1468 AUROC = 0.8256
2023-11-08 14:41:15,346 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 29.1468 ------------
2023-11-08 14:41:15,653 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8256 ------------
2023-11-08 14:41:15,655 - __main__ - INFO - Fold 3, mse = 29.1468, mad = 3.9417
2023-11-08 14:41:16,383 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 2.0192
2023-11-08 14:41:54,946 - __main__ - INFO - Fold 3 Epoch 1 Batch 50: Train Loss = 1.9516
2023-11-08 14:42:33,623 - __main__ - INFO - Fold 3 Epoch 1 Batch 100: Train Loss = 1.9519
2023-11-08 14:43:13,257 - __main__ - INFO - Fold 3 Epoch 1 Batch 150: Train Loss = 1.6606
2023-11-08 14:43:51,413 - __main__ - INFO - Fold 3 Epoch 1 Batch 200: Train Loss = 1.8586
2023-11-08 14:44:30,303 - __main__ - INFO - Fold 3 Epoch 1 Batch 250: Train Loss = 2.3787
2023-11-08 14:45:10,203 - __main__ - INFO - Fold 3 Epoch 1 Batch 300: Train Loss = 1.4897
2023-11-08 14:46:23,207 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 28.7591 ------------
2023-11-08 14:46:23,709 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8574 ------------
2023-11-08 14:46:23,711 - __main__ - INFO - Fold 3, mse = 28.7591, mad = 3.8813
2023-11-08 14:46:24,523 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 1.9203
2023-11-08 14:47:04,508 - __main__ - INFO - Fold 3 Epoch 2 Batch 50: Train Loss = 3.4047
2023-11-08 14:47:43,510 - __main__ - INFO - Fold 3 Epoch 2 Batch 100: Train Loss = 1.8455
2023-11-08 14:48:23,359 - __main__ - INFO - Fold 3 Epoch 2 Batch 150: Train Loss = 1.8461
2023-11-08 14:49:02,635 - __main__ - INFO - Fold 3 Epoch 2 Batch 200: Train Loss = 2.9717
2023-11-08 14:49:41,914 - __main__ - INFO - Fold 3 Epoch 2 Batch 250: Train Loss = 1.3833
2023-11-08 14:50:22,797 - __main__ - INFO - Fold 3 Epoch 2 Batch 300: Train Loss = 1.6381
2023-11-08 14:51:37,670 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 28.3051 ------------
2023-11-08 14:51:38,236 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8671 ------------
2023-11-08 14:51:38,238 - __main__ - INFO - Fold 3, mse = 28.3051, mad = 3.8711
2023-11-08 14:51:39,034 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 1.7293
2023-11-08 14:52:18,261 - __main__ - INFO - Fold 3 Epoch 3 Batch 50: Train Loss = 2.0402
2023-11-08 14:52:57,769 - __main__ - INFO - Fold 3 Epoch 3 Batch 100: Train Loss = 1.9324
2023-11-08 14:53:37,255 - __main__ - INFO - Fold 3 Epoch 3 Batch 150: Train Loss = 1.8982
2023-11-08 14:54:16,891 - __main__ - INFO - Fold 3 Epoch 3 Batch 200: Train Loss = 2.1611
2023-11-08 14:54:56,545 - __main__ - INFO - Fold 3 Epoch 3 Batch 250: Train Loss = 2.2605
2023-11-08 14:55:35,301 - __main__ - INFO - Fold 3 Epoch 3 Batch 300: Train Loss = 1.3699
2023-11-08 14:56:49,064 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 28.1252 ------------
2023-11-08 14:56:49,520 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8710 ------------
2023-11-08 14:56:49,522 - __main__ - INFO - Fold 3, mse = 28.1252, mad = 3.8777
2023-11-08 14:56:50,375 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 1.7769
2023-11-08 14:57:28,035 - __main__ - INFO - Fold 3 Epoch 4 Batch 50: Train Loss = 1.6914
2023-11-08 14:58:06,828 - __main__ - INFO - Fold 3 Epoch 4 Batch 100: Train Loss = 1.5861
2023-11-08 14:58:44,764 - __main__ - INFO - Fold 3 Epoch 4 Batch 150: Train Loss = 1.7819
2023-11-08 14:59:21,431 - __main__ - INFO - Fold 3 Epoch 4 Batch 200: Train Loss = 1.7149
2023-11-08 15:00:01,222 - __main__ - INFO - Fold 3 Epoch 4 Batch 250: Train Loss = 1.5789
2023-11-08 15:00:39,452 - __main__ - INFO - Fold 3 Epoch 4 Batch 300: Train Loss = 2.4925
2023-11-08 15:01:51,787 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 28.0833 ------------
2023-11-08 15:01:52,303 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8746 ------------
2023-11-08 15:01:52,306 - __main__ - INFO - Fold 3, mse = 28.0833, mad = 3.8579
2023-11-08 15:01:53,154 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 1.8400
2023-11-08 15:02:30,440 - __main__ - INFO - Fold 3 Epoch 5 Batch 50: Train Loss = 2.0523
2023-11-08 15:03:09,033 - __main__ - INFO - Fold 3 Epoch 5 Batch 100: Train Loss = 2.0951
2023-11-08 15:03:47,577 - __main__ - INFO - Fold 3 Epoch 5 Batch 150: Train Loss = 2.8550
2023-11-08 15:04:24,764 - __main__ - INFO - Fold 3 Epoch 5 Batch 200: Train Loss = 1.5179
2023-11-08 15:05:04,882 - __main__ - INFO - Fold 3 Epoch 5 Batch 250: Train Loss = 2.1160
2023-11-08 15:05:43,992 - __main__ - INFO - Fold 3 Epoch 5 Batch 300: Train Loss = 1.4565
2023-11-08 15:06:56,988 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8771 ------------
2023-11-08 15:06:56,994 - __main__ - INFO - Fold 3, mse = 28.3834, mad = 3.8264
2023-11-08 15:06:57,820 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 3.0941
2023-11-08 15:07:36,675 - __main__ - INFO - Fold 3 Epoch 6 Batch 50: Train Loss = 2.9878
2023-11-08 15:08:14,946 - __main__ - INFO - Fold 3 Epoch 6 Batch 100: Train Loss = 1.5212
2023-11-08 15:08:54,720 - __main__ - INFO - Fold 3 Epoch 6 Batch 150: Train Loss = 1.3508
2023-11-08 15:09:33,514 - __main__ - INFO - Fold 3 Epoch 6 Batch 200: Train Loss = 1.8987
2023-11-08 15:10:11,824 - __main__ - INFO - Fold 3 Epoch 6 Batch 250: Train Loss = 1.2351
2023-11-08 15:10:51,104 - __main__ - INFO - Fold 3 Epoch 6 Batch 300: Train Loss = 2.1368
2023-11-08 15:12:01,520 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.8930 ------------
2023-11-08 15:12:02,136 - __main__ - INFO - ------------ Save best model - MSE: 27.8930 ------------
2023-11-08 15:12:02,139 - __main__ - INFO - Fold 3, mse = 27.8930, mad = 3.8142
2023-11-08 15:12:02,858 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 1.8332
2023-11-08 15:12:41,109 - __main__ - INFO - Fold 3 Epoch 7 Batch 50: Train Loss = 1.6332
2023-11-08 15:13:20,024 - __main__ - INFO - Fold 3 Epoch 7 Batch 100: Train Loss = 2.1879
2023-11-08 15:13:57,912 - __main__ - INFO - Fold 3 Epoch 7 Batch 150: Train Loss = 2.9527
2023-11-08 15:14:37,058 - __main__ - INFO - Fold 3 Epoch 7 Batch 200: Train Loss = 1.6545
2023-11-08 15:15:15,042 - __main__ - INFO - Fold 3 Epoch 7 Batch 250: Train Loss = 1.8420
2023-11-08 15:15:53,757 - __main__ - INFO - Fold 3 Epoch 7 Batch 300: Train Loss = 1.3969
2023-11-08 15:17:06,882 - __main__ - INFO - Fold 3, mse = 28.0306, mad = 3.8942
2023-11-08 15:17:07,514 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 2.5794
2023-11-08 15:17:44,824 - __main__ - INFO - Fold 3 Epoch 8 Batch 50: Train Loss = 1.6615
2023-11-08 15:18:23,574 - __main__ - INFO - Fold 3 Epoch 8 Batch 100: Train Loss = 1.8204
2023-11-08 15:19:00,744 - __main__ - INFO - Fold 3 Epoch 8 Batch 150: Train Loss = 1.4901
2023-11-08 15:19:37,486 - __main__ - INFO - Fold 3 Epoch 8 Batch 200: Train Loss = 1.7092
2023-11-08 15:20:16,101 - __main__ - INFO - Fold 3 Epoch 8 Batch 250: Train Loss = 1.1915
2023-11-08 15:20:54,666 - __main__ - INFO - Fold 3 Epoch 8 Batch 300: Train Loss = 1.9013
2023-11-08 15:22:05,449 - __main__ - INFO - Fold 3, mse = 27.9955, mad = 3.9432
2023-11-08 15:22:06,092 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 2.0364
2023-11-08 15:22:42,702 - __main__ - INFO - Fold 3 Epoch 9 Batch 50: Train Loss = 2.7446
2023-11-08 15:23:20,277 - __main__ - INFO - Fold 3 Epoch 9 Batch 100: Train Loss = 1.6538
2023-11-08 15:23:56,944 - __main__ - INFO - Fold 3 Epoch 9 Batch 150: Train Loss = 1.6159
2023-11-08 15:24:33,182 - __main__ - INFO - Fold 3 Epoch 9 Batch 200: Train Loss = 1.7462
2023-11-08 15:25:10,222 - __main__ - INFO - Fold 3 Epoch 9 Batch 250: Train Loss = 2.0308
2023-11-08 15:25:47,340 - __main__ - INFO - Fold 3 Epoch 9 Batch 300: Train Loss = 1.3490
2023-11-08 15:26:58,313 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8794 ------------
2023-11-08 15:26:58,315 - __main__ - INFO - Fold 3, mse = 27.9300, mad = 3.9055
2023-11-08 15:26:59,111 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 1.5561
2023-11-08 15:27:35,546 - __main__ - INFO - Fold 3 Epoch 10 Batch 50: Train Loss = 1.7144
2023-11-08 15:28:13,120 - __main__ - INFO - Fold 3 Epoch 10 Batch 100: Train Loss = 1.4454
2023-11-08 15:28:50,081 - __main__ - INFO - Fold 3 Epoch 10 Batch 150: Train Loss = 1.7601
2023-11-08 15:29:26,345 - __main__ - INFO - Fold 3 Epoch 10 Batch 200: Train Loss = 1.6552
2023-11-08 15:30:03,535 - __main__ - INFO - Fold 3 Epoch 10 Batch 250: Train Loss = 2.4567
2023-11-08 15:30:42,428 - __main__ - INFO - Fold 3 Epoch 10 Batch 300: Train Loss = 2.1443
2023-11-08 15:31:53,617 - __main__ - INFO - Fold 3, epoch 10: Loss = 1.8740 Valid loss = 1.8898 MSE = 27.5321 AUROC = 0.8776
2023-11-08 15:31:53,620 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.5321 ------------
2023-11-08 15:31:54,344 - __main__ - INFO - ------------ Save best model - MSE: 27.5321 ------------
2023-11-08 15:31:54,346 - __main__ - INFO - Fold 3, mse = 27.5321, mad = 3.8024
2023-11-08 15:31:55,080 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 2.5035
2023-11-08 15:32:31,294 - __main__ - INFO - Fold 3 Epoch 11 Batch 50: Train Loss = 1.8068
2023-11-08 15:33:07,716 - __main__ - INFO - Fold 3 Epoch 11 Batch 100: Train Loss = 1.8970
2023-11-08 15:33:44,144 - __main__ - INFO - Fold 3 Epoch 11 Batch 150: Train Loss = 2.0250
2023-11-08 15:34:22,116 - __main__ - INFO - Fold 3 Epoch 11 Batch 200: Train Loss = 1.7183
2023-11-08 15:34:58,790 - __main__ - INFO - Fold 3 Epoch 11 Batch 250: Train Loss = 1.9327
2023-11-08 15:35:37,142 - __main__ - INFO - Fold 3 Epoch 11 Batch 300: Train Loss = 1.8550
2023-11-08 15:36:47,506 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8828 ------------
2023-11-08 15:36:47,507 - __main__ - INFO - Fold 3, mse = 27.6541, mad = 3.8822
2023-11-08 15:36:48,220 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 1.7885
2023-11-08 15:37:24,776 - __main__ - INFO - Fold 3 Epoch 12 Batch 50: Train Loss = 1.6153
2023-11-08 15:38:01,932 - __main__ - INFO - Fold 3 Epoch 12 Batch 100: Train Loss = 2.0640
2023-11-08 15:38:38,308 - __main__ - INFO - Fold 3 Epoch 12 Batch 150: Train Loss = 2.4870
2023-11-08 15:39:15,132 - __main__ - INFO - Fold 3 Epoch 12 Batch 200: Train Loss = 1.5710
2023-11-08 15:39:52,345 - __main__ - INFO - Fold 3 Epoch 12 Batch 250: Train Loss = 1.9572
2023-11-08 15:40:32,319 - __main__ - INFO - Fold 3 Epoch 12 Batch 300: Train Loss = 2.1210
2023-11-08 15:41:40,332 - __main__ - INFO - Fold 3, mse = 27.6489, mad = 3.8607
2023-11-08 15:41:41,026 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 1.9113
2023-11-08 15:42:17,504 - __main__ - INFO - Fold 3 Epoch 13 Batch 50: Train Loss = 1.9041
2023-11-08 15:42:54,638 - __main__ - INFO - Fold 3 Epoch 13 Batch 100: Train Loss = 2.0234
2023-11-08 15:43:31,426 - __main__ - INFO - Fold 3 Epoch 13 Batch 150: Train Loss = 1.9401
2023-11-08 15:44:08,550 - __main__ - INFO - Fold 3 Epoch 13 Batch 200: Train Loss = 2.0525
2023-11-08 15:44:47,107 - __main__ - INFO - Fold 3 Epoch 13 Batch 250: Train Loss = 1.2850
2023-11-08 15:45:25,082 - __main__ - INFO - Fold 3 Epoch 13 Batch 300: Train Loss = 1.4761
2023-11-08 15:46:33,237 - __main__ - INFO - Fold 3, mse = 27.7094, mad = 3.8136
2023-11-08 15:46:33,979 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 2.4872
2023-11-08 15:47:10,174 - __main__ - INFO - Fold 3 Epoch 14 Batch 50: Train Loss = 1.9013
2023-11-08 15:47:47,783 - __main__ - INFO - Fold 3 Epoch 14 Batch 100: Train Loss = 1.2463
2023-11-08 15:48:24,207 - __main__ - INFO - Fold 3 Epoch 14 Batch 150: Train Loss = 2.4820
2023-11-08 15:49:01,158 - __main__ - INFO - Fold 3 Epoch 14 Batch 200: Train Loss = 1.8101
2023-11-08 15:49:41,269 - __main__ - INFO - Fold 3 Epoch 14 Batch 250: Train Loss = 4.0185
2023-11-08 15:50:17,140 - __main__ - INFO - Fold 3 Epoch 14 Batch 300: Train Loss = 1.6715
2023-11-08 15:51:27,044 - __main__ - INFO - Fold 3, mse = 27.8376, mad = 3.8769
2023-11-08 15:51:27,862 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 1.9909
2023-11-08 15:52:04,755 - __main__ - INFO - Fold 3 Epoch 15 Batch 50: Train Loss = 1.4192
2023-11-08 15:52:40,430 - __main__ - INFO - Fold 3 Epoch 15 Batch 100: Train Loss = 1.6490
2023-11-08 15:53:16,886 - __main__ - INFO - Fold 3 Epoch 15 Batch 150: Train Loss = 2.5339
2023-11-08 15:53:54,869 - __main__ - INFO - Fold 3 Epoch 15 Batch 200: Train Loss = 2.0041
2023-11-08 15:54:33,978 - __main__ - INFO - Fold 3 Epoch 15 Batch 250: Train Loss = 2.1874
2023-11-08 15:55:10,208 - __main__ - INFO - Fold 3 Epoch 15 Batch 300: Train Loss = 1.4795
2023-11-08 15:56:17,927 - __main__ - INFO - Fold 3, mse = 27.5734, mad = 3.8077
2023-11-08 15:56:18,684 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 1.4897
2023-11-08 15:56:56,095 - __main__ - INFO - Fold 3 Epoch 16 Batch 50: Train Loss = 2.0875
2023-11-08 15:57:32,432 - __main__ - INFO - Fold 3 Epoch 16 Batch 100: Train Loss = 1.4687
2023-11-08 15:58:09,778 - __main__ - INFO - Fold 3 Epoch 16 Batch 150: Train Loss = 1.4852
2023-11-08 15:58:49,999 - __main__ - INFO - Fold 3 Epoch 16 Batch 200: Train Loss = 1.8953
2023-11-08 15:59:28,021 - __main__ - INFO - Fold 3 Epoch 16 Batch 250: Train Loss = 2.0803
2023-11-08 16:00:05,082 - __main__ - INFO - Fold 3 Epoch 16 Batch 300: Train Loss = 1.9097
2023-11-08 16:01:14,612 - __main__ - INFO - Fold 3, mse = 27.6282, mad = 3.8600
2023-11-08 16:01:15,407 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 1.6190
2023-11-08 16:01:52,816 - __main__ - INFO - Fold 3 Epoch 17 Batch 50: Train Loss = 1.4057
2023-11-08 16:02:29,967 - __main__ - INFO - Fold 3 Epoch 17 Batch 100: Train Loss = 1.8509
2023-11-08 16:03:06,912 - __main__ - INFO - Fold 3 Epoch 17 Batch 150: Train Loss = 1.6377
2023-11-08 16:03:47,044 - __main__ - INFO - Fold 3 Epoch 17 Batch 200: Train Loss = 1.4619
2023-11-08 16:04:24,178 - __main__ - INFO - Fold 3 Epoch 17 Batch 250: Train Loss = 1.6016
2023-11-08 16:05:00,229 - __main__ - INFO - Fold 3 Epoch 17 Batch 300: Train Loss = 1.3643
2023-11-08 16:06:09,755 - __main__ - INFO - Fold 3, mse = 27.5411, mad = 3.8407
2023-11-08 16:06:10,726 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 1.6167
2023-11-08 16:06:47,656 - __main__ - INFO - Fold 3 Epoch 18 Batch 50: Train Loss = 1.5654
2023-11-08 16:07:23,707 - __main__ - INFO - Fold 3 Epoch 18 Batch 100: Train Loss = 2.3014
2023-11-08 16:08:03,067 - __main__ - INFO - Fold 3 Epoch 18 Batch 150: Train Loss = 1.3642
2023-11-08 16:08:41,257 - __main__ - INFO - Fold 3 Epoch 18 Batch 200: Train Loss = 1.1999
2023-11-08 16:09:17,961 - __main__ - INFO - Fold 3 Epoch 18 Batch 250: Train Loss = 2.3126
2023-11-08 16:09:54,604 - __main__ - INFO - Fold 3 Epoch 18 Batch 300: Train Loss = 2.0416
2023-11-08 16:11:02,650 - __main__ - INFO - Fold 3, mse = 27.6687, mad = 3.8307
2023-11-08 16:11:03,442 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 1.2495
2023-11-08 16:11:40,913 - __main__ - INFO - Fold 3 Epoch 19 Batch 50: Train Loss = 1.8643
2023-11-08 16:12:17,675 - __main__ - INFO - Fold 3 Epoch 19 Batch 100: Train Loss = 1.4564
2023-11-08 16:12:57,013 - __main__ - INFO - Fold 3 Epoch 19 Batch 150: Train Loss = 1.6775
2023-11-08 16:13:33,214 - __main__ - INFO - Fold 3 Epoch 19 Batch 200: Train Loss = 3.3289
2023-11-08 16:14:08,929 - __main__ - INFO - Fold 3 Epoch 19 Batch 250: Train Loss = 1.1038
2023-11-08 16:14:46,527 - __main__ - INFO - Fold 3 Epoch 19 Batch 300: Train Loss = 1.7672
2023-11-08 16:15:54,266 - __main__ - INFO - Fold 3, mse = 27.8500, mad = 3.8792
2023-11-08 16:15:54,268 - __main__ - INFO - mse 28.9084(1.6591)
2023-11-08 16:15:54,270 - __main__ - INFO - mad 3.8227(0.0145)
2023-11-08 16:15:54,271 - __main__ - INFO - auroc 0.8821(0.0090)
2023-11-08 16:15:54,272 - __main__ - INFO - auprc 0.5492(0.0244)
2023-11-08 16:15:54,282 - __main__ - INFO - mse 28.9084(1.6591)
2023-11-08 16:15:54,284 - __main__ - INFO - mad 3.8227(0.0145)
2023-11-08 16:15:54,285 - __main__ - INFO - auroc 0.8821(0.0090)
2023-11-08 16:15:54,286 - __main__ - INFO - auprc 0.5492(0.0244)
