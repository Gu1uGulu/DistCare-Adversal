2023-11-08 00:37:09,171 - __main__ - INFO - 这是希望输出的info内容
2023-11-08 00:37:09,173 - __main__ - WARNING - 这是希望输出的warning内容
2023-11-08 00:37:18,108 - __main__ - INFO - 这是希望输出的info内容
2023-11-08 00:37:18,109 - __main__ - WARNING - 这是希望输出的warning内容
2023-11-08 00:37:39,856 - __main__ - INFO - [[-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 1.2605692444279462, 0.585371321489137, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.9546043520029015, 0.2987563180527675, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 1.0769903089729194, 0.4420638197709522, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.566534136852991, 1.803485086093707, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, 0.46506052412282983, -0.05951243624269434, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, 0.281481588667803, -0.2744736888199714, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.8934113735178925, 0.9436400757845987, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592]]
2023-11-08 00:37:39,860 - __main__ - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
2023-11-08 00:37:39,861 - __main__ - INFO - 110609
2023-11-08 00:37:39,861 - __main__ - INFO - [[-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8962118618028821, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8617355345389544, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8272592072750267, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7927828800110989, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7583065527471711, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7238302254832434, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6893538982193156, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6548775709553878, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.62040124369146, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5859249164275323, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5514485891636045, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5169722618996767, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.48249593463574897, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.4480196073718212, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.41354328010789343, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3790669528439657, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3445906255800379, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.31011429831611015, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.27563797105218235, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.2411616437882546, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.20668531652432684, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.17220898926039907, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.1377326619964713, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.10325633473254354, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.06878000746861578, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.034303680204688006, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.0001726470592397616, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.034648974323167527, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.06912530158709529, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.10360162885102306, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.13807795611495083, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.1725542833788786, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.20703061064280637, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.24150693790673414, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.2759832651706619, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.31045959243458965, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.34493591969851745, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.3794122469624452, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.413888574226373, '0']]
2023-11-08 00:37:56,917 - __main__ - INFO - [[-0.32211134163582006, -0.06645805008034258, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, -0.14487324959363315, -0.0899704549996704, -0.14828727498338712, -0.24435830491350327, -0.34587251014597875, 0.014295447077977357, -0.05532679134287014, -0.1935716453287135, -0.3766591890459441, -0.01724690479013476, 0.3051487380880145, -0.33684497792590695, -0.3160730875843661, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.32211134163582006, -0.06645805008034258, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, -0.14487324959363315, -0.0899704549996704, -0.14828727498338712, -0.24435830491350327, -0.34587251014597875, 0.014295447077977357, -0.05532679134287014, -0.1935716453287135, -0.3766591890459441, -0.01724690479013476, 0.3051487380880145, -0.33684497792590695, -0.3160730875843661, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.32211134163582006, -0.06645805008034258, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, -0.14487324959363315, -0.0899704549996704, -0.14828727498338712, -0.24435830491350327, -0.34587251014597875, 0.014295447077977357, -0.05532679134287014, -0.1935716453287135, -0.3766591890459441, -0.01724690479013476, 0.3051487380880145, -0.33684497792590695, -0.3160730875843661, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 1.2605692444279462, 0.585371321489137, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.9546043520029015, 0.2987563180527675, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 1.0769903089729194, 0.4420638197709522, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.566534136852991, 1.803485086093707, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, 0.46506052412282983, -0.05951243624269434, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, 0.281481588667803, -0.2744736888199714, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.8934113735178925, 0.9436400757845987, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.5053978624091442, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.5053978624091442, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.5053978624091442, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.5053978624091442, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592]]
2023-11-08 00:37:56,923 - __main__ - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
2023-11-08 00:37:56,924 - __main__ - INFO - 34
2023-11-08 00:37:58,423 - __main__ - INFO - 32269
2023-11-08 00:37:58,425 - __main__ - INFO - 4034
2023-11-08 00:37:58,426 - __main__ - INFO - 4033
2023-11-08 00:38:05,438 - __main__ - INFO - Training Teacher
2023-11-08 00:38:06,720 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 0.6807
2023-11-08 00:38:19,083 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 0.4993
2023-11-08 00:38:31,564 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 0.2431
2023-11-08 00:38:44,161 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 0.2449
2023-11-08 00:38:56,909 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 0.2732
2023-11-08 00:39:09,749 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 0.2980
2023-11-08 00:39:23,089 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 0.2893
2023-11-08 00:39:33,146 - __main__ - INFO - Epoch 0: Loss = 0.3323 Valid loss = 0.2420 roc = 0.7876
2023-11-08 00:39:34,189 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 0.1548
2023-11-08 00:39:48,714 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 0.2485
2023-11-08 00:40:02,715 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 0.2693
2023-11-08 00:40:16,252 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 0.2384
2023-11-08 00:40:30,286 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 0.2495
2023-11-08 00:40:46,532 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 0.2690
2023-11-08 00:41:02,407 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 0.2143
2023-11-08 00:41:15,135 - __main__ - INFO - Epoch 1: Loss = 0.2475 Valid loss = 0.2320 roc = 0.7929
2023-11-08 00:41:16,224 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 0.1795
2023-11-08 00:41:31,729 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 0.2371
2023-11-08 00:41:47,740 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 0.3451
2023-11-08 00:42:02,347 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 0.2039
2023-11-08 00:42:18,936 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 0.2576
2023-11-08 00:42:34,683 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 0.2710
2023-11-08 00:42:50,538 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 0.2088
2023-11-08 00:43:02,423 - __main__ - INFO - Epoch 2: Loss = 0.2389 Valid loss = 0.2249 roc = 0.8206
2023-11-08 00:43:03,338 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 0.2632
2023-11-08 00:43:18,311 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 0.2973
2023-11-08 00:43:33,649 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 0.2312
2023-11-08 00:43:48,576 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 0.2769
2023-11-08 00:44:05,025 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 0.2401
2023-11-08 00:44:20,172 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 0.2660
2023-11-08 00:44:35,388 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 0.1624
2023-11-08 00:44:47,770 - __main__ - INFO - Epoch 3: Loss = 0.2368 Valid loss = 0.2141 roc = 0.8313
2023-11-08 00:44:48,742 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 0.1999
2023-11-08 00:45:05,263 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 0.2330
2023-11-08 00:45:21,090 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 0.2225
2023-11-08 00:45:37,675 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 0.2664
2023-11-08 00:45:52,759 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 0.2710
2023-11-08 00:46:08,199 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 0.2163
2023-11-08 00:46:23,799 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 0.2480
2023-11-08 00:46:35,502 - __main__ - INFO - Epoch 4: Loss = 0.2291 Valid loss = 0.2054 roc = 0.8559
2023-11-08 00:46:36,523 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 0.2703
2023-11-08 00:46:52,423 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 0.2441
2023-11-08 00:47:08,458 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 0.2538
2023-11-08 00:47:24,710 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 0.2270
2023-11-08 00:47:39,440 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 0.2189
2023-11-08 00:47:55,730 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 0.2151
2023-11-08 00:48:11,681 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 0.1819
2023-11-08 00:48:23,958 - __main__ - INFO - Epoch 5: Loss = 0.2232 Valid loss = 0.1974 roc = 0.8771
2023-11-08 00:48:24,632 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 0.2562
2023-11-08 00:48:40,131 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 0.2245
2023-11-08 00:48:55,815 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 0.2283
2023-11-08 00:49:11,301 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 0.2145
2023-11-08 00:49:27,104 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 0.3544
2023-11-08 00:49:43,999 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 0.2073
2023-11-08 00:50:01,168 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 0.2001
2023-11-08 00:50:13,552 - __main__ - INFO - Epoch 6: Loss = 0.2135 Valid loss = 0.1845 roc = 0.8984
2023-11-08 00:50:14,391 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 0.2113
2023-11-08 00:50:29,977 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 0.2014
2023-11-08 00:50:46,041 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 0.2063
2023-11-08 00:51:01,939 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 0.1913
2023-11-08 00:51:16,959 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 0.1777
2023-11-08 00:51:34,042 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 0.2286
2023-11-08 00:51:49,711 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 0.2236
2023-11-08 00:52:02,011 - __main__ - INFO - Epoch 7: Loss = 0.2072 Valid loss = 0.1851 roc = 0.8985
2023-11-08 00:52:02,791 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 0.2383
2023-11-08 00:52:18,414 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 0.1390
2023-11-08 00:52:33,885 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 0.1678
2023-11-08 00:52:49,398 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 0.1574
2023-11-08 00:53:04,424 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 0.2043
2023-11-08 00:53:21,469 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 0.2310
2023-11-08 00:53:36,808 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 0.2260
2023-11-08 00:53:48,781 - __main__ - INFO - Epoch 8: Loss = 0.1995 Valid loss = 0.1674 roc = 0.9053
2023-11-08 00:53:49,592 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 0.1938
2023-11-08 00:54:05,354 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 0.2056
2023-11-08 00:54:20,175 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 0.2219
2023-11-08 00:54:37,513 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 0.2044
2023-11-08 00:54:54,016 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 0.1851
2023-11-08 00:55:10,215 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 0.1432
2023-11-08 00:55:26,336 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 0.1268
2023-11-08 00:55:38,730 - __main__ - INFO - Epoch 9: Loss = 0.1923 Valid loss = 0.1650 roc = 0.9051
2023-11-08 00:55:39,631 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 0.1454
2023-11-08 00:55:56,268 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 0.1992
2023-11-08 00:56:12,764 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 0.1344
2023-11-08 00:56:29,446 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 0.2554
2023-11-08 00:56:45,019 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 0.2162
2023-11-08 00:57:00,308 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 0.1640
2023-11-08 00:57:16,781 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 0.2038
2023-11-08 00:57:28,929 - __main__ - INFO - Epoch 10: Loss = 0.1838 Valid loss = 0.1609 roc = 0.9094
2023-11-08 00:57:29,826 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 0.1175
2023-11-08 00:57:43,788 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 0.2451
2023-11-08 00:57:59,246 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 0.2413
2023-11-08 00:58:16,175 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 0.1707
2023-11-08 00:58:31,117 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 0.2294
2023-11-08 00:58:47,155 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 0.2016
2023-11-08 00:59:03,615 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 0.2003
2023-11-08 00:59:15,963 - __main__ - INFO - Epoch 11: Loss = 0.1846 Valid loss = 0.1580 roc = 0.9101
2023-11-08 00:59:16,870 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 0.2626
2023-11-08 00:59:33,549 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 0.1224
2023-11-08 00:59:50,324 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 0.2007
2023-11-08 01:00:06,359 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 0.1853
2023-11-08 01:00:22,013 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 0.1280
2023-11-08 01:00:37,238 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 0.2109
2023-11-08 01:00:52,663 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 0.2228
2023-11-08 01:01:04,346 - __main__ - INFO - Epoch 12: Loss = 0.1829 Valid loss = 0.1594 roc = 0.9125
2023-11-08 01:01:05,099 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 0.1858
2023-11-08 01:01:20,567 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 0.1722
2023-11-08 01:01:36,562 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 0.1731
2023-11-08 01:01:52,564 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 0.1733
2023-11-08 01:02:08,337 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 0.1745
2023-11-08 01:02:23,099 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 0.2092
2023-11-08 01:02:38,298 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 0.2372
2023-11-08 01:02:50,112 - __main__ - INFO - Epoch 13: Loss = 0.1821 Valid loss = 0.1551 roc = 0.9159
2023-11-08 01:02:51,116 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 0.1422
2023-11-08 01:03:06,617 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 0.2918
2023-11-08 01:03:22,011 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 0.1688
2023-11-08 01:03:37,374 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 0.1953
2023-11-08 01:03:54,003 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 0.1607
2023-11-08 01:04:10,155 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 0.1884
2023-11-08 01:04:25,267 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 0.1363
2023-11-08 01:04:37,311 - __main__ - INFO - Epoch 14: Loss = 0.1803 Valid loss = 0.1530 roc = 0.9094
2023-11-08 01:04:38,225 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 0.2860
2023-11-08 01:04:54,466 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 0.2447
2023-11-08 01:05:09,806 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 0.1965
2023-11-08 01:05:25,441 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 0.1831
2023-11-08 01:05:43,163 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 0.1457
2023-11-08 01:05:58,578 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 0.2384
2023-11-08 01:06:14,464 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 0.1715
2023-11-08 01:06:26,617 - __main__ - INFO - Epoch 15: Loss = 0.1767 Valid loss = 0.1579 roc = 0.9121
2023-11-08 01:06:27,695 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 0.2095
2023-11-08 01:06:43,511 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 0.1638
2023-11-08 01:06:59,897 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 0.1872
2023-11-08 01:07:15,202 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 0.1762
2023-11-08 01:07:30,946 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 0.2230
2023-11-08 01:07:46,767 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 0.1302
2023-11-08 01:08:02,514 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 0.1993
2023-11-08 01:08:14,764 - __main__ - INFO - Epoch 16: Loss = 0.1818 Valid loss = 0.1510 roc = 0.9158
2023-11-08 01:08:15,581 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 0.1507
2023-11-08 01:08:31,400 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 0.1895
2023-11-08 01:08:47,459 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 0.1430
2023-11-08 01:09:02,897 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 0.1504
2023-11-08 01:09:17,191 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 0.1067
2023-11-08 01:09:32,563 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 0.1960
2023-11-08 01:09:48,281 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 0.1836
2023-11-08 01:09:59,796 - __main__ - INFO - Epoch 17: Loss = 0.1784 Valid loss = 0.1496 roc = 0.9170
2023-11-08 01:10:00,659 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 0.2271
2023-11-08 01:10:15,677 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 0.2651
2023-11-08 01:10:31,403 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 0.1254
2023-11-08 01:10:46,794 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 0.1772
2023-11-08 01:11:02,269 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 0.1427
2023-11-08 01:11:18,583 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 0.1594
2023-11-08 01:11:34,205 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 0.1247
2023-11-08 01:11:46,559 - __main__ - INFO - Epoch 18: Loss = 0.1758 Valid loss = 0.1547 roc = 0.9160
2023-11-08 01:11:47,204 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 0.1677
2023-11-08 01:12:02,625 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 0.1666
2023-11-08 01:12:19,060 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 0.2147
2023-11-08 01:12:36,352 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 0.1596
2023-11-08 01:12:54,051 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 0.1673
2023-11-08 01:13:10,460 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 0.1715
2023-11-08 01:13:28,549 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 0.2081
2023-11-08 01:13:41,831 - __main__ - INFO - Epoch 19: Loss = 0.1712 Valid loss = 0.1467 roc = 0.9104
2023-11-08 01:13:43,174 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 0.1510
2023-11-08 01:13:59,150 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 0.1970
2023-11-08 01:14:14,975 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 0.1272
2023-11-08 01:14:32,049 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 0.2094
2023-11-08 01:14:48,030 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 0.1694
2023-11-08 01:15:06,269 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 0.1538
2023-11-08 01:15:24,569 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 0.1530
2023-11-08 01:15:38,503 - __main__ - INFO - Epoch 20: Loss = 0.1725 Valid loss = 0.1456 roc = 0.9199
2023-11-08 01:15:39,404 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 0.1474
2023-11-08 01:15:55,440 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 0.2067
2023-11-08 01:16:12,046 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 0.1389
2023-11-08 01:16:27,772 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 0.1781
2023-11-08 01:16:43,128 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 0.1636
2023-11-08 01:16:59,502 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 0.1314
2023-11-08 01:17:15,757 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 0.2342
2023-11-08 01:17:26,936 - __main__ - INFO - Epoch 21: Loss = 0.1718 Valid loss = 0.1508 roc = 0.9127
2023-11-08 01:17:27,592 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 0.2290
2023-11-08 01:17:42,425 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 0.1856
2023-11-08 01:17:59,206 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 0.1206
2023-11-08 01:18:17,080 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 0.1872
2023-11-08 01:18:35,599 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 0.1633
2023-11-08 01:18:53,053 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 0.1456
2023-11-08 01:19:08,697 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 0.1749
2023-11-08 01:19:20,787 - __main__ - INFO - Epoch 22: Loss = 0.1738 Valid loss = 0.1485 roc = 0.9122
2023-11-08 01:19:21,565 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 0.2109
2023-11-08 01:19:39,033 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 0.1972
2023-11-08 01:19:55,799 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 0.1796
2023-11-08 01:20:12,095 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 0.1798
2023-11-08 01:20:29,760 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 0.2086
2023-11-08 01:20:44,496 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 0.1943
2023-11-08 01:21:01,102 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 0.1655
2023-11-08 01:21:14,010 - __main__ - INFO - Epoch 23: Loss = 0.1707 Valid loss = 0.1489 roc = 0.9148
2023-11-08 01:21:14,865 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 0.1863
2023-11-08 01:21:31,899 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 0.1173
2023-11-08 01:21:48,303 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 0.1642
2023-11-08 01:22:04,906 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 0.2254
2023-11-08 01:22:20,624 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 0.2124
2023-11-08 01:22:34,525 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 0.1879
2023-11-08 01:22:48,598 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 0.1972
2023-11-08 01:22:59,819 - __main__ - INFO - Epoch 24: Loss = 0.1702 Valid loss = 0.1469 roc = 0.9137
2023-11-08 01:23:01,020 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 0.1355
2023-11-08 01:23:16,675 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 0.1424
2023-11-08 01:23:32,837 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 0.1579
2023-11-08 01:23:49,608 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 0.1788
2023-11-08 01:24:05,110 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 0.1705
2023-11-08 01:24:21,035 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 0.2057
2023-11-08 01:24:35,692 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 0.1089
2023-11-08 01:24:47,953 - __main__ - INFO - Epoch 25: Loss = 0.1733 Valid loss = 0.1468 roc = 0.9146
2023-11-08 01:24:48,766 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 0.1283
2023-11-08 01:25:04,893 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 0.1167
2023-11-08 01:25:22,969 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 0.1535
2023-11-08 01:25:39,475 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 0.1682
2023-11-08 01:25:54,319 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 0.2119
2023-11-08 01:26:09,460 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 0.1622
2023-11-08 01:26:25,693 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 0.1834
2023-11-08 01:26:37,034 - __main__ - INFO - Epoch 26: Loss = 0.1664 Valid loss = 0.1466 roc = 0.9109
2023-11-08 01:26:37,675 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 0.1733
2023-11-08 01:26:54,115 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 0.1182
2023-11-08 01:27:09,895 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 0.1474
2023-11-08 01:27:25,858 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 0.1352
2023-11-08 01:27:40,749 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 0.1823
2023-11-08 01:27:55,667 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 0.1960
2023-11-08 01:28:11,162 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 0.1702
2023-11-08 01:28:21,552 - __main__ - INFO - Epoch 27: Loss = 0.1658 Valid loss = 0.1475 roc = 0.9147
2023-11-08 01:28:22,486 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 0.2070
2023-11-08 01:28:38,529 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 0.0871
2023-11-08 01:28:53,411 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 0.1734
2023-11-08 01:29:07,238 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 0.1673
2023-11-08 01:29:22,071 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 0.2599
2023-11-08 01:29:39,063 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 0.1687
2023-11-08 01:29:56,445 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 0.1382
2023-11-08 01:30:09,096 - __main__ - INFO - Epoch 28: Loss = 0.1640 Valid loss = 0.1448 roc = 0.9109
2023-11-08 01:30:09,977 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 0.1687
2023-11-08 01:30:26,779 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 0.1971
2023-11-08 01:30:44,364 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 0.2494
2023-11-08 01:31:00,356 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 0.1578
2023-11-08 01:31:16,966 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 0.2093
2023-11-08 01:31:32,469 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 0.1242
2023-11-08 01:31:49,739 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 0.1638
2023-11-08 01:32:03,274 - __main__ - INFO - Epoch 29: Loss = 0.1672 Valid loss = 0.1447 roc = 0.9163
2023-11-08 01:32:03,911 - __main__ - INFO - Epoch 30 Batch 0: Train Loss = 0.1599
2023-11-08 01:32:20,722 - __main__ - INFO - Epoch 30 Batch 20: Train Loss = 0.1938
2023-11-08 01:32:39,669 - __main__ - INFO - Epoch 30 Batch 40: Train Loss = 0.1558
2023-11-08 01:32:57,407 - __main__ - INFO - Epoch 30 Batch 60: Train Loss = 0.1280
2023-11-08 01:33:16,352 - __main__ - INFO - Epoch 30 Batch 80: Train Loss = 0.1650
2023-11-08 01:33:33,878 - __main__ - INFO - Epoch 30 Batch 100: Train Loss = 0.1950
2023-11-08 01:33:51,777 - __main__ - INFO - Epoch 30 Batch 120: Train Loss = 0.1687
2023-11-08 01:34:05,102 - __main__ - INFO - Epoch 30: Loss = 0.1650 Valid loss = 0.1487 roc = 0.9144
2023-11-08 01:34:05,865 - __main__ - INFO - Epoch 31 Batch 0: Train Loss = 0.1693
2023-11-08 01:34:24,180 - __main__ - INFO - Epoch 31 Batch 20: Train Loss = 0.1773
2023-11-08 01:34:43,006 - __main__ - INFO - Epoch 31 Batch 40: Train Loss = 0.2393
2023-11-08 01:35:00,011 - __main__ - INFO - Epoch 31 Batch 60: Train Loss = 0.1351
2023-11-08 01:35:16,527 - __main__ - INFO - Epoch 31 Batch 80: Train Loss = 0.1217
2023-11-08 01:35:33,206 - __main__ - INFO - Epoch 31 Batch 100: Train Loss = 0.2067
2023-11-08 01:35:51,525 - __main__ - INFO - Epoch 31 Batch 120: Train Loss = 0.1087
2023-11-08 01:36:04,837 - __main__ - INFO - Epoch 31: Loss = 0.1617 Valid loss = 0.1436 roc = 0.9173
2023-11-08 01:36:05,783 - __main__ - INFO - Epoch 32 Batch 0: Train Loss = 0.0982
2023-11-08 01:36:23,341 - __main__ - INFO - Epoch 32 Batch 20: Train Loss = 0.1615
2023-11-08 01:36:40,255 - __main__ - INFO - Epoch 32 Batch 40: Train Loss = 0.1312
2023-11-08 01:36:57,140 - __main__ - INFO - Epoch 32 Batch 60: Train Loss = 0.1669
2023-11-08 01:37:14,453 - __main__ - INFO - Epoch 32 Batch 80: Train Loss = 0.1905
2023-11-08 01:37:32,429 - __main__ - INFO - Epoch 32 Batch 100: Train Loss = 0.1682
2023-11-08 01:37:49,917 - __main__ - INFO - Epoch 32 Batch 120: Train Loss = 0.1325
2023-11-08 01:38:03,564 - __main__ - INFO - Epoch 32: Loss = 0.1637 Valid loss = 0.1427 roc = 0.9237
2023-11-08 01:38:04,659 - __main__ - INFO - Epoch 33 Batch 0: Train Loss = 0.1035
2023-11-08 01:38:22,126 - __main__ - INFO - Epoch 33 Batch 20: Train Loss = 0.1600
2023-11-08 01:38:39,875 - __main__ - INFO - Epoch 33 Batch 40: Train Loss = 0.1054
2023-11-08 01:38:57,965 - __main__ - INFO - Epoch 33 Batch 60: Train Loss = 0.2169
2023-11-08 01:39:15,506 - __main__ - INFO - Epoch 33 Batch 80: Train Loss = 0.1873
2023-11-08 01:39:32,216 - __main__ - INFO - Epoch 33 Batch 100: Train Loss = 0.1426
2023-11-08 01:39:48,710 - __main__ - INFO - Epoch 33 Batch 120: Train Loss = 0.1511
2023-11-08 01:40:02,787 - __main__ - INFO - Epoch 33: Loss = 0.1646 Valid loss = 0.1455 roc = 0.9162
2023-11-08 01:40:03,731 - __main__ - INFO - Epoch 34 Batch 0: Train Loss = 0.1281
2023-11-08 01:40:23,261 - __main__ - INFO - Epoch 34 Batch 20: Train Loss = 0.2290
2023-11-08 01:40:41,298 - __main__ - INFO - Epoch 34 Batch 40: Train Loss = 0.1092
2023-11-08 01:40:58,320 - __main__ - INFO - Epoch 34 Batch 60: Train Loss = 0.1461
2023-11-08 01:41:17,518 - __main__ - INFO - Epoch 34 Batch 80: Train Loss = 0.2105
2023-11-08 01:41:35,127 - __main__ - INFO - Epoch 34 Batch 100: Train Loss = 0.1864
2023-11-08 01:41:52,560 - __main__ - INFO - Epoch 34 Batch 120: Train Loss = 0.1581
2023-11-08 01:42:05,845 - __main__ - INFO - Epoch 34: Loss = 0.1634 Valid loss = 0.1445 roc = 0.9183
2023-11-08 01:42:06,730 - __main__ - INFO - Epoch 35 Batch 0: Train Loss = 0.1502
2023-11-08 01:42:23,454 - __main__ - INFO - Epoch 35 Batch 20: Train Loss = 0.1910
2023-11-08 01:42:41,651 - __main__ - INFO - Epoch 35 Batch 40: Train Loss = 0.1992
2023-11-08 01:42:59,222 - __main__ - INFO - Epoch 35 Batch 60: Train Loss = 0.1237
2023-11-08 01:43:17,363 - __main__ - INFO - Epoch 35 Batch 80: Train Loss = 0.1428
2023-11-08 01:43:36,623 - __main__ - INFO - Epoch 35 Batch 100: Train Loss = 0.1536
2023-11-08 01:43:54,786 - __main__ - INFO - Epoch 35 Batch 120: Train Loss = 0.1999
2023-11-08 01:44:08,460 - __main__ - INFO - Epoch 35: Loss = 0.1617 Valid loss = 0.1448 roc = 0.9159
2023-11-08 01:44:09,368 - __main__ - INFO - Epoch 36 Batch 0: Train Loss = 0.0945
2023-11-08 01:44:25,943 - __main__ - INFO - Epoch 36 Batch 20: Train Loss = 0.2434
2023-11-08 01:44:42,935 - __main__ - INFO - Epoch 36 Batch 40: Train Loss = 0.1296
2023-11-08 01:45:00,443 - __main__ - INFO - Epoch 36 Batch 60: Train Loss = 0.2035
2023-11-08 01:45:18,776 - __main__ - INFO - Epoch 36 Batch 80: Train Loss = 0.2347
2023-11-08 01:45:36,875 - __main__ - INFO - Epoch 36 Batch 100: Train Loss = 0.1433
2023-11-08 01:45:55,595 - __main__ - INFO - Epoch 36 Batch 120: Train Loss = 0.1974
2023-11-08 01:46:09,507 - __main__ - INFO - Epoch 36: Loss = 0.1609 Valid loss = 0.1420 roc = 0.9193
2023-11-08 01:46:10,413 - __main__ - INFO - Epoch 37 Batch 0: Train Loss = 0.1725
2023-11-08 01:46:29,910 - __main__ - INFO - Epoch 37 Batch 20: Train Loss = 0.1341
2023-11-08 01:46:47,129 - __main__ - INFO - Epoch 37 Batch 40: Train Loss = 0.1305
2023-11-08 01:47:04,787 - __main__ - INFO - Epoch 37 Batch 60: Train Loss = 0.1745
2023-11-08 01:47:21,780 - __main__ - INFO - Epoch 37 Batch 80: Train Loss = 0.2728
2023-11-08 01:47:39,700 - __main__ - INFO - Epoch 37 Batch 100: Train Loss = 0.0959
2023-11-08 01:47:57,423 - __main__ - INFO - Epoch 37 Batch 120: Train Loss = 0.1725
2023-11-08 01:48:10,872 - __main__ - INFO - Epoch 37: Loss = 0.1636 Valid loss = 0.1444 roc = 0.9138
2023-11-08 01:48:11,812 - __main__ - INFO - Epoch 38 Batch 0: Train Loss = 0.2028
2023-11-08 01:48:28,988 - __main__ - INFO - Epoch 38 Batch 20: Train Loss = 0.1750
2023-11-08 01:48:46,010 - __main__ - INFO - Epoch 38 Batch 40: Train Loss = 0.1300
2023-11-08 01:49:03,546 - __main__ - INFO - Epoch 38 Batch 60: Train Loss = 0.1838
2023-11-08 01:49:21,588 - __main__ - INFO - Epoch 38 Batch 80: Train Loss = 0.1385
2023-11-08 01:49:38,492 - __main__ - INFO - Epoch 38 Batch 100: Train Loss = 0.1659
2023-11-08 01:49:55,255 - __main__ - INFO - Epoch 38 Batch 120: Train Loss = 0.1511
2023-11-08 01:50:09,453 - __main__ - INFO - Epoch 38: Loss = 0.1678 Valid loss = 0.1417 roc = 0.9176
2023-11-08 01:50:10,135 - __main__ - INFO - Epoch 39 Batch 0: Train Loss = 0.1677
2023-11-08 01:50:28,359 - __main__ - INFO - Epoch 39 Batch 20: Train Loss = 0.1594
2023-11-08 01:50:46,237 - __main__ - INFO - Epoch 39 Batch 40: Train Loss = 0.1567
2023-11-08 01:51:04,261 - __main__ - INFO - Epoch 39 Batch 60: Train Loss = 0.1278
2023-11-08 01:51:21,871 - __main__ - INFO - Epoch 39 Batch 80: Train Loss = 0.1654
2023-11-08 01:51:39,403 - __main__ - INFO - Epoch 39 Batch 100: Train Loss = 0.1525
2023-11-08 01:51:56,718 - __main__ - INFO - Epoch 39 Batch 120: Train Loss = 0.1551
2023-11-08 01:52:09,088 - __main__ - INFO - Epoch 39: Loss = 0.1600 Valid loss = 0.1422 roc = 0.9176
2023-11-08 01:52:10,116 - __main__ - INFO - Epoch 40 Batch 0: Train Loss = 0.1537
2023-11-08 01:52:28,637 - __main__ - INFO - Epoch 40 Batch 20: Train Loss = 0.1332
2023-11-08 01:52:45,870 - __main__ - INFO - Epoch 40 Batch 40: Train Loss = 0.1192
2023-11-08 01:53:02,349 - __main__ - INFO - Epoch 40 Batch 60: Train Loss = 0.1700
2023-11-08 01:53:20,246 - __main__ - INFO - Epoch 40 Batch 80: Train Loss = 0.1170
2023-11-08 01:53:36,931 - __main__ - INFO - Epoch 40 Batch 100: Train Loss = 0.1657
2023-11-08 01:53:53,703 - __main__ - INFO - Epoch 40 Batch 120: Train Loss = 0.1292
2023-11-08 01:54:08,027 - __main__ - INFO - Epoch 40: Loss = 0.1588 Valid loss = 0.1406 roc = 0.9212
2023-11-08 01:54:09,051 - __main__ - INFO - Epoch 41 Batch 0: Train Loss = 0.1490
2023-11-08 01:54:26,701 - __main__ - INFO - Epoch 41 Batch 20: Train Loss = 0.1490
2023-11-08 01:54:44,513 - __main__ - INFO - Epoch 41 Batch 40: Train Loss = 0.1177
2023-11-08 01:55:03,640 - __main__ - INFO - Epoch 41 Batch 60: Train Loss = 0.1609
2023-11-08 01:55:20,512 - __main__ - INFO - Epoch 41 Batch 80: Train Loss = 0.2011
2023-11-08 01:55:37,888 - __main__ - INFO - Epoch 41 Batch 100: Train Loss = 0.1050
2023-11-08 01:55:55,276 - __main__ - INFO - Epoch 41 Batch 120: Train Loss = 0.1390
2023-11-08 01:56:08,565 - __main__ - INFO - Epoch 41: Loss = 0.1551 Valid loss = 0.1405 roc = 0.9190
2023-11-08 01:56:09,342 - __main__ - INFO - Epoch 42 Batch 0: Train Loss = 0.2277
2023-11-08 01:56:25,828 - __main__ - INFO - Epoch 42 Batch 20: Train Loss = 0.2042
2023-11-08 01:56:44,390 - __main__ - INFO - Epoch 42 Batch 40: Train Loss = 0.1375
2023-11-08 01:57:02,627 - __main__ - INFO - Epoch 42 Batch 60: Train Loss = 0.1591
2023-11-08 01:57:20,626 - __main__ - INFO - Epoch 42 Batch 80: Train Loss = 0.1869
2023-11-08 01:57:38,350 - __main__ - INFO - Epoch 42 Batch 100: Train Loss = 0.1574
2023-11-08 01:57:56,802 - __main__ - INFO - Epoch 42 Batch 120: Train Loss = 0.1981
2023-11-08 01:58:10,963 - __main__ - INFO - Epoch 42: Loss = 0.1584 Valid loss = 0.1422 roc = 0.9179
2023-11-08 01:58:11,833 - __main__ - INFO - Epoch 43 Batch 0: Train Loss = 0.1681
2023-11-08 01:58:27,213 - __main__ - INFO - Epoch 43 Batch 20: Train Loss = 0.1647
2023-11-08 01:58:44,463 - __main__ - INFO - Epoch 43 Batch 40: Train Loss = 0.1520
2023-11-08 01:59:03,612 - __main__ - INFO - Epoch 43 Batch 60: Train Loss = 0.1755
2023-11-08 01:59:21,912 - __main__ - INFO - Epoch 43 Batch 80: Train Loss = 0.1125
2023-11-08 01:59:40,194 - __main__ - INFO - Epoch 43 Batch 100: Train Loss = 0.1414
2023-11-08 01:59:57,361 - __main__ - INFO - Epoch 43 Batch 120: Train Loss = 0.1725
2023-11-08 02:00:10,600 - __main__ - INFO - Epoch 43: Loss = 0.1582 Valid loss = 0.1415 roc = 0.9176
2023-11-08 02:00:11,570 - __main__ - INFO - Epoch 44 Batch 0: Train Loss = 0.2057
2023-11-08 02:00:27,922 - __main__ - INFO - Epoch 44 Batch 20: Train Loss = 0.1471
2023-11-08 02:00:46,456 - __main__ - INFO - Epoch 44 Batch 40: Train Loss = 0.1319
2023-11-08 02:01:03,099 - __main__ - INFO - Epoch 44 Batch 60: Train Loss = 0.1618
2023-11-08 02:01:20,982 - __main__ - INFO - Epoch 44 Batch 80: Train Loss = 0.1397
2023-11-08 02:01:38,687 - __main__ - INFO - Epoch 44 Batch 100: Train Loss = 0.1116
2023-11-08 02:01:54,113 - __main__ - INFO - Epoch 44 Batch 120: Train Loss = 0.1901
2023-11-08 02:02:07,798 - __main__ - INFO - Epoch 44: Loss = 0.1594 Valid loss = 0.1410 roc = 0.9159
2023-11-08 02:02:08,723 - __main__ - INFO - Epoch 45 Batch 0: Train Loss = 0.1323
2023-11-08 02:02:27,243 - __main__ - INFO - Epoch 45 Batch 20: Train Loss = 0.1501
2023-11-08 02:02:45,537 - __main__ - INFO - Epoch 45 Batch 40: Train Loss = 0.1494
2023-11-08 02:03:02,909 - __main__ - INFO - Epoch 45 Batch 60: Train Loss = 0.1292
2023-11-08 02:03:21,635 - __main__ - INFO - Epoch 45 Batch 80: Train Loss = 0.1569
2023-11-08 02:03:38,830 - __main__ - INFO - Epoch 45 Batch 100: Train Loss = 0.1663
2023-11-08 02:03:57,034 - __main__ - INFO - Epoch 45 Batch 120: Train Loss = 0.1618
2023-11-08 02:04:10,218 - __main__ - INFO - Epoch 45: Loss = 0.1584 Valid loss = 0.1421 roc = 0.9179
2023-11-08 02:04:11,200 - __main__ - INFO - Epoch 46 Batch 0: Train Loss = 0.2064
2023-11-08 02:04:28,624 - __main__ - INFO - Epoch 46 Batch 20: Train Loss = 0.0989
2023-11-08 02:04:46,853 - __main__ - INFO - Epoch 46 Batch 40: Train Loss = 0.1765
2023-11-08 02:05:03,271 - __main__ - INFO - Epoch 46 Batch 60: Train Loss = 0.1430
2023-11-08 02:05:20,386 - __main__ - INFO - Epoch 46 Batch 80: Train Loss = 0.1339
2023-11-08 02:05:36,968 - __main__ - INFO - Epoch 46 Batch 100: Train Loss = 0.1367
2023-11-08 02:05:55,481 - __main__ - INFO - Epoch 46 Batch 120: Train Loss = 0.1353
2023-11-08 02:06:09,693 - __main__ - INFO - Epoch 46: Loss = 0.1576 Valid loss = 0.1408 roc = 0.9213
2023-11-08 02:06:10,826 - __main__ - INFO - Epoch 47 Batch 0: Train Loss = 0.1800
2023-11-08 02:06:27,897 - __main__ - INFO - Epoch 47 Batch 20: Train Loss = 0.1347
2023-11-08 02:06:47,190 - __main__ - INFO - Epoch 47 Batch 40: Train Loss = 0.2277
2023-11-08 02:07:04,777 - __main__ - INFO - Epoch 47 Batch 60: Train Loss = 0.0960
2023-11-08 02:07:21,485 - __main__ - INFO - Epoch 47 Batch 80: Train Loss = 0.2009
2023-11-08 02:07:39,620 - __main__ - INFO - Epoch 47 Batch 100: Train Loss = 0.1143
2023-11-08 02:07:57,285 - __main__ - INFO - Epoch 47 Batch 120: Train Loss = 0.1581
2023-11-08 02:08:11,001 - __main__ - INFO - Epoch 47: Loss = 0.1580 Valid loss = 0.1407 roc = 0.9182
2023-11-08 02:08:12,031 - __main__ - INFO - Epoch 48 Batch 0: Train Loss = 0.1522
2023-11-08 02:08:30,293 - __main__ - INFO - Epoch 48 Batch 20: Train Loss = 0.1306
2023-11-08 02:08:48,996 - __main__ - INFO - Epoch 48 Batch 40: Train Loss = 0.1504
2023-11-08 02:09:05,144 - __main__ - INFO - Epoch 48 Batch 60: Train Loss = 0.1690
2023-11-08 02:09:22,891 - __main__ - INFO - Epoch 48 Batch 80: Train Loss = 0.1883
2023-11-08 02:09:40,016 - __main__ - INFO - Epoch 48 Batch 100: Train Loss = 0.1748
2023-11-08 02:09:56,785 - __main__ - INFO - Epoch 48 Batch 120: Train Loss = 0.1180
2023-11-08 02:10:11,002 - __main__ - INFO - Epoch 48: Loss = 0.1590 Valid loss = 0.1407 roc = 0.9175
2023-11-08 02:10:11,824 - __main__ - INFO - Epoch 49 Batch 0: Train Loss = 0.1161
2023-11-08 02:10:30,197 - __main__ - INFO - Epoch 49 Batch 20: Train Loss = 0.2290
2023-11-08 02:10:47,940 - __main__ - INFO - Epoch 49 Batch 40: Train Loss = 0.1595
2023-11-08 02:11:06,039 - __main__ - INFO - Epoch 49 Batch 60: Train Loss = 0.1409
2023-11-08 02:11:24,184 - __main__ - INFO - Epoch 49 Batch 80: Train Loss = 0.1321
2023-11-08 02:11:41,647 - __main__ - INFO - Epoch 49 Batch 100: Train Loss = 0.1087
2023-11-08 02:11:59,227 - __main__ - INFO - Epoch 49 Batch 120: Train Loss = 0.1435
2023-11-08 02:12:12,603 - __main__ - INFO - Epoch 49: Loss = 0.1560 Valid loss = 0.1409 roc = 0.9128
2023-11-08 02:12:13,648 - __main__ - INFO - Epoch 50 Batch 0: Train Loss = 0.1978
2023-11-08 02:12:31,935 - __main__ - INFO - Epoch 50 Batch 20: Train Loss = 0.1694
2023-11-08 02:12:50,315 - __main__ - INFO - Epoch 50 Batch 40: Train Loss = 0.1401
2023-11-08 02:13:06,257 - __main__ - INFO - Epoch 50 Batch 60: Train Loss = 0.2685
2023-11-08 02:13:23,204 - __main__ - INFO - Epoch 50 Batch 80: Train Loss = 0.1239
2023-11-08 02:13:40,310 - __main__ - INFO - Epoch 50 Batch 100: Train Loss = 0.1347
2023-11-08 02:13:57,278 - __main__ - INFO - Epoch 50 Batch 120: Train Loss = 0.1697
2023-11-08 02:14:11,326 - __main__ - INFO - Epoch 50: Loss = 0.1566 Valid loss = 0.1432 roc = 0.9162
2023-11-08 02:14:12,323 - __main__ - INFO - Epoch 51 Batch 0: Train Loss = 0.1612
2023-11-08 02:14:30,380 - __main__ - INFO - Epoch 51 Batch 20: Train Loss = 0.1588
2023-11-08 02:14:48,715 - __main__ - INFO - Epoch 51 Batch 40: Train Loss = 0.1455
2023-11-08 02:15:06,370 - __main__ - INFO - Epoch 51 Batch 60: Train Loss = 0.1525
2023-11-08 02:15:24,935 - __main__ - INFO - Epoch 51 Batch 80: Train Loss = 0.1111
2023-11-08 02:15:41,639 - __main__ - INFO - Epoch 51 Batch 100: Train Loss = 0.1830
2023-11-08 02:16:00,279 - __main__ - INFO - Epoch 51 Batch 120: Train Loss = 0.1053
2023-11-08 02:16:14,039 - __main__ - INFO - Epoch 51: Loss = 0.1579 Valid loss = 0.1408 roc = 0.9210
2023-11-08 02:16:14,979 - __main__ - INFO - Epoch 52 Batch 0: Train Loss = 0.1469
2023-11-08 02:16:32,583 - __main__ - INFO - Epoch 52 Batch 20: Train Loss = 0.1590
2023-11-08 02:16:51,515 - __main__ - INFO - Epoch 52 Batch 40: Train Loss = 0.1525
2023-11-08 02:17:09,532 - __main__ - INFO - Epoch 52 Batch 60: Train Loss = 0.1796
2023-11-08 02:17:27,073 - __main__ - INFO - Epoch 52 Batch 80: Train Loss = 0.1263
2023-11-08 02:17:44,390 - __main__ - INFO - Epoch 52 Batch 100: Train Loss = 0.1103
2023-11-08 02:18:03,225 - __main__ - INFO - Epoch 52 Batch 120: Train Loss = 0.1300
2023-11-08 02:18:16,548 - __main__ - INFO - Epoch 52: Loss = 0.1565 Valid loss = 0.1400 roc = 0.9232
2023-11-08 02:18:17,384 - __main__ - INFO - Epoch 53 Batch 0: Train Loss = 0.1455
2023-11-08 02:18:35,204 - __main__ - INFO - Epoch 53 Batch 20: Train Loss = 0.1856
2023-11-08 02:18:53,678 - __main__ - INFO - Epoch 53 Batch 40: Train Loss = 0.1237
2023-11-08 02:19:10,119 - __main__ - INFO - Epoch 53 Batch 60: Train Loss = 0.1660
2023-11-08 02:19:28,293 - __main__ - INFO - Epoch 53 Batch 80: Train Loss = 0.2006
2023-11-08 02:19:45,361 - __main__ - INFO - Epoch 53 Batch 100: Train Loss = 0.1306
2023-11-08 02:20:04,774 - __main__ - INFO - Epoch 53 Batch 120: Train Loss = 0.1629
2023-11-08 02:20:17,759 - __main__ - INFO - Epoch 53: Loss = 0.1570 Valid loss = 0.1384 roc = 0.9248
2023-11-08 02:20:18,904 - __main__ - INFO - Epoch 54 Batch 0: Train Loss = 0.1347
2023-11-08 02:20:35,792 - __main__ - INFO - Epoch 54 Batch 20: Train Loss = 0.1646
2023-11-08 02:20:53,094 - __main__ - INFO - Epoch 54 Batch 40: Train Loss = 0.1879
2023-11-08 02:21:10,655 - __main__ - INFO - Epoch 54 Batch 60: Train Loss = 0.1634
2023-11-08 02:21:28,592 - __main__ - INFO - Epoch 54 Batch 80: Train Loss = 0.1477
2023-11-08 02:21:45,269 - __main__ - INFO - Epoch 54 Batch 100: Train Loss = 0.1686
2023-11-08 02:22:03,051 - __main__ - INFO - Epoch 54 Batch 120: Train Loss = 0.1337
2023-11-08 02:22:16,128 - __main__ - INFO - Epoch 54: Loss = 0.1576 Valid loss = 0.1400 roc = 0.9262
2023-11-08 02:22:17,144 - __main__ - INFO - Epoch 55 Batch 0: Train Loss = 0.1737
2023-11-08 02:22:34,858 - __main__ - INFO - Epoch 55 Batch 20: Train Loss = 0.1266
2023-11-08 02:22:52,474 - __main__ - INFO - Epoch 55 Batch 40: Train Loss = 0.1877
2023-11-08 02:23:09,051 - __main__ - INFO - Epoch 55 Batch 60: Train Loss = 0.2059
2023-11-08 02:23:27,054 - __main__ - INFO - Epoch 55 Batch 80: Train Loss = 0.1094
2023-11-08 02:23:45,741 - __main__ - INFO - Epoch 55 Batch 100: Train Loss = 0.1213
2023-11-08 02:24:03,543 - __main__ - INFO - Epoch 55 Batch 120: Train Loss = 0.1769
2023-11-08 02:24:17,233 - __main__ - INFO - Epoch 55: Loss = 0.1595 Valid loss = 0.1389 roc = 0.9255
2023-11-08 02:24:18,016 - __main__ - INFO - Epoch 56 Batch 0: Train Loss = 0.1349
2023-11-08 02:24:35,206 - __main__ - INFO - Epoch 56 Batch 20: Train Loss = 0.1667
2023-11-08 02:24:53,371 - __main__ - INFO - Epoch 56 Batch 40: Train Loss = 0.1795
2023-11-08 02:25:10,861 - __main__ - INFO - Epoch 56 Batch 60: Train Loss = 0.1789
2023-11-08 02:25:27,685 - __main__ - INFO - Epoch 56 Batch 80: Train Loss = 0.0901
2023-11-08 02:25:45,991 - __main__ - INFO - Epoch 56 Batch 100: Train Loss = 0.1052
2023-11-08 02:26:02,125 - __main__ - INFO - Epoch 56 Batch 120: Train Loss = 0.1164
2023-11-08 02:26:15,589 - __main__ - INFO - Epoch 56: Loss = 0.1592 Valid loss = 0.1431 roc = 0.9255
2023-11-08 02:26:16,643 - __main__ - INFO - Epoch 57 Batch 0: Train Loss = 0.1859
2023-11-08 02:26:34,807 - __main__ - INFO - Epoch 57 Batch 20: Train Loss = 0.1730
2023-11-08 02:26:53,080 - __main__ - INFO - Epoch 57 Batch 40: Train Loss = 0.1687
2023-11-08 02:27:09,329 - __main__ - INFO - Epoch 57 Batch 60: Train Loss = 0.1086
2023-11-08 02:27:26,018 - __main__ - INFO - Epoch 57 Batch 80: Train Loss = 0.1798
2023-11-08 02:27:43,579 - __main__ - INFO - Epoch 57 Batch 100: Train Loss = 0.1477
2023-11-08 02:28:01,155 - __main__ - INFO - Epoch 57 Batch 120: Train Loss = 0.1591
2023-11-08 02:28:13,907 - __main__ - INFO - Epoch 57: Loss = 0.1591 Valid loss = 0.1374 roc = 0.9240
2023-11-08 02:28:14,799 - __main__ - INFO - Epoch 58 Batch 0: Train Loss = 0.1828
2023-11-08 02:28:33,877 - __main__ - INFO - Epoch 58 Batch 20: Train Loss = 0.1187
2023-11-08 02:28:51,932 - __main__ - INFO - Epoch 58 Batch 40: Train Loss = 0.1721
2023-11-08 02:29:10,357 - __main__ - INFO - Epoch 58 Batch 60: Train Loss = 0.1700
2023-11-08 02:29:27,427 - __main__ - INFO - Epoch 58 Batch 80: Train Loss = 0.1711
2023-11-08 02:29:45,137 - __main__ - INFO - Epoch 58 Batch 100: Train Loss = 0.0824
2023-11-08 02:30:02,946 - __main__ - INFO - Epoch 58 Batch 120: Train Loss = 0.2081
2023-11-08 02:30:16,421 - __main__ - INFO - Epoch 58: Loss = 0.1584 Valid loss = 0.1371 roc = 0.9266
2023-11-08 02:30:17,100 - __main__ - INFO - Epoch 59 Batch 0: Train Loss = 0.1318
2023-11-08 02:30:35,340 - __main__ - INFO - Epoch 59 Batch 20: Train Loss = 0.1248
2023-11-08 02:30:53,580 - __main__ - INFO - Epoch 59 Batch 40: Train Loss = 0.1787
2023-11-08 02:31:09,990 - __main__ - INFO - Epoch 59 Batch 60: Train Loss = 0.1376
2023-11-08 02:31:26,351 - __main__ - INFO - Epoch 59 Batch 80: Train Loss = 0.1224
2023-11-08 02:31:45,150 - __main__ - INFO - Epoch 59 Batch 100: Train Loss = 0.1107
2023-11-08 02:32:03,266 - __main__ - INFO - Epoch 59 Batch 120: Train Loss = 0.1030
2023-11-08 02:32:17,502 - __main__ - INFO - Epoch 59: Loss = 0.1556 Valid loss = 0.1371 roc = 0.9253
2023-11-08 02:32:18,478 - __main__ - INFO - Epoch 60 Batch 0: Train Loss = 0.1139
2023-11-08 02:32:36,355 - __main__ - INFO - Epoch 60 Batch 20: Train Loss = 0.1639
2023-11-08 02:32:54,530 - __main__ - INFO - Epoch 60 Batch 40: Train Loss = 0.1377
2023-11-08 02:33:12,488 - __main__ - INFO - Epoch 60 Batch 60: Train Loss = 0.1657
2023-11-08 02:33:30,928 - __main__ - INFO - Epoch 60 Batch 80: Train Loss = 0.1147
2023-11-08 02:33:49,310 - __main__ - INFO - Epoch 60 Batch 100: Train Loss = 0.1814
2023-11-08 02:34:06,626 - __main__ - INFO - Epoch 60 Batch 120: Train Loss = 0.2032
2023-11-08 02:34:20,371 - __main__ - INFO - Epoch 60: Loss = 0.1554 Valid loss = 0.1413 roc = 0.9219
2023-11-08 02:34:21,296 - __main__ - INFO - Epoch 61 Batch 0: Train Loss = 0.1867
2023-11-08 02:34:38,505 - __main__ - INFO - Epoch 61 Batch 20: Train Loss = 0.1067
2023-11-08 02:34:56,292 - __main__ - INFO - Epoch 61 Batch 40: Train Loss = 0.1416
2023-11-08 02:35:13,977 - __main__ - INFO - Epoch 61 Batch 60: Train Loss = 0.1448
2023-11-08 02:35:32,591 - __main__ - INFO - Epoch 61 Batch 80: Train Loss = 0.1256
2023-11-08 02:35:51,037 - __main__ - INFO - Epoch 61 Batch 100: Train Loss = 0.1725
2023-11-08 02:36:08,096 - __main__ - INFO - Epoch 61 Batch 120: Train Loss = 0.1841
2023-11-08 02:36:21,864 - __main__ - INFO - Epoch 61: Loss = 0.1543 Valid loss = 0.1391 roc = 0.9238
2023-11-08 02:36:22,907 - __main__ - INFO - Epoch 62 Batch 0: Train Loss = 0.1130
2023-11-08 02:36:39,273 - __main__ - INFO - Epoch 62 Batch 20: Train Loss = 0.1441
2023-11-08 02:36:57,062 - __main__ - INFO - Epoch 62 Batch 40: Train Loss = 0.1973
2023-11-08 02:37:15,025 - __main__ - INFO - Epoch 62 Batch 60: Train Loss = 0.2071
2023-11-08 02:37:31,852 - __main__ - INFO - Epoch 62 Batch 80: Train Loss = 0.1519
2023-11-08 02:37:49,857 - __main__ - INFO - Epoch 62 Batch 100: Train Loss = 0.1505
2023-11-08 02:38:08,653 - __main__ - INFO - Epoch 62 Batch 120: Train Loss = 0.1072
2023-11-08 02:38:21,913 - __main__ - INFO - Epoch 62: Loss = 0.1564 Valid loss = 0.1371 roc = 0.9268
2023-11-08 02:38:22,894 - __main__ - INFO - Epoch 63 Batch 0: Train Loss = 0.1411
2023-11-08 02:38:39,750 - __main__ - INFO - Epoch 63 Batch 20: Train Loss = 0.1790
2023-11-08 02:38:57,313 - __main__ - INFO - Epoch 63 Batch 40: Train Loss = 0.1118
2023-11-08 02:39:13,606 - __main__ - INFO - Epoch 63 Batch 60: Train Loss = 0.1454
2023-11-08 02:39:29,730 - __main__ - INFO - Epoch 63 Batch 80: Train Loss = 0.1984
2023-11-08 02:39:48,340 - __main__ - INFO - Epoch 63 Batch 100: Train Loss = 0.1888
2023-11-08 02:40:04,451 - __main__ - INFO - Epoch 63 Batch 120: Train Loss = 0.1117
2023-11-08 02:40:18,634 - __main__ - INFO - Epoch 63: Loss = 0.1563 Valid loss = 0.1379 roc = 0.9230
2023-11-08 02:40:19,456 - __main__ - INFO - Epoch 64 Batch 0: Train Loss = 0.2269
2023-11-08 02:40:35,776 - __main__ - INFO - Epoch 64 Batch 20: Train Loss = 0.1209
2023-11-08 02:40:54,120 - __main__ - INFO - Epoch 64 Batch 40: Train Loss = 0.1267
2023-11-08 02:41:12,845 - __main__ - INFO - Epoch 64 Batch 60: Train Loss = 0.1201
2023-11-08 02:41:30,114 - __main__ - INFO - Epoch 64 Batch 80: Train Loss = 0.1436
2023-11-08 02:41:46,955 - __main__ - INFO - Epoch 64 Batch 100: Train Loss = 0.1077
2023-11-08 02:42:04,291 - __main__ - INFO - Epoch 64 Batch 120: Train Loss = 0.1609
2023-11-08 02:42:18,661 - __main__ - INFO - Epoch 64: Loss = 0.1548 Valid loss = 0.1374 roc = 0.9238
2023-11-08 02:42:19,573 - __main__ - INFO - Epoch 65 Batch 0: Train Loss = 0.2171
2023-11-08 02:42:37,570 - __main__ - INFO - Epoch 65 Batch 20: Train Loss = 0.1347
2023-11-08 02:42:53,239 - __main__ - INFO - Epoch 65 Batch 40: Train Loss = 0.1180
2023-11-08 02:43:09,995 - __main__ - INFO - Epoch 65 Batch 60: Train Loss = 0.1293
2023-11-08 02:43:26,867 - __main__ - INFO - Epoch 65 Batch 80: Train Loss = 0.1760
2023-11-08 02:43:45,895 - __main__ - INFO - Epoch 65 Batch 100: Train Loss = 0.1862
2023-11-08 02:44:03,527 - __main__ - INFO - Epoch 65 Batch 120: Train Loss = 0.1372
2023-11-08 02:44:15,901 - __main__ - INFO - Epoch 65: Loss = 0.1548 Valid loss = 0.1365 roc = 0.9247
2023-11-08 02:44:16,869 - __main__ - INFO - Epoch 66 Batch 0: Train Loss = 0.1415
2023-11-08 02:44:34,637 - __main__ - INFO - Epoch 66 Batch 20: Train Loss = 0.1991
2023-11-08 02:44:51,536 - __main__ - INFO - Epoch 66 Batch 40: Train Loss = 0.1544
2023-11-08 02:45:09,333 - __main__ - INFO - Epoch 66 Batch 60: Train Loss = 0.1502
2023-11-08 02:45:25,765 - __main__ - INFO - Epoch 66 Batch 80: Train Loss = 0.1786
2023-11-08 02:45:44,206 - __main__ - INFO - Epoch 66 Batch 100: Train Loss = 0.1406
2023-11-08 02:46:01,242 - __main__ - INFO - Epoch 66 Batch 120: Train Loss = 0.1287
2023-11-08 02:46:13,672 - __main__ - INFO - Epoch 66: Loss = 0.1538 Valid loss = 0.1355 roc = 0.9239
2023-11-08 02:46:14,703 - __main__ - INFO - Epoch 67 Batch 0: Train Loss = 0.0970
2023-11-08 02:46:32,755 - __main__ - INFO - Epoch 67 Batch 20: Train Loss = 0.1563
2023-11-08 02:46:50,215 - __main__ - INFO - Epoch 67 Batch 40: Train Loss = 0.1433
2023-11-08 02:47:07,915 - __main__ - INFO - Epoch 67 Batch 60: Train Loss = 0.1441
2023-11-08 02:47:25,957 - __main__ - INFO - Epoch 67 Batch 80: Train Loss = 0.1403
2023-11-08 02:47:43,868 - __main__ - INFO - Epoch 67 Batch 100: Train Loss = 0.1345
2023-11-08 02:48:00,145 - __main__ - INFO - Epoch 67 Batch 120: Train Loss = 0.1687
2023-11-08 02:48:14,202 - __main__ - INFO - Epoch 67: Loss = 0.1509 Valid loss = 0.1382 roc = 0.9255
2023-11-08 02:48:14,852 - __main__ - INFO - Epoch 68 Batch 0: Train Loss = 0.1411
2023-11-08 02:48:32,002 - __main__ - INFO - Epoch 68 Batch 20: Train Loss = 0.1764
2023-11-08 02:48:49,959 - __main__ - INFO - Epoch 68 Batch 40: Train Loss = 0.1814
2023-11-08 02:49:06,748 - __main__ - INFO - Epoch 68 Batch 60: Train Loss = 0.1571
2023-11-08 02:49:24,272 - __main__ - INFO - Epoch 68 Batch 80: Train Loss = 0.2151
2023-11-08 02:49:41,221 - __main__ - INFO - Epoch 68 Batch 100: Train Loss = 0.1793
2023-11-08 02:49:57,890 - __main__ - INFO - Epoch 68 Batch 120: Train Loss = 0.2063
2023-11-08 02:50:11,907 - __main__ - INFO - Epoch 68: Loss = 0.1505 Valid loss = 0.1351 roc = 0.9235
2023-11-08 02:50:12,783 - __main__ - INFO - Epoch 69 Batch 0: Train Loss = 0.1747
2023-11-08 02:50:30,007 - __main__ - INFO - Epoch 69 Batch 20: Train Loss = 0.1867
2023-11-08 02:50:47,793 - __main__ - INFO - Epoch 69 Batch 40: Train Loss = 0.1824
2023-11-08 02:51:06,150 - __main__ - INFO - Epoch 69 Batch 60: Train Loss = 0.1129
2023-11-08 02:51:23,558 - __main__ - INFO - Epoch 69 Batch 80: Train Loss = 0.1509
2023-11-08 02:51:42,213 - __main__ - INFO - Epoch 69 Batch 100: Train Loss = 0.2071
2023-11-08 02:52:00,133 - __main__ - INFO - Epoch 69 Batch 120: Train Loss = 0.1456
2023-11-08 02:52:12,368 - __main__ - INFO - Epoch 69: Loss = 0.1493 Valid loss = 0.1353 roc = 0.9264
2023-11-08 02:52:13,417 - __main__ - INFO - Epoch 70 Batch 0: Train Loss = 0.1793
2023-11-08 02:52:31,042 - __main__ - INFO - Epoch 70 Batch 20: Train Loss = 0.1893
2023-11-08 02:52:47,986 - __main__ - INFO - Epoch 70 Batch 40: Train Loss = 0.1256
2023-11-08 02:53:06,904 - __main__ - INFO - Epoch 70 Batch 60: Train Loss = 0.1748
2023-11-08 02:53:24,027 - __main__ - INFO - Epoch 70 Batch 80: Train Loss = 0.1497
2023-11-08 02:53:40,371 - __main__ - INFO - Epoch 70 Batch 100: Train Loss = 0.1574
2023-11-08 02:53:57,947 - __main__ - INFO - Epoch 70 Batch 120: Train Loss = 0.1027
2023-11-08 02:54:10,537 - __main__ - INFO - Epoch 70: Loss = 0.1503 Valid loss = 0.1383 roc = 0.9241
2023-11-08 02:54:11,247 - __main__ - INFO - Epoch 71 Batch 0: Train Loss = 0.1419
2023-11-08 02:54:29,368 - __main__ - INFO - Epoch 71 Batch 20: Train Loss = 0.1279
2023-11-08 02:54:46,076 - __main__ - INFO - Epoch 71 Batch 40: Train Loss = 0.1703
2023-11-08 02:55:04,728 - __main__ - INFO - Epoch 71 Batch 60: Train Loss = 0.1751
2023-11-08 02:55:22,727 - __main__ - INFO - Epoch 71 Batch 80: Train Loss = 0.1274
2023-11-08 02:55:39,522 - __main__ - INFO - Epoch 71 Batch 100: Train Loss = 0.2008
2023-11-08 02:55:56,270 - __main__ - INFO - Epoch 71 Batch 120: Train Loss = 0.1681
2023-11-08 02:56:10,454 - __main__ - INFO - Epoch 71: Loss = 0.1535 Valid loss = 0.1408 roc = 0.9223
2023-11-08 02:56:11,555 - __main__ - INFO - Epoch 72 Batch 0: Train Loss = 0.1613
2023-11-08 02:56:29,523 - __main__ - INFO - Epoch 72 Batch 20: Train Loss = 0.1867
2023-11-08 02:56:47,502 - __main__ - INFO - Epoch 72 Batch 40: Train Loss = 0.1882
2023-11-08 02:57:05,038 - __main__ - INFO - Epoch 72 Batch 60: Train Loss = 0.1351
2023-11-08 02:57:21,780 - __main__ - INFO - Epoch 72 Batch 80: Train Loss = 0.1090
2023-11-08 02:57:40,652 - __main__ - INFO - Epoch 72 Batch 100: Train Loss = 0.1799
2023-11-08 02:57:57,696 - __main__ - INFO - Epoch 72 Batch 120: Train Loss = 0.1619
2023-11-08 02:58:10,540 - __main__ - INFO - Epoch 72: Loss = 0.1514 Valid loss = 0.1384 roc = 0.9235
2023-11-08 02:58:11,460 - __main__ - INFO - Epoch 73 Batch 0: Train Loss = 0.1618
2023-11-08 02:58:29,508 - __main__ - INFO - Epoch 73 Batch 20: Train Loss = 0.1411
2023-11-08 02:58:46,544 - __main__ - INFO - Epoch 73 Batch 40: Train Loss = 0.1660
2023-11-08 02:59:04,937 - __main__ - INFO - Epoch 73 Batch 60: Train Loss = 0.1114
2023-11-08 02:59:22,294 - __main__ - INFO - Epoch 73 Batch 80: Train Loss = 0.1483
2023-11-08 02:59:39,700 - __main__ - INFO - Epoch 73 Batch 100: Train Loss = 0.0946
2023-11-08 02:59:56,991 - __main__ - INFO - Epoch 73 Batch 120: Train Loss = 0.1398
2023-11-08 03:00:10,498 - __main__ - INFO - Epoch 73: Loss = 0.1511 Valid loss = 0.1373 roc = 0.9233
2023-11-08 03:00:11,510 - __main__ - INFO - Epoch 74 Batch 0: Train Loss = 0.1507
2023-11-08 03:00:29,279 - __main__ - INFO - Epoch 74 Batch 20: Train Loss = 0.1963
2023-11-08 03:00:47,467 - __main__ - INFO - Epoch 74 Batch 40: Train Loss = 0.1835
2023-11-08 03:01:06,312 - __main__ - INFO - Epoch 74 Batch 60: Train Loss = 0.2043
2023-11-08 03:01:23,876 - __main__ - INFO - Epoch 74 Batch 80: Train Loss = 0.1486
2023-11-08 03:01:41,150 - __main__ - INFO - Epoch 74 Batch 100: Train Loss = 0.1076
2023-11-08 03:01:58,296 - __main__ - INFO - Epoch 74 Batch 120: Train Loss = 0.2629
2023-11-08 03:02:09,774 - __main__ - INFO - Epoch 74: Loss = 0.1483 Valid loss = 0.1369 roc = 0.9241
2023-11-08 03:02:10,401 - __main__ - INFO - Epoch 75 Batch 0: Train Loss = 0.1955
2023-11-08 03:02:28,894 - __main__ - INFO - Epoch 75 Batch 20: Train Loss = 0.1486
2023-11-08 03:02:46,603 - __main__ - INFO - Epoch 75 Batch 40: Train Loss = 0.1795
2023-11-08 03:03:04,817 - __main__ - INFO - Epoch 75 Batch 60: Train Loss = 0.1308
2023-11-08 03:03:22,936 - __main__ - INFO - Epoch 75 Batch 80: Train Loss = 0.1307
2023-11-08 03:03:40,558 - __main__ - INFO - Epoch 75 Batch 100: Train Loss = 0.1308
2023-11-08 03:03:59,088 - __main__ - INFO - Epoch 75 Batch 120: Train Loss = 0.1490
2023-11-08 03:04:12,616 - __main__ - INFO - Epoch 75: Loss = 0.1503 Valid loss = 0.1378 roc = 0.9211
2023-11-08 03:04:13,338 - __main__ - INFO - Epoch 76 Batch 0: Train Loss = 0.1325
2023-11-08 03:04:30,957 - __main__ - INFO - Epoch 76 Batch 20: Train Loss = 0.1270
2023-11-08 03:04:48,321 - __main__ - INFO - Epoch 76 Batch 40: Train Loss = 0.1627
2023-11-08 03:05:06,047 - __main__ - INFO - Epoch 76 Batch 60: Train Loss = 0.1625
2023-11-08 03:05:24,084 - __main__ - INFO - Epoch 76 Batch 80: Train Loss = 0.1186
2023-11-08 03:05:43,245 - __main__ - INFO - Epoch 76 Batch 100: Train Loss = 0.1242
2023-11-08 03:06:00,285 - __main__ - INFO - Epoch 76 Batch 120: Train Loss = 0.1587
2023-11-08 03:06:13,746 - __main__ - INFO - Epoch 76: Loss = 0.1512 Valid loss = 0.1370 roc = 0.9246
2023-11-08 03:06:14,368 - __main__ - INFO - Epoch 77 Batch 0: Train Loss = 0.1171
2023-11-08 03:06:32,032 - __main__ - INFO - Epoch 77 Batch 20: Train Loss = 0.1546
2023-11-08 03:06:48,951 - __main__ - INFO - Epoch 77 Batch 40: Train Loss = 0.2615
2023-11-08 03:07:07,212 - __main__ - INFO - Epoch 77 Batch 60: Train Loss = 0.1984
2023-11-08 03:07:24,077 - __main__ - INFO - Epoch 77 Batch 80: Train Loss = 0.1417
2023-11-08 03:07:39,531 - __main__ - INFO - Epoch 77 Batch 100: Train Loss = 0.1255
2023-11-08 03:07:57,013 - __main__ - INFO - Epoch 77 Batch 120: Train Loss = 0.0751
2023-11-08 03:08:10,323 - __main__ - INFO - Epoch 77: Loss = 0.1509 Valid loss = 0.1370 roc = 0.9239
2023-11-08 03:08:11,320 - __main__ - INFO - Epoch 78 Batch 0: Train Loss = 0.0732
2023-11-08 03:08:29,275 - __main__ - INFO - Epoch 78 Batch 20: Train Loss = 0.1333
2023-11-08 03:08:46,711 - __main__ - INFO - Epoch 78 Batch 40: Train Loss = 0.1999
2023-11-08 03:09:04,498 - __main__ - INFO - Epoch 78 Batch 60: Train Loss = 0.0960
2023-11-08 03:09:21,166 - __main__ - INFO - Epoch 78 Batch 80: Train Loss = 0.1538
2023-11-08 03:09:38,012 - __main__ - INFO - Epoch 78 Batch 100: Train Loss = 0.1386
2023-11-08 03:09:54,827 - __main__ - INFO - Epoch 78 Batch 120: Train Loss = 0.1656
2023-11-08 03:10:09,022 - __main__ - INFO - Epoch 78: Loss = 0.1495 Valid loss = 0.1366 roc = 0.9257
2023-11-08 03:10:09,965 - __main__ - INFO - Epoch 79 Batch 0: Train Loss = 0.1288
2023-11-08 03:10:26,833 - __main__ - INFO - Epoch 79 Batch 20: Train Loss = 0.0904
2023-11-08 03:10:44,299 - __main__ - INFO - Epoch 79 Batch 40: Train Loss = 0.1336
2023-11-08 03:11:01,256 - __main__ - INFO - Epoch 79 Batch 60: Train Loss = 0.0708
2023-11-08 03:11:18,870 - __main__ - INFO - Epoch 79 Batch 80: Train Loss = 0.1430
2023-11-08 03:11:36,801 - __main__ - INFO - Epoch 79 Batch 100: Train Loss = 0.1486
2023-11-08 03:11:55,115 - __main__ - INFO - Epoch 79 Batch 120: Train Loss = 0.0937
2023-11-08 03:12:08,128 - __main__ - INFO - Epoch 79: Loss = 0.1484 Valid loss = 0.1373 roc = 0.9259
2023-11-08 03:12:08,764 - __main__ - INFO - Epoch 80 Batch 0: Train Loss = 0.1105
2023-11-08 03:12:25,957 - __main__ - INFO - Epoch 80 Batch 20: Train Loss = 0.1617
2023-11-08 03:12:43,742 - __main__ - INFO - Epoch 80 Batch 40: Train Loss = 0.1274
2023-11-08 03:13:01,348 - __main__ - INFO - Epoch 80 Batch 60: Train Loss = 0.1912
2023-11-08 03:13:19,767 - __main__ - INFO - Epoch 80 Batch 80: Train Loss = 0.1525
2023-11-08 03:13:37,812 - __main__ - INFO - Epoch 80 Batch 100: Train Loss = 0.1194
2023-11-08 03:13:55,207 - __main__ - INFO - Epoch 80 Batch 120: Train Loss = 0.1281
2023-11-08 03:14:08,234 - __main__ - INFO - Epoch 80: Loss = 0.1481 Valid loss = 0.1356 roc = 0.9270
2023-11-08 03:14:09,265 - __main__ - INFO - Epoch 81 Batch 0: Train Loss = 0.1229
2023-11-08 03:14:26,818 - __main__ - INFO - Epoch 81 Batch 20: Train Loss = 0.1110
2023-11-08 03:14:44,258 - __main__ - INFO - Epoch 81 Batch 40: Train Loss = 0.2074
2023-11-08 03:15:02,858 - __main__ - INFO - Epoch 81 Batch 60: Train Loss = 0.1676
2023-11-08 03:15:20,315 - __main__ - INFO - Epoch 81 Batch 80: Train Loss = 0.1688
2023-11-08 03:15:37,635 - __main__ - INFO - Epoch 81 Batch 100: Train Loss = 0.1103
2023-11-08 03:15:54,213 - __main__ - INFO - Epoch 81 Batch 120: Train Loss = 0.1579
2023-11-08 03:16:06,892 - __main__ - INFO - Epoch 81: Loss = 0.1492 Valid loss = 0.1364 roc = 0.9265
2023-11-08 03:16:07,539 - __main__ - INFO - Epoch 82 Batch 0: Train Loss = 0.1200
2023-11-08 03:16:25,445 - __main__ - INFO - Epoch 82 Batch 20: Train Loss = 0.1371
2023-11-08 03:16:42,769 - __main__ - INFO - Epoch 82 Batch 40: Train Loss = 0.1175
2023-11-08 03:17:00,341 - __main__ - INFO - Epoch 82 Batch 60: Train Loss = 0.1928
2023-11-08 03:17:17,129 - __main__ - INFO - Epoch 82 Batch 80: Train Loss = 0.2100
2023-11-08 03:17:34,683 - __main__ - INFO - Epoch 82 Batch 100: Train Loss = 0.1160
2023-11-08 03:17:52,767 - __main__ - INFO - Epoch 82 Batch 120: Train Loss = 0.1476
2023-11-08 03:18:05,858 - __main__ - INFO - Epoch 82: Loss = 0.1456 Valid loss = 0.1348 roc = 0.9243
2023-11-08 03:18:06,712 - __main__ - INFO - Epoch 83 Batch 0: Train Loss = 0.1457
2023-11-08 03:18:24,317 - __main__ - INFO - Epoch 83 Batch 20: Train Loss = 0.1159
2023-11-08 03:18:40,696 - __main__ - INFO - Epoch 83 Batch 40: Train Loss = 0.1723
2023-11-08 03:18:58,166 - __main__ - INFO - Epoch 83 Batch 60: Train Loss = 0.1521
2023-11-08 03:19:14,128 - __main__ - INFO - Epoch 83 Batch 80: Train Loss = 0.1391
2023-11-08 03:19:31,997 - __main__ - INFO - Epoch 83 Batch 100: Train Loss = 0.1108
2023-11-08 03:19:50,923 - __main__ - INFO - Epoch 83 Batch 120: Train Loss = 0.1421
2023-11-08 03:20:05,325 - __main__ - INFO - Epoch 83: Loss = 0.1485 Valid loss = 0.1352 roc = 0.9247
2023-11-08 03:20:06,032 - __main__ - INFO - Epoch 84 Batch 0: Train Loss = 0.0829
2023-11-08 03:20:24,278 - __main__ - INFO - Epoch 84 Batch 20: Train Loss = 0.1209
2023-11-08 03:20:42,141 - __main__ - INFO - Epoch 84 Batch 40: Train Loss = 0.1598
2023-11-08 03:20:58,771 - __main__ - INFO - Epoch 84 Batch 60: Train Loss = 0.0930
2023-11-08 03:21:15,652 - __main__ - INFO - Epoch 84 Batch 80: Train Loss = 0.1195
2023-11-08 03:21:33,307 - __main__ - INFO - Epoch 84 Batch 100: Train Loss = 0.2139
2023-11-08 03:21:50,241 - __main__ - INFO - Epoch 84 Batch 120: Train Loss = 0.2470
2023-11-08 03:22:03,893 - __main__ - INFO - Epoch 84: Loss = 0.1471 Valid loss = 0.1340 roc = 0.9260
2023-11-08 03:22:04,507 - __main__ - INFO - Epoch 85 Batch 0: Train Loss = 0.1724
2023-11-08 03:22:21,137 - __main__ - INFO - Epoch 85 Batch 20: Train Loss = 0.1579
2023-11-08 03:22:38,751 - __main__ - INFO - Epoch 85 Batch 40: Train Loss = 0.1517
2023-11-08 03:22:56,567 - __main__ - INFO - Epoch 85 Batch 60: Train Loss = 0.1443
2023-11-08 03:23:14,035 - __main__ - INFO - Epoch 85 Batch 80: Train Loss = 0.1071
2023-11-08 03:23:31,929 - __main__ - INFO - Epoch 85 Batch 100: Train Loss = 0.1522
2023-11-08 03:23:49,297 - __main__ - INFO - Epoch 85 Batch 120: Train Loss = 0.1808
2023-11-08 03:24:01,134 - __main__ - INFO - Epoch 85: Loss = 0.1468 Valid loss = 0.1386 roc = 0.9224
2023-11-08 03:24:02,048 - __main__ - INFO - Epoch 86 Batch 0: Train Loss = 0.1341
2023-11-08 03:24:18,851 - __main__ - INFO - Epoch 86 Batch 20: Train Loss = 0.1208
2023-11-08 03:24:35,671 - __main__ - INFO - Epoch 86 Batch 40: Train Loss = 0.1833
2023-11-08 03:24:53,203 - __main__ - INFO - Epoch 86 Batch 60: Train Loss = 0.1824
2023-11-08 03:25:11,149 - __main__ - INFO - Epoch 86 Batch 80: Train Loss = 0.1261
2023-11-08 03:25:29,191 - __main__ - INFO - Epoch 86 Batch 100: Train Loss = 0.1856
2023-11-08 03:25:47,815 - __main__ - INFO - Epoch 86 Batch 120: Train Loss = 0.1612
2023-11-08 03:26:01,533 - __main__ - INFO - Epoch 86: Loss = 0.1516 Valid loss = 0.1355 roc = 0.9253
2023-11-08 03:26:02,323 - __main__ - INFO - Epoch 87 Batch 0: Train Loss = 0.0967
2023-11-08 03:26:19,164 - __main__ - INFO - Epoch 87 Batch 20: Train Loss = 0.1345
2023-11-08 03:26:37,823 - __main__ - INFO - Epoch 87 Batch 40: Train Loss = 0.1045
2023-11-08 03:26:55,283 - __main__ - INFO - Epoch 87 Batch 60: Train Loss = 0.1112
2023-11-08 03:27:12,402 - __main__ - INFO - Epoch 87 Batch 80: Train Loss = 0.1180
2023-11-08 03:27:29,604 - __main__ - INFO - Epoch 87 Batch 100: Train Loss = 0.1826
2023-11-08 03:27:48,024 - __main__ - INFO - Epoch 87 Batch 120: Train Loss = 0.1617
2023-11-08 03:28:00,762 - __main__ - INFO - Epoch 87: Loss = 0.1489 Valid loss = 0.1351 roc = 0.9285
2023-11-08 03:28:01,661 - __main__ - INFO - Epoch 88 Batch 0: Train Loss = 0.1346
2023-11-08 03:28:18,763 - __main__ - INFO - Epoch 88 Batch 20: Train Loss = 0.1559
2023-11-08 03:28:36,184 - __main__ - INFO - Epoch 88 Batch 40: Train Loss = 0.1415
2023-11-08 03:28:54,236 - __main__ - INFO - Epoch 88 Batch 60: Train Loss = 0.1205
2023-11-08 03:29:12,040 - __main__ - INFO - Epoch 88 Batch 80: Train Loss = 0.1338
2023-11-08 03:29:30,549 - __main__ - INFO - Epoch 88 Batch 100: Train Loss = 0.1102
2023-11-08 03:29:46,994 - __main__ - INFO - Epoch 88 Batch 120: Train Loss = 0.1533
2023-11-08 03:29:59,728 - __main__ - INFO - Epoch 88: Loss = 0.1498 Valid loss = 0.1360 roc = 0.9262
2023-11-08 03:30:01,032 - __main__ - INFO - Epoch 89 Batch 0: Train Loss = 0.1043
2023-11-08 03:30:17,999 - __main__ - INFO - Epoch 89 Batch 20: Train Loss = 0.1784
2023-11-08 03:30:35,623 - __main__ - INFO - Epoch 89 Batch 40: Train Loss = 0.1425
2023-11-08 03:30:53,103 - __main__ - INFO - Epoch 89 Batch 60: Train Loss = 0.1226
2023-11-08 03:31:10,886 - __main__ - INFO - Epoch 89 Batch 80: Train Loss = 0.1316
2023-11-08 03:31:27,811 - __main__ - INFO - Epoch 89 Batch 100: Train Loss = 0.1412
2023-11-08 03:31:45,575 - __main__ - INFO - Epoch 89 Batch 120: Train Loss = 0.2151
2023-11-08 03:31:59,771 - __main__ - INFO - Epoch 89: Loss = 0.1465 Valid loss = 0.1366 roc = 0.9266
2023-11-08 03:32:00,380 - __main__ - INFO - Epoch 90 Batch 0: Train Loss = 0.1135
2023-11-08 03:32:19,462 - __main__ - INFO - Epoch 90 Batch 20: Train Loss = 0.1212
2023-11-08 03:32:37,767 - __main__ - INFO - Epoch 90 Batch 40: Train Loss = 0.1580
2023-11-08 03:32:56,352 - __main__ - INFO - Epoch 90 Batch 60: Train Loss = 0.1667
2023-11-08 03:33:13,301 - __main__ - INFO - Epoch 90 Batch 80: Train Loss = 0.1577
2023-11-08 03:33:32,444 - __main__ - INFO - Epoch 90 Batch 100: Train Loss = 0.1372
2023-11-08 03:33:49,198 - __main__ - INFO - Epoch 90 Batch 120: Train Loss = 0.1654
2023-11-08 03:34:02,531 - __main__ - INFO - Epoch 90: Loss = 0.1473 Valid loss = 0.1358 roc = 0.9258
2023-11-08 03:34:03,765 - __main__ - INFO - Epoch 91 Batch 0: Train Loss = 0.1768
2023-11-08 03:34:22,222 - __main__ - INFO - Epoch 91 Batch 20: Train Loss = 0.1077
2023-11-08 03:34:39,936 - __main__ - INFO - Epoch 91 Batch 40: Train Loss = 0.1705
2023-11-08 03:34:57,710 - __main__ - INFO - Epoch 91 Batch 60: Train Loss = 0.1092
2023-11-08 03:35:15,586 - __main__ - INFO - Epoch 91 Batch 80: Train Loss = 0.1356
2023-11-08 03:35:33,822 - __main__ - INFO - Epoch 91 Batch 100: Train Loss = 0.1970
2023-11-08 03:35:51,959 - __main__ - INFO - Epoch 91 Batch 120: Train Loss = 0.0873
2023-11-08 03:36:04,958 - __main__ - INFO - Epoch 91: Loss = 0.1474 Valid loss = 0.1350 roc = 0.9262
2023-11-08 03:36:06,086 - __main__ - INFO - Epoch 92 Batch 0: Train Loss = 0.1529
2023-11-08 03:36:23,165 - __main__ - INFO - Epoch 92 Batch 20: Train Loss = 0.1169
2023-11-08 03:36:41,038 - __main__ - INFO - Epoch 92 Batch 40: Train Loss = 0.1383
2023-11-08 03:36:59,598 - __main__ - INFO - Epoch 92 Batch 60: Train Loss = 0.1062
2023-11-08 03:37:17,418 - __main__ - INFO - Epoch 92 Batch 80: Train Loss = 0.1156
2023-11-08 03:37:34,939 - __main__ - INFO - Epoch 92 Batch 100: Train Loss = 0.1945
2023-11-08 03:37:52,063 - __main__ - INFO - Epoch 92 Batch 120: Train Loss = 0.1497
2023-11-08 03:38:05,399 - __main__ - INFO - Epoch 92: Loss = 0.1464 Valid loss = 0.1363 roc = 0.9234
2023-11-08 03:38:06,149 - __main__ - INFO - Epoch 93 Batch 0: Train Loss = 0.1553
2023-11-08 03:38:23,874 - __main__ - INFO - Epoch 93 Batch 20: Train Loss = 0.1048
2023-11-08 03:38:42,803 - __main__ - INFO - Epoch 93 Batch 40: Train Loss = 0.1551
2023-11-08 03:38:59,550 - __main__ - INFO - Epoch 93 Batch 60: Train Loss = 0.1063
2023-11-08 03:39:16,489 - __main__ - INFO - Epoch 93 Batch 80: Train Loss = 0.1261
2023-11-08 03:39:34,812 - __main__ - INFO - Epoch 93 Batch 100: Train Loss = 0.1472
2023-11-08 03:39:51,249 - __main__ - INFO - Epoch 93 Batch 120: Train Loss = 0.1777
2023-11-08 03:40:03,871 - __main__ - INFO - Epoch 93: Loss = 0.1469 Valid loss = 0.1359 roc = 0.9235
2023-11-08 03:40:04,451 - __main__ - INFO - Epoch 94 Batch 0: Train Loss = 0.0956
2023-11-08 03:40:20,632 - __main__ - INFO - Epoch 94 Batch 20: Train Loss = 0.1168
2023-11-08 03:40:37,005 - __main__ - INFO - Epoch 94 Batch 40: Train Loss = 0.1615
2023-11-08 03:40:55,775 - __main__ - INFO - Epoch 94 Batch 60: Train Loss = 0.1171
2023-11-08 03:41:12,759 - __main__ - INFO - Epoch 94 Batch 80: Train Loss = 0.1761
2023-11-08 03:41:30,588 - __main__ - INFO - Epoch 94 Batch 100: Train Loss = 0.1371
2023-11-08 03:41:47,733 - __main__ - INFO - Epoch 94 Batch 120: Train Loss = 0.1178
2023-11-08 03:42:01,645 - __main__ - INFO - Epoch 94: Loss = 0.1471 Valid loss = 0.1384 roc = 0.9235
2023-11-08 03:42:02,713 - __main__ - INFO - Epoch 95 Batch 0: Train Loss = 0.1291
2023-11-08 03:42:20,720 - __main__ - INFO - Epoch 95 Batch 20: Train Loss = 0.1152
2023-11-08 03:42:38,548 - __main__ - INFO - Epoch 95 Batch 40: Train Loss = 0.2062
2023-11-08 03:42:57,384 - __main__ - INFO - Epoch 95 Batch 60: Train Loss = 0.1864
2023-11-08 03:43:15,387 - __main__ - INFO - Epoch 95 Batch 80: Train Loss = 0.2242
2023-11-08 03:43:33,260 - __main__ - INFO - Epoch 95 Batch 100: Train Loss = 0.1034
2023-11-08 03:43:49,565 - __main__ - INFO - Epoch 95 Batch 120: Train Loss = 0.1354
2023-11-08 03:44:01,855 - __main__ - INFO - Epoch 95: Loss = 0.1464 Valid loss = 0.1347 roc = 0.9263
2023-11-08 03:44:02,630 - __main__ - INFO - Epoch 96 Batch 0: Train Loss = 0.1906
2023-11-08 03:44:20,244 - __main__ - INFO - Epoch 96 Batch 20: Train Loss = 0.1983
2023-11-08 03:44:39,247 - __main__ - INFO - Epoch 96 Batch 40: Train Loss = 0.2397
2023-11-08 03:44:57,312 - __main__ - INFO - Epoch 96 Batch 60: Train Loss = 0.1241
2023-11-08 03:45:13,773 - __main__ - INFO - Epoch 96 Batch 80: Train Loss = 0.1650
2023-11-08 03:45:32,238 - __main__ - INFO - Epoch 96 Batch 100: Train Loss = 0.1650
2023-11-08 03:45:49,894 - __main__ - INFO - Epoch 96 Batch 120: Train Loss = 0.2181
2023-11-08 03:46:01,808 - __main__ - INFO - Epoch 96: Loss = 0.1431 Valid loss = 0.1344 roc = 0.9262
2023-11-08 03:46:02,663 - __main__ - INFO - Epoch 97 Batch 0: Train Loss = 0.1442
2023-11-08 03:46:19,977 - __main__ - INFO - Epoch 97 Batch 20: Train Loss = 0.1172
2023-11-08 03:46:37,990 - __main__ - INFO - Epoch 97 Batch 40: Train Loss = 0.1493
2023-11-08 03:46:55,471 - __main__ - INFO - Epoch 97 Batch 60: Train Loss = 0.1595
2023-11-08 03:47:13,061 - __main__ - INFO - Epoch 97 Batch 80: Train Loss = 0.1326
2023-11-08 03:47:30,138 - __main__ - INFO - Epoch 97 Batch 100: Train Loss = 0.1474
2023-11-08 03:47:46,933 - __main__ - INFO - Epoch 97 Batch 120: Train Loss = 0.0912
2023-11-08 03:47:59,367 - __main__ - INFO - Epoch 97: Loss = 0.1443 Valid loss = 0.1381 roc = 0.9229
2023-11-08 03:48:00,259 - __main__ - INFO - Epoch 98 Batch 0: Train Loss = 0.1740
2023-11-08 03:48:16,881 - __main__ - INFO - Epoch 98 Batch 20: Train Loss = 0.1568
2023-11-08 03:48:35,599 - __main__ - INFO - Epoch 98 Batch 40: Train Loss = 0.1112
2023-11-08 03:48:53,411 - __main__ - INFO - Epoch 98 Batch 60: Train Loss = 0.1417
2023-11-08 03:49:11,655 - __main__ - INFO - Epoch 98 Batch 80: Train Loss = 0.1394
2023-11-08 03:49:29,618 - __main__ - INFO - Epoch 98 Batch 100: Train Loss = 0.1172
2023-11-08 03:49:47,243 - __main__ - INFO - Epoch 98 Batch 120: Train Loss = 0.1714
2023-11-08 03:49:59,042 - __main__ - INFO - Epoch 98: Loss = 0.1464 Valid loss = 0.1354 roc = 0.9272
2023-11-08 03:49:59,640 - __main__ - INFO - Epoch 99 Batch 0: Train Loss = 0.1316
2023-11-08 03:50:16,706 - __main__ - INFO - Epoch 99 Batch 20: Train Loss = 0.1746
2023-11-08 03:50:35,037 - __main__ - INFO - Epoch 99 Batch 40: Train Loss = 0.1133
2023-11-08 03:50:53,196 - __main__ - INFO - Epoch 99 Batch 60: Train Loss = 0.1346
2023-11-08 03:51:11,694 - __main__ - INFO - Epoch 99 Batch 80: Train Loss = 0.1094
2023-11-08 03:51:29,328 - __main__ - INFO - Epoch 99 Batch 100: Train Loss = 0.1696
2023-11-08 03:51:46,901 - __main__ - INFO - Epoch 99 Batch 120: Train Loss = 0.1311
2023-11-08 03:52:00,355 - __main__ - INFO - Epoch 99: Loss = 0.1458 Valid loss = 0.1349 roc = 0.9251
2023-11-08 03:52:01,351 - __main__ - INFO - Epoch 100 Batch 0: Train Loss = 0.1635
2023-11-08 03:52:20,590 - __main__ - INFO - Epoch 100 Batch 20: Train Loss = 0.0725
2023-11-08 03:52:37,853 - __main__ - INFO - Epoch 100 Batch 40: Train Loss = 0.1704
2023-11-08 03:52:56,179 - __main__ - INFO - Epoch 100 Batch 60: Train Loss = 0.1549
2023-11-08 03:53:13,273 - __main__ - INFO - Epoch 100 Batch 80: Train Loss = 0.1536
2023-11-08 03:53:29,226 - __main__ - INFO - Epoch 100 Batch 100: Train Loss = 0.1449
2023-11-08 03:53:47,501 - __main__ - INFO - Epoch 100 Batch 120: Train Loss = 0.1615
2023-11-08 03:54:01,238 - __main__ - INFO - Epoch 100: Loss = 0.1447 Valid loss = 0.1372 roc = 0.9242
2023-11-08 03:54:02,452 - __main__ - INFO - Epoch 101 Batch 0: Train Loss = 0.1405
2023-11-08 03:54:19,334 - __main__ - INFO - Epoch 101 Batch 20: Train Loss = 0.1202
2023-11-08 03:54:38,314 - __main__ - INFO - Epoch 101 Batch 40: Train Loss = 0.1876
2023-11-08 03:54:56,719 - __main__ - INFO - Epoch 101 Batch 60: Train Loss = 0.1255
2023-11-08 03:55:14,835 - __main__ - INFO - Epoch 101 Batch 80: Train Loss = 0.1416
2023-11-08 03:55:31,705 - __main__ - INFO - Epoch 101 Batch 100: Train Loss = 0.0921
2023-11-08 03:55:46,885 - __main__ - INFO - Epoch 101 Batch 120: Train Loss = 0.1294
2023-11-08 03:56:00,886 - __main__ - INFO - Epoch 101: Loss = 0.1476 Valid loss = 0.1352 roc = 0.9244
2023-11-08 03:56:02,019 - __main__ - INFO - Epoch 102 Batch 0: Train Loss = 0.1559
2023-11-08 03:56:18,690 - __main__ - INFO - Epoch 102 Batch 20: Train Loss = 0.1180
2023-11-08 03:56:37,480 - __main__ - INFO - Epoch 102 Batch 40: Train Loss = 0.0973
2023-11-08 03:56:56,631 - __main__ - INFO - Epoch 102 Batch 60: Train Loss = 0.1110
2023-11-08 03:57:15,140 - __main__ - INFO - Epoch 102 Batch 80: Train Loss = 0.1290
2023-11-08 03:57:31,677 - __main__ - INFO - Epoch 102 Batch 100: Train Loss = 0.2167
2023-11-08 03:57:49,589 - __main__ - INFO - Epoch 102 Batch 120: Train Loss = 0.1281
2023-11-08 03:58:02,759 - __main__ - INFO - Epoch 102: Loss = 0.1442 Valid loss = 0.1342 roc = 0.9287
2023-11-08 03:58:03,682 - __main__ - INFO - Epoch 103 Batch 0: Train Loss = 0.1636
2023-11-08 03:58:20,719 - __main__ - INFO - Epoch 103 Batch 20: Train Loss = 0.1423
2023-11-08 03:58:39,275 - __main__ - INFO - Epoch 103 Batch 40: Train Loss = 0.1681
2023-11-08 03:58:57,415 - __main__ - INFO - Epoch 103 Batch 60: Train Loss = 0.1168
2023-11-08 03:59:16,205 - __main__ - INFO - Epoch 103 Batch 80: Train Loss = 0.0644
2023-11-08 03:59:31,971 - __main__ - INFO - Epoch 103 Batch 100: Train Loss = 0.1581
2023-11-08 03:59:48,957 - __main__ - INFO - Epoch 103 Batch 120: Train Loss = 0.0667
2023-11-08 04:00:02,707 - __main__ - INFO - Epoch 103: Loss = 0.1470 Valid loss = 0.1355 roc = 0.9271
2023-11-08 04:00:03,583 - __main__ - INFO - Epoch 104 Batch 0: Train Loss = 0.1735
2023-11-08 04:00:20,432 - __main__ - INFO - Epoch 104 Batch 20: Train Loss = 0.1699
2023-11-08 04:00:38,839 - __main__ - INFO - Epoch 104 Batch 40: Train Loss = 0.1758
2023-11-08 04:00:56,831 - __main__ - INFO - Epoch 104 Batch 60: Train Loss = 0.0916
2023-11-08 04:01:13,547 - __main__ - INFO - Epoch 104 Batch 80: Train Loss = 0.1663
2023-11-08 04:01:30,514 - __main__ - INFO - Epoch 104 Batch 100: Train Loss = 0.1384
2023-11-08 04:01:49,038 - __main__ - INFO - Epoch 104 Batch 120: Train Loss = 0.2015
2023-11-08 04:02:02,265 - __main__ - INFO - Epoch 104: Loss = 0.1486 Valid loss = 0.1346 roc = 0.9244
2023-11-08 04:02:02,832 - __main__ - INFO - Epoch 105 Batch 0: Train Loss = 0.1523
2023-11-08 04:02:20,083 - __main__ - INFO - Epoch 105 Batch 20: Train Loss = 0.1751
2023-11-08 04:02:38,736 - __main__ - INFO - Epoch 105 Batch 40: Train Loss = 0.1400
2023-11-08 04:02:56,167 - __main__ - INFO - Epoch 105 Batch 60: Train Loss = 0.1519
2023-11-08 04:03:14,036 - __main__ - INFO - Epoch 105 Batch 80: Train Loss = 0.1226
2023-11-08 04:03:31,499 - __main__ - INFO - Epoch 105 Batch 100: Train Loss = 0.1165
2023-11-08 04:03:47,582 - __main__ - INFO - Epoch 105 Batch 120: Train Loss = 0.1625
2023-11-08 04:04:00,168 - __main__ - INFO - Epoch 105: Loss = 0.1464 Valid loss = 0.1359 roc = 0.9238
2023-11-08 04:04:01,228 - __main__ - INFO - Epoch 106 Batch 0: Train Loss = 0.1961
2023-11-08 04:04:20,746 - __main__ - INFO - Epoch 106 Batch 20: Train Loss = 0.2132
2023-11-08 04:04:39,381 - __main__ - INFO - Epoch 106 Batch 40: Train Loss = 0.1060
2023-11-08 04:04:57,012 - __main__ - INFO - Epoch 106 Batch 60: Train Loss = 0.1709
2023-11-08 04:05:14,426 - __main__ - INFO - Epoch 106 Batch 80: Train Loss = 0.1467
2023-11-08 04:05:30,839 - __main__ - INFO - Epoch 106 Batch 100: Train Loss = 0.1120
2023-11-08 04:05:48,520 - __main__ - INFO - Epoch 106 Batch 120: Train Loss = 0.1334
2023-11-08 04:06:02,378 - __main__ - INFO - Epoch 106: Loss = 0.1496 Valid loss = 0.1374 roc = 0.9203
2023-11-08 04:06:03,306 - __main__ - INFO - Epoch 107 Batch 0: Train Loss = 0.1415
2023-11-08 04:06:21,855 - __main__ - INFO - Epoch 107 Batch 20: Train Loss = 0.1509
2023-11-08 04:06:39,960 - __main__ - INFO - Epoch 107 Batch 40: Train Loss = 0.1597
2023-11-08 04:06:57,781 - __main__ - INFO - Epoch 107 Batch 60: Train Loss = 0.1778
2023-11-08 04:07:14,303 - __main__ - INFO - Epoch 107 Batch 80: Train Loss = 0.1440
2023-11-08 04:07:32,581 - __main__ - INFO - Epoch 107 Batch 100: Train Loss = 0.1617
2023-11-08 04:07:49,237 - __main__ - INFO - Epoch 107 Batch 120: Train Loss = 0.1201
2023-11-08 04:08:03,535 - __main__ - INFO - Epoch 107: Loss = 0.1477 Valid loss = 0.1393 roc = 0.9197
2023-11-08 04:08:04,540 - __main__ - INFO - Epoch 108 Batch 0: Train Loss = 0.2225
2023-11-08 04:08:20,889 - __main__ - INFO - Epoch 108 Batch 20: Train Loss = 0.1503
2023-11-08 04:08:36,551 - __main__ - INFO - Epoch 108 Batch 40: Train Loss = 0.1764
2023-11-08 04:08:55,153 - __main__ - INFO - Epoch 108 Batch 60: Train Loss = 0.1572
2023-11-08 04:09:13,670 - __main__ - INFO - Epoch 108 Batch 80: Train Loss = 0.1962
2023-11-08 04:09:30,791 - __main__ - INFO - Epoch 108 Batch 100: Train Loss = 0.1192
2023-11-08 04:09:48,228 - __main__ - INFO - Epoch 108 Batch 120: Train Loss = 0.1949
2023-11-08 04:10:01,913 - __main__ - INFO - Epoch 108: Loss = 0.1488 Valid loss = 0.1366 roc = 0.9235
2023-11-08 04:10:02,742 - __main__ - INFO - Epoch 109 Batch 0: Train Loss = 0.1353
2023-11-08 04:10:19,683 - __main__ - INFO - Epoch 109 Batch 20: Train Loss = 0.1430
2023-11-08 04:10:37,392 - __main__ - INFO - Epoch 109 Batch 40: Train Loss = 0.1892
2023-11-08 04:10:54,424 - __main__ - INFO - Epoch 109 Batch 60: Train Loss = 0.1847
2023-11-08 04:11:12,106 - __main__ - INFO - Epoch 109 Batch 80: Train Loss = 0.1966
2023-11-08 04:11:30,224 - __main__ - INFO - Epoch 109 Batch 100: Train Loss = 0.1128
2023-11-08 04:11:48,051 - __main__ - INFO - Epoch 109 Batch 120: Train Loss = 0.1285
2023-11-08 04:12:01,608 - __main__ - INFO - Epoch 109: Loss = 0.1507 Valid loss = 0.1367 roc = 0.9272
2023-11-08 04:12:02,512 - __main__ - INFO - Epoch 110 Batch 0: Train Loss = 0.1707
2023-11-08 04:12:19,375 - __main__ - INFO - Epoch 110 Batch 20: Train Loss = 0.1933
2023-11-08 04:12:37,892 - __main__ - INFO - Epoch 110 Batch 40: Train Loss = 0.1745
2023-11-08 04:12:55,147 - __main__ - INFO - Epoch 110 Batch 60: Train Loss = 0.0842
2023-11-08 04:13:12,682 - __main__ - INFO - Epoch 110 Batch 80: Train Loss = 0.1699
2023-11-08 04:13:29,028 - __main__ - INFO - Epoch 110 Batch 100: Train Loss = 0.1404
2023-11-08 04:13:46,575 - __main__ - INFO - Epoch 110 Batch 120: Train Loss = 0.1440
2023-11-08 04:14:00,023 - __main__ - INFO - Epoch 110: Loss = 0.1464 Valid loss = 0.1348 roc = 0.9274
2023-11-08 04:14:01,088 - __main__ - INFO - Epoch 111 Batch 0: Train Loss = 0.1367
2023-11-08 04:14:17,276 - __main__ - INFO - Epoch 111 Batch 20: Train Loss = 0.1771
2023-11-08 04:14:35,132 - __main__ - INFO - Epoch 111 Batch 40: Train Loss = 0.1596
2023-11-08 04:14:50,356 - __main__ - INFO - Epoch 111 Batch 60: Train Loss = 0.1158
2023-11-08 04:15:08,167 - __main__ - INFO - Epoch 111 Batch 80: Train Loss = 0.2084
2023-11-08 04:15:25,692 - __main__ - INFO - Epoch 111 Batch 100: Train Loss = 0.1520
2023-11-08 04:15:43,003 - __main__ - INFO - Epoch 111 Batch 120: Train Loss = 0.1187
2023-11-08 04:15:56,623 - __main__ - INFO - Epoch 111: Loss = 0.1479 Valid loss = 0.1365 roc = 0.9246
2023-11-08 04:15:57,340 - __main__ - INFO - Epoch 112 Batch 0: Train Loss = 0.1208
2023-11-08 04:16:14,379 - __main__ - INFO - Epoch 112 Batch 20: Train Loss = 0.2090
2023-11-08 04:16:32,810 - __main__ - INFO - Epoch 112 Batch 40: Train Loss = 0.1813
2023-11-08 04:16:50,465 - __main__ - INFO - Epoch 112 Batch 60: Train Loss = 0.1398
2023-11-08 04:17:08,618 - __main__ - INFO - Epoch 112 Batch 80: Train Loss = 0.0856
2023-11-08 04:17:26,524 - __main__ - INFO - Epoch 112 Batch 100: Train Loss = 0.1212
2023-11-08 04:17:43,870 - __main__ - INFO - Epoch 112 Batch 120: Train Loss = 0.1332
2023-11-08 04:17:57,652 - __main__ - INFO - Epoch 112: Loss = 0.1509 Valid loss = 0.1369 roc = 0.9228
2023-11-08 04:17:58,560 - __main__ - INFO - Epoch 113 Batch 0: Train Loss = 0.1708
2023-11-08 04:18:16,007 - __main__ - INFO - Epoch 113 Batch 20: Train Loss = 0.1813
2023-11-08 04:18:32,892 - __main__ - INFO - Epoch 113 Batch 40: Train Loss = 0.0879
2023-11-08 04:18:51,342 - __main__ - INFO - Epoch 113 Batch 60: Train Loss = 0.0857
2023-11-08 04:19:09,461 - __main__ - INFO - Epoch 113 Batch 80: Train Loss = 0.1107
2023-11-08 04:19:26,874 - __main__ - INFO - Epoch 113 Batch 100: Train Loss = 0.1340
2023-11-08 04:19:44,725 - __main__ - INFO - Epoch 113 Batch 120: Train Loss = 0.1105
2023-11-08 04:19:57,704 - __main__ - INFO - Epoch 113: Loss = 0.1479 Valid loss = 0.1371 roc = 0.9205
2023-11-08 04:19:58,644 - __main__ - INFO - Epoch 114 Batch 0: Train Loss = 0.1318
2023-11-08 04:20:15,784 - __main__ - INFO - Epoch 114 Batch 20: Train Loss = 0.1540
2023-11-08 04:20:34,487 - __main__ - INFO - Epoch 114 Batch 40: Train Loss = 0.1550
2023-11-08 04:20:52,509 - __main__ - INFO - Epoch 114 Batch 60: Train Loss = 0.1791
2023-11-08 04:21:10,351 - __main__ - INFO - Epoch 114 Batch 80: Train Loss = 0.1836
2023-11-08 04:21:28,228 - __main__ - INFO - Epoch 114 Batch 100: Train Loss = 0.1334
2023-11-08 04:21:45,125 - __main__ - INFO - Epoch 114 Batch 120: Train Loss = 0.2103
2023-11-08 04:21:58,302 - __main__ - INFO - Epoch 114: Loss = 0.1519 Valid loss = 0.1389 roc = 0.9205
2023-11-08 04:21:59,344 - __main__ - INFO - Epoch 115 Batch 0: Train Loss = 0.1309
2023-11-08 04:22:17,816 - __main__ - INFO - Epoch 115 Batch 20: Train Loss = 0.1372
2023-11-08 04:22:35,839 - __main__ - INFO - Epoch 115 Batch 40: Train Loss = 0.1733
2023-11-08 04:22:53,496 - __main__ - INFO - Epoch 115 Batch 60: Train Loss = 0.1392
2023-11-08 04:23:10,874 - __main__ - INFO - Epoch 115 Batch 80: Train Loss = 0.1105
2023-11-08 04:23:28,213 - __main__ - INFO - Epoch 115 Batch 100: Train Loss = 0.1614
2023-11-08 04:23:45,445 - __main__ - INFO - Epoch 115 Batch 120: Train Loss = 0.2181
2023-11-08 04:23:58,891 - __main__ - INFO - Epoch 115: Loss = 0.1547 Valid loss = 0.1370 roc = 0.9235
2023-11-08 04:23:59,576 - __main__ - INFO - Epoch 116 Batch 0: Train Loss = 0.1373
2023-11-08 04:24:17,399 - __main__ - INFO - Epoch 116 Batch 20: Train Loss = 0.1376
2023-11-08 04:24:35,315 - __main__ - INFO - Epoch 116 Batch 40: Train Loss = 0.1209
2023-11-08 04:24:52,946 - __main__ - INFO - Epoch 116 Batch 60: Train Loss = 0.1540
2023-11-08 04:25:12,215 - __main__ - INFO - Epoch 116 Batch 80: Train Loss = 0.1214
2023-11-08 04:25:28,890 - __main__ - INFO - Epoch 116 Batch 100: Train Loss = 0.1261
2023-11-08 04:25:46,663 - __main__ - INFO - Epoch 116 Batch 120: Train Loss = 0.1353
2023-11-08 04:26:00,106 - __main__ - INFO - Epoch 116: Loss = 0.1493 Valid loss = 0.1345 roc = 0.9250
2023-11-08 04:26:00,872 - __main__ - INFO - Epoch 117 Batch 0: Train Loss = 0.1378
2023-11-08 04:26:17,917 - __main__ - INFO - Epoch 117 Batch 20: Train Loss = 0.1365
2023-11-08 04:26:36,623 - __main__ - INFO - Epoch 117 Batch 40: Train Loss = 0.2270
2023-11-08 04:26:52,832 - __main__ - INFO - Epoch 117 Batch 60: Train Loss = 0.1334
2023-11-08 04:27:10,970 - __main__ - INFO - Epoch 117 Batch 80: Train Loss = 0.1612
2023-11-08 04:27:28,846 - __main__ - INFO - Epoch 117 Batch 100: Train Loss = 0.1665
2023-11-08 04:27:46,559 - __main__ - INFO - Epoch 117 Batch 120: Train Loss = 0.1604
2023-11-08 04:27:59,464 - __main__ - INFO - Epoch 117: Loss = 0.1483 Valid loss = 0.1338 roc = 0.9273
2023-11-08 04:28:00,401 - __main__ - INFO - Epoch 118 Batch 0: Train Loss = 0.1031
2023-11-08 04:28:17,610 - __main__ - INFO - Epoch 118 Batch 20: Train Loss = 0.1501
2023-11-08 04:28:35,683 - __main__ - INFO - Epoch 118 Batch 40: Train Loss = 0.1210
2023-11-08 04:28:52,505 - __main__ - INFO - Epoch 118 Batch 60: Train Loss = 0.1909
2023-11-08 04:29:09,983 - __main__ - INFO - Epoch 118 Batch 80: Train Loss = 0.1712
2023-11-08 04:29:26,768 - __main__ - INFO - Epoch 118 Batch 100: Train Loss = 0.1190
2023-11-08 04:29:44,430 - __main__ - INFO - Epoch 118 Batch 120: Train Loss = 0.0963
2023-11-08 04:29:56,417 - __main__ - INFO - Epoch 118: Loss = 0.1517 Valid loss = 0.1364 roc = 0.9252
2023-11-08 04:29:57,177 - __main__ - INFO - Epoch 119 Batch 0: Train Loss = 0.1227
2023-11-08 04:30:15,039 - __main__ - INFO - Epoch 119 Batch 20: Train Loss = 0.1526
2023-11-08 04:30:34,376 - __main__ - INFO - Epoch 119 Batch 40: Train Loss = 0.1349
2023-11-08 04:30:53,863 - __main__ - INFO - Epoch 119 Batch 60: Train Loss = 0.1515
2023-11-08 04:31:11,027 - __main__ - INFO - Epoch 119 Batch 80: Train Loss = 0.1276
2023-11-08 04:31:27,547 - __main__ - INFO - Epoch 119 Batch 100: Train Loss = 0.1261
2023-11-08 04:31:45,487 - __main__ - INFO - Epoch 119 Batch 120: Train Loss = 0.1691
2023-11-08 04:31:58,897 - __main__ - INFO - Epoch 119: Loss = 0.1483 Valid loss = 0.1365 roc = 0.9239
2023-11-08 04:31:59,829 - __main__ - INFO - Epoch 120 Batch 0: Train Loss = 0.1241
2023-11-08 04:32:15,542 - __main__ - INFO - Epoch 120 Batch 20: Train Loss = 0.1401
2023-11-08 04:32:32,937 - __main__ - INFO - Epoch 120 Batch 40: Train Loss = 0.1156
2023-11-08 04:32:50,687 - __main__ - INFO - Epoch 120 Batch 60: Train Loss = 0.1217
2023-11-08 04:33:09,334 - __main__ - INFO - Epoch 120 Batch 80: Train Loss = 0.1366
2023-11-08 04:33:27,117 - __main__ - INFO - Epoch 120 Batch 100: Train Loss = 0.1795
2023-11-08 04:33:45,182 - __main__ - INFO - Epoch 120 Batch 120: Train Loss = 0.1190
2023-11-08 04:33:59,364 - __main__ - INFO - Epoch 120: Loss = 0.1477 Valid loss = 0.1332 roc = 0.9269
2023-11-08 04:34:00,274 - __main__ - INFO - Epoch 121 Batch 0: Train Loss = 0.1697
2023-11-08 04:34:17,567 - __main__ - INFO - Epoch 121 Batch 20: Train Loss = 0.1356
2023-11-08 04:34:36,006 - __main__ - INFO - Epoch 121 Batch 40: Train Loss = 0.2078
2023-11-08 04:34:53,732 - __main__ - INFO - Epoch 121 Batch 60: Train Loss = 0.1113
2023-11-08 04:35:10,696 - __main__ - INFO - Epoch 121 Batch 80: Train Loss = 0.1804
2023-11-08 04:35:28,896 - __main__ - INFO - Epoch 121 Batch 100: Train Loss = 0.2223
2023-11-08 04:35:45,192 - __main__ - INFO - Epoch 121 Batch 120: Train Loss = 0.1433
2023-11-08 04:35:58,984 - __main__ - INFO - Epoch 121: Loss = 0.1501 Valid loss = 0.1346 roc = 0.9244
2023-11-08 04:35:59,723 - __main__ - INFO - Epoch 122 Batch 0: Train Loss = 0.1402
2023-11-08 04:36:16,858 - __main__ - INFO - Epoch 122 Batch 20: Train Loss = 0.0978
2023-11-08 04:36:33,514 - __main__ - INFO - Epoch 122 Batch 40: Train Loss = 0.1788
2023-11-08 04:36:51,089 - __main__ - INFO - Epoch 122 Batch 60: Train Loss = 0.1743
2023-11-08 04:37:09,153 - __main__ - INFO - Epoch 122 Batch 80: Train Loss = 0.1606
2023-11-08 04:37:25,979 - __main__ - INFO - Epoch 122 Batch 100: Train Loss = 0.1559
2023-11-08 04:37:42,823 - __main__ - INFO - Epoch 122 Batch 120: Train Loss = 0.1558
2023-11-08 04:37:55,579 - __main__ - INFO - Epoch 122: Loss = 0.1467 Valid loss = 0.1347 roc = 0.9253
2023-11-08 04:37:56,679 - __main__ - INFO - Epoch 123 Batch 0: Train Loss = 0.1636
2023-11-08 04:38:13,512 - __main__ - INFO - Epoch 123 Batch 20: Train Loss = 0.1142
2023-11-08 04:38:31,846 - __main__ - INFO - Epoch 123 Batch 40: Train Loss = 0.1593
2023-11-08 04:38:50,180 - __main__ - INFO - Epoch 123 Batch 60: Train Loss = 0.1042
2023-11-08 04:39:06,228 - __main__ - INFO - Epoch 123 Batch 80: Train Loss = 0.1482
2023-11-08 04:39:23,419 - __main__ - INFO - Epoch 123 Batch 100: Train Loss = 0.1412
2023-11-08 04:39:42,985 - __main__ - INFO - Epoch 123 Batch 120: Train Loss = 0.1322
2023-11-08 04:39:56,715 - __main__ - INFO - Epoch 123: Loss = 0.1493 Valid loss = 0.1350 roc = 0.9245
2023-11-08 04:39:57,558 - __main__ - INFO - Epoch 124 Batch 0: Train Loss = 0.1787
2023-11-08 04:40:14,601 - __main__ - INFO - Epoch 124 Batch 20: Train Loss = 0.1675
2023-11-08 04:40:34,080 - __main__ - INFO - Epoch 124 Batch 40: Train Loss = 0.2139
2023-11-08 04:40:51,321 - __main__ - INFO - Epoch 124 Batch 60: Train Loss = 0.1042
2023-11-08 04:41:09,362 - __main__ - INFO - Epoch 124 Batch 80: Train Loss = 0.1364
2023-11-08 04:41:26,354 - __main__ - INFO - Epoch 124 Batch 100: Train Loss = 0.1092
2023-11-08 04:41:44,455 - __main__ - INFO - Epoch 124 Batch 120: Train Loss = 0.1254
2023-11-08 04:41:58,020 - __main__ - INFO - Epoch 124: Loss = 0.1462 Valid loss = 0.1324 roc = 0.9298
2023-11-08 04:41:59,170 - __main__ - INFO - Epoch 125 Batch 0: Train Loss = 0.1197
2023-11-08 04:42:16,356 - __main__ - INFO - Epoch 125 Batch 20: Train Loss = 0.1512
2023-11-08 04:42:34,163 - __main__ - INFO - Epoch 125 Batch 40: Train Loss = 0.1344
2023-11-08 04:42:51,795 - __main__ - INFO - Epoch 125 Batch 60: Train Loss = 0.1559
2023-11-08 04:43:10,452 - __main__ - INFO - Epoch 125 Batch 80: Train Loss = 0.1101
2023-11-08 04:43:27,805 - __main__ - INFO - Epoch 125 Batch 100: Train Loss = 0.1613
2023-11-08 04:43:44,663 - __main__ - INFO - Epoch 125 Batch 120: Train Loss = 0.1543
2023-11-08 04:43:57,642 - __main__ - INFO - Epoch 125: Loss = 0.1442 Valid loss = 0.1333 roc = 0.9282
2023-11-08 04:43:58,334 - __main__ - INFO - Epoch 126 Batch 0: Train Loss = 0.1292
2023-11-08 04:44:14,267 - __main__ - INFO - Epoch 126 Batch 20: Train Loss = 0.1617
2023-11-08 04:44:33,454 - __main__ - INFO - Epoch 126 Batch 40: Train Loss = 0.1129
2023-11-08 04:44:51,437 - __main__ - INFO - Epoch 126 Batch 60: Train Loss = 0.1459
2023-11-08 04:45:10,107 - __main__ - INFO - Epoch 126 Batch 80: Train Loss = 0.1148
2023-11-08 04:45:26,843 - __main__ - INFO - Epoch 126 Batch 100: Train Loss = 0.1821
2023-11-08 04:45:44,612 - __main__ - INFO - Epoch 126 Batch 120: Train Loss = 0.1897
2023-11-08 04:45:58,641 - __main__ - INFO - Epoch 126: Loss = 0.1498 Valid loss = 0.1343 roc = 0.9283
2023-11-08 04:45:59,424 - __main__ - INFO - Epoch 127 Batch 0: Train Loss = 0.1260
2023-11-08 04:46:15,847 - __main__ - INFO - Epoch 127 Batch 20: Train Loss = 0.1534
2023-11-08 04:46:33,395 - __main__ - INFO - Epoch 127 Batch 40: Train Loss = 0.1455
2023-11-08 04:46:49,404 - __main__ - INFO - Epoch 127 Batch 60: Train Loss = 0.1169
2023-11-08 04:47:06,869 - __main__ - INFO - Epoch 127 Batch 80: Train Loss = 0.0866
2023-11-08 04:47:23,053 - __main__ - INFO - Epoch 127 Batch 100: Train Loss = 0.1672
2023-11-08 04:47:40,369 - __main__ - INFO - Epoch 127 Batch 120: Train Loss = 0.1626
2023-11-08 04:47:53,885 - __main__ - INFO - Epoch 127: Loss = 0.1470 Valid loss = 0.1330 roc = 0.9266
2023-11-08 04:47:54,823 - __main__ - INFO - Epoch 128 Batch 0: Train Loss = 0.1869
2023-11-08 04:48:12,510 - __main__ - INFO - Epoch 128 Batch 20: Train Loss = 0.1301
2023-11-08 04:48:30,326 - __main__ - INFO - Epoch 128 Batch 40: Train Loss = 0.1160
2023-11-08 04:48:47,498 - __main__ - INFO - Epoch 128 Batch 60: Train Loss = 0.1621
2023-11-08 04:49:04,543 - __main__ - INFO - Epoch 128 Batch 80: Train Loss = 0.1842
2023-11-08 04:49:20,668 - __main__ - INFO - Epoch 128 Batch 100: Train Loss = 0.1707
2023-11-08 04:49:37,399 - __main__ - INFO - Epoch 128 Batch 120: Train Loss = 0.1491
2023-11-08 04:49:51,210 - __main__ - INFO - Epoch 128: Loss = 0.1483 Valid loss = 0.1348 roc = 0.9278
2023-11-08 04:49:52,375 - __main__ - INFO - Epoch 129 Batch 0: Train Loss = 0.1744
2023-11-08 04:50:12,110 - __main__ - INFO - Epoch 129 Batch 20: Train Loss = 0.1492
2023-11-08 04:50:30,544 - __main__ - INFO - Epoch 129 Batch 40: Train Loss = 0.1835
2023-11-08 04:50:47,870 - __main__ - INFO - Epoch 129 Batch 60: Train Loss = 0.1205
2023-11-08 04:51:05,449 - __main__ - INFO - Epoch 129 Batch 80: Train Loss = 0.0911
2023-11-08 04:51:21,893 - __main__ - INFO - Epoch 129 Batch 100: Train Loss = 0.1046
2023-11-08 04:51:39,316 - __main__ - INFO - Epoch 129 Batch 120: Train Loss = 0.1901
2023-11-08 04:51:51,421 - __main__ - INFO - Epoch 129: Loss = 0.1455 Valid loss = 0.1347 roc = 0.9247
2023-11-08 04:51:52,122 - __main__ - INFO - Epoch 130 Batch 0: Train Loss = 0.1529
2023-11-08 04:52:08,909 - __main__ - INFO - Epoch 130 Batch 20: Train Loss = 0.1222
2023-11-08 04:52:26,605 - __main__ - INFO - Epoch 130 Batch 40: Train Loss = 0.0822
2023-11-08 04:52:43,574 - __main__ - INFO - Epoch 130 Batch 60: Train Loss = 0.1414
2023-11-08 04:53:01,699 - __main__ - INFO - Epoch 130 Batch 80: Train Loss = 0.1160
2023-11-08 04:53:19,498 - __main__ - INFO - Epoch 130 Batch 100: Train Loss = 0.1653
2023-11-08 04:53:36,865 - __main__ - INFO - Epoch 130 Batch 120: Train Loss = 0.1250
2023-11-08 04:53:50,738 - __main__ - INFO - Epoch 130: Loss = 0.1461 Valid loss = 0.1351 roc = 0.9279
2023-11-08 04:53:51,511 - __main__ - INFO - Epoch 131 Batch 0: Train Loss = 0.1770
2023-11-08 04:54:09,079 - __main__ - INFO - Epoch 131 Batch 20: Train Loss = 0.1601
2023-11-08 04:54:26,478 - __main__ - INFO - Epoch 131 Batch 40: Train Loss = 0.1719
2023-11-08 04:54:44,741 - __main__ - INFO - Epoch 131 Batch 60: Train Loss = 0.1581
2023-11-08 04:55:03,205 - __main__ - INFO - Epoch 131 Batch 80: Train Loss = 0.1187
2023-11-08 04:55:21,244 - __main__ - INFO - Epoch 131 Batch 100: Train Loss = 0.1916
2023-11-08 04:55:38,000 - __main__ - INFO - Epoch 131 Batch 120: Train Loss = 0.1032
2023-11-08 04:55:51,163 - __main__ - INFO - Epoch 131: Loss = 0.1453 Valid loss = 0.1354 roc = 0.9269
2023-11-08 04:55:52,191 - __main__ - INFO - Epoch 132 Batch 0: Train Loss = 0.2115
2023-11-08 04:56:09,398 - __main__ - INFO - Epoch 132 Batch 20: Train Loss = 0.1619
2023-11-08 04:56:26,652 - __main__ - INFO - Epoch 132 Batch 40: Train Loss = 0.1251
2023-11-08 04:56:44,289 - __main__ - INFO - Epoch 132 Batch 60: Train Loss = 0.1088
2023-11-08 04:57:01,843 - __main__ - INFO - Epoch 132 Batch 80: Train Loss = 0.1636
2023-11-08 04:57:18,870 - __main__ - INFO - Epoch 132 Batch 100: Train Loss = 0.1322
2023-11-08 04:57:36,252 - __main__ - INFO - Epoch 132 Batch 120: Train Loss = 0.1139
2023-11-08 04:57:50,369 - __main__ - INFO - Epoch 132: Loss = 0.1439 Valid loss = 0.1351 roc = 0.9254
2023-11-08 04:57:50,999 - __main__ - INFO - Epoch 133 Batch 0: Train Loss = 0.1613
2023-11-08 04:58:08,821 - __main__ - INFO - Epoch 133 Batch 20: Train Loss = 0.2120
2023-11-08 04:58:26,606 - __main__ - INFO - Epoch 133 Batch 40: Train Loss = 0.1703
2023-11-08 04:58:45,047 - __main__ - INFO - Epoch 133 Batch 60: Train Loss = 0.1274
2023-11-08 04:59:02,940 - __main__ - INFO - Epoch 133 Batch 80: Train Loss = 0.1495
2023-11-08 04:59:20,944 - __main__ - INFO - Epoch 133 Batch 100: Train Loss = 0.1480
2023-11-08 04:59:38,673 - __main__ - INFO - Epoch 133 Batch 120: Train Loss = 0.1708
2023-11-08 04:59:52,063 - __main__ - INFO - Epoch 133: Loss = 0.1456 Valid loss = 0.1339 roc = 0.9274
2023-11-08 04:59:52,949 - __main__ - INFO - Epoch 134 Batch 0: Train Loss = 0.0975
2023-11-08 05:00:10,258 - __main__ - INFO - Epoch 134 Batch 20: Train Loss = 0.1566
2023-11-08 05:00:29,455 - __main__ - INFO - Epoch 134 Batch 40: Train Loss = 0.1510
2023-11-08 05:00:46,856 - __main__ - INFO - Epoch 134 Batch 60: Train Loss = 0.1290
2023-11-08 05:01:04,423 - __main__ - INFO - Epoch 134 Batch 80: Train Loss = 0.1206
2023-11-08 05:01:21,612 - __main__ - INFO - Epoch 134 Batch 100: Train Loss = 0.1801
2023-11-08 05:01:38,551 - __main__ - INFO - Epoch 134 Batch 120: Train Loss = 0.1125
2023-11-08 05:01:51,170 - __main__ - INFO - Epoch 134: Loss = 0.1445 Valid loss = 0.1346 roc = 0.9270
2023-11-08 05:01:52,038 - __main__ - INFO - Epoch 135 Batch 0: Train Loss = 0.1581
2023-11-08 05:02:09,055 - __main__ - INFO - Epoch 135 Batch 20: Train Loss = 0.0987
2023-11-08 05:02:26,671 - __main__ - INFO - Epoch 135 Batch 40: Train Loss = 0.0936
2023-11-08 05:02:43,937 - __main__ - INFO - Epoch 135 Batch 60: Train Loss = 0.1244
2023-11-08 05:03:00,544 - __main__ - INFO - Epoch 135 Batch 80: Train Loss = 0.1794
2023-11-08 05:03:17,902 - __main__ - INFO - Epoch 135 Batch 100: Train Loss = 0.1275
2023-11-08 05:03:35,815 - __main__ - INFO - Epoch 135 Batch 120: Train Loss = 0.1284
2023-11-08 05:03:49,692 - __main__ - INFO - Epoch 135: Loss = 0.1465 Valid loss = 0.1346 roc = 0.9277
2023-11-08 05:03:50,529 - __main__ - INFO - Epoch 136 Batch 0: Train Loss = 0.2447
2023-11-08 05:04:08,620 - __main__ - INFO - Epoch 136 Batch 20: Train Loss = 0.0966
2023-11-08 05:04:25,862 - __main__ - INFO - Epoch 136 Batch 40: Train Loss = 0.1667
2023-11-08 05:04:43,115 - __main__ - INFO - Epoch 136 Batch 60: Train Loss = 0.1683
2023-11-08 05:05:00,858 - __main__ - INFO - Epoch 136 Batch 80: Train Loss = 0.1774
2023-11-08 05:05:17,372 - __main__ - INFO - Epoch 136 Batch 100: Train Loss = 0.1332
2023-11-08 05:05:34,060 - __main__ - INFO - Epoch 136 Batch 120: Train Loss = 0.1297
2023-11-08 05:05:48,630 - __main__ - INFO - Epoch 136: Loss = 0.1489 Valid loss = 0.1390 roc = 0.9236
2023-11-08 05:05:49,401 - __main__ - INFO - Epoch 137 Batch 0: Train Loss = 0.1073
2023-11-08 05:06:05,618 - __main__ - INFO - Epoch 137 Batch 20: Train Loss = 0.1815
2023-11-08 05:06:21,821 - __main__ - INFO - Epoch 137 Batch 40: Train Loss = 0.1893
2023-11-08 05:06:39,601 - __main__ - INFO - Epoch 137 Batch 60: Train Loss = 0.0878
2023-11-08 05:06:56,700 - __main__ - INFO - Epoch 137 Batch 80: Train Loss = 0.1128
2023-11-08 05:07:15,748 - __main__ - INFO - Epoch 137 Batch 100: Train Loss = 0.1425
2023-11-08 05:07:32,913 - __main__ - INFO - Epoch 137 Batch 120: Train Loss = 0.2149
2023-11-08 05:07:46,451 - __main__ - INFO - Epoch 137: Loss = 0.1514 Valid loss = 0.1374 roc = 0.9207
2023-11-08 05:07:47,347 - __main__ - INFO - Epoch 138 Batch 0: Train Loss = 0.1873
2023-11-08 05:08:03,760 - __main__ - INFO - Epoch 138 Batch 20: Train Loss = 0.1287
2023-11-08 05:08:22,681 - __main__ - INFO - Epoch 138 Batch 40: Train Loss = 0.1568
2023-11-08 05:08:40,764 - __main__ - INFO - Epoch 138 Batch 60: Train Loss = 0.1677
2023-11-08 05:08:58,376 - __main__ - INFO - Epoch 138 Batch 80: Train Loss = 0.1639
2023-11-08 05:09:16,143 - __main__ - INFO - Epoch 138 Batch 100: Train Loss = 0.1245
2023-11-08 05:09:33,485 - __main__ - INFO - Epoch 138 Batch 120: Train Loss = 0.1314
2023-11-08 05:09:45,911 - __main__ - INFO - Epoch 138: Loss = 0.1480 Valid loss = 0.1442 roc = 0.9134
2023-11-08 05:09:46,880 - __main__ - INFO - Epoch 139 Batch 0: Train Loss = 0.1178
2023-11-08 05:10:04,452 - __main__ - INFO - Epoch 139 Batch 20: Train Loss = 0.2003
2023-11-08 05:10:23,427 - __main__ - INFO - Epoch 139 Batch 40: Train Loss = 0.1688
2023-11-08 05:10:41,417 - __main__ - INFO - Epoch 139 Batch 60: Train Loss = 0.1300
2023-11-08 05:10:57,226 - __main__ - INFO - Epoch 139 Batch 80: Train Loss = 0.1238
2023-11-08 05:11:14,922 - __main__ - INFO - Epoch 139 Batch 100: Train Loss = 0.1731
2023-11-08 05:11:32,209 - __main__ - INFO - Epoch 139 Batch 120: Train Loss = 0.1622
2023-11-08 05:11:46,255 - __main__ - INFO - Epoch 139: Loss = 0.1471 Valid loss = 0.1402 roc = 0.9177
2023-11-08 05:11:47,051 - __main__ - INFO - Epoch 140 Batch 0: Train Loss = 0.0861
2023-11-08 05:12:03,683 - __main__ - INFO - Epoch 140 Batch 20: Train Loss = 0.1031
2023-11-08 05:12:19,843 - __main__ - INFO - Epoch 140 Batch 40: Train Loss = 0.1922
2023-11-08 05:12:37,315 - __main__ - INFO - Epoch 140 Batch 60: Train Loss = 0.1615
2023-11-08 05:12:53,089 - __main__ - INFO - Epoch 140 Batch 80: Train Loss = 0.1799
2023-11-08 05:13:11,355 - __main__ - INFO - Epoch 140 Batch 100: Train Loss = 0.2148
2023-11-08 05:13:28,810 - __main__ - INFO - Epoch 140 Batch 120: Train Loss = 0.1122
2023-11-08 05:13:42,679 - __main__ - INFO - Epoch 140: Loss = 0.1491 Valid loss = 0.1375 roc = 0.9214
2023-11-08 05:13:43,564 - __main__ - INFO - Epoch 141 Batch 0: Train Loss = 0.1520
2023-11-08 05:14:00,034 - __main__ - INFO - Epoch 141 Batch 20: Train Loss = 0.1213
2023-11-08 05:14:18,219 - __main__ - INFO - Epoch 141 Batch 40: Train Loss = 0.1447
2023-11-08 05:14:36,196 - __main__ - INFO - Epoch 141 Batch 60: Train Loss = 0.1222
2023-11-08 05:14:54,331 - __main__ - INFO - Epoch 141 Batch 80: Train Loss = 0.1787
2023-11-08 05:15:12,464 - __main__ - INFO - Epoch 141 Batch 100: Train Loss = 0.1358
2023-11-08 05:15:30,053 - __main__ - INFO - Epoch 141 Batch 120: Train Loss = 0.1460
2023-11-08 05:15:42,761 - __main__ - INFO - Epoch 141: Loss = 0.1469 Valid loss = 0.1367 roc = 0.9244
2023-11-08 05:15:43,501 - __main__ - INFO - Epoch 142 Batch 0: Train Loss = 0.1714
2023-11-08 05:15:59,909 - __main__ - INFO - Epoch 142 Batch 20: Train Loss = 0.1672
2023-11-08 05:16:17,692 - __main__ - INFO - Epoch 142 Batch 40: Train Loss = 0.0994
2023-11-08 05:16:34,849 - __main__ - INFO - Epoch 142 Batch 60: Train Loss = 0.1129
2023-11-08 05:16:53,045 - __main__ - INFO - Epoch 142 Batch 80: Train Loss = 0.1634
2023-11-08 05:17:10,813 - __main__ - INFO - Epoch 142 Batch 100: Train Loss = 0.2010
2023-11-08 05:17:28,494 - __main__ - INFO - Epoch 142 Batch 120: Train Loss = 0.1043
2023-11-08 05:17:42,661 - __main__ - INFO - Epoch 142: Loss = 0.1448 Valid loss = 0.1373 roc = 0.9216
2023-11-08 05:17:43,599 - __main__ - INFO - Epoch 143 Batch 0: Train Loss = 0.1395
2023-11-08 05:18:01,295 - __main__ - INFO - Epoch 143 Batch 20: Train Loss = 0.1172
2023-11-08 05:18:18,617 - __main__ - INFO - Epoch 143 Batch 40: Train Loss = 0.1488
2023-11-08 05:18:35,932 - __main__ - INFO - Epoch 143 Batch 60: Train Loss = 0.1772
2023-11-08 05:18:53,984 - __main__ - INFO - Epoch 143 Batch 80: Train Loss = 0.1644
2023-11-08 05:19:11,024 - __main__ - INFO - Epoch 143 Batch 100: Train Loss = 0.1095
2023-11-08 05:19:27,481 - __main__ - INFO - Epoch 143 Batch 120: Train Loss = 0.1662
2023-11-08 05:19:40,734 - __main__ - INFO - Epoch 143: Loss = 0.1464 Valid loss = 0.1346 roc = 0.9254
2023-11-08 05:19:41,892 - __main__ - INFO - Epoch 144 Batch 0: Train Loss = 0.1812
2023-11-08 05:19:58,930 - __main__ - INFO - Epoch 144 Batch 20: Train Loss = 0.1701
2023-11-08 05:20:14,775 - __main__ - INFO - Epoch 144 Batch 40: Train Loss = 0.1134
2023-11-08 05:20:32,448 - __main__ - INFO - Epoch 144 Batch 60: Train Loss = 0.1198
2023-11-08 05:20:49,980 - __main__ - INFO - Epoch 144 Batch 80: Train Loss = 0.1221
2023-11-08 05:21:07,320 - __main__ - INFO - Epoch 144 Batch 100: Train Loss = 0.1820
2023-11-08 05:21:24,261 - __main__ - INFO - Epoch 144 Batch 120: Train Loss = 0.1880
2023-11-08 05:21:37,142 - __main__ - INFO - Epoch 144: Loss = 0.1460 Valid loss = 0.1365 roc = 0.9262
2023-11-08 05:21:38,341 - __main__ - INFO - Epoch 145 Batch 0: Train Loss = 0.1740
2023-11-08 05:21:55,647 - __main__ - INFO - Epoch 145 Batch 20: Train Loss = 0.1215
2023-11-08 05:22:13,216 - __main__ - INFO - Epoch 145 Batch 40: Train Loss = 0.1424
2023-11-08 05:22:29,585 - __main__ - INFO - Epoch 145 Batch 60: Train Loss = 0.1705
2023-11-08 05:22:46,682 - __main__ - INFO - Epoch 145 Batch 80: Train Loss = 0.1516
2023-11-08 05:23:04,219 - __main__ - INFO - Epoch 145 Batch 100: Train Loss = 0.1681
2023-11-08 05:23:22,547 - __main__ - INFO - Epoch 145 Batch 120: Train Loss = 0.2004
2023-11-08 05:23:35,890 - __main__ - INFO - Epoch 145: Loss = 0.1465 Valid loss = 0.1355 roc = 0.9269
2023-11-08 05:23:36,401 - __main__ - INFO - Epoch 146 Batch 0: Train Loss = 0.1132
2023-11-08 05:23:54,146 - __main__ - INFO - Epoch 146 Batch 20: Train Loss = 0.1620
2023-11-08 05:24:13,712 - __main__ - INFO - Epoch 146 Batch 40: Train Loss = 0.1427
2023-11-08 05:24:30,972 - __main__ - INFO - Epoch 146 Batch 60: Train Loss = 0.1003
2023-11-08 05:24:47,087 - __main__ - INFO - Epoch 146 Batch 80: Train Loss = 0.2023
2023-11-08 05:25:04,645 - __main__ - INFO - Epoch 146 Batch 100: Train Loss = 0.0815
2023-11-08 05:25:22,129 - __main__ - INFO - Epoch 146 Batch 120: Train Loss = 0.1637
2023-11-08 05:25:35,998 - __main__ - INFO - Epoch 146: Loss = 0.1508 Valid loss = 0.1387 roc = 0.9226
2023-11-08 05:25:36,671 - __main__ - INFO - Epoch 147 Batch 0: Train Loss = 0.1627
2023-11-08 05:25:54,313 - __main__ - INFO - Epoch 147 Batch 20: Train Loss = 0.1847
2023-11-08 05:26:11,652 - __main__ - INFO - Epoch 147 Batch 40: Train Loss = 0.1453
2023-11-08 05:26:29,075 - __main__ - INFO - Epoch 147 Batch 60: Train Loss = 0.1359
2023-11-08 05:26:47,928 - __main__ - INFO - Epoch 147 Batch 80: Train Loss = 0.1115
2023-11-08 05:27:05,443 - __main__ - INFO - Epoch 147 Batch 100: Train Loss = 0.1580
2023-11-08 05:27:24,806 - __main__ - INFO - Epoch 147 Batch 120: Train Loss = 0.1705
2023-11-08 05:27:38,278 - __main__ - INFO - Epoch 147: Loss = 0.1475 Valid loss = 0.1373 roc = 0.9234
2023-11-08 05:27:39,176 - __main__ - INFO - Epoch 148 Batch 0: Train Loss = 0.1537
2023-11-08 05:27:56,484 - __main__ - INFO - Epoch 148 Batch 20: Train Loss = 0.1884
2023-11-08 05:28:13,760 - __main__ - INFO - Epoch 148 Batch 40: Train Loss = 0.1490
2023-11-08 05:28:31,342 - __main__ - INFO - Epoch 148 Batch 60: Train Loss = 0.1737
2023-11-08 05:28:48,572 - __main__ - INFO - Epoch 148 Batch 80: Train Loss = 0.1690
2023-11-08 05:29:06,863 - __main__ - INFO - Epoch 148 Batch 100: Train Loss = 0.2307
2023-11-08 05:29:23,775 - __main__ - INFO - Epoch 148 Batch 120: Train Loss = 0.1492
2023-11-08 05:29:36,721 - __main__ - INFO - Epoch 148: Loss = 0.1514 Valid loss = 0.1402 roc = 0.9172
2023-11-08 05:29:37,631 - __main__ - INFO - Epoch 149 Batch 0: Train Loss = 0.1463
2023-11-08 05:29:55,209 - __main__ - INFO - Epoch 149 Batch 20: Train Loss = 0.1485
2023-11-08 05:30:12,778 - __main__ - INFO - Epoch 149 Batch 40: Train Loss = 0.1272
2023-11-08 05:30:31,927 - __main__ - INFO - Epoch 149 Batch 60: Train Loss = 0.2373
2023-11-08 05:30:50,910 - __main__ - INFO - Epoch 149 Batch 80: Train Loss = 0.1140
2023-11-08 05:31:09,000 - __main__ - INFO - Epoch 149 Batch 100: Train Loss = 0.1328
2023-11-08 05:31:26,525 - __main__ - INFO - Epoch 149 Batch 120: Train Loss = 0.1648
2023-11-08 05:31:39,172 - __main__ - INFO - Epoch 149: Loss = 0.1500 Valid loss = 0.1391 roc = 0.9223
2023-11-08 05:31:39,257 - __main__ - INFO - auroc 0.9298
2023-11-08 05:31:39,277 - __main__ - INFO - auprc 0.7227
2023-11-08 05:31:39,279 - __main__ - INFO - minpse 0.6644
2023-11-08 05:31:39,578 - __main__ - INFO - last saved model is in epoch 124
2023-11-08 05:31:40,269 - __main__ - INFO - Batch 0: Test Loss = 0.1333
2023-11-08 05:31:48,237 - __main__ - INFO - 
==>Predicting on test
2023-11-08 05:31:48,242 - __main__ - INFO - Test Loss = 0.1238
2023-11-08 05:31:48,370 - __main__ - INFO - load target data
2023-11-08 05:31:48,455 - __main__ - INFO - [[-0.43249782  0.7734225   0.11121635 -1.15107593 -0.57751738 -0.89497949
   0.64182481 -1.1001333  -0.09626591  0.07837977  0.27346416 -0.29216681
  -0.20501606 -0.4488011  -0.6600819  -0.53421373 -0.16268639  1.01340074]
 [-0.43249782  0.7734225   0.11121635 -1.15107593 -0.57751738 -0.89497949
   0.64182481 -1.1001333  -0.09626591  0.07837977  0.27346416 -0.29216681
  -0.20501606 -0.4488011  -0.6600819  -0.53421373 -0.16268639  1.01340074]
 [-0.43249782  1.06266555 -0.26070216 -0.8723303  -0.69876022  0.42060383
   0.77351631 -0.44220536 -0.09626591 -0.57033811  0.27346416  0.40542139
  -0.64784203 -0.65264891  1.05825687  1.3623417  -0.16268639  0.06953911]
 [-0.43249782  0.33955793 -0.89515138 -0.76780069  0.54061107 -0.13459647
   0.32576523  0.68002608 -0.73854097 -0.0884334   0.27346416 -0.50144327
   2.83902535 -0.1708268   0.71458912 -0.36929586 -0.4389748   0.28190798]
 [-0.43249782  0.26724717 -0.98266161 -0.62842788 -0.40238883 -0.43633576
   0.03604394 -0.01741705 -0.73854097 -0.21817698  0.27346416  0.19614493
  -0.4868144  -0.52292757  2.40168536  1.3623417  -0.4389748   0.02234603]
 [-0.43249782  0.41186869 -0.56698799 -0.27999585 -0.84694592 -0.46047491
   0.32576523  0.10508004 -0.82053353 -0.29231616  0.27346416  1.13788901
  -0.39081717 -0.70824377  1.46440967  1.3623417  -0.26629454  0.02234603]
 [-0.43249782  0.41186869 -0.56698799 -0.27999585 -0.84694592 -0.46047491
   0.32576523  0.10508004 -0.82053353 -0.29231616  0.27346416  1.13788901
  -0.39081717 -0.70824377  1.46440967  1.3623417  -0.26629454  0.02234603]]
2023-11-08 05:31:48,461 - __main__ - INFO - 18
2023-11-08 05:31:48,462 - __main__ - INFO - 361
2023-11-08 05:31:49,163 - __main__ - INFO - Batch 0: Test Loss = 0.1324
2023-11-08 05:32:01,166 - __main__ - INFO - Batch 20: Test Loss = 0.1649
2023-11-08 05:32:11,135 - __main__ - INFO - Batch 40: Test Loss = 0.1500
2023-11-08 05:32:21,469 - __main__ - INFO - Batch 60: Test Loss = 0.1281
2023-11-08 05:32:32,524 - __main__ - INFO - Batch 80: Test Loss = 0.0840
2023-11-08 05:32:43,047 - __main__ - INFO - Batch 100: Test Loss = 0.0736
2023-11-08 05:32:52,503 - __main__ - INFO - Batch 120: Test Loss = 0.1101
2023-11-08 05:32:55,088 - __main__ - INFO - Training Student
2023-11-08 05:32:55,906 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 0.8395
2023-11-08 05:33:14,296 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 0.6238
2023-11-08 05:33:31,924 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 0.4930
2023-11-08 05:33:48,575 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 0.4157
2023-11-08 05:34:06,636 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 0.2620
2023-11-08 05:34:24,746 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 0.2482
2023-11-08 05:34:42,307 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 0.2762
2023-11-08 05:34:55,715 - __main__ - INFO - ------------ Save best model - AUROC: 0.6792 ------------
2023-11-08 05:34:56,567 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 0.3080
2023-11-08 05:35:14,759 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 0.3202
2023-11-08 05:35:33,256 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 0.3169
2023-11-08 05:35:51,630 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 0.3486
2023-11-08 05:36:10,795 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 0.2567
2023-11-08 05:36:30,025 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 0.2479
2023-11-08 05:36:46,756 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 0.2642
2023-11-08 05:36:58,924 - __main__ - INFO - ------------ Save best model - AUROC: 0.7152 ------------
2023-11-08 05:36:59,620 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 0.3361
2023-11-08 05:37:16,979 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 0.3177
2023-11-08 05:37:33,196 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 0.2874
2023-11-08 05:37:49,903 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 0.3405
2023-11-08 05:38:05,942 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 0.2426
2023-11-08 05:38:22,726 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 0.2420
2023-11-08 05:38:38,449 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 0.2493
2023-11-08 05:38:50,958 - __main__ - INFO - ------------ Save best model - AUROC: 0.7610 ------------
2023-11-08 05:38:51,756 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 0.2922
2023-11-08 05:39:09,310 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 0.2854
2023-11-08 05:39:25,311 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 0.3177
2023-11-08 05:39:41,581 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 0.3249
2023-11-08 05:39:58,162 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 0.2174
2023-11-08 05:40:14,585 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 0.2033
2023-11-08 05:40:30,970 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 0.2454
2023-11-08 05:40:43,263 - __main__ - INFO - ------------ Save best model - AUROC: 0.8379 ------------
2023-11-08 05:40:44,085 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 0.2761
2023-11-08 05:41:02,307 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 0.2665
2023-11-08 05:41:19,041 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 0.2420
2023-11-08 05:41:34,960 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 0.2415
2023-11-08 05:41:51,821 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 0.1621
2023-11-08 05:42:08,910 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 0.1652
2023-11-08 05:42:24,545 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 0.1803
2023-11-08 05:42:37,838 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 0.2436
2023-11-08 05:42:54,335 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 0.2424
2023-11-08 05:43:10,074 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 0.2281
2023-11-08 05:43:26,130 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 0.2523
2023-11-08 05:43:43,111 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 0.1539
2023-11-08 05:43:59,097 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 0.1476
2023-11-08 05:44:15,066 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 0.1559
2023-11-08 05:44:28,557 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 0.2636
2023-11-08 05:44:46,535 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 0.2362
2023-11-08 05:45:02,072 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 0.2279
2023-11-08 05:45:18,354 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 0.2400
2023-11-08 05:45:35,014 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 0.3984
2023-11-08 05:45:51,822 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 0.1594
2023-11-08 05:46:08,559 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 0.1720
2023-11-08 05:46:21,939 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 0.2285
2023-11-08 05:46:38,671 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 0.2517
2023-11-08 05:46:54,207 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 0.2532
2023-11-08 05:47:10,469 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 0.2347
2023-11-08 05:47:27,749 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 0.1605
2023-11-08 05:47:44,468 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 0.1783
2023-11-08 05:48:00,577 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 0.1428
2023-11-08 05:48:12,739 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 0.2556
2023-11-08 05:48:29,778 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 0.2251
2023-11-08 05:48:45,554 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 0.2154
2023-11-08 05:49:02,518 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 0.2305
2023-11-08 05:49:19,104 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 0.1668
2023-11-08 05:49:34,318 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 0.1462
2023-11-08 05:49:49,794 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 0.1571
2023-11-08 05:50:03,143 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 0.2445
2023-11-08 05:50:19,291 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 0.2193
2023-11-08 05:50:35,302 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 0.2070
2023-11-08 05:50:51,799 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 0.2300
2023-11-08 05:51:08,995 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 0.1555
2023-11-08 05:51:25,524 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 0.1544
2023-11-08 05:51:41,583 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 0.1545
2023-11-08 05:51:55,374 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 0.2801
2023-11-08 05:52:12,005 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 0.2352
2023-11-08 05:52:28,482 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 0.2100
2023-11-08 05:52:44,392 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 0.2237
2023-11-08 05:53:01,519 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 0.1582
2023-11-08 05:53:17,915 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 0.1469
2023-11-08 05:53:35,410 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 0.1626
2023-11-08 05:53:48,627 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 0.2147
2023-11-08 05:54:05,678 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 0.2497
2023-11-08 05:54:21,959 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 0.2310
2023-11-08 05:54:38,221 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 0.2075
2023-11-08 05:54:55,054 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 0.1499
2023-11-08 05:55:12,089 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 0.1379
2023-11-08 05:55:28,481 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 0.1481
2023-11-08 05:55:40,160 - __main__ - INFO - ------------ Save best model - AUROC: 0.8425 ------------
2023-11-08 05:55:41,109 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 0.2505
2023-11-08 05:55:57,935 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 0.2334
2023-11-08 05:56:13,648 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 0.1975
2023-11-08 05:56:31,251 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 0.1971
2023-11-08 05:56:47,155 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 0.1628
2023-11-08 05:57:03,585 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 0.1336
2023-11-08 05:57:19,621 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 0.1505
2023-11-08 05:57:32,016 - __main__ - INFO - ------------ Save best model - AUROC: 0.8431 ------------
2023-11-08 05:57:32,897 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 0.2196
2023-11-08 05:57:50,034 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 0.2563
2023-11-08 05:58:06,608 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 0.2250
2023-11-08 05:58:22,744 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 0.2031
2023-11-08 05:58:38,367 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 0.1523
2023-11-08 05:58:54,300 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 0.1514
2023-11-08 05:59:10,604 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 0.1560
2023-11-08 05:59:23,853 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 0.2366
2023-11-08 05:59:40,677 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 0.2343
2023-11-08 05:59:57,187 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 0.1982
2023-11-08 06:00:13,568 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 0.1994
2023-11-08 06:00:30,215 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 0.1618
2023-11-08 06:00:46,989 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 0.1349
2023-11-08 06:01:03,247 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 0.1522
2023-11-08 06:01:15,618 - __main__ - INFO - ------------ Save best model - AUROC: 0.8492 ------------
2023-11-08 06:01:16,330 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 0.2179
2023-11-08 06:01:32,896 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 0.2568
2023-11-08 06:01:48,762 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 0.2271
2023-11-08 06:02:04,586 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 0.2189
2023-11-08 06:02:20,932 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 0.1610
2023-11-08 06:02:36,835 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 0.1278
2023-11-08 06:02:53,312 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 0.1502
2023-11-08 06:03:04,268 - __main__ - INFO - ------------ Save best model - AUROC: 0.8500 ------------
2023-11-08 06:03:04,823 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 0.2642
2023-11-08 06:03:21,439 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 0.2325
2023-11-08 06:03:37,389 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 0.2177
2023-11-08 06:03:53,376 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 0.2149
2023-11-08 06:04:09,843 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 0.1726
2023-11-08 06:04:27,321 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 0.1464
2023-11-08 06:04:44,668 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 0.1681
2023-11-08 06:04:56,419 - __main__ - INFO - ------------ Save best model - AUROC: 0.8501 ------------
2023-11-08 06:04:57,245 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 0.2176
2023-11-08 06:05:13,228 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 0.2464
2023-11-08 06:05:28,199 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 0.2373
2023-11-08 06:05:43,674 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 0.2183
2023-11-08 06:06:00,380 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 0.1513
2023-11-08 06:06:17,857 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 0.1374
2023-11-08 06:06:34,845 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 0.1546
2023-11-08 06:06:47,122 - __main__ - INFO - ------------ Save best model - AUROC: 0.8556 ------------
2023-11-08 06:06:47,822 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 0.2123
2023-11-08 06:07:04,033 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 0.2430
2023-11-08 06:07:20,495 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 0.2193
2023-11-08 06:07:37,170 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 0.2130
2023-11-08 06:07:53,443 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 0.1617
2023-11-08 06:08:10,072 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 0.1400
2023-11-08 06:08:25,750 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 0.1692
2023-11-08 06:08:38,430 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 0.2145
2023-11-08 06:08:54,734 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 0.2286
2023-11-08 06:09:10,611 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 0.2156
2023-11-08 06:09:26,786 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 0.2494
2023-11-08 06:09:43,498 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 0.1495
2023-11-08 06:09:59,164 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 0.1471
2023-11-08 06:10:14,824 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 0.1562
2023-11-08 06:10:27,869 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 0.2256
2023-11-08 06:10:44,190 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 0.2141
2023-11-08 06:10:59,935 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 0.2203
2023-11-08 06:11:15,537 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 0.2398
2023-11-08 06:11:31,556 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 0.1712
2023-11-08 06:11:48,610 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 0.1523
2023-11-08 06:12:05,240 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 0.1550
2023-11-08 06:12:18,221 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 0.2186
2023-11-08 06:12:34,988 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 0.2290
2023-11-08 06:12:51,194 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 0.2382
2023-11-08 06:13:07,885 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 0.3002
2023-11-08 06:13:25,401 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 0.1568
2023-11-08 06:13:41,682 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 0.1206
2023-11-08 06:13:57,763 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 0.1716
2023-11-08 06:14:11,415 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 0.2112
2023-11-08 06:14:28,188 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 0.2126
2023-11-08 06:14:44,782 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 0.2105
2023-11-08 06:15:00,675 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 0.2225
2023-11-08 06:15:17,702 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 0.1623
2023-11-08 06:15:34,383 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 0.1436
2023-11-08 06:15:49,728 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 0.1565
2023-11-08 06:16:01,442 - __main__ - INFO - ------------ Save best model - AUROC: 0.8558 ------------
2023-11-08 06:16:02,235 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 0.2182
2023-11-08 06:16:17,957 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 0.2058
2023-11-08 06:16:34,677 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 0.2150
2023-11-08 06:16:51,016 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 0.2295
2023-11-08 06:17:07,044 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 0.1573
2023-11-08 06:17:24,545 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 0.1342
2023-11-08 06:17:40,963 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 0.1518
2023-11-08 06:17:53,599 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 0.2145
2023-11-08 06:18:10,750 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 0.2125
2023-11-08 06:18:27,052 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 0.2303
2023-11-08 06:18:42,692 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 0.2115
2023-11-08 06:18:59,237 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 0.1642
2023-11-08 06:19:15,922 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 0.1211
2023-11-08 06:19:32,146 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 0.1585
2023-11-08 06:19:45,123 - __main__ - INFO - ------------ Save best model - AUROC: 0.8614 ------------
2023-11-08 06:19:45,843 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 0.2252
2023-11-08 06:20:01,811 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 0.2212
2023-11-08 06:20:17,861 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 0.2047
2023-11-08 06:20:34,126 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 0.1853
2023-11-08 06:20:50,439 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 0.1713
2023-11-08 06:21:06,500 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 0.1286
2023-11-08 06:21:21,813 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 0.1471
2023-11-08 06:21:35,248 - __main__ - INFO - ------------ Save best model - AUROC: 0.8618 ------------
2023-11-08 06:21:35,946 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 0.2190
2023-11-08 06:21:53,455 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 0.2253
2023-11-08 06:22:10,690 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 0.2108
2023-11-08 06:22:26,947 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 0.2291
2023-11-08 06:22:45,376 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 0.1573
2023-11-08 06:23:01,968 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 0.1239
2023-11-08 06:23:18,534 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 0.1354
2023-11-08 06:23:30,554 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 0.2131
2023-11-08 06:23:47,430 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 0.2202
2023-11-08 06:24:02,448 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 0.2326
2023-11-08 06:24:17,331 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 0.2157
2023-11-08 06:24:32,804 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 0.1587
2023-11-08 06:24:47,562 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 0.1384
2023-11-08 06:25:01,473 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 0.1692
2023-11-08 06:25:12,832 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 0.2079
2023-11-08 06:25:28,564 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 0.2396
2023-11-08 06:25:43,227 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 0.2349
2023-11-08 06:25:57,813 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 0.2073
2023-11-08 06:26:12,490 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 0.1627
2023-11-08 06:26:27,270 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 0.1373
2023-11-08 06:26:42,341 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 0.1675
2023-11-08 06:26:53,808 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 0.2021
2023-11-08 06:27:09,241 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 0.2210
2023-11-08 06:27:23,848 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 0.2286
2023-11-08 06:27:38,799 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 0.2254
2023-11-08 06:27:55,056 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 0.1551
2023-11-08 06:28:10,269 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 0.1362
2023-11-08 06:28:24,568 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 0.1534
2023-11-08 06:28:35,638 - __main__ - INFO - ------------ Save best model - AUROC: 0.8652 ------------
2023-11-08 06:28:36,492 - __main__ - INFO - Epoch 30 Batch 0: Train Loss = 0.2707
2023-11-08 06:28:52,089 - __main__ - INFO - Epoch 30 Batch 20: Train Loss = 0.2370
2023-11-08 06:29:06,965 - __main__ - INFO - Epoch 30 Batch 40: Train Loss = 0.2376
2023-11-08 06:29:20,612 - __main__ - INFO - Epoch 30 Batch 60: Train Loss = 0.2240
2023-11-08 06:29:35,210 - __main__ - INFO - Epoch 30 Batch 80: Train Loss = 0.1433
2023-11-08 06:29:50,178 - __main__ - INFO - Epoch 30 Batch 100: Train Loss = 0.1404
2023-11-08 06:30:04,663 - __main__ - INFO - Epoch 30 Batch 120: Train Loss = 0.1396
2023-11-08 06:30:15,860 - __main__ - INFO - ------------ Save best model - AUROC: 0.8672 ------------
2023-11-08 06:30:16,658 - __main__ - INFO - Epoch 31 Batch 0: Train Loss = 0.2156
2023-11-08 06:30:31,285 - __main__ - INFO - Epoch 31 Batch 20: Train Loss = 0.2009
2023-11-08 06:30:46,770 - __main__ - INFO - Epoch 31 Batch 40: Train Loss = 0.2215
2023-11-08 06:31:01,644 - __main__ - INFO - Epoch 31 Batch 60: Train Loss = 0.2430
2023-11-08 06:31:16,266 - __main__ - INFO - Epoch 31 Batch 80: Train Loss = 0.1492
2023-11-08 06:31:31,416 - __main__ - INFO - Epoch 31 Batch 100: Train Loss = 0.1353
2023-11-08 06:31:45,972 - __main__ - INFO - Epoch 31 Batch 120: Train Loss = 0.1427
2023-11-08 06:31:57,534 - __main__ - INFO - Epoch 32 Batch 0: Train Loss = 0.2390
2023-11-08 06:32:12,410 - __main__ - INFO - Epoch 32 Batch 20: Train Loss = 0.2193
2023-11-08 06:32:26,415 - __main__ - INFO - Epoch 32 Batch 40: Train Loss = 0.2133
2023-11-08 06:32:41,479 - __main__ - INFO - Epoch 32 Batch 60: Train Loss = 0.2371
2023-11-08 06:32:56,191 - __main__ - INFO - Epoch 32 Batch 80: Train Loss = 0.1602
2023-11-08 06:33:11,786 - __main__ - INFO - Epoch 32 Batch 100: Train Loss = 0.1283
2023-11-08 06:33:26,453 - __main__ - INFO - Epoch 32 Batch 120: Train Loss = 0.1588
2023-11-08 06:33:38,264 - __main__ - INFO - Epoch 33 Batch 0: Train Loss = 0.2072
2023-11-08 06:33:54,215 - __main__ - INFO - Epoch 33 Batch 20: Train Loss = 0.2373
2023-11-08 06:34:08,804 - __main__ - INFO - Epoch 33 Batch 40: Train Loss = 0.2217
2023-11-08 06:34:23,809 - __main__ - INFO - Epoch 33 Batch 60: Train Loss = 0.2234
2023-11-08 06:34:38,949 - __main__ - INFO - Epoch 33 Batch 80: Train Loss = 0.1614
2023-11-08 06:34:53,635 - __main__ - INFO - Epoch 33 Batch 100: Train Loss = 0.1371
2023-11-08 06:35:07,537 - __main__ - INFO - Epoch 33 Batch 120: Train Loss = 0.1784
2023-11-08 06:35:19,640 - __main__ - INFO - Epoch 34 Batch 0: Train Loss = 0.2403
2023-11-08 06:35:35,056 - __main__ - INFO - Epoch 34 Batch 20: Train Loss = 0.2052
2023-11-08 06:35:50,037 - __main__ - INFO - Epoch 34 Batch 40: Train Loss = 0.2159
2023-11-08 06:36:05,238 - __main__ - INFO - Epoch 34 Batch 60: Train Loss = 0.2156
2023-11-08 06:36:20,806 - __main__ - INFO - Epoch 34 Batch 80: Train Loss = 0.1572
2023-11-08 06:36:36,380 - __main__ - INFO - Epoch 34 Batch 100: Train Loss = 0.1304
2023-11-08 06:36:51,162 - __main__ - INFO - Epoch 34 Batch 120: Train Loss = 0.1478
2023-11-08 06:37:01,874 - __main__ - INFO - ------------ Save best model - AUROC: 0.8680 ------------
2023-11-08 06:37:02,440 - __main__ - INFO - Epoch 35 Batch 0: Train Loss = 0.2335
2023-11-08 06:37:17,647 - __main__ - INFO - Epoch 35 Batch 20: Train Loss = 0.2282
2023-11-08 06:37:31,663 - __main__ - INFO - Epoch 35 Batch 40: Train Loss = 0.2396
2023-11-08 06:37:46,697 - __main__ - INFO - Epoch 35 Batch 60: Train Loss = 0.2359
2023-11-08 06:38:01,540 - __main__ - INFO - Epoch 35 Batch 80: Train Loss = 0.1854
2023-11-08 06:38:16,764 - __main__ - INFO - Epoch 35 Batch 100: Train Loss = 0.1254
2023-11-08 06:38:29,923 - __main__ - INFO - Epoch 35 Batch 120: Train Loss = 0.1388
2023-11-08 06:38:40,770 - __main__ - INFO - ------------ Save best model - AUROC: 0.8683 ------------
2023-11-08 06:38:41,397 - __main__ - INFO - Epoch 36 Batch 0: Train Loss = 0.1791
2023-11-08 06:38:55,932 - __main__ - INFO - Epoch 36 Batch 20: Train Loss = 0.2323
2023-11-08 06:39:10,008 - __main__ - INFO - Epoch 36 Batch 40: Train Loss = 0.2417
2023-11-08 06:39:24,512 - __main__ - INFO - Epoch 36 Batch 60: Train Loss = 0.2120
2023-11-08 06:39:38,591 - __main__ - INFO - Epoch 36 Batch 80: Train Loss = 0.1504
2023-11-08 06:39:52,692 - __main__ - INFO - Epoch 36 Batch 100: Train Loss = 0.1634
2023-11-08 06:40:06,894 - __main__ - INFO - Epoch 36 Batch 120: Train Loss = 0.1624
2023-11-08 06:40:18,486 - __main__ - INFO - Epoch 37 Batch 0: Train Loss = 0.1967
2023-11-08 06:40:33,360 - __main__ - INFO - Epoch 37 Batch 20: Train Loss = 0.2409
2023-11-08 06:40:48,764 - __main__ - INFO - Epoch 37 Batch 40: Train Loss = 0.2133
2023-11-08 06:41:03,002 - __main__ - INFO - Epoch 37 Batch 60: Train Loss = 0.2271
2023-11-08 06:41:17,161 - __main__ - INFO - Epoch 37 Batch 80: Train Loss = 0.1555
2023-11-08 06:41:31,543 - __main__ - INFO - Epoch 37 Batch 100: Train Loss = 0.1290
2023-11-08 06:41:46,103 - __main__ - INFO - Epoch 37 Batch 120: Train Loss = 0.1531
2023-11-08 06:41:56,780 - __main__ - INFO - ------------ Save best model - AUROC: 0.8751 ------------
2023-11-08 06:41:57,407 - __main__ - INFO - Epoch 38 Batch 0: Train Loss = 0.2581
2023-11-08 06:42:12,299 - __main__ - INFO - Epoch 38 Batch 20: Train Loss = 0.2529
2023-11-08 06:42:27,285 - __main__ - INFO - Epoch 38 Batch 40: Train Loss = 0.2254
2023-11-08 06:42:41,546 - __main__ - INFO - Epoch 38 Batch 60: Train Loss = 0.2174
2023-11-08 06:42:57,106 - __main__ - INFO - Epoch 38 Batch 80: Train Loss = 0.1877
2023-11-08 06:43:11,668 - __main__ - INFO - Epoch 38 Batch 100: Train Loss = 0.1502
2023-11-08 06:43:25,607 - __main__ - INFO - Epoch 38 Batch 120: Train Loss = 0.1330
2023-11-08 06:43:36,641 - __main__ - INFO - Epoch 39 Batch 0: Train Loss = 0.1990
2023-11-08 06:43:51,688 - __main__ - INFO - Epoch 39 Batch 20: Train Loss = 0.2200
2023-11-08 06:44:06,575 - __main__ - INFO - Epoch 39 Batch 40: Train Loss = 0.2310
2023-11-08 06:44:21,016 - __main__ - INFO - Epoch 39 Batch 60: Train Loss = 0.2412
2023-11-08 06:44:36,312 - __main__ - INFO - Epoch 39 Batch 80: Train Loss = 0.1699
2023-11-08 06:44:50,660 - __main__ - INFO - Epoch 39 Batch 100: Train Loss = 0.1332
2023-11-08 06:45:05,195 - __main__ - INFO - Epoch 39 Batch 120: Train Loss = 0.1635
2023-11-08 06:45:15,986 - __main__ - INFO - Epoch 40 Batch 0: Train Loss = 0.2151
2023-11-08 06:45:31,212 - __main__ - INFO - Epoch 40 Batch 20: Train Loss = 0.2160
2023-11-08 06:45:45,700 - __main__ - INFO - Epoch 40 Batch 40: Train Loss = 0.2274
2023-11-08 06:46:00,393 - __main__ - INFO - Epoch 40 Batch 60: Train Loss = 0.2121
2023-11-08 06:46:14,890 - __main__ - INFO - Epoch 40 Batch 80: Train Loss = 0.1582
2023-11-08 06:46:28,709 - __main__ - INFO - Epoch 40 Batch 100: Train Loss = 0.1283
2023-11-08 06:46:42,099 - __main__ - INFO - Epoch 40 Batch 120: Train Loss = 0.1570
2023-11-08 06:46:53,743 - __main__ - INFO - Epoch 41 Batch 0: Train Loss = 0.1944
2023-11-08 06:47:08,633 - __main__ - INFO - Epoch 41 Batch 20: Train Loss = 0.2540
2023-11-08 06:47:24,703 - __main__ - INFO - Epoch 41 Batch 40: Train Loss = 0.2387
2023-11-08 06:47:39,415 - __main__ - INFO - Epoch 41 Batch 60: Train Loss = 0.2223
2023-11-08 06:47:53,648 - __main__ - INFO - Epoch 41 Batch 80: Train Loss = 0.1532
2023-11-08 06:48:08,979 - __main__ - INFO - Epoch 41 Batch 100: Train Loss = 0.1341
2023-11-08 06:48:23,754 - __main__ - INFO - Epoch 41 Batch 120: Train Loss = 0.1442
2023-11-08 06:48:34,748 - __main__ - INFO - Epoch 42 Batch 0: Train Loss = 0.1962
2023-11-08 06:48:49,820 - __main__ - INFO - Epoch 42 Batch 20: Train Loss = 0.2503
2023-11-08 06:49:04,641 - __main__ - INFO - Epoch 42 Batch 40: Train Loss = 0.2193
2023-11-08 06:49:18,567 - __main__ - INFO - Epoch 42 Batch 60: Train Loss = 0.2217
2023-11-08 06:49:32,883 - __main__ - INFO - Epoch 42 Batch 80: Train Loss = 0.2223
2023-11-08 06:49:47,719 - __main__ - INFO - Epoch 42 Batch 100: Train Loss = 0.1431
2023-11-08 06:50:02,005 - __main__ - INFO - Epoch 42 Batch 120: Train Loss = 0.1537
2023-11-08 06:50:13,771 - __main__ - INFO - Epoch 43 Batch 0: Train Loss = 0.2186
2023-11-08 06:50:28,908 - __main__ - INFO - Epoch 43 Batch 20: Train Loss = 0.2419
2023-11-08 06:50:43,159 - __main__ - INFO - Epoch 43 Batch 40: Train Loss = 0.2885
2023-11-08 06:50:58,046 - __main__ - INFO - Epoch 43 Batch 60: Train Loss = 0.3083
2023-11-08 06:51:12,873 - __main__ - INFO - Epoch 43 Batch 80: Train Loss = 0.1599
2023-11-08 06:51:27,435 - __main__ - INFO - Epoch 43 Batch 100: Train Loss = 0.1426
2023-11-08 06:51:42,370 - __main__ - INFO - Epoch 43 Batch 120: Train Loss = 0.1474
2023-11-08 06:51:52,678 - __main__ - INFO - Epoch 44 Batch 0: Train Loss = 0.2142
2023-11-08 06:52:08,395 - __main__ - INFO - Epoch 44 Batch 20: Train Loss = 0.2290
2023-11-08 06:52:22,405 - __main__ - INFO - Epoch 44 Batch 40: Train Loss = 0.2129
2023-11-08 06:52:37,543 - __main__ - INFO - Epoch 44 Batch 60: Train Loss = 0.2259
2023-11-08 06:52:52,584 - __main__ - INFO - Epoch 44 Batch 80: Train Loss = 0.1654
2023-11-08 06:53:07,041 - __main__ - INFO - Epoch 44 Batch 100: Train Loss = 0.1424
2023-11-08 06:53:22,080 - __main__ - INFO - Epoch 44 Batch 120: Train Loss = 0.1649
2023-11-08 06:53:33,618 - __main__ - INFO - Epoch 45 Batch 0: Train Loss = 0.2397
2023-11-08 06:53:48,121 - __main__ - INFO - Epoch 45 Batch 20: Train Loss = 0.2265
2023-11-08 06:54:02,115 - __main__ - INFO - Epoch 45 Batch 40: Train Loss = 0.2311
2023-11-08 06:54:16,815 - __main__ - INFO - Epoch 45 Batch 60: Train Loss = 0.2446
2023-11-08 06:54:32,809 - __main__ - INFO - Epoch 45 Batch 80: Train Loss = 0.1760
2023-11-08 06:54:48,087 - __main__ - INFO - Epoch 45 Batch 100: Train Loss = 0.1546
2023-11-08 06:55:02,743 - __main__ - INFO - Epoch 45 Batch 120: Train Loss = 0.1926
2023-11-08 06:55:15,152 - __main__ - INFO - Epoch 46 Batch 0: Train Loss = 0.2346
2023-11-08 06:55:29,627 - __main__ - INFO - Epoch 46 Batch 20: Train Loss = 0.2394
2023-11-08 06:55:43,608 - __main__ - INFO - Epoch 46 Batch 40: Train Loss = 0.2001
2023-11-08 06:55:57,950 - __main__ - INFO - Epoch 46 Batch 60: Train Loss = 0.1879
2023-11-08 06:56:12,410 - __main__ - INFO - Epoch 46 Batch 80: Train Loss = 0.1503
2023-11-08 06:56:28,038 - __main__ - INFO - Epoch 46 Batch 100: Train Loss = 0.1425
2023-11-08 06:56:42,455 - __main__ - INFO - Epoch 46 Batch 120: Train Loss = 0.1687
2023-11-08 06:56:53,956 - __main__ - INFO - Epoch 47 Batch 0: Train Loss = 0.2354
2023-11-08 06:57:08,797 - __main__ - INFO - Epoch 47 Batch 20: Train Loss = 0.2513
2023-11-08 06:57:23,022 - __main__ - INFO - Epoch 47 Batch 40: Train Loss = 0.2377
2023-11-08 06:57:37,601 - __main__ - INFO - Epoch 47 Batch 60: Train Loss = 0.2259
2023-11-08 06:57:52,291 - __main__ - INFO - Epoch 47 Batch 80: Train Loss = 0.1448
2023-11-08 06:58:06,807 - __main__ - INFO - Epoch 47 Batch 100: Train Loss = 0.1264
2023-11-08 06:58:21,815 - __main__ - INFO - Epoch 47 Batch 120: Train Loss = 0.1659
2023-11-08 06:58:33,552 - __main__ - INFO - Epoch 48 Batch 0: Train Loss = 0.2108
2023-11-08 06:58:49,988 - __main__ - INFO - Epoch 48 Batch 20: Train Loss = 0.2297
2023-11-08 06:59:04,651 - __main__ - INFO - Epoch 48 Batch 40: Train Loss = 0.2262
2023-11-08 06:59:19,818 - __main__ - INFO - Epoch 48 Batch 60: Train Loss = 0.2174
2023-11-08 06:59:34,190 - __main__ - INFO - Epoch 48 Batch 80: Train Loss = 0.1645
2023-11-08 06:59:49,468 - __main__ - INFO - Epoch 48 Batch 100: Train Loss = 0.1410
2023-11-08 07:00:04,429 - __main__ - INFO - Epoch 48 Batch 120: Train Loss = 0.1693
2023-11-08 07:00:15,864 - __main__ - INFO - Epoch 49 Batch 0: Train Loss = 0.2173
2023-11-08 07:00:30,128 - __main__ - INFO - Epoch 49 Batch 20: Train Loss = 0.2408
2023-11-08 07:00:43,666 - __main__ - INFO - Epoch 49 Batch 40: Train Loss = 0.2108
2023-11-08 07:00:57,610 - __main__ - INFO - Epoch 49 Batch 60: Train Loss = 0.2614
2023-11-08 07:01:11,479 - __main__ - INFO - Epoch 49 Batch 80: Train Loss = 0.1599
2023-11-08 07:01:26,608 - __main__ - INFO - Epoch 49 Batch 100: Train Loss = 0.1197
2023-11-08 07:01:41,277 - __main__ - INFO - Epoch 49 Batch 120: Train Loss = 0.1682
2023-11-08 07:01:52,787 - __main__ - INFO - Epoch 50 Batch 0: Train Loss = 0.2146
2023-11-08 07:02:07,699 - __main__ - INFO - Epoch 50 Batch 20: Train Loss = 0.3138
2023-11-08 07:02:21,224 - __main__ - INFO - Epoch 50 Batch 40: Train Loss = 0.2300
2023-11-08 07:02:35,822 - __main__ - INFO - Epoch 50 Batch 60: Train Loss = 0.2132
2023-11-08 07:02:50,418 - __main__ - INFO - Epoch 50 Batch 80: Train Loss = 0.1637
2023-11-08 07:03:04,437 - __main__ - INFO - Epoch 50 Batch 100: Train Loss = 0.1275
2023-11-08 07:03:19,664 - __main__ - INFO - Epoch 50 Batch 120: Train Loss = 0.1910
2023-11-08 07:03:30,673 - __main__ - INFO - Epoch 51 Batch 0: Train Loss = 0.2120
2023-11-08 07:03:45,962 - __main__ - INFO - Epoch 51 Batch 20: Train Loss = 0.2309
2023-11-08 07:03:59,940 - __main__ - INFO - Epoch 51 Batch 40: Train Loss = 0.2265
2023-11-08 07:04:14,093 - __main__ - INFO - Epoch 51 Batch 60: Train Loss = 0.2132
2023-11-08 07:04:29,162 - __main__ - INFO - Epoch 51 Batch 80: Train Loss = 0.1749
2023-11-08 07:04:43,261 - __main__ - INFO - Epoch 51 Batch 100: Train Loss = 0.1616
2023-11-08 07:04:57,509 - __main__ - INFO - Epoch 51 Batch 120: Train Loss = 0.1600
2023-11-08 07:05:08,588 - __main__ - INFO - Epoch 52 Batch 0: Train Loss = 0.2454
2023-11-08 07:05:23,290 - __main__ - INFO - Epoch 52 Batch 20: Train Loss = 0.2202
2023-11-08 07:05:37,608 - __main__ - INFO - Epoch 52 Batch 40: Train Loss = 0.2149
2023-11-08 07:05:51,771 - __main__ - INFO - Epoch 52 Batch 60: Train Loss = 0.1951
2023-11-08 07:06:06,841 - __main__ - INFO - Epoch 52 Batch 80: Train Loss = 0.1367
2023-11-08 07:06:21,520 - __main__ - INFO - Epoch 52 Batch 100: Train Loss = 0.1465
2023-11-08 07:06:36,302 - __main__ - INFO - Epoch 52 Batch 120: Train Loss = 0.1320
2023-11-08 07:06:46,440 - __main__ - INFO - ------------ Save best model - AUROC: 0.8757 ------------
2023-11-08 07:06:46,985 - __main__ - INFO - Epoch 53 Batch 0: Train Loss = 0.2117
2023-11-08 07:07:01,151 - __main__ - INFO - Epoch 53 Batch 20: Train Loss = 0.2410
2023-11-08 07:07:15,135 - __main__ - INFO - Epoch 53 Batch 40: Train Loss = 0.1982
2023-11-08 07:07:29,667 - __main__ - INFO - Epoch 53 Batch 60: Train Loss = 0.2536
2023-11-08 07:07:44,685 - __main__ - INFO - Epoch 53 Batch 80: Train Loss = 0.1584
2023-11-08 07:07:59,154 - __main__ - INFO - Epoch 53 Batch 100: Train Loss = 0.1537
2023-11-08 07:08:14,424 - __main__ - INFO - Epoch 53 Batch 120: Train Loss = 0.1615
2023-11-08 07:08:26,376 - __main__ - INFO - Epoch 54 Batch 0: Train Loss = 0.2144
2023-11-08 07:08:41,893 - __main__ - INFO - Epoch 54 Batch 20: Train Loss = 0.2409
2023-11-08 07:08:56,053 - __main__ - INFO - Epoch 54 Batch 40: Train Loss = 0.1926
2023-11-08 07:09:10,512 - __main__ - INFO - Epoch 54 Batch 60: Train Loss = 0.2228
2023-11-08 07:09:25,978 - __main__ - INFO - Epoch 54 Batch 80: Train Loss = 0.1440
2023-11-08 07:09:40,881 - __main__ - INFO - Epoch 54 Batch 100: Train Loss = 0.1299
2023-11-08 07:09:55,198 - __main__ - INFO - Epoch 54 Batch 120: Train Loss = 0.1533
2023-11-08 07:10:07,530 - __main__ - INFO - Epoch 55 Batch 0: Train Loss = 0.2350
2023-11-08 07:10:22,462 - __main__ - INFO - Epoch 55 Batch 20: Train Loss = 0.2505
2023-11-08 07:10:36,919 - __main__ - INFO - Epoch 55 Batch 40: Train Loss = 0.2258
2023-11-08 07:10:51,581 - __main__ - INFO - Epoch 55 Batch 60: Train Loss = 0.2002
2023-11-08 07:11:07,165 - __main__ - INFO - Epoch 55 Batch 80: Train Loss = 0.1606
2023-11-08 07:11:21,778 - __main__ - INFO - Epoch 55 Batch 100: Train Loss = 0.1120
2023-11-08 07:11:35,864 - __main__ - INFO - Epoch 55 Batch 120: Train Loss = 0.1505
2023-11-08 07:11:46,352 - __main__ - INFO - Epoch 56 Batch 0: Train Loss = 0.1990
2023-11-08 07:12:01,302 - __main__ - INFO - Epoch 56 Batch 20: Train Loss = 0.2372
2023-11-08 07:12:15,569 - __main__ - INFO - Epoch 56 Batch 40: Train Loss = 0.2058
2023-11-08 07:12:29,535 - __main__ - INFO - Epoch 56 Batch 60: Train Loss = 0.2216
2023-11-08 07:12:44,462 - __main__ - INFO - Epoch 56 Batch 80: Train Loss = 0.1649
2023-11-08 07:12:59,715 - __main__ - INFO - Epoch 56 Batch 100: Train Loss = 0.1269
2023-11-08 07:13:14,652 - __main__ - INFO - Epoch 56 Batch 120: Train Loss = 0.1535
2023-11-08 07:13:26,274 - __main__ - INFO - Epoch 57 Batch 0: Train Loss = 0.2752
2023-11-08 07:13:41,435 - __main__ - INFO - Epoch 57 Batch 20: Train Loss = 0.2180
2023-11-08 07:13:55,156 - __main__ - INFO - Epoch 57 Batch 40: Train Loss = 0.2236
2023-11-08 07:14:09,274 - __main__ - INFO - Epoch 57 Batch 60: Train Loss = 0.2442
2023-11-08 07:14:23,985 - __main__ - INFO - Epoch 57 Batch 80: Train Loss = 0.1790
2023-11-08 07:14:39,104 - __main__ - INFO - Epoch 57 Batch 100: Train Loss = 0.1199
2023-11-08 07:14:53,364 - __main__ - INFO - Epoch 57 Batch 120: Train Loss = 0.1487
2023-11-08 07:15:04,561 - __main__ - INFO - Epoch 58 Batch 0: Train Loss = 0.2191
2023-11-08 07:15:20,019 - __main__ - INFO - Epoch 58 Batch 20: Train Loss = 0.2225
2023-11-08 07:15:34,386 - __main__ - INFO - Epoch 58 Batch 40: Train Loss = 0.2056
2023-11-08 07:15:48,362 - __main__ - INFO - Epoch 58 Batch 60: Train Loss = 0.2126
2023-11-08 07:16:03,793 - __main__ - INFO - Epoch 58 Batch 80: Train Loss = 0.1584
2023-11-08 07:16:18,007 - __main__ - INFO - Epoch 58 Batch 100: Train Loss = 0.1627
2023-11-08 07:16:33,163 - __main__ - INFO - Epoch 58 Batch 120: Train Loss = 0.1490
2023-11-08 07:16:43,415 - __main__ - INFO - Epoch 59 Batch 0: Train Loss = 0.2096
2023-11-08 07:16:59,131 - __main__ - INFO - Epoch 59 Batch 20: Train Loss = 0.2046
2023-11-08 07:17:13,097 - __main__ - INFO - Epoch 59 Batch 40: Train Loss = 0.2188
2023-11-08 07:17:28,157 - __main__ - INFO - Epoch 59 Batch 60: Train Loss = 0.2149
2023-11-08 07:17:42,518 - __main__ - INFO - Epoch 59 Batch 80: Train Loss = 0.1487
2023-11-08 07:17:56,847 - __main__ - INFO - Epoch 59 Batch 100: Train Loss = 0.1281
2023-11-08 07:18:11,633 - __main__ - INFO - Epoch 59 Batch 120: Train Loss = 0.1652
2023-11-08 07:18:23,429 - __main__ - INFO - Epoch 60 Batch 0: Train Loss = 0.2259
2023-11-08 07:18:38,376 - __main__ - INFO - Epoch 60 Batch 20: Train Loss = 0.3465
2023-11-08 07:18:52,055 - __main__ - INFO - Epoch 60 Batch 40: Train Loss = 0.3004
2023-11-08 07:19:06,270 - __main__ - INFO - Epoch 60 Batch 60: Train Loss = 0.2390
2023-11-08 07:19:20,929 - __main__ - INFO - Epoch 60 Batch 80: Train Loss = 0.1775
2023-11-08 07:19:36,269 - __main__ - INFO - Epoch 60 Batch 100: Train Loss = 0.1423
2023-11-08 07:19:50,831 - __main__ - INFO - Epoch 60 Batch 120: Train Loss = 0.1494
2023-11-08 07:20:02,073 - __main__ - INFO - Epoch 61 Batch 0: Train Loss = 0.2073
2023-11-08 07:20:18,075 - __main__ - INFO - Epoch 61 Batch 20: Train Loss = 0.2080
2023-11-08 07:20:33,290 - __main__ - INFO - Epoch 61 Batch 40: Train Loss = 0.2478
2023-11-08 07:20:47,537 - __main__ - INFO - Epoch 61 Batch 60: Train Loss = 0.2424
2023-11-08 07:21:02,440 - __main__ - INFO - Epoch 61 Batch 80: Train Loss = 0.1930
2023-11-08 07:21:17,075 - __main__ - INFO - Epoch 61 Batch 100: Train Loss = 0.1243
2023-11-08 07:21:32,028 - __main__ - INFO - Epoch 61 Batch 120: Train Loss = 0.1647
2023-11-08 07:21:43,272 - __main__ - INFO - Epoch 62 Batch 0: Train Loss = 0.2009
2023-11-08 07:21:58,508 - __main__ - INFO - Epoch 62 Batch 20: Train Loss = 0.2295
2023-11-08 07:22:12,313 - __main__ - INFO - Epoch 62 Batch 40: Train Loss = 0.2017
2023-11-08 07:22:27,238 - __main__ - INFO - Epoch 62 Batch 60: Train Loss = 0.2460
2023-11-08 07:22:42,282 - __main__ - INFO - Epoch 62 Batch 80: Train Loss = 0.1637
2023-11-08 07:22:56,354 - __main__ - INFO - Epoch 62 Batch 100: Train Loss = 0.1336
2023-11-08 07:23:10,376 - __main__ - INFO - Epoch 62 Batch 120: Train Loss = 0.1359
2023-11-08 07:23:21,831 - __main__ - INFO - Epoch 63 Batch 0: Train Loss = 0.2482
2023-11-08 07:23:36,571 - __main__ - INFO - Epoch 63 Batch 20: Train Loss = 0.2488
2023-11-08 07:23:51,200 - __main__ - INFO - Epoch 63 Batch 40: Train Loss = 0.2387
2023-11-08 07:24:05,776 - __main__ - INFO - Epoch 63 Batch 60: Train Loss = 0.2137
2023-11-08 07:24:20,778 - __main__ - INFO - Epoch 63 Batch 80: Train Loss = 0.1571
2023-11-08 07:24:35,371 - __main__ - INFO - Epoch 63 Batch 100: Train Loss = 0.1197
2023-11-08 07:24:50,047 - __main__ - INFO - Epoch 63 Batch 120: Train Loss = 0.1852
2023-11-08 07:25:01,047 - __main__ - INFO - Epoch 64 Batch 0: Train Loss = 0.2179
2023-11-08 07:25:16,405 - __main__ - INFO - Epoch 64 Batch 20: Train Loss = 0.2332
2023-11-08 07:25:30,629 - __main__ - INFO - Epoch 64 Batch 40: Train Loss = 0.2151
2023-11-08 07:25:46,647 - __main__ - INFO - Epoch 64 Batch 60: Train Loss = 0.2298
2023-11-08 07:26:01,887 - __main__ - INFO - Epoch 64 Batch 80: Train Loss = 0.1969
2023-11-08 07:26:16,211 - __main__ - INFO - Epoch 64 Batch 100: Train Loss = 0.1712
2023-11-08 07:26:30,705 - __main__ - INFO - Epoch 64 Batch 120: Train Loss = 0.1952
2023-11-08 07:26:42,175 - __main__ - INFO - Epoch 65 Batch 0: Train Loss = 0.2490
2023-11-08 07:26:56,428 - __main__ - INFO - Epoch 65 Batch 20: Train Loss = 0.2475
2023-11-08 07:27:10,552 - __main__ - INFO - Epoch 65 Batch 40: Train Loss = 0.2540
2023-11-08 07:27:26,343 - __main__ - INFO - Epoch 65 Batch 60: Train Loss = 0.1854
2023-11-08 07:27:41,290 - __main__ - INFO - Epoch 65 Batch 80: Train Loss = 0.1427
2023-11-08 07:27:56,409 - __main__ - INFO - Epoch 65 Batch 100: Train Loss = 0.1459
2023-11-08 07:28:10,939 - __main__ - INFO - Epoch 65 Batch 120: Train Loss = 0.1685
2023-11-08 07:28:22,573 - __main__ - INFO - Epoch 66 Batch 0: Train Loss = 0.2641
2023-11-08 07:28:38,100 - __main__ - INFO - Epoch 66 Batch 20: Train Loss = 0.2268
2023-11-08 07:28:52,416 - __main__ - INFO - Epoch 66 Batch 40: Train Loss = 0.2094
2023-11-08 07:29:06,622 - __main__ - INFO - Epoch 66 Batch 60: Train Loss = 0.2340
2023-11-08 07:29:21,437 - __main__ - INFO - Epoch 66 Batch 80: Train Loss = 0.1632
2023-11-08 07:29:36,140 - __main__ - INFO - Epoch 66 Batch 100: Train Loss = 0.1284
2023-11-08 07:29:50,857 - __main__ - INFO - Epoch 66 Batch 120: Train Loss = 0.1832
2023-11-08 07:30:02,200 - __main__ - INFO - Epoch 67 Batch 0: Train Loss = 0.3292
2023-11-08 07:30:17,077 - __main__ - INFO - Epoch 67 Batch 20: Train Loss = 0.2470
2023-11-08 07:30:31,732 - __main__ - INFO - Epoch 67 Batch 40: Train Loss = 0.2511
2023-11-08 07:30:46,719 - __main__ - INFO - Epoch 67 Batch 60: Train Loss = 0.1903
2023-11-08 07:31:01,354 - __main__ - INFO - Epoch 67 Batch 80: Train Loss = 0.1458
2023-11-08 07:31:15,877 - __main__ - INFO - Epoch 67 Batch 100: Train Loss = 0.1182
2023-11-08 07:31:30,295 - __main__ - INFO - Epoch 67 Batch 120: Train Loss = 0.1685
2023-11-08 07:31:42,377 - __main__ - INFO - Epoch 68 Batch 0: Train Loss = 0.3594
2023-11-08 07:31:57,221 - __main__ - INFO - Epoch 68 Batch 20: Train Loss = 0.2962
2023-11-08 07:32:11,129 - __main__ - INFO - Epoch 68 Batch 40: Train Loss = 0.2037
2023-11-08 07:32:26,121 - __main__ - INFO - Epoch 68 Batch 60: Train Loss = 0.2233
2023-11-08 07:32:40,544 - __main__ - INFO - Epoch 68 Batch 80: Train Loss = 0.1429
2023-11-08 07:32:55,191 - __main__ - INFO - Epoch 68 Batch 100: Train Loss = 0.1141
2023-11-08 07:33:09,630 - __main__ - INFO - Epoch 68 Batch 120: Train Loss = 0.2175
2023-11-08 07:33:21,101 - __main__ - INFO - Epoch 69 Batch 0: Train Loss = 0.2221
2023-11-08 07:33:36,229 - __main__ - INFO - Epoch 69 Batch 20: Train Loss = 0.2227
2023-11-08 07:33:50,662 - __main__ - INFO - Epoch 69 Batch 40: Train Loss = 0.2597
2023-11-08 07:34:04,474 - __main__ - INFO - Epoch 69 Batch 60: Train Loss = 0.1999
2023-11-08 07:34:19,488 - __main__ - INFO - Epoch 69 Batch 80: Train Loss = 0.2045
2023-11-08 07:34:34,344 - __main__ - INFO - Epoch 69 Batch 100: Train Loss = 0.1642
2023-11-08 07:34:48,894 - __main__ - INFO - Epoch 69 Batch 120: Train Loss = 0.1558
2023-11-08 07:35:00,525 - __main__ - INFO - Epoch 70 Batch 0: Train Loss = 0.2202
2023-11-08 07:35:15,589 - __main__ - INFO - Epoch 70 Batch 20: Train Loss = 0.2534
2023-11-08 07:35:30,425 - __main__ - INFO - Epoch 70 Batch 40: Train Loss = 0.2320
2023-11-08 07:35:45,318 - __main__ - INFO - Epoch 70 Batch 60: Train Loss = 0.2553
2023-11-08 07:35:59,451 - __main__ - INFO - Epoch 70 Batch 80: Train Loss = 0.1507
2023-11-08 07:36:13,969 - __main__ - INFO - Epoch 70 Batch 100: Train Loss = 0.1246
2023-11-08 07:36:28,311 - __main__ - INFO - Epoch 70 Batch 120: Train Loss = 0.1907
2023-11-08 07:36:39,704 - __main__ - INFO - Epoch 71 Batch 0: Train Loss = 0.2663
2023-11-08 07:36:55,142 - __main__ - INFO - Epoch 71 Batch 20: Train Loss = 0.2232
2023-11-08 07:37:09,541 - __main__ - INFO - Epoch 71 Batch 40: Train Loss = 0.2120
2023-11-08 07:37:23,583 - __main__ - INFO - Epoch 71 Batch 60: Train Loss = 0.1999
2023-11-08 07:37:37,831 - __main__ - INFO - Epoch 71 Batch 80: Train Loss = 0.1571
2023-11-08 07:37:52,856 - __main__ - INFO - Epoch 71 Batch 100: Train Loss = 0.1367
2023-11-08 07:38:08,020 - __main__ - INFO - Epoch 71 Batch 120: Train Loss = 0.1727
2023-11-08 07:38:19,641 - __main__ - INFO - Epoch 72 Batch 0: Train Loss = 0.2415
2023-11-08 07:38:36,182 - __main__ - INFO - Epoch 72 Batch 20: Train Loss = 0.2437
2023-11-08 07:38:50,897 - __main__ - INFO - Epoch 72 Batch 40: Train Loss = 0.2065
2023-11-08 07:39:05,007 - __main__ - INFO - Epoch 72 Batch 60: Train Loss = 0.2245
2023-11-08 07:39:20,486 - __main__ - INFO - Epoch 72 Batch 80: Train Loss = 0.1470
2023-11-08 07:39:34,931 - __main__ - INFO - Epoch 72 Batch 100: Train Loss = 0.1278
2023-11-08 07:39:49,163 - __main__ - INFO - Epoch 72 Batch 120: Train Loss = 0.1491
2023-11-08 07:40:00,133 - __main__ - INFO - Epoch 73 Batch 0: Train Loss = 0.2057
2023-11-08 07:40:15,116 - __main__ - INFO - Epoch 73 Batch 20: Train Loss = 0.2362
2023-11-08 07:40:30,064 - __main__ - INFO - Epoch 73 Batch 40: Train Loss = 0.2363
2023-11-08 07:40:44,807 - __main__ - INFO - Epoch 73 Batch 60: Train Loss = 0.1933
2023-11-08 07:40:59,065 - __main__ - INFO - Epoch 73 Batch 80: Train Loss = 0.1697
2023-11-08 07:41:13,680 - __main__ - INFO - Epoch 73 Batch 100: Train Loss = 0.1274
2023-11-08 07:41:28,297 - __main__ - INFO - Epoch 73 Batch 120: Train Loss = 0.1474
2023-11-08 07:41:40,340 - __main__ - INFO - Epoch 74 Batch 0: Train Loss = 0.2111
2023-11-08 07:41:55,214 - __main__ - INFO - Epoch 74 Batch 20: Train Loss = 0.2434
2023-11-08 07:42:10,094 - __main__ - INFO - Epoch 74 Batch 40: Train Loss = 0.2060
2023-11-08 07:42:24,067 - __main__ - INFO - Epoch 74 Batch 60: Train Loss = 0.2542
2023-11-08 07:42:38,750 - __main__ - INFO - Epoch 74 Batch 80: Train Loss = 0.1528
2023-11-08 07:42:53,675 - __main__ - INFO - Epoch 74 Batch 100: Train Loss = 0.1170
2023-11-08 07:43:08,214 - __main__ - INFO - Epoch 74 Batch 120: Train Loss = 0.1627
2023-11-08 07:43:20,167 - __main__ - INFO - Epoch 75 Batch 0: Train Loss = 0.1934
2023-11-08 07:43:34,481 - __main__ - INFO - Epoch 75 Batch 20: Train Loss = 0.2372
2023-11-08 07:43:48,883 - __main__ - INFO - Epoch 75 Batch 40: Train Loss = 0.2139
2023-11-08 07:44:03,568 - __main__ - INFO - Epoch 75 Batch 60: Train Loss = 0.2057
2023-11-08 07:44:17,709 - __main__ - INFO - Epoch 75 Batch 80: Train Loss = 0.1497
2023-11-08 07:44:32,385 - __main__ - INFO - Epoch 75 Batch 100: Train Loss = 0.1250
2023-11-08 07:44:47,076 - __main__ - INFO - Epoch 75 Batch 120: Train Loss = 0.1547
2023-11-08 07:44:59,005 - __main__ - INFO - ------------ Save best model - AUROC: 0.8765 ------------
2023-11-08 07:44:59,677 - __main__ - INFO - Epoch 76 Batch 0: Train Loss = 0.2194
2023-11-08 07:45:14,720 - __main__ - INFO - Epoch 76 Batch 20: Train Loss = 0.2075
2023-11-08 07:45:28,926 - __main__ - INFO - Epoch 76 Batch 40: Train Loss = 0.2119
2023-11-08 07:45:42,959 - __main__ - INFO - Epoch 76 Batch 60: Train Loss = 0.2262
2023-11-08 07:45:58,437 - __main__ - INFO - Epoch 76 Batch 80: Train Loss = 0.1591
2023-11-08 07:46:13,323 - __main__ - INFO - Epoch 76 Batch 100: Train Loss = 0.1362
2023-11-08 07:46:29,017 - __main__ - INFO - Epoch 76 Batch 120: Train Loss = 0.1472
2023-11-08 07:46:40,551 - __main__ - INFO - Epoch 77 Batch 0: Train Loss = 0.2237
2023-11-08 07:46:55,610 - __main__ - INFO - Epoch 77 Batch 20: Train Loss = 0.2342
2023-11-08 07:47:10,482 - __main__ - INFO - Epoch 77 Batch 40: Train Loss = 0.2471
2023-11-08 07:47:25,895 - __main__ - INFO - Epoch 77 Batch 60: Train Loss = 0.2177
2023-11-08 07:47:41,151 - __main__ - INFO - Epoch 77 Batch 80: Train Loss = 0.1717
2023-11-08 07:47:55,659 - __main__ - INFO - Epoch 77 Batch 100: Train Loss = 0.1483
2023-11-08 07:48:09,975 - __main__ - INFO - Epoch 77 Batch 120: Train Loss = 0.1710
2023-11-08 07:48:21,182 - __main__ - INFO - Epoch 78 Batch 0: Train Loss = 0.2712
2023-11-08 07:48:36,084 - __main__ - INFO - Epoch 78 Batch 20: Train Loss = 0.2551
2023-11-08 07:48:49,915 - __main__ - INFO - Epoch 78 Batch 40: Train Loss = 0.2409
2023-11-08 07:49:04,273 - __main__ - INFO - Epoch 78 Batch 60: Train Loss = 0.2003
2023-11-08 07:49:18,681 - __main__ - INFO - Epoch 78 Batch 80: Train Loss = 0.1442
2023-11-08 07:49:34,131 - __main__ - INFO - Epoch 78 Batch 100: Train Loss = 0.1209
2023-11-08 07:49:48,522 - __main__ - INFO - Epoch 78 Batch 120: Train Loss = 0.1859
2023-11-08 07:49:59,957 - __main__ - INFO - ------------ Save best model - AUROC: 0.8773 ------------
2023-11-08 07:50:00,588 - __main__ - INFO - Epoch 79 Batch 0: Train Loss = 0.2265
2023-11-08 07:50:15,635 - __main__ - INFO - Epoch 79 Batch 20: Train Loss = 0.2083
2023-11-08 07:50:30,166 - __main__ - INFO - Epoch 79 Batch 40: Train Loss = 0.2248
2023-11-08 07:50:44,939 - __main__ - INFO - Epoch 79 Batch 60: Train Loss = 0.2130
2023-11-08 07:50:59,266 - __main__ - INFO - Epoch 79 Batch 80: Train Loss = 0.1582
2023-11-08 07:51:14,404 - __main__ - INFO - Epoch 79 Batch 100: Train Loss = 0.1336
2023-11-08 07:51:29,111 - __main__ - INFO - Epoch 79 Batch 120: Train Loss = 0.1581
2023-11-08 07:51:40,024 - __main__ - INFO - ------------ Save best model - AUROC: 0.8794 ------------
2023-11-08 07:51:40,725 - __main__ - INFO - Epoch 80 Batch 0: Train Loss = 0.2121
2023-11-08 07:51:55,296 - __main__ - INFO - Epoch 80 Batch 20: Train Loss = 0.2226
2023-11-08 07:52:09,209 - __main__ - INFO - Epoch 80 Batch 40: Train Loss = 0.2255
2023-11-08 07:52:23,343 - __main__ - INFO - Epoch 80 Batch 60: Train Loss = 0.2210
2023-11-08 07:52:37,586 - __main__ - INFO - Epoch 80 Batch 80: Train Loss = 0.1610
2023-11-08 07:52:51,969 - __main__ - INFO - Epoch 80 Batch 100: Train Loss = 0.1310
2023-11-08 07:53:06,572 - __main__ - INFO - Epoch 80 Batch 120: Train Loss = 0.1289
2023-11-08 07:53:17,874 - __main__ - INFO - Epoch 81 Batch 0: Train Loss = 0.3555
2023-11-08 07:53:32,250 - __main__ - INFO - Epoch 81 Batch 20: Train Loss = 0.2448
2023-11-08 07:53:46,310 - __main__ - INFO - Epoch 81 Batch 40: Train Loss = 0.2010
2023-11-08 07:54:00,446 - __main__ - INFO - Epoch 81 Batch 60: Train Loss = 0.2122
2023-11-08 07:54:14,441 - __main__ - INFO - Epoch 81 Batch 80: Train Loss = 0.1475
2023-11-08 07:54:29,252 - __main__ - INFO - Epoch 81 Batch 100: Train Loss = 0.1371
2023-11-08 07:54:42,296 - __main__ - INFO - Epoch 81 Batch 120: Train Loss = 0.1774
2023-11-08 07:54:54,300 - __main__ - INFO - Epoch 82 Batch 0: Train Loss = 0.2340
2023-11-08 07:55:08,782 - __main__ - INFO - Epoch 82 Batch 20: Train Loss = 0.2498
2023-11-08 07:55:23,364 - __main__ - INFO - Epoch 82 Batch 40: Train Loss = 0.2250
2023-11-08 07:55:38,439 - __main__ - INFO - Epoch 82 Batch 60: Train Loss = 0.2128
2023-11-08 07:55:53,209 - __main__ - INFO - Epoch 82 Batch 80: Train Loss = 0.1436
2023-11-08 07:56:08,260 - __main__ - INFO - Epoch 82 Batch 100: Train Loss = 0.1391
2023-11-08 07:56:22,205 - __main__ - INFO - Epoch 82 Batch 120: Train Loss = 0.1645
2023-11-08 07:56:34,068 - __main__ - INFO - Epoch 83 Batch 0: Train Loss = 0.2380
2023-11-08 07:56:49,340 - __main__ - INFO - Epoch 83 Batch 20: Train Loss = 0.2290
2023-11-08 07:57:03,359 - __main__ - INFO - Epoch 83 Batch 40: Train Loss = 0.2089
2023-11-08 07:57:18,300 - __main__ - INFO - Epoch 83 Batch 60: Train Loss = 0.2313
2023-11-08 07:57:33,166 - __main__ - INFO - Epoch 83 Batch 80: Train Loss = 0.1518
2023-11-08 07:57:47,943 - __main__ - INFO - Epoch 83 Batch 100: Train Loss = 0.1240
2023-11-08 07:58:02,423 - __main__ - INFO - Epoch 83 Batch 120: Train Loss = 0.1565
2023-11-08 07:58:14,106 - __main__ - INFO - Epoch 84 Batch 0: Train Loss = 0.2282
2023-11-08 07:58:28,642 - __main__ - INFO - Epoch 84 Batch 20: Train Loss = 0.2254
2023-11-08 07:58:44,211 - __main__ - INFO - Epoch 84 Batch 40: Train Loss = 0.2042
2023-11-08 07:58:59,061 - __main__ - INFO - Epoch 84 Batch 60: Train Loss = 0.2244
2023-11-08 07:59:14,159 - __main__ - INFO - Epoch 84 Batch 80: Train Loss = 0.1595
2023-11-08 07:59:28,845 - __main__ - INFO - Epoch 84 Batch 100: Train Loss = 0.1935
2023-11-08 07:59:43,137 - __main__ - INFO - Epoch 84 Batch 120: Train Loss = 0.1587
2023-11-08 07:59:54,552 - __main__ - INFO - Epoch 85 Batch 0: Train Loss = 0.2255
2023-11-08 08:00:09,954 - __main__ - INFO - Epoch 85 Batch 20: Train Loss = 0.2456
2023-11-08 08:00:23,265 - __main__ - INFO - Epoch 85 Batch 40: Train Loss = 0.2155
2023-11-08 08:00:37,590 - __main__ - INFO - Epoch 85 Batch 60: Train Loss = 0.2296
2023-11-08 08:00:53,188 - __main__ - INFO - Epoch 85 Batch 80: Train Loss = 0.1616
2023-11-08 08:01:09,234 - __main__ - INFO - Epoch 85 Batch 100: Train Loss = 0.1372
2023-11-08 08:01:23,572 - __main__ - INFO - Epoch 85 Batch 120: Train Loss = 0.1558
2023-11-08 08:01:35,049 - __main__ - INFO - Epoch 86 Batch 0: Train Loss = 0.2019
2023-11-08 08:01:50,026 - __main__ - INFO - Epoch 86 Batch 20: Train Loss = 0.2319
2023-11-08 08:02:03,837 - __main__ - INFO - Epoch 86 Batch 40: Train Loss = 0.2216
2023-11-08 08:02:18,175 - __main__ - INFO - Epoch 86 Batch 60: Train Loss = 0.2122
2023-11-08 08:02:32,730 - __main__ - INFO - Epoch 86 Batch 80: Train Loss = 0.1520
2023-11-08 08:02:47,402 - __main__ - INFO - Epoch 86 Batch 100: Train Loss = 0.1250
2023-11-08 08:03:02,431 - __main__ - INFO - Epoch 86 Batch 120: Train Loss = 0.1785
2023-11-08 08:03:13,581 - __main__ - INFO - Epoch 87 Batch 0: Train Loss = 0.2126
2023-11-08 08:03:29,163 - __main__ - INFO - Epoch 87 Batch 20: Train Loss = 0.2074
2023-11-08 08:03:43,674 - __main__ - INFO - Epoch 87 Batch 40: Train Loss = 0.2397
2023-11-08 08:03:58,546 - __main__ - INFO - Epoch 87 Batch 60: Train Loss = 0.2198
2023-11-08 08:04:13,655 - __main__ - INFO - Epoch 87 Batch 80: Train Loss = 0.1655
2023-11-08 08:04:28,323 - __main__ - INFO - Epoch 87 Batch 100: Train Loss = 0.1564
2023-11-08 08:04:43,018 - __main__ - INFO - Epoch 87 Batch 120: Train Loss = 0.1458
2023-11-08 08:04:54,867 - __main__ - INFO - Epoch 88 Batch 0: Train Loss = 0.2421
2023-11-08 08:05:09,940 - __main__ - INFO - Epoch 88 Batch 20: Train Loss = 0.2284
2023-11-08 08:05:24,688 - __main__ - INFO - Epoch 88 Batch 40: Train Loss = 0.2141
2023-11-08 08:05:40,651 - __main__ - INFO - Epoch 88 Batch 60: Train Loss = 0.2053
2023-11-08 08:05:54,935 - __main__ - INFO - Epoch 88 Batch 80: Train Loss = 0.1413
2023-11-08 08:06:09,406 - __main__ - INFO - Epoch 88 Batch 100: Train Loss = 0.1316
2023-11-08 08:06:23,811 - __main__ - INFO - Epoch 88 Batch 120: Train Loss = 0.1571
2023-11-08 08:06:35,065 - __main__ - INFO - Epoch 89 Batch 0: Train Loss = 0.2914
2023-11-08 08:06:50,169 - __main__ - INFO - Epoch 89 Batch 20: Train Loss = 0.2209
2023-11-08 08:07:04,572 - __main__ - INFO - Epoch 89 Batch 40: Train Loss = 0.2444
2023-11-08 08:07:20,220 - __main__ - INFO - Epoch 89 Batch 60: Train Loss = 0.2235
2023-11-08 08:07:35,127 - __main__ - INFO - Epoch 89 Batch 80: Train Loss = 0.1700
2023-11-08 08:07:50,819 - __main__ - INFO - Epoch 89 Batch 100: Train Loss = 0.1609
2023-11-08 08:08:06,212 - __main__ - INFO - Epoch 89 Batch 120: Train Loss = 0.1695
2023-11-08 08:08:17,512 - __main__ - INFO - Epoch 90 Batch 0: Train Loss = 0.2455
2023-11-08 08:08:32,432 - __main__ - INFO - Epoch 90 Batch 20: Train Loss = 0.2417
2023-11-08 08:08:47,068 - __main__ - INFO - Epoch 90 Batch 40: Train Loss = 0.2143
2023-11-08 08:09:01,354 - __main__ - INFO - Epoch 90 Batch 60: Train Loss = 0.2093
2023-11-08 08:09:16,264 - __main__ - INFO - Epoch 90 Batch 80: Train Loss = 0.1475
2023-11-08 08:09:31,176 - __main__ - INFO - Epoch 90 Batch 100: Train Loss = 0.1317
2023-11-08 08:09:46,059 - __main__ - INFO - Epoch 90 Batch 120: Train Loss = 0.1540
2023-11-08 08:09:57,196 - __main__ - INFO - Epoch 91 Batch 0: Train Loss = 0.2316
2023-11-08 08:10:12,191 - __main__ - INFO - Epoch 91 Batch 20: Train Loss = 0.2074
2023-11-08 08:10:27,546 - __main__ - INFO - Epoch 91 Batch 40: Train Loss = 0.2236
2023-11-08 08:10:42,523 - __main__ - INFO - Epoch 91 Batch 60: Train Loss = 0.2184
2023-11-08 08:10:56,684 - __main__ - INFO - Epoch 91 Batch 80: Train Loss = 0.1655
2023-11-08 08:11:11,184 - __main__ - INFO - Epoch 91 Batch 100: Train Loss = 0.1311
2023-11-08 08:11:25,643 - __main__ - INFO - Epoch 91 Batch 120: Train Loss = 0.1587
2023-11-08 08:11:36,967 - __main__ - INFO - Epoch 92 Batch 0: Train Loss = 0.2172
2023-11-08 08:11:52,196 - __main__ - INFO - Epoch 92 Batch 20: Train Loss = 0.2359
2023-11-08 08:12:07,193 - __main__ - INFO - Epoch 92 Batch 40: Train Loss = 0.2204
2023-11-08 08:12:21,485 - __main__ - INFO - Epoch 92 Batch 60: Train Loss = 0.2338
2023-11-08 08:12:35,973 - __main__ - INFO - Epoch 92 Batch 80: Train Loss = 0.1558
2023-11-08 08:12:50,562 - __main__ - INFO - Epoch 92 Batch 100: Train Loss = 0.1261
2023-11-08 08:13:05,420 - __main__ - INFO - Epoch 92 Batch 120: Train Loss = 0.1618
2023-11-08 08:13:16,991 - __main__ - INFO - ------------ Save best model - AUROC: 0.8806 ------------
2023-11-08 08:13:17,704 - __main__ - INFO - Epoch 93 Batch 0: Train Loss = 0.3552
2023-11-08 08:13:33,024 - __main__ - INFO - Epoch 93 Batch 20: Train Loss = 0.2189
2023-11-08 08:13:47,068 - __main__ - INFO - Epoch 93 Batch 40: Train Loss = 0.2150
2023-11-08 08:14:01,529 - __main__ - INFO - Epoch 93 Batch 60: Train Loss = 0.2063
2023-11-08 08:14:16,131 - __main__ - INFO - Epoch 93 Batch 80: Train Loss = 0.1448
2023-11-08 08:14:30,712 - __main__ - INFO - Epoch 93 Batch 100: Train Loss = 0.1331
2023-11-08 08:14:45,705 - __main__ - INFO - Epoch 93 Batch 120: Train Loss = 0.1746
2023-11-08 08:14:57,921 - __main__ - INFO - Epoch 94 Batch 0: Train Loss = 0.2134
2023-11-08 08:15:13,088 - __main__ - INFO - Epoch 94 Batch 20: Train Loss = 0.2355
2023-11-08 08:15:26,882 - __main__ - INFO - Epoch 94 Batch 40: Train Loss = 0.2203
2023-11-08 08:15:40,824 - __main__ - INFO - Epoch 94 Batch 60: Train Loss = 0.1976
2023-11-08 08:15:56,315 - __main__ - INFO - Epoch 94 Batch 80: Train Loss = 0.1536
2023-11-08 08:16:11,231 - __main__ - INFO - Epoch 94 Batch 100: Train Loss = 0.1170
2023-11-08 08:16:25,875 - __main__ - INFO - Epoch 94 Batch 120: Train Loss = 0.2153
2023-11-08 08:16:36,461 - __main__ - INFO - ------------ Save best model - AUROC: 0.8810 ------------
2023-11-08 08:16:37,123 - __main__ - INFO - Epoch 95 Batch 0: Train Loss = 0.2606
2023-11-08 08:16:52,385 - __main__ - INFO - Epoch 95 Batch 20: Train Loss = 0.2328
2023-11-08 08:17:07,881 - __main__ - INFO - Epoch 95 Batch 40: Train Loss = 0.2322
2023-11-08 08:17:21,898 - __main__ - INFO - Epoch 95 Batch 60: Train Loss = 0.2390
2023-11-08 08:17:36,420 - __main__ - INFO - Epoch 95 Batch 80: Train Loss = 0.1487
2023-11-08 08:17:50,940 - __main__ - INFO - Epoch 95 Batch 100: Train Loss = 0.1160
2023-11-08 08:18:05,108 - __main__ - INFO - Epoch 95 Batch 120: Train Loss = 0.1657
2023-11-08 08:18:17,140 - __main__ - INFO - Epoch 96 Batch 0: Train Loss = 0.3161
2023-11-08 08:18:32,000 - __main__ - INFO - Epoch 96 Batch 20: Train Loss = 0.2522
2023-11-08 08:18:46,667 - __main__ - INFO - Epoch 96 Batch 40: Train Loss = 0.2230
2023-11-08 08:19:01,131 - __main__ - INFO - Epoch 96 Batch 60: Train Loss = 0.2037
2023-11-08 08:19:15,335 - __main__ - INFO - Epoch 96 Batch 80: Train Loss = 0.1490
2023-11-08 08:19:30,942 - __main__ - INFO - Epoch 96 Batch 100: Train Loss = 0.1438
2023-11-08 08:19:46,471 - __main__ - INFO - Epoch 96 Batch 120: Train Loss = 0.1803
2023-11-08 08:19:57,806 - __main__ - INFO - Epoch 97 Batch 0: Train Loss = 0.2401
2023-11-08 08:20:12,771 - __main__ - INFO - Epoch 97 Batch 20: Train Loss = 0.2621
2023-11-08 08:20:27,835 - __main__ - INFO - Epoch 97 Batch 40: Train Loss = 0.2254
2023-11-08 08:20:42,304 - __main__ - INFO - Epoch 97 Batch 60: Train Loss = 0.2046
2023-11-08 08:20:56,639 - __main__ - INFO - Epoch 97 Batch 80: Train Loss = 0.1557
2023-11-08 08:21:11,537 - __main__ - INFO - Epoch 97 Batch 100: Train Loss = 0.1129
2023-11-08 08:21:26,267 - __main__ - INFO - Epoch 97 Batch 120: Train Loss = 0.1502
2023-11-08 08:21:37,410 - __main__ - INFO - ------------ Save best model - AUROC: 0.8819 ------------
2023-11-08 08:21:37,937 - __main__ - INFO - Epoch 98 Batch 0: Train Loss = 0.3366
2023-11-08 08:21:53,289 - __main__ - INFO - Epoch 98 Batch 20: Train Loss = 0.2274
2023-11-08 08:22:07,676 - __main__ - INFO - Epoch 98 Batch 40: Train Loss = 0.2227
2023-11-08 08:22:22,911 - __main__ - INFO - Epoch 98 Batch 60: Train Loss = 0.1993
2023-11-08 08:22:36,911 - __main__ - INFO - Epoch 98 Batch 80: Train Loss = 0.1480
2023-11-08 08:22:51,775 - __main__ - INFO - Epoch 98 Batch 100: Train Loss = 0.1037
2023-11-08 08:23:05,694 - __main__ - INFO - Epoch 98 Batch 120: Train Loss = 0.1788
2023-11-08 08:23:17,155 - __main__ - INFO - Epoch 99 Batch 0: Train Loss = 0.2406
2023-11-08 08:23:31,394 - __main__ - INFO - Epoch 99 Batch 20: Train Loss = 0.2759
2023-11-08 08:23:45,889 - __main__ - INFO - Epoch 99 Batch 40: Train Loss = 0.2149
2023-11-08 08:24:00,205 - __main__ - INFO - Epoch 99 Batch 60: Train Loss = 0.2305
2023-11-08 08:24:14,759 - __main__ - INFO - Epoch 99 Batch 80: Train Loss = 0.1513
2023-11-08 08:24:29,124 - __main__ - INFO - Epoch 99 Batch 100: Train Loss = 0.1610
2023-11-08 08:24:44,480 - __main__ - INFO - Epoch 99 Batch 120: Train Loss = 0.1691
2023-11-08 08:24:56,350 - __main__ - INFO - Epoch 100 Batch 0: Train Loss = 0.1987
2023-11-08 08:25:11,021 - __main__ - INFO - Epoch 100 Batch 20: Train Loss = 0.2224
2023-11-08 08:25:25,291 - __main__ - INFO - Epoch 100 Batch 40: Train Loss = 0.2109
2023-11-08 08:25:39,489 - __main__ - INFO - Epoch 100 Batch 60: Train Loss = 0.2483
2023-11-08 08:25:55,047 - __main__ - INFO - Epoch 100 Batch 80: Train Loss = 0.1462
2023-11-08 08:26:09,584 - __main__ - INFO - Epoch 100 Batch 100: Train Loss = 0.1298
2023-11-08 08:26:24,896 - __main__ - INFO - Epoch 100 Batch 120: Train Loss = 0.1574
2023-11-08 08:26:35,239 - __main__ - INFO - ------------ Save best model - AUROC: 0.8867 ------------
2023-11-08 08:26:35,875 - __main__ - INFO - Epoch 101 Batch 0: Train Loss = 0.3045
2023-11-08 08:26:51,591 - __main__ - INFO - Epoch 101 Batch 20: Train Loss = 0.2293
2023-11-08 08:27:05,328 - __main__ - INFO - Epoch 101 Batch 40: Train Loss = 0.2547
2023-11-08 08:27:20,427 - __main__ - INFO - Epoch 101 Batch 60: Train Loss = 0.1991
2023-11-08 08:27:35,328 - __main__ - INFO - Epoch 101 Batch 80: Train Loss = 0.1533
2023-11-08 08:27:50,127 - __main__ - INFO - Epoch 101 Batch 100: Train Loss = 0.1008
2023-11-08 08:28:04,545 - __main__ - INFO - Epoch 101 Batch 120: Train Loss = 0.2202
2023-11-08 08:28:15,883 - __main__ - INFO - Epoch 102 Batch 0: Train Loss = 0.2722
2023-11-08 08:28:30,824 - __main__ - INFO - Epoch 102 Batch 20: Train Loss = 0.2569
2023-11-08 08:28:45,689 - __main__ - INFO - Epoch 102 Batch 40: Train Loss = 0.2199
2023-11-08 08:28:59,985 - __main__ - INFO - Epoch 102 Batch 60: Train Loss = 0.2149
2023-11-08 08:29:14,201 - __main__ - INFO - Epoch 102 Batch 80: Train Loss = 0.1469
2023-11-08 08:29:28,997 - __main__ - INFO - Epoch 102 Batch 100: Train Loss = 0.1174
2023-11-08 08:29:43,885 - __main__ - INFO - Epoch 102 Batch 120: Train Loss = 0.1763
2023-11-08 08:29:55,646 - __main__ - INFO - Epoch 103 Batch 0: Train Loss = 0.3382
2023-11-08 08:30:10,845 - __main__ - INFO - Epoch 103 Batch 20: Train Loss = 0.2373
2023-11-08 08:30:25,019 - __main__ - INFO - Epoch 103 Batch 40: Train Loss = 0.2623
2023-11-08 08:30:39,105 - __main__ - INFO - Epoch 103 Batch 60: Train Loss = 0.3146
2023-11-08 08:30:53,887 - __main__ - INFO - Epoch 103 Batch 80: Train Loss = 0.1531
2023-11-08 08:31:09,046 - __main__ - INFO - Epoch 103 Batch 100: Train Loss = 0.1024
2023-11-08 08:31:24,245 - __main__ - INFO - Epoch 103 Batch 120: Train Loss = 0.1843
2023-11-08 08:31:36,097 - __main__ - INFO - Epoch 104 Batch 0: Train Loss = 0.2255
2023-11-08 08:31:50,728 - __main__ - INFO - Epoch 104 Batch 20: Train Loss = 0.2158
2023-11-08 08:32:04,896 - __main__ - INFO - Epoch 104 Batch 40: Train Loss = 0.2252
2023-11-08 08:32:19,391 - __main__ - INFO - Epoch 104 Batch 60: Train Loss = 0.1892
2023-11-08 08:32:33,446 - __main__ - INFO - Epoch 104 Batch 80: Train Loss = 0.1445
2023-11-08 08:32:49,310 - __main__ - INFO - Epoch 104 Batch 100: Train Loss = 0.1267
2023-11-08 08:33:03,741 - __main__ - INFO - Epoch 104 Batch 120: Train Loss = 0.1734
2023-11-08 08:33:14,871 - __main__ - INFO - Epoch 105 Batch 0: Train Loss = 0.2482
2023-11-08 08:33:30,032 - __main__ - INFO - Epoch 105 Batch 20: Train Loss = 0.2433
2023-11-08 08:33:44,385 - __main__ - INFO - Epoch 105 Batch 40: Train Loss = 0.2095
2023-11-08 08:33:58,951 - __main__ - INFO - Epoch 105 Batch 60: Train Loss = 0.1913
2023-11-08 08:34:14,017 - __main__ - INFO - Epoch 105 Batch 80: Train Loss = 0.1288
2023-11-08 08:34:28,833 - __main__ - INFO - Epoch 105 Batch 100: Train Loss = 0.1136
2023-11-08 08:34:43,700 - __main__ - INFO - Epoch 105 Batch 120: Train Loss = 0.1616
2023-11-08 08:34:55,751 - __main__ - INFO - Epoch 106 Batch 0: Train Loss = 0.2200
2023-11-08 08:35:11,083 - __main__ - INFO - Epoch 106 Batch 20: Train Loss = 0.2648
2023-11-08 08:35:25,325 - __main__ - INFO - Epoch 106 Batch 40: Train Loss = 0.2096
2023-11-08 08:35:39,845 - __main__ - INFO - Epoch 106 Batch 60: Train Loss = 0.1893
2023-11-08 08:35:54,593 - __main__ - INFO - Epoch 106 Batch 80: Train Loss = 0.1462
2023-11-08 08:36:09,218 - __main__ - INFO - Epoch 106 Batch 100: Train Loss = 0.1143
2023-11-08 08:36:23,198 - __main__ - INFO - Epoch 106 Batch 120: Train Loss = 0.1556
2023-11-08 08:36:35,008 - __main__ - INFO - Epoch 107 Batch 0: Train Loss = 0.3785
2023-11-08 08:36:50,288 - __main__ - INFO - Epoch 107 Batch 20: Train Loss = 0.2482
2023-11-08 08:37:03,861 - __main__ - INFO - Epoch 107 Batch 40: Train Loss = 0.1887
2023-11-08 08:37:18,295 - __main__ - INFO - Epoch 107 Batch 60: Train Loss = 0.2231
2023-11-08 08:37:33,324 - __main__ - INFO - Epoch 107 Batch 80: Train Loss = 0.1308
2023-11-08 08:37:48,452 - __main__ - INFO - Epoch 107 Batch 100: Train Loss = 0.0974
2023-11-08 08:38:02,888 - __main__ - INFO - Epoch 107 Batch 120: Train Loss = 0.1565
2023-11-08 08:38:14,527 - __main__ - INFO - Epoch 108 Batch 0: Train Loss = 0.3062
2023-11-08 08:38:28,836 - __main__ - INFO - Epoch 108 Batch 20: Train Loss = 0.2203
2023-11-08 08:38:42,938 - __main__ - INFO - Epoch 108 Batch 40: Train Loss = 0.2300
2023-11-08 08:38:57,908 - __main__ - INFO - Epoch 108 Batch 60: Train Loss = 0.2330
2023-11-08 08:39:12,684 - __main__ - INFO - Epoch 108 Batch 80: Train Loss = 0.1301
2023-11-08 08:39:26,710 - __main__ - INFO - Epoch 108 Batch 100: Train Loss = 0.1190
2023-11-08 08:39:40,680 - __main__ - INFO - Epoch 108 Batch 120: Train Loss = 0.1613
2023-11-08 08:39:51,844 - __main__ - INFO - Epoch 109 Batch 0: Train Loss = 0.1865
2023-11-08 08:40:06,184 - __main__ - INFO - Epoch 109 Batch 20: Train Loss = 0.2543
2023-11-08 08:40:19,878 - __main__ - INFO - Epoch 109 Batch 40: Train Loss = 0.2034
2023-11-08 08:40:35,666 - __main__ - INFO - Epoch 109 Batch 60: Train Loss = 0.1869
2023-11-08 08:40:50,817 - __main__ - INFO - Epoch 109 Batch 80: Train Loss = 0.1490
2023-11-08 08:41:05,556 - __main__ - INFO - Epoch 109 Batch 100: Train Loss = 0.1335
2023-11-08 08:41:19,696 - __main__ - INFO - Epoch 109 Batch 120: Train Loss = 0.1947
2023-11-08 08:41:31,297 - __main__ - INFO - Epoch 110 Batch 0: Train Loss = 0.2641
2023-11-08 08:41:46,883 - __main__ - INFO - Epoch 110 Batch 20: Train Loss = 0.2443
2023-11-08 08:42:00,802 - __main__ - INFO - Epoch 110 Batch 40: Train Loss = 0.2333
2023-11-08 08:42:15,771 - __main__ - INFO - Epoch 110 Batch 60: Train Loss = 0.2000
2023-11-08 08:42:31,237 - __main__ - INFO - Epoch 110 Batch 80: Train Loss = 0.1662
2023-11-08 08:42:46,079 - __main__ - INFO - Epoch 110 Batch 100: Train Loss = 0.0972
2023-11-08 08:43:00,299 - __main__ - INFO - Epoch 110 Batch 120: Train Loss = 0.1641
2023-11-08 08:43:11,421 - __main__ - INFO - Epoch 111 Batch 0: Train Loss = 0.2634
2023-11-08 08:43:25,947 - __main__ - INFO - Epoch 111 Batch 20: Train Loss = 0.3692
2023-11-08 08:43:39,445 - __main__ - INFO - Epoch 111 Batch 40: Train Loss = 0.2116
2023-11-08 08:43:53,507 - __main__ - INFO - Epoch 111 Batch 60: Train Loss = 0.3003
2023-11-08 08:44:08,678 - __main__ - INFO - Epoch 111 Batch 80: Train Loss = 0.1661
2023-11-08 08:44:23,349 - __main__ - INFO - Epoch 111 Batch 100: Train Loss = 0.1145
2023-11-08 08:44:37,500 - __main__ - INFO - Epoch 111 Batch 120: Train Loss = 0.1702
2023-11-08 08:44:49,968 - __main__ - INFO - Epoch 112 Batch 0: Train Loss = 0.2210
2023-11-08 08:45:04,502 - __main__ - INFO - Epoch 112 Batch 20: Train Loss = 0.2200
2023-11-08 08:45:18,412 - __main__ - INFO - Epoch 112 Batch 40: Train Loss = 0.2374
2023-11-08 08:45:33,111 - __main__ - INFO - Epoch 112 Batch 60: Train Loss = 0.2170
2023-11-08 08:45:47,063 - __main__ - INFO - Epoch 112 Batch 80: Train Loss = 0.1440
2023-11-08 08:46:02,683 - __main__ - INFO - Epoch 112 Batch 100: Train Loss = 0.1227
2023-11-08 08:46:17,460 - __main__ - INFO - Epoch 112 Batch 120: Train Loss = 0.1797
2023-11-08 08:46:28,971 - __main__ - INFO - Epoch 113 Batch 0: Train Loss = 0.2362
2023-11-08 08:46:43,437 - __main__ - INFO - Epoch 113 Batch 20: Train Loss = 0.2476
2023-11-08 08:46:58,286 - __main__ - INFO - Epoch 113 Batch 40: Train Loss = 0.2438
2023-11-08 08:47:13,130 - __main__ - INFO - Epoch 113 Batch 60: Train Loss = 0.2800
2023-11-08 08:47:27,821 - __main__ - INFO - Epoch 113 Batch 80: Train Loss = 0.1955
2023-11-08 08:47:43,161 - __main__ - INFO - Epoch 113 Batch 100: Train Loss = 0.1585
2023-11-08 08:47:57,985 - __main__ - INFO - Epoch 113 Batch 120: Train Loss = 0.1626
2023-11-08 08:48:08,656 - __main__ - INFO - Epoch 114 Batch 0: Train Loss = 0.2251
2023-11-08 08:48:23,145 - __main__ - INFO - Epoch 114 Batch 20: Train Loss = 0.2152
2023-11-08 08:48:36,784 - __main__ - INFO - Epoch 114 Batch 40: Train Loss = 0.2235
2023-11-08 08:48:52,264 - __main__ - INFO - Epoch 114 Batch 60: Train Loss = 0.2134
2023-11-08 08:49:06,881 - __main__ - INFO - Epoch 114 Batch 80: Train Loss = 0.1561
2023-11-08 08:49:20,842 - __main__ - INFO - Epoch 114 Batch 100: Train Loss = 0.1385
2023-11-08 08:49:35,546 - __main__ - INFO - Epoch 114 Batch 120: Train Loss = 0.1871
2023-11-08 08:49:46,744 - __main__ - INFO - Epoch 115 Batch 0: Train Loss = 0.2419
2023-11-08 08:50:01,772 - __main__ - INFO - Epoch 115 Batch 20: Train Loss = 0.2248
2023-11-08 08:50:15,968 - __main__ - INFO - Epoch 115 Batch 40: Train Loss = 0.2035
2023-11-08 08:50:30,515 - __main__ - INFO - Epoch 115 Batch 60: Train Loss = 0.2312
2023-11-08 08:50:45,226 - __main__ - INFO - Epoch 115 Batch 80: Train Loss = 0.1534
2023-11-08 08:50:59,867 - __main__ - INFO - Epoch 115 Batch 100: Train Loss = 0.1503
2023-11-08 08:51:13,908 - __main__ - INFO - Epoch 115 Batch 120: Train Loss = 0.1555
2023-11-08 08:51:26,321 - __main__ - INFO - Epoch 116 Batch 0: Train Loss = 0.2432
2023-11-08 08:51:41,744 - __main__ - INFO - Epoch 116 Batch 20: Train Loss = 0.2756
2023-11-08 08:51:56,601 - __main__ - INFO - Epoch 116 Batch 40: Train Loss = 0.2276
2023-11-08 08:52:11,384 - __main__ - INFO - Epoch 116 Batch 60: Train Loss = 0.2542
2023-11-08 08:52:26,262 - __main__ - INFO - Epoch 116 Batch 80: Train Loss = 0.1480
2023-11-08 08:52:41,098 - __main__ - INFO - Epoch 116 Batch 100: Train Loss = 0.1244
2023-11-08 08:52:55,410 - __main__ - INFO - Epoch 116 Batch 120: Train Loss = 0.1707
2023-11-08 08:53:06,897 - __main__ - INFO - Epoch 117 Batch 0: Train Loss = 0.2270
2023-11-08 08:53:22,022 - __main__ - INFO - Epoch 117 Batch 20: Train Loss = 0.2141
2023-11-08 08:53:36,635 - __main__ - INFO - Epoch 117 Batch 40: Train Loss = 0.2041
2023-11-08 08:53:51,572 - __main__ - INFO - Epoch 117 Batch 60: Train Loss = 0.2214
2023-11-08 08:54:06,217 - __main__ - INFO - Epoch 117 Batch 80: Train Loss = 0.1436
2023-11-08 08:54:21,546 - __main__ - INFO - Epoch 117 Batch 100: Train Loss = 0.1200
2023-11-08 08:54:36,011 - __main__ - INFO - Epoch 117 Batch 120: Train Loss = 0.1471
2023-11-08 08:54:47,669 - __main__ - INFO - Epoch 118 Batch 0: Train Loss = 0.2076
2023-11-08 08:55:02,816 - __main__ - INFO - Epoch 118 Batch 20: Train Loss = 0.2089
2023-11-08 08:55:16,669 - __main__ - INFO - Epoch 118 Batch 40: Train Loss = 0.2461
2023-11-08 08:55:30,564 - __main__ - INFO - Epoch 118 Batch 60: Train Loss = 0.1875
2023-11-08 08:55:45,549 - __main__ - INFO - Epoch 118 Batch 80: Train Loss = 0.1533
2023-11-08 08:55:59,369 - __main__ - INFO - Epoch 118 Batch 100: Train Loss = 0.1177
2023-11-08 08:56:13,870 - __main__ - INFO - Epoch 118 Batch 120: Train Loss = 0.1674
2023-11-08 08:56:25,481 - __main__ - INFO - Epoch 119 Batch 0: Train Loss = 0.3524
2023-11-08 08:56:40,752 - __main__ - INFO - Epoch 119 Batch 20: Train Loss = 0.2332
2023-11-08 08:56:56,367 - __main__ - INFO - Epoch 119 Batch 40: Train Loss = 0.2183
2023-11-08 08:57:11,681 - __main__ - INFO - Epoch 119 Batch 60: Train Loss = 0.2556
2023-11-08 08:57:25,636 - __main__ - INFO - Epoch 119 Batch 80: Train Loss = 0.1422
2023-11-08 08:57:40,098 - __main__ - INFO - Epoch 119 Batch 100: Train Loss = 0.1194
2023-11-08 08:57:54,393 - __main__ - INFO - Epoch 119 Batch 120: Train Loss = 0.1801
2023-11-08 08:58:06,241 - __main__ - INFO - Epoch 120 Batch 0: Train Loss = 0.2562
2023-11-08 08:58:23,047 - __main__ - INFO - Epoch 120 Batch 20: Train Loss = 0.2252
2023-11-08 08:58:38,309 - __main__ - INFO - Epoch 120 Batch 40: Train Loss = 0.2222
2023-11-08 08:58:53,592 - __main__ - INFO - Epoch 120 Batch 60: Train Loss = 0.2171
2023-11-08 08:59:08,685 - __main__ - INFO - Epoch 120 Batch 80: Train Loss = 0.1397
2023-11-08 08:59:23,127 - __main__ - INFO - Epoch 120 Batch 100: Train Loss = 0.1114
2023-11-08 08:59:38,624 - __main__ - INFO - Epoch 120 Batch 120: Train Loss = 0.1619
2023-11-08 08:59:50,167 - __main__ - INFO - Epoch 121 Batch 0: Train Loss = 0.2448
2023-11-08 09:00:05,540 - __main__ - INFO - Epoch 121 Batch 20: Train Loss = 0.3355
2023-11-08 09:00:19,208 - __main__ - INFO - Epoch 121 Batch 40: Train Loss = 0.2260
2023-11-08 09:00:33,558 - __main__ - INFO - Epoch 121 Batch 60: Train Loss = 0.1899
2023-11-08 09:00:47,240 - __main__ - INFO - Epoch 121 Batch 80: Train Loss = 0.1278
2023-11-08 09:01:01,855 - __main__ - INFO - Epoch 121 Batch 100: Train Loss = 0.1296
2023-11-08 09:01:16,437 - __main__ - INFO - Epoch 121 Batch 120: Train Loss = 0.1599
2023-11-08 09:01:28,738 - __main__ - INFO - Epoch 122 Batch 0: Train Loss = 0.1979
2023-11-08 09:01:43,415 - __main__ - INFO - Epoch 122 Batch 20: Train Loss = 0.2484
2023-11-08 09:01:57,299 - __main__ - INFO - Epoch 122 Batch 40: Train Loss = 0.2300
2023-11-08 09:02:12,014 - __main__ - INFO - Epoch 122 Batch 60: Train Loss = 0.2352
2023-11-08 09:02:27,079 - __main__ - INFO - Epoch 122 Batch 80: Train Loss = 0.1470
2023-11-08 09:02:41,280 - __main__ - INFO - Epoch 122 Batch 100: Train Loss = 0.1202
2023-11-08 09:02:55,302 - __main__ - INFO - Epoch 122 Batch 120: Train Loss = 0.1843
2023-11-08 09:03:06,964 - __main__ - INFO - Epoch 123 Batch 0: Train Loss = 0.2761
2023-11-08 09:03:21,370 - __main__ - INFO - Epoch 123 Batch 20: Train Loss = 0.2061
2023-11-08 09:03:35,525 - __main__ - INFO - Epoch 123 Batch 40: Train Loss = 0.2027
2023-11-08 09:03:49,876 - __main__ - INFO - Epoch 123 Batch 60: Train Loss = 0.2138
2023-11-08 09:04:04,827 - __main__ - INFO - Epoch 123 Batch 80: Train Loss = 0.1409
2023-11-08 09:04:19,382 - __main__ - INFO - Epoch 123 Batch 100: Train Loss = 0.1151
2023-11-08 09:04:33,376 - __main__ - INFO - Epoch 123 Batch 120: Train Loss = 0.1495
2023-11-08 09:04:44,530 - __main__ - INFO - Epoch 124 Batch 0: Train Loss = 0.2115
2023-11-08 09:04:59,846 - __main__ - INFO - Epoch 124 Batch 20: Train Loss = 0.1973
2023-11-08 09:05:13,441 - __main__ - INFO - Epoch 124 Batch 40: Train Loss = 0.2177
2023-11-08 09:05:27,978 - __main__ - INFO - Epoch 124 Batch 60: Train Loss = 0.2434
2023-11-08 09:05:44,159 - __main__ - INFO - Epoch 124 Batch 80: Train Loss = 0.1380
2023-11-08 09:05:59,726 - __main__ - INFO - Epoch 124 Batch 100: Train Loss = 0.1251
2023-11-08 09:06:14,240 - __main__ - INFO - Epoch 124 Batch 120: Train Loss = 0.2431
2023-11-08 09:06:25,702 - __main__ - INFO - Epoch 125 Batch 0: Train Loss = 0.2195
2023-11-08 09:06:40,369 - __main__ - INFO - Epoch 125 Batch 20: Train Loss = 0.2778
2023-11-08 09:06:53,958 - __main__ - INFO - Epoch 125 Batch 40: Train Loss = 0.1963
2023-11-08 09:07:09,671 - __main__ - INFO - Epoch 125 Batch 60: Train Loss = 0.2318
2023-11-08 09:07:24,594 - __main__ - INFO - Epoch 125 Batch 80: Train Loss = 0.1437
2023-11-08 09:07:39,195 - __main__ - INFO - Epoch 125 Batch 100: Train Loss = 0.1276
2023-11-08 09:07:54,373 - __main__ - INFO - Epoch 125 Batch 120: Train Loss = 0.1550
2023-11-08 09:08:06,042 - __main__ - INFO - Epoch 126 Batch 0: Train Loss = 0.2838
2023-11-08 09:08:21,377 - __main__ - INFO - Epoch 126 Batch 20: Train Loss = 0.2675
2023-11-08 09:08:35,428 - __main__ - INFO - Epoch 126 Batch 40: Train Loss = 0.2451
2023-11-08 09:08:49,115 - __main__ - INFO - Epoch 126 Batch 60: Train Loss = 0.2028
2023-11-08 09:09:04,381 - __main__ - INFO - Epoch 126 Batch 80: Train Loss = 0.1320
2023-11-08 09:09:18,487 - __main__ - INFO - Epoch 126 Batch 100: Train Loss = 0.1112
2023-11-08 09:09:33,361 - __main__ - INFO - Epoch 126 Batch 120: Train Loss = 0.1604
2023-11-08 09:09:45,114 - __main__ - INFO - Epoch 127 Batch 0: Train Loss = 0.4522
2023-11-08 09:10:00,185 - __main__ - INFO - Epoch 127 Batch 20: Train Loss = 0.2103
2023-11-08 09:10:14,803 - __main__ - INFO - Epoch 127 Batch 40: Train Loss = 0.2132
2023-11-08 09:10:28,788 - __main__ - INFO - Epoch 127 Batch 60: Train Loss = 0.2148
2023-11-08 09:10:43,558 - __main__ - INFO - Epoch 127 Batch 80: Train Loss = 0.1327
2023-11-08 09:10:57,883 - __main__ - INFO - Epoch 127 Batch 100: Train Loss = 0.1098
2023-11-08 09:11:13,220 - __main__ - INFO - Epoch 127 Batch 120: Train Loss = 0.1689
2023-11-08 09:11:24,344 - __main__ - INFO - Epoch 128 Batch 0: Train Loss = 0.2684
2023-11-08 09:11:40,087 - __main__ - INFO - Epoch 128 Batch 20: Train Loss = 0.2652
2023-11-08 09:11:54,847 - __main__ - INFO - Epoch 128 Batch 40: Train Loss = 0.2226
2023-11-08 09:12:09,748 - __main__ - INFO - Epoch 128 Batch 60: Train Loss = 0.2033
2023-11-08 09:12:24,960 - __main__ - INFO - Epoch 128 Batch 80: Train Loss = 0.1474
2023-11-08 09:12:39,483 - __main__ - INFO - Epoch 128 Batch 100: Train Loss = 0.1259
2023-11-08 09:12:53,694 - __main__ - INFO - Epoch 128 Batch 120: Train Loss = 0.1441
2023-11-08 09:13:05,001 - __main__ - INFO - Epoch 129 Batch 0: Train Loss = 0.3694
2023-11-08 09:13:20,779 - __main__ - INFO - Epoch 129 Batch 20: Train Loss = 0.2207
2023-11-08 09:13:34,915 - __main__ - INFO - Epoch 129 Batch 40: Train Loss = 0.2077
2023-11-08 09:13:48,751 - __main__ - INFO - Epoch 129 Batch 60: Train Loss = 0.1903
2023-11-08 09:14:03,939 - __main__ - INFO - Epoch 129 Batch 80: Train Loss = 0.1392
2023-11-08 09:14:19,150 - __main__ - INFO - Epoch 129 Batch 100: Train Loss = 0.1412
2023-11-08 09:14:34,106 - __main__ - INFO - Epoch 129 Batch 120: Train Loss = 0.1582
2023-11-08 09:14:46,042 - __main__ - INFO - Epoch 130 Batch 0: Train Loss = 0.2106
2023-11-08 09:15:02,390 - __main__ - INFO - Epoch 130 Batch 20: Train Loss = 0.2807
2023-11-08 09:15:17,122 - __main__ - INFO - Epoch 130 Batch 40: Train Loss = 0.2031
2023-11-08 09:15:31,671 - __main__ - INFO - Epoch 130 Batch 60: Train Loss = 0.2042
2023-11-08 09:15:46,027 - __main__ - INFO - Epoch 130 Batch 80: Train Loss = 0.1390
2023-11-08 09:16:00,555 - __main__ - INFO - Epoch 130 Batch 100: Train Loss = 0.1175
2023-11-08 09:16:14,697 - __main__ - INFO - Epoch 130 Batch 120: Train Loss = 0.1457
2023-11-08 09:16:25,934 - __main__ - INFO - Epoch 131 Batch 0: Train Loss = 0.2041
2023-11-08 09:16:40,772 - __main__ - INFO - Epoch 131 Batch 20: Train Loss = 0.3047
2023-11-08 09:16:55,696 - __main__ - INFO - Epoch 131 Batch 40: Train Loss = 0.2171
2023-11-08 09:17:09,910 - __main__ - INFO - Epoch 131 Batch 60: Train Loss = 0.2643
2023-11-08 09:17:25,020 - __main__ - INFO - Epoch 131 Batch 80: Train Loss = 0.1533
2023-11-08 09:17:39,226 - __main__ - INFO - Epoch 131 Batch 100: Train Loss = 0.1254
2023-11-08 09:17:53,722 - __main__ - INFO - Epoch 131 Batch 120: Train Loss = 0.1520
2023-11-08 09:18:05,665 - __main__ - INFO - Epoch 132 Batch 0: Train Loss = 0.2079
2023-11-08 09:18:20,387 - __main__ - INFO - Epoch 132 Batch 20: Train Loss = 0.2200
2023-11-08 09:18:34,359 - __main__ - INFO - Epoch 132 Batch 40: Train Loss = 0.2167
2023-11-08 09:18:48,987 - __main__ - INFO - Epoch 132 Batch 60: Train Loss = 0.1907
2023-11-08 09:19:04,525 - __main__ - INFO - Epoch 132 Batch 80: Train Loss = 0.1435
2023-11-08 09:19:20,087 - __main__ - INFO - Epoch 132 Batch 100: Train Loss = 0.1359
2023-11-08 09:19:35,268 - __main__ - INFO - Epoch 132 Batch 120: Train Loss = 0.1518
2023-11-08 09:19:47,116 - __main__ - INFO - Epoch 133 Batch 0: Train Loss = 0.2423
2023-11-08 09:20:02,432 - __main__ - INFO - Epoch 133 Batch 20: Train Loss = 0.2556
2023-11-08 09:20:16,571 - __main__ - INFO - Epoch 133 Batch 40: Train Loss = 0.2280
2023-11-08 09:20:30,538 - __main__ - INFO - Epoch 133 Batch 60: Train Loss = 0.2284
2023-11-08 09:20:46,141 - __main__ - INFO - Epoch 133 Batch 80: Train Loss = 0.1336
2023-11-08 09:21:01,156 - __main__ - INFO - Epoch 133 Batch 100: Train Loss = 0.1529
2023-11-08 09:21:15,364 - __main__ - INFO - Epoch 133 Batch 120: Train Loss = 0.1484
2023-11-08 09:21:26,348 - __main__ - INFO - Epoch 134 Batch 0: Train Loss = 0.2268
2023-11-08 09:21:41,753 - __main__ - INFO - Epoch 134 Batch 20: Train Loss = 0.2267
2023-11-08 09:21:56,785 - __main__ - INFO - Epoch 134 Batch 40: Train Loss = 0.2040
2023-11-08 09:22:11,012 - __main__ - INFO - Epoch 134 Batch 60: Train Loss = 0.1992
2023-11-08 09:22:26,585 - __main__ - INFO - Epoch 134 Batch 80: Train Loss = 0.1376
2023-11-08 09:22:40,762 - __main__ - INFO - Epoch 134 Batch 100: Train Loss = 0.1234
2023-11-08 09:22:55,444 - __main__ - INFO - Epoch 134 Batch 120: Train Loss = 0.1631
2023-11-08 09:23:06,774 - __main__ - INFO - Epoch 135 Batch 0: Train Loss = 0.2284
2023-11-08 09:23:21,955 - __main__ - INFO - Epoch 135 Batch 20: Train Loss = 0.1970
2023-11-08 09:23:35,910 - __main__ - INFO - Epoch 135 Batch 40: Train Loss = 0.2093
2023-11-08 09:23:50,517 - __main__ - INFO - Epoch 135 Batch 60: Train Loss = 0.2125
2023-11-08 09:24:05,069 - __main__ - INFO - Epoch 135 Batch 80: Train Loss = 0.1311
2023-11-08 09:24:20,479 - __main__ - INFO - Epoch 135 Batch 100: Train Loss = 0.1275
2023-11-08 09:24:35,286 - __main__ - INFO - Epoch 135 Batch 120: Train Loss = 0.1975
2023-11-08 09:24:46,850 - __main__ - INFO - Epoch 136 Batch 0: Train Loss = 0.3361
2023-11-08 09:25:01,872 - __main__ - INFO - Epoch 136 Batch 20: Train Loss = 0.2359
2023-11-08 09:25:15,586 - __main__ - INFO - Epoch 136 Batch 40: Train Loss = 0.2213
2023-11-08 09:25:30,375 - __main__ - INFO - Epoch 136 Batch 60: Train Loss = 0.2285
2023-11-08 09:25:45,768 - __main__ - INFO - Epoch 136 Batch 80: Train Loss = 0.1470
2023-11-08 09:26:00,466 - __main__ - INFO - Epoch 136 Batch 100: Train Loss = 0.1080
2023-11-08 09:26:15,380 - __main__ - INFO - Epoch 136 Batch 120: Train Loss = 0.1593
2023-11-08 09:26:27,626 - __main__ - INFO - Epoch 137 Batch 0: Train Loss = 0.2087
2023-11-08 09:26:42,222 - __main__ - INFO - Epoch 137 Batch 20: Train Loss = 0.2433
2023-11-08 09:26:56,856 - __main__ - INFO - Epoch 137 Batch 40: Train Loss = 0.2107
2023-11-08 09:27:11,494 - __main__ - INFO - Epoch 137 Batch 60: Train Loss = 0.1846
2023-11-08 09:27:26,471 - __main__ - INFO - Epoch 137 Batch 80: Train Loss = 0.1320
2023-11-08 09:27:41,395 - __main__ - INFO - Epoch 137 Batch 100: Train Loss = 0.1213
2023-11-08 09:27:56,040 - __main__ - INFO - Epoch 137 Batch 120: Train Loss = 0.1546
2023-11-08 09:28:07,362 - __main__ - INFO - Epoch 138 Batch 0: Train Loss = 0.4903
2023-11-08 09:28:22,139 - __main__ - INFO - Epoch 138 Batch 20: Train Loss = 0.1935
2023-11-08 09:28:36,007 - __main__ - INFO - Epoch 138 Batch 40: Train Loss = 0.1957
2023-11-08 09:28:51,099 - __main__ - INFO - Epoch 138 Batch 60: Train Loss = 0.1901
2023-11-08 09:29:06,035 - __main__ - INFO - Epoch 138 Batch 80: Train Loss = 0.1342
2023-11-08 09:29:20,637 - __main__ - INFO - Epoch 138 Batch 100: Train Loss = 0.0890
2023-11-08 09:29:35,150 - __main__ - INFO - Epoch 138 Batch 120: Train Loss = 0.1865
2023-11-08 09:29:46,326 - __main__ - INFO - Epoch 139 Batch 0: Train Loss = 0.2096
2023-11-08 09:30:00,950 - __main__ - INFO - Epoch 139 Batch 20: Train Loss = 0.2436
2023-11-08 09:30:15,578 - __main__ - INFO - Epoch 139 Batch 40: Train Loss = 0.2230
2023-11-08 09:30:30,102 - __main__ - INFO - Epoch 139 Batch 60: Train Loss = 0.1997
2023-11-08 09:30:44,893 - __main__ - INFO - Epoch 139 Batch 80: Train Loss = 0.1543
2023-11-08 09:30:59,750 - __main__ - INFO - Epoch 139 Batch 100: Train Loss = 0.1167
2023-11-08 09:31:14,268 - __main__ - INFO - Epoch 139 Batch 120: Train Loss = 0.1602
2023-11-08 09:31:25,715 - __main__ - INFO - Epoch 140 Batch 0: Train Loss = 0.2867
2023-11-08 09:31:40,863 - __main__ - INFO - Epoch 140 Batch 20: Train Loss = 0.2043
2023-11-08 09:31:55,625 - __main__ - INFO - Epoch 140 Batch 40: Train Loss = 0.2016
2023-11-08 09:32:09,911 - __main__ - INFO - Epoch 140 Batch 60: Train Loss = 0.2123
2023-11-08 09:32:24,746 - __main__ - INFO - Epoch 140 Batch 80: Train Loss = 0.1483
2023-11-08 09:32:39,540 - __main__ - INFO - Epoch 140 Batch 100: Train Loss = 0.1141
2023-11-08 09:32:54,115 - __main__ - INFO - Epoch 140 Batch 120: Train Loss = 0.1458
2023-11-08 09:33:05,922 - __main__ - INFO - Epoch 141 Batch 0: Train Loss = 0.2924
2023-11-08 09:33:20,140 - __main__ - INFO - Epoch 141 Batch 20: Train Loss = 0.4053
2023-11-08 09:33:35,129 - __main__ - INFO - Epoch 141 Batch 40: Train Loss = 0.3129
2023-11-08 09:33:48,878 - __main__ - INFO - Epoch 141 Batch 60: Train Loss = 0.2027
2023-11-08 09:34:03,752 - __main__ - INFO - Epoch 141 Batch 80: Train Loss = 0.1510
2023-11-08 09:34:18,955 - __main__ - INFO - Epoch 141 Batch 100: Train Loss = 0.1218
2023-11-08 09:34:33,805 - __main__ - INFO - Epoch 141 Batch 120: Train Loss = 0.1668
2023-11-08 09:34:45,353 - __main__ - INFO - Epoch 142 Batch 0: Train Loss = 0.2654
2023-11-08 09:35:00,495 - __main__ - INFO - Epoch 142 Batch 20: Train Loss = 0.2103
2023-11-08 09:35:16,307 - __main__ - INFO - Epoch 142 Batch 40: Train Loss = 0.2045
2023-11-08 09:35:31,161 - __main__ - INFO - Epoch 142 Batch 60: Train Loss = 0.1926
2023-11-08 09:35:46,039 - __main__ - INFO - Epoch 142 Batch 80: Train Loss = 0.1679
2023-11-08 09:36:00,529 - __main__ - INFO - Epoch 142 Batch 100: Train Loss = 0.1089
2023-11-08 09:36:14,499 - __main__ - INFO - Epoch 142 Batch 120: Train Loss = 0.1574
2023-11-08 09:36:26,064 - __main__ - INFO - Epoch 143 Batch 0: Train Loss = 0.2338
2023-11-08 09:36:41,654 - __main__ - INFO - Epoch 143 Batch 20: Train Loss = 0.2317
2023-11-08 09:36:56,513 - __main__ - INFO - Epoch 143 Batch 40: Train Loss = 0.2307
2023-11-08 09:37:11,452 - __main__ - INFO - Epoch 143 Batch 60: Train Loss = 0.1887
2023-11-08 09:37:26,175 - __main__ - INFO - Epoch 143 Batch 80: Train Loss = 0.1626
2023-11-08 09:37:40,903 - __main__ - INFO - Epoch 143 Batch 100: Train Loss = 0.1317
2023-11-08 09:37:55,528 - __main__ - INFO - Epoch 143 Batch 120: Train Loss = 0.1451
2023-11-08 09:38:07,527 - __main__ - INFO - Epoch 144 Batch 0: Train Loss = 0.2358
2023-11-08 09:38:23,451 - __main__ - INFO - Epoch 144 Batch 20: Train Loss = 0.2040
2023-11-08 09:38:38,407 - __main__ - INFO - Epoch 144 Batch 40: Train Loss = 0.2542
2023-11-08 09:38:53,240 - __main__ - INFO - Epoch 144 Batch 60: Train Loss = 0.1921
2023-11-08 09:39:09,472 - __main__ - INFO - Epoch 144 Batch 80: Train Loss = 0.1542
2023-11-08 09:39:25,583 - __main__ - INFO - Epoch 144 Batch 100: Train Loss = 0.0960
2023-11-08 09:39:40,680 - __main__ - INFO - Epoch 144 Batch 120: Train Loss = 0.1509
2023-11-08 09:39:51,720 - __main__ - INFO - Epoch 145 Batch 0: Train Loss = 0.1981
2023-11-08 09:40:07,774 - __main__ - INFO - Epoch 145 Batch 20: Train Loss = 0.2088
2023-11-08 09:40:23,137 - __main__ - INFO - Epoch 145 Batch 40: Train Loss = 0.2244
2023-11-08 09:40:39,725 - __main__ - INFO - Epoch 145 Batch 60: Train Loss = 0.2058
2023-11-08 09:40:56,184 - __main__ - INFO - Epoch 145 Batch 80: Train Loss = 0.1477
2023-11-08 09:41:13,139 - __main__ - INFO - Epoch 145 Batch 100: Train Loss = 0.1774
2023-11-08 09:41:29,714 - __main__ - INFO - Epoch 145 Batch 120: Train Loss = 0.1663
2023-11-08 09:41:42,637 - __main__ - INFO - Epoch 146 Batch 0: Train Loss = 0.2672
2023-11-08 09:41:58,672 - __main__ - INFO - Epoch 146 Batch 20: Train Loss = 0.1864
2023-11-08 09:42:13,363 - __main__ - INFO - Epoch 146 Batch 40: Train Loss = 0.2351
2023-11-08 09:42:29,560 - __main__ - INFO - Epoch 146 Batch 60: Train Loss = 0.1872
2023-11-08 09:42:45,536 - __main__ - INFO - Epoch 146 Batch 80: Train Loss = 0.1382
2023-11-08 09:43:02,433 - __main__ - INFO - Epoch 146 Batch 100: Train Loss = 0.1049
2023-11-08 09:43:18,452 - __main__ - INFO - Epoch 146 Batch 120: Train Loss = 0.1465
2023-11-08 09:43:31,391 - __main__ - INFO - Epoch 147 Batch 0: Train Loss = 0.3045
2023-11-08 09:43:47,168 - __main__ - INFO - Epoch 147 Batch 20: Train Loss = 0.2017
2023-11-08 09:44:03,160 - __main__ - INFO - Epoch 147 Batch 40: Train Loss = 0.2160
2023-11-08 09:44:19,919 - __main__ - INFO - Epoch 147 Batch 60: Train Loss = 0.1908
2023-11-08 09:44:36,043 - __main__ - INFO - Epoch 147 Batch 80: Train Loss = 0.1600
2023-11-08 09:44:52,301 - __main__ - INFO - Epoch 147 Batch 100: Train Loss = 0.1664
2023-11-08 09:45:08,330 - __main__ - INFO - Epoch 147 Batch 120: Train Loss = 0.2569
2023-11-08 09:45:21,043 - __main__ - INFO - Epoch 148 Batch 0: Train Loss = 0.2697
2023-11-08 09:45:36,951 - __main__ - INFO - Epoch 148 Batch 20: Train Loss = 0.2410
2023-11-08 09:45:52,913 - __main__ - INFO - Epoch 148 Batch 40: Train Loss = 0.2200
2023-11-08 09:46:07,670 - __main__ - INFO - Epoch 148 Batch 60: Train Loss = 0.2835
2023-11-08 09:46:23,540 - __main__ - INFO - Epoch 148 Batch 80: Train Loss = 0.1358
2023-11-08 09:46:38,890 - __main__ - INFO - Epoch 148 Batch 100: Train Loss = 0.1139
2023-11-08 09:46:54,517 - __main__ - INFO - Epoch 148 Batch 120: Train Loss = 0.2182
2023-11-08 09:47:06,939 - __main__ - INFO - Epoch 149 Batch 0: Train Loss = 0.2288
2023-11-08 09:47:23,126 - __main__ - INFO - Epoch 149 Batch 20: Train Loss = 0.2328
2023-11-08 09:47:39,021 - __main__ - INFO - Epoch 149 Batch 40: Train Loss = 0.2277
2023-11-08 09:47:54,411 - __main__ - INFO - Epoch 149 Batch 60: Train Loss = 0.2394
2023-11-08 09:48:10,288 - __main__ - INFO - Epoch 149 Batch 80: Train Loss = 0.1505
2023-11-08 09:48:26,338 - __main__ - INFO - Epoch 149 Batch 100: Train Loss = 0.1210
2023-11-08 09:48:42,272 - __main__ - INFO - Epoch 149 Batch 120: Train Loss = 0.1815
2023-11-08 09:48:55,291 - __main__ - INFO - auroc 0.8867
2023-11-08 09:48:55,292 - __main__ - INFO - auprc 0.6031
2023-11-08 09:48:55,293 - __main__ - INFO - minpse 0.5890
2023-11-08 09:48:55,393 - __main__ - INFO - last saved model is in epoch 100
2023-11-08 09:48:55,889 - __main__ - INFO - Batch 0: Test Loss = 0.1380
2023-11-08 09:49:02,653 - __main__ - INFO - 
==>Predicting on test
2023-11-08 09:49:02,655 - __main__ - INFO - Test Loss = 0.1504
2023-11-08 09:49:02,763 - __main__ - INFO - Transfer Target Dataset & Model
2023-11-08 09:49:02,872 - __main__ - INFO - [[-0.43249781572948887, 0.7734225027254102, 0.11121635247274919, -1.1510759266125783, -0.5775173789921769, -0.894979488306686, 0.6418248141325145, -1.1001333024577382, -0.09626591113773565, 0.07837976568684282, 0.27346416059268763, -0.29216681084789653, -0.20501606288246071, -0.44880109579951655, -0.6600819004124745, -0.5342137257388455, -0.16268638728171492, 1.013400735854239, 1.0, 1.098202336859675, -0.6863245135763378, -0.4670502544450039, -0.41420989908626493, -0.2941064849292665, 0.2534399947977164, 0.6689953308348103, -0.2780944554864925, 1.6237612679911448, -0.016555502218216205, -0.411288788529273, -0.5683812105284773, -0.9512610100808374, -0.9403624977548863, 0.651423723145103, -0.43323556246024986, 0.6921334800493779, 2.9116903104103184, 0.2360670258723392, -0.2674665445678642, 0.4444334033508993, -0.21164668014859825, -0.16135240337065737, -0.7969887859671778, -0.8495988149987506, -0.705221190542043, -1.0534078933590525, 1.3058941843049534, -0.3135827086480343, -0.3167900638717034, -0.24477434302814813, 0.7655720433538256, -0.33782515371340677, -0.18389192944425495, 0.311524270793097, -0.03429036235566567, 1.6532381373162057, -0.6035840289958837, -0.4798165125287964, 1.3136602297615485, -0.38113328114517314, -0.438838853292927, -0.25994688807198424, 1.8115519460700398, 0.6532487251600559, -0.6813987150690228, -0.6881330975707701, -0.8608674113288034, 0.5282372331730917, -0.3927744234337778, -0.2332558526002517, -0.7532798583884561, -1.2230128823047004, 0.4609091575849162, -0.7849239794277927, -1.4109277280424448], [-0.43249781572948887, 0.7734225027254102, 0.11121635247274919, -1.1510759266125783, -0.5775173789921769, -0.894979488306686, 0.6418248141325145, -1.1001333024577382, -0.09626591113773565, 0.07837976568684282, 0.27346416059268763, -0.29216681084789653, -0.20501606288246071, -0.44880109579951655, -0.6600819004124745, -0.5342137257388455, -0.16268638728171492, 1.013400735854239, 1.0, 1.098202336859675, -0.6863245135763378, -0.4670502544450039, -0.41420989908626493, -0.2941064849292665, 0.2534399947977164, 0.6689953308348103, -0.2780944554864925, 1.6237612679911448, -0.016555502218216205, -0.411288788529273, -0.5683812105284773, -0.9512610100808374, -0.9403624977548863, 0.651423723145103, -0.43323556246024986, 0.6921334800493779, 2.9116903104103184, 0.2360670258723392, -0.2674665445678642, 0.4444334033508993, -0.21164668014859825, -0.16135240337065737, -0.7969887859671778, -0.8495988149987506, -0.705221190542043, -1.0534078933590525, 1.3058941843049534, -0.3135827086480343, -0.3167900638717034, -0.24477434302814813, 0.7655720433538256, -0.33782515371340677, -0.18389192944425495, 0.311524270793097, -0.03429036235566567, 1.6532381373162057, -0.6035840289958837, -0.4798165125287964, 1.3136602297615485, -0.38113328114517314, -0.438838853292927, -0.25994688807198424, 1.8115519460700398, 0.6532487251600559, -0.6813987150690228, -0.6881330975707701, -0.8608674113288034, 0.5282372331730917, -0.3927744234337778, -0.2332558526002517, -0.7532798583884561, -1.2230128823047004, 0.4609091575849162, -0.7849239794277927, -1.4109277280424448], [-0.43249781572948887, 1.0626655489546322, -0.2607021557915042, -0.8723303017698648, -0.6987602223992506, 0.42060382604722935, 0.7735163080078745, -0.4422053625222244, -0.09626591113773565, -0.5703381110324975, 0.27346416059268763, 0.4054213944799434, -0.6478420259418122, -0.652648910528702, 1.0582568698308186, 1.3623416964867365, -0.16268638728171492, 0.06953911374846142, 1.0, 1.098202336859675, -0.6863245135763378, -0.4670502544450039, -0.8301874079837086, -0.2941064849292665, 0.23429139098427385, -0.9597305030519863, -0.2780944554864925, 0.44841735124674564, -0.016555502218216205, -0.411288788529273, -0.49230413633903536, -1.2379951878994253, -0.8899655603378251, 0.4144052468488665, -0.43323556246024986, 0.6921334800493779, 2.9116903104103184, 0.07172496731033204, -0.2674665445678642, 1.0100887085497614, -0.21164668014859825, 1.943067011208341, -0.08881508319448383, -0.8495988149987506, 1.7124080848737049, -0.5711764060634537, 0.07961653401158347, -0.3135827086480343, -0.7495061187450142, -0.24477434302814813, 1.0712982894377368, -0.33782515371340677, -0.18389192944425495, 0.1906271826667854, 0.08009979585285867, 0.2295633031960889, -0.6035840289958837, -0.7241072055173139, 0.12678603163309085, -0.38113328114517314, -0.438838853292927, 0.37611362926166203, -0.07561878133196119, 0.3448973731726548, -0.45301547558529076, -0.6881330975707701, -0.8608674113288034, 0.6571388363948929, -0.9767247139266083, -0.2332558526002517, 0.4743340898355563, 0.31603580948316384, 0.4609091575849162, 0.5732318714191863, -0.40170913406717196], [-0.43249781572948887, 0.3395579333815775, -0.8951513757717067, -0.7678006924538472, 0.5406110657619471, -0.13459647175350561, 0.3257652288316494, 0.680026078568982, -0.738540973511107, -0.08843340261241621, 0.27346416059268763, -0.5014432724462485, 2.8390253475185365, -0.170826802986991, 0.7145891157821607, -0.36929586293662087, -0.43897479874595047, 0.28190797872226137, 1.0, 1.098202336859675, -0.5896780760673638, -0.4670502544450039, -0.96884657761619, -0.2941064849292665, 0.08110256047672398, -0.9597305030519863, -0.2780944554864925, -0.32395150832814545, -0.016555502218216205, -0.411288788529273, 2.0182393119125503, -0.9512610100808374, 0.3915565625531523, -0.5505985495001029, -0.43323556246024986, 0.572828751735985, 2.9116903104103184, 0.44736395830920606, -0.2674665445678642, -0.026946017648152392, -0.21164668014859825, 0.9439992083274025, -0.21802572370037915, -0.8495988149987506, -0.302282977972751, 0.721203979888751, -0.4108945261057649, -0.3135827086480343, -0.38890940635058796, 1.1668382976193643, -0.35542419228718275, -0.33782515371340677, -0.5335587241304727, -0.024300974002213524, -1.0066067071281226, 1.247958440011939, -0.6035840289958837, -0.942223895685633, -0.29927137282327887, -0.38113328114517314, -0.438838853292927, 1.0121741465953082, -0.43265108111071765, -0.8173500304721677, -0.583520183861709, -0.5895349223964507, -0.8608674113288034, 0.39933562995129274, -0.9767247139266083, -0.2332558526002517, -0.35194452916137237, -0.3835317776931382, 0.4609091575849162, -0.10584605400430319, -0.7071814287953199], [-0.43249781572948887, 0.2672471718242721, -0.9826616130103564, -0.6284278800324904, -0.40238882740418175, -0.43633576403651375, 0.03604394230585671, -0.017417052954250063, -0.738540973511107, -0.21817697795628416, 0.27346416059268763, 0.19614493288159143, -0.48681440301113876, -0.5229275738828567, 2.4016853629301207, 1.3623416964867365, -0.43897479874595047, 0.022346032643172534, 1.0, 1.098202336859675, -0.5896780760673638, -0.4670502544450039, 0.2790859490761416, -0.2941064849292665, 0.751303693947251, 0.6689953308348103, -0.2780944554864925, 1.0528799370010082, -0.016555502218216205, -0.411288788529273, 0.07827392008177957, -0.9512610100808374, -1.2931410596743127, 0.295896008700747, -0.43323556246024986, 0.572828751735985, 2.9116903104103184, 0.5647511429963535, -0.2674665445678642, 0.7272610559503303, -0.21164668014859825, 3.6648647140457036, -0.287600683972784, 0.972012082109433, 1.1080007660197713, -0.3718540579812729, 0.815383124187605, -0.3135827086480343, -0.3408298446979971, 1.1668382976193643, 1.3307023770240858, -0.33782515371340677, -0.5335587241304727, 1.2787009758035917, -0.7206313116068118, 1.3414845240052313, -0.6035840289958837, -0.9509485632923658, 0.9028191611786214, -0.38113328114517314, -0.438838853292927, 2.0192699657069153, 1.0974873465125259, -0.4615600089482426, -0.09412752782514046, -0.5895349223964507, 1.2819970892257146, 0.9793928444493915, -0.9915082655846547, -0.2332558526002517, -0.21029676590475738, -0.6633588125636588, 0.4609091575849162, -0.10584605400430319, -0.3243743759081472], [-0.43249781572948887, 0.411868694938883, -0.566987986126775, -0.2799958489790985, -0.8469459198967851, -0.46047490741915437, 0.3257652288316494, 0.10508004096767726, -0.8205335346651542, -0.292316163867066, 0.27346416059268763, 1.1378890100741745, -0.3908171662640067, -0.7082437690912071, 1.4644096700701428, 1.3623416964867365, -0.2662945415808034, 0.022346032643172534, 1.0, 1.098202336859675, -1.4111727948936457, -0.4670502544450039, 1.111040966871029, -0.2941064849292665, 1.0768299587757935, -0.14536758610858785, -0.2780944554864925, 0.6834861345956256, -0.016555502218216205, -0.411288788529273, -0.7966124330968032, -0.9512610100808374, -1.0483559350771596, 0.380545464520832, -0.43323556246024986, 2.1237902198100924, 2.9116903104103184, 0.7056157646209292, -0.2674665445678642, 0.25588163495127864, -0.21164668014859825, 3.452297096411461, -0.24287392379766656, 2.3382202549405706, 0.8057971065927999, -0.21753998204668118, 0.5701275941289319, -0.35234396942435836, -0.1485115980876371, -0.4706323655317493, 1.0712982894377368, -0.33782515371340677, -0.47452407048215023, 1.5742271912234647, -0.6062411533982874, 1.2167830786808416, -0.6035840289958837, -0.9160498928654347, 0.8571701535582954, -0.38113328114517314, -0.438838853292927, 1.754244750151229, 0.7914596609878773, -0.7461920261673826, -0.12675370489424503, -1.3783203237910062, 0.21056483894845573, 0.7860404396166896, -0.9915082655846547, -0.2332558526002517, 0.4271181687500135, -0.6633588125636588, 0.4609091575849162, -0.05360929051018861, -0.3243743759081472], [-0.43249781572948887, 0.411868694938883, -0.566987986126775, -0.2799958489790985, -0.8469459198967851, -0.46047490741915437, 0.3257652288316494, 0.10508004096767726, -0.8205335346651542, -0.292316163867066, 0.27346416059268763, 1.1378890100741745, -0.3908171662640067, -0.7082437690912071, 1.4644096700701428, 1.3623416964867365, -0.2662945415808034, 0.022346032643172534, 1.0, 1.098202336859675, -1.4111727948936457, -0.4670502544450039, 1.111040966871029, -0.2941064849292665, 1.0768299587757935, -0.14536758610858785, -0.2780944554864925, 0.6834861345956256, -0.016555502218216205, -0.411288788529273, -0.7966124330968032, -0.9512610100808374, -1.0483559350771596, 0.380545464520832, -0.43323556246024986, 2.1237902198100924, 2.9116903104103184, 0.7056157646209292, -0.2674665445678642, 0.25588163495127864, -0.21164668014859825, 3.452297096411461, -0.24287392379766656, 2.3382202549405706, 0.8057971065927999, -0.21753998204668118, 0.5701275941289319, -0.35234396942435836, -0.1485115980876371, -0.4706323655317493, 1.0712982894377368, -0.33782515371340677, -0.47452407048215023, 1.5742271912234647, -0.6062411533982874, 1.2167830786808416, -0.6035840289958837, -0.9160498928654347, 0.8571701535582954, -0.38113328114517314, -0.438838853292927, 1.754244750151229, 0.7914596609878773, -0.7461920261673826, -0.12675370489424503, -1.3783203237910062, 0.21056483894845573, 0.7860404396166896, -0.9915082655846547, -0.2332558526002517, 0.4271181687500135, -0.6633588125636588, 0.4609091575849162, -0.05360929051018861, -0.3243743759081472]]
2023-11-08 09:49:02,875 - __main__ - INFO - 75
2023-11-08 09:49:02,876 - __main__ - INFO - 361
2023-11-08 09:49:03,036 - __main__ - INFO - {'los_mean': 6.927731092436975, 'los_std': 5.1253246527009555, 'los_median': 6.0, 'large_los': 26.649999999999977, 'threshold': 4.9562737642585555}
2023-11-08 09:49:06,145 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 2.9160
2023-11-08 09:49:07,839 - __main__ - INFO - Fold 1, epoch 0: Loss = 2.4436 Valid loss = 2.3138 MSE = 40.6316 AUROC = 0.8089
2023-11-08 09:49:07,840 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 40.6316 ------------
2023-11-08 09:49:08,174 - __main__ - INFO - ------------ Save best model - MSE: 40.6316 ------------
2023-11-08 09:49:08,176 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8089 ------------
2023-11-08 09:49:08,177 - __main__ - INFO - Fold 1, mse = 40.6316, mad = 4.8427
2023-11-08 09:49:08,457 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 2.2476
2023-11-08 09:49:10,099 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 39.1958 ------------
2023-11-08 09:49:10,495 - __main__ - INFO - ------------ Save best model - MSE: 39.1958 ------------
2023-11-08 09:49:10,497 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8686 ------------
2023-11-08 09:49:10,499 - __main__ - INFO - Fold 1, mse = 39.1958, mad = 4.7195
2023-11-08 09:49:10,981 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 2.2680
2023-11-08 09:49:12,959 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 38.2477 ------------
2023-11-08 09:49:13,314 - __main__ - INFO - ------------ Save best model - MSE: 38.2477 ------------
2023-11-08 09:49:13,319 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8726 ------------
2023-11-08 09:49:13,323 - __main__ - INFO - Fold 1, mse = 38.2477, mad = 4.6128
2023-11-08 09:49:13,737 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 2.1518
2023-11-08 09:49:15,225 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.7000 ------------
2023-11-08 09:49:15,571 - __main__ - INFO - ------------ Save best model - MSE: 37.7000 ------------
2023-11-08 09:49:15,575 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8747 ------------
2023-11-08 09:49:15,579 - __main__ - INFO - Fold 1, mse = 37.7000, mad = 4.5698
2023-11-08 09:49:15,802 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 2.5739
2023-11-08 09:49:17,467 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.4814 ------------
2023-11-08 09:49:17,865 - __main__ - INFO - ------------ Save best model - MSE: 37.4814 ------------
2023-11-08 09:49:17,867 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8801 ------------
2023-11-08 09:49:17,869 - __main__ - INFO - Fold 1, mse = 37.4814, mad = 4.6092
2023-11-08 09:49:18,177 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 1.8771
2023-11-08 09:49:19,878 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.2130 ------------
2023-11-08 09:49:20,239 - __main__ - INFO - ------------ Save best model - MSE: 37.2130 ------------
2023-11-08 09:49:20,243 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8966 ------------
2023-11-08 09:49:20,245 - __main__ - INFO - Fold 1, mse = 37.2130, mad = 4.5629
2023-11-08 09:49:20,582 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 2.0421
2023-11-08 09:49:22,369 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.0859 ------------
2023-11-08 09:49:22,723 - __main__ - INFO - ------------ Save best model - MSE: 37.0859 ------------
2023-11-08 09:49:22,728 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9111 ------------
2023-11-08 09:49:22,731 - __main__ - INFO - Fold 1, mse = 37.0859, mad = 4.5676
2023-11-08 09:49:23,060 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 2.3480
2023-11-08 09:49:25,083 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.8659 ------------
2023-11-08 09:49:25,439 - __main__ - INFO - ------------ Save best model - MSE: 36.8659 ------------
2023-11-08 09:49:25,441 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9236 ------------
2023-11-08 09:49:25,443 - __main__ - INFO - Fold 1, mse = 36.8659, mad = 4.5619
2023-11-08 09:49:25,648 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 2.3040
2023-11-08 09:49:27,471 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.6235 ------------
2023-11-08 09:49:27,966 - __main__ - INFO - ------------ Save best model - MSE: 36.6235 ------------
2023-11-08 09:49:27,968 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9341 ------------
2023-11-08 09:49:27,970 - __main__ - INFO - Fold 1, mse = 36.6235, mad = 4.5438
2023-11-08 09:49:28,350 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 2.0215
2023-11-08 09:49:30,708 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.3781 ------------
2023-11-08 09:49:31,106 - __main__ - INFO - ------------ Save best model - MSE: 36.3781 ------------
2023-11-08 09:49:31,123 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9384 ------------
2023-11-08 09:49:31,126 - __main__ - INFO - Fold 1, mse = 36.3781, mad = 4.5507
2023-11-08 09:49:31,369 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 1.8051
2023-11-08 09:49:33,188 - __main__ - INFO - Fold 1, epoch 10: Loss = 1.9422 Valid loss = 2.0276 MSE = 36.2755 AUROC = 0.9444
2023-11-08 09:49:33,192 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.2755 ------------
2023-11-08 09:49:33,508 - __main__ - INFO - ------------ Save best model - MSE: 36.2755 ------------
2023-11-08 09:49:33,510 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9444 ------------
2023-11-08 09:49:33,511 - __main__ - INFO - Fold 1, mse = 36.2755, mad = 4.5772
2023-11-08 09:49:33,790 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 1.8521
2023-11-08 09:49:35,416 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.1679 ------------
2023-11-08 09:49:35,747 - __main__ - INFO - ------------ Save best model - MSE: 36.1679 ------------
2023-11-08 09:49:35,749 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9506 ------------
2023-11-08 09:49:35,751 - __main__ - INFO - Fold 1, mse = 36.1679, mad = 4.5062
2023-11-08 09:49:36,018 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 2.0766
2023-11-08 09:49:38,020 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.0652 ------------
2023-11-08 09:49:38,348 - __main__ - INFO - ------------ Save best model - MSE: 36.0652 ------------
2023-11-08 09:49:38,351 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9531 ------------
2023-11-08 09:49:38,353 - __main__ - INFO - Fold 1, mse = 36.0652, mad = 4.5328
2023-11-08 09:49:38,609 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 2.1861
2023-11-08 09:49:40,533 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9549 ------------
2023-11-08 09:49:40,537 - __main__ - INFO - Fold 1, mse = 36.0749, mad = 4.5529
2023-11-08 09:49:40,832 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 1.9250
2023-11-08 09:49:42,641 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.9605 ------------
2023-11-08 09:49:43,021 - __main__ - INFO - ------------ Save best model - MSE: 35.9605 ------------
2023-11-08 09:49:43,024 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9572 ------------
2023-11-08 09:49:43,026 - __main__ - INFO - Fold 1, mse = 35.9605, mad = 4.4850
2023-11-08 09:49:43,359 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 2.0193
2023-11-08 09:49:45,024 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.8303 ------------
2023-11-08 09:49:45,340 - __main__ - INFO - ------------ Save best model - MSE: 35.8303 ------------
2023-11-08 09:49:45,342 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9593 ------------
2023-11-08 09:49:45,344 - __main__ - INFO - Fold 1, mse = 35.8303, mad = 4.5131
2023-11-08 09:49:45,550 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 1.6495
2023-11-08 09:49:47,212 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9612 ------------
2023-11-08 09:49:47,214 - __main__ - INFO - Fold 1, mse = 35.8458, mad = 4.4746
2023-11-08 09:49:47,493 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 1.7736
2023-11-08 09:49:49,346 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.6670 ------------
2023-11-08 09:49:49,595 - __main__ - INFO - ------------ Save best model - MSE: 35.6670 ------------
2023-11-08 09:49:49,599 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9630 ------------
2023-11-08 09:49:49,601 - __main__ - INFO - Fold 1, mse = 35.6670, mad = 4.4602
2023-11-08 09:49:49,842 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 1.6881
2023-11-08 09:49:51,338 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9646 ------------
2023-11-08 09:49:51,341 - __main__ - INFO - Fold 1, mse = 35.7606, mad = 4.5197
2023-11-08 09:49:51,601 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 1.7679
2023-11-08 09:49:53,185 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9667 ------------
2023-11-08 09:49:53,186 - __main__ - INFO - Fold 1, mse = 35.9154, mad = 4.5125
2023-11-08 09:49:53,452 - __main__ - INFO - Fold 1 Epoch 20 Batch 0: Train Loss = 1.7997
2023-11-08 09:49:55,487 - __main__ - INFO - Fold 1, epoch 20: Loss = 1.6533 Valid loss = 1.8512 MSE = 35.8450 AUROC = 0.9685
2023-11-08 09:49:55,488 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9685 ------------
2023-11-08 09:49:55,490 - __main__ - INFO - Fold 1, mse = 35.8450, mad = 4.5109
2023-11-08 09:49:55,880 - __main__ - INFO - Fold 1 Epoch 21 Batch 0: Train Loss = 1.6262
2023-11-08 09:49:57,923 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.6208 ------------
2023-11-08 09:49:58,358 - __main__ - INFO - ------------ Save best model - MSE: 35.6208 ------------
2023-11-08 09:49:58,362 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9702 ------------
2023-11-08 09:49:58,366 - __main__ - INFO - Fold 1, mse = 35.6208, mad = 4.4867
2023-11-08 09:49:58,677 - __main__ - INFO - Fold 1 Epoch 22 Batch 0: Train Loss = 1.7728
2023-11-08 09:50:00,409 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.3789 ------------
2023-11-08 09:50:00,827 - __main__ - INFO - ------------ Save best model - MSE: 35.3789 ------------
2023-11-08 09:50:00,832 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9721 ------------
2023-11-08 09:50:00,836 - __main__ - INFO - Fold 1, mse = 35.3789, mad = 4.4721
2023-11-08 09:50:01,161 - __main__ - INFO - Fold 1 Epoch 23 Batch 0: Train Loss = 1.9796
2023-11-08 09:50:03,051 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.1558 ------------
2023-11-08 09:50:03,426 - __main__ - INFO - ------------ Save best model - MSE: 35.1558 ------------
2023-11-08 09:50:03,430 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9733 ------------
2023-11-08 09:50:03,434 - __main__ - INFO - Fold 1, mse = 35.1558, mad = 4.4664
2023-11-08 09:50:03,681 - __main__ - INFO - Fold 1 Epoch 24 Batch 0: Train Loss = 1.7861
2023-11-08 09:50:05,251 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.0537 ------------
2023-11-08 09:50:05,581 - __main__ - INFO - ------------ Save best model - MSE: 35.0537 ------------
2023-11-08 09:50:05,583 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9745 ------------
2023-11-08 09:50:05,585 - __main__ - INFO - Fold 1, mse = 35.0537, mad = 4.4275
2023-11-08 09:50:05,895 - __main__ - INFO - Fold 1 Epoch 25 Batch 0: Train Loss = 1.2967
2023-11-08 09:50:07,669 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.9725 ------------
2023-11-08 09:50:07,983 - __main__ - INFO - ------------ Save best model - MSE: 34.9725 ------------
2023-11-08 09:50:07,988 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9750 ------------
2023-11-08 09:50:07,990 - __main__ - INFO - Fold 1, mse = 34.9725, mad = 4.4117
2023-11-08 09:50:08,343 - __main__ - INFO - Fold 1 Epoch 26 Batch 0: Train Loss = 1.8012
2023-11-08 09:50:09,879 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.7283 ------------
2023-11-08 09:50:10,184 - __main__ - INFO - ------------ Save best model - MSE: 34.7283 ------------
2023-11-08 09:50:10,188 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9754 ------------
2023-11-08 09:50:10,192 - __main__ - INFO - Fold 1, mse = 34.7283, mad = 4.4130
2023-11-08 09:50:10,452 - __main__ - INFO - Fold 1 Epoch 27 Batch 0: Train Loss = 1.4064
2023-11-08 09:50:12,311 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9756 ------------
2023-11-08 09:50:12,312 - __main__ - INFO - Fold 1, mse = 34.7486, mad = 4.3869
2023-11-08 09:50:12,640 - __main__ - INFO - Fold 1 Epoch 28 Batch 0: Train Loss = 1.4664
2023-11-08 09:50:14,315 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9759 ------------
2023-11-08 09:50:14,317 - __main__ - INFO - Fold 1, mse = 34.9080, mad = 4.4570
2023-11-08 09:50:14,652 - __main__ - INFO - Fold 1 Epoch 29 Batch 0: Train Loss = 1.2519
2023-11-08 09:50:16,308 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9769 ------------
2023-11-08 09:50:16,310 - __main__ - INFO - Fold 1, mse = 35.0373, mad = 4.4453
2023-11-08 09:50:16,671 - __main__ - INFO - Fold 1 Epoch 30 Batch 0: Train Loss = 1.4721
2023-11-08 09:50:18,502 - __main__ - INFO - Fold 1, epoch 30: Loss = 1.5032 Valid loss = 1.7681 MSE = 34.9628 AUROC = 0.9783
2023-11-08 09:50:18,505 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9783 ------------
2023-11-08 09:50:18,507 - __main__ - INFO - Fold 1, mse = 34.9628, mad = 4.4100
2023-11-08 09:50:18,846 - __main__ - INFO - Fold 1 Epoch 31 Batch 0: Train Loss = 1.5559
2023-11-08 09:50:20,442 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.5659 ------------
2023-11-08 09:50:20,866 - __main__ - INFO - ------------ Save best model - MSE: 34.5659 ------------
2023-11-08 09:50:20,869 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9787 ------------
2023-11-08 09:50:20,871 - __main__ - INFO - Fold 1, mse = 34.5659, mad = 4.3713
2023-11-08 09:50:21,301 - __main__ - INFO - Fold 1 Epoch 32 Batch 0: Train Loss = 1.4372
2023-11-08 09:50:23,033 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.4607 ------------
2023-11-08 09:50:23,425 - __main__ - INFO - ------------ Save best model - MSE: 34.4607 ------------
2023-11-08 09:50:23,427 - __main__ - INFO - Fold 1, mse = 34.4607, mad = 4.3652
2023-11-08 09:50:23,742 - __main__ - INFO - Fold 1 Epoch 33 Batch 0: Train Loss = 1.4392
2023-11-08 09:50:25,639 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9791 ------------
2023-11-08 09:50:25,643 - __main__ - INFO - Fold 1, mse = 34.5591, mad = 4.3573
2023-11-08 09:50:25,974 - __main__ - INFO - Fold 1 Epoch 34 Batch 0: Train Loss = 1.3608
2023-11-08 09:50:27,945 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9793 ------------
2023-11-08 09:50:27,948 - __main__ - INFO - Fold 1, mse = 34.4806, mad = 4.3725
2023-11-08 09:50:28,287 - __main__ - INFO - Fold 1 Epoch 35 Batch 0: Train Loss = 1.4468
2023-11-08 09:50:30,193 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9797 ------------
2023-11-08 09:50:30,197 - __main__ - INFO - Fold 1, mse = 34.8126, mad = 4.4165
2023-11-08 09:50:30,456 - __main__ - INFO - Fold 1 Epoch 36 Batch 0: Train Loss = 1.2208
2023-11-08 09:50:32,127 - __main__ - INFO - Fold 1, mse = 34.4827, mad = 4.3760
2023-11-08 09:50:32,449 - __main__ - INFO - Fold 1 Epoch 37 Batch 0: Train Loss = 1.2017
2023-11-08 09:50:34,223 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.2435 ------------
2023-11-08 09:50:34,695 - __main__ - INFO - ------------ Save best model - MSE: 34.2435 ------------
2023-11-08 09:50:34,697 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9800 ------------
2023-11-08 09:50:34,699 - __main__ - INFO - Fold 1, mse = 34.2435, mad = 4.3454
2023-11-08 09:50:35,095 - __main__ - INFO - Fold 1 Epoch 38 Batch 0: Train Loss = 1.4125
2023-11-08 09:50:36,741 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.2268 ------------
2023-11-08 09:50:37,045 - __main__ - INFO - ------------ Save best model - MSE: 34.2268 ------------
2023-11-08 09:50:37,047 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9803 ------------
2023-11-08 09:50:37,049 - __main__ - INFO - Fold 1, mse = 34.2268, mad = 4.3389
2023-11-08 09:50:37,357 - __main__ - INFO - Fold 1 Epoch 39 Batch 0: Train Loss = 1.3775
2023-11-08 09:50:38,919 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9807 ------------
2023-11-08 09:50:38,923 - __main__ - INFO - Fold 1, mse = 34.2280, mad = 4.3549
2023-11-08 09:50:39,192 - __main__ - INFO - Fold 1 Epoch 40 Batch 0: Train Loss = 1.4313
2023-11-08 09:50:40,867 - __main__ - INFO - Fold 1, epoch 40: Loss = 1.4205 Valid loss = 1.7228 MSE = 34.0448 AUROC = 0.9811
2023-11-08 09:50:40,870 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.0448 ------------
2023-11-08 09:50:41,225 - __main__ - INFO - ------------ Save best model - MSE: 34.0448 ------------
2023-11-08 09:50:41,227 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9811 ------------
2023-11-08 09:50:41,229 - __main__ - INFO - Fold 1, mse = 34.0448, mad = 4.3460
2023-11-08 09:50:41,523 - __main__ - INFO - Fold 1 Epoch 41 Batch 0: Train Loss = 1.3001
2023-11-08 09:50:42,795 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9816 ------------
2023-11-08 09:50:42,799 - __main__ - INFO - Fold 1, mse = 34.1482, mad = 4.3278
2023-11-08 09:50:43,039 - __main__ - INFO - Fold 1 Epoch 42 Batch 0: Train Loss = 1.5525
2023-11-08 09:50:44,554 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.3448 ------------
2023-11-08 09:50:44,884 - __main__ - INFO - ------------ Save best model - MSE: 33.3448 ------------
2023-11-08 09:50:44,890 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9816 ------------
2023-11-08 09:50:44,895 - __main__ - INFO - Fold 1, mse = 33.3448, mad = 4.2830
2023-11-08 09:50:45,152 - __main__ - INFO - Fold 1 Epoch 43 Batch 0: Train Loss = 1.5279
2023-11-08 09:50:46,780 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.2568 ------------
2023-11-08 09:50:47,087 - __main__ - INFO - ------------ Save best model - MSE: 33.2568 ------------
2023-11-08 09:50:47,089 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9819 ------------
2023-11-08 09:50:47,091 - __main__ - INFO - Fold 1, mse = 33.2568, mad = 4.2453
2023-11-08 09:50:47,307 - __main__ - INFO - Fold 1 Epoch 44 Batch 0: Train Loss = 1.2378
2023-11-08 09:50:48,966 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.1765 ------------
2023-11-08 09:50:49,324 - __main__ - INFO - ------------ Save best model - MSE: 33.1765 ------------
2023-11-08 09:50:49,326 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9822 ------------
2023-11-08 09:50:49,329 - __main__ - INFO - Fold 1, mse = 33.1765, mad = 4.2407
2023-11-08 09:50:49,618 - __main__ - INFO - Fold 1 Epoch 45 Batch 0: Train Loss = 1.2637
2023-11-08 09:50:51,630 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.0805 ------------
2023-11-08 09:50:51,957 - __main__ - INFO - ------------ Save best model - MSE: 33.0805 ------------
2023-11-08 09:50:51,959 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9828 ------------
2023-11-08 09:50:51,961 - __main__ - INFO - Fold 1, mse = 33.0805, mad = 4.2440
2023-11-08 09:50:52,363 - __main__ - INFO - Fold 1 Epoch 46 Batch 0: Train Loss = 1.6413
2023-11-08 09:50:54,015 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.8626 ------------
2023-11-08 09:50:54,339 - __main__ - INFO - ------------ Save best model - MSE: 32.8626 ------------
2023-11-08 09:50:54,341 - __main__ - INFO - Fold 1, mse = 32.8626, mad = 4.2427
2023-11-08 09:50:54,669 - __main__ - INFO - Fold 1 Epoch 47 Batch 0: Train Loss = 1.2502
2023-11-08 09:50:56,736 - __main__ - INFO - Fold 1, mse = 33.1015, mad = 4.2015
2023-11-08 09:50:56,991 - __main__ - INFO - Fold 1 Epoch 48 Batch 0: Train Loss = 1.5225
2023-11-08 09:50:58,776 - __main__ - INFO - Fold 1, mse = 32.8912, mad = 4.2410
2023-11-08 09:50:59,087 - __main__ - INFO - Fold 1 Epoch 49 Batch 0: Train Loss = 1.6074
2023-11-08 09:51:01,266 - __main__ - INFO - Fold 1, mse = 33.2313, mad = 4.2103
2023-11-08 09:51:01,524 - __main__ - INFO - Fold 1 Epoch 50 Batch 0: Train Loss = 1.2492
2023-11-08 09:51:03,280 - __main__ - INFO - Fold 1, epoch 50: Loss = 1.3832 Valid loss = 1.6306 MSE = 33.0169 AUROC = 0.9827
2023-11-08 09:51:03,282 - __main__ - INFO - Fold 1, mse = 33.0169, mad = 4.2440
2023-11-08 09:51:03,522 - __main__ - INFO - Fold 1 Epoch 51 Batch 0: Train Loss = 1.3713
2023-11-08 09:51:05,115 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.7611 ------------
2023-11-08 09:51:05,458 - __main__ - INFO - ------------ Save best model - MSE: 32.7611 ------------
2023-11-08 09:51:05,460 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9830 ------------
2023-11-08 09:51:05,461 - __main__ - INFO - Fold 1, mse = 32.7611, mad = 4.2116
2023-11-08 09:51:05,819 - __main__ - INFO - Fold 1 Epoch 52 Batch 0: Train Loss = 1.2550
2023-11-08 09:51:07,480 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9837 ------------
2023-11-08 09:51:07,482 - __main__ - INFO - Fold 1, mse = 32.7915, mad = 4.1559
2023-11-08 09:51:07,788 - __main__ - INFO - Fold 1 Epoch 53 Batch 0: Train Loss = 1.4771
2023-11-08 09:51:09,754 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.4463 ------------
2023-11-08 09:51:10,060 - __main__ - INFO - ------------ Save best model - MSE: 32.4463 ------------
2023-11-08 09:51:10,062 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9839 ------------
2023-11-08 09:51:10,063 - __main__ - INFO - Fold 1, mse = 32.4463, mad = 4.1868
2023-11-08 09:51:10,343 - __main__ - INFO - Fold 1 Epoch 54 Batch 0: Train Loss = 1.1976
2023-11-08 09:51:12,078 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9840 ------------
2023-11-08 09:51:12,081 - __main__ - INFO - Fold 1, mse = 32.6562, mad = 4.1754
2023-11-08 09:51:12,421 - __main__ - INFO - Fold 1 Epoch 55 Batch 0: Train Loss = 1.1129
2023-11-08 09:51:14,286 - __main__ - INFO - Fold 1, mse = 32.6416, mad = 4.2114
2023-11-08 09:51:14,567 - __main__ - INFO - Fold 1 Epoch 56 Batch 0: Train Loss = 1.3117
2023-11-08 09:51:16,322 - __main__ - INFO - Fold 1, mse = 32.8612, mad = 4.2017
2023-11-08 09:51:16,667 - __main__ - INFO - Fold 1 Epoch 57 Batch 0: Train Loss = 1.2124
2023-11-08 09:51:18,392 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9843 ------------
2023-11-08 09:51:18,394 - __main__ - INFO - Fold 1, mse = 32.4468, mad = 4.1843
2023-11-08 09:51:18,681 - __main__ - INFO - Fold 1 Epoch 58 Batch 0: Train Loss = 1.3891
2023-11-08 09:51:20,565 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.2475 ------------
2023-11-08 09:51:20,898 - __main__ - INFO - ------------ Save best model - MSE: 32.2475 ------------
2023-11-08 09:51:20,899 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9851 ------------
2023-11-08 09:51:20,900 - __main__ - INFO - Fold 1, mse = 32.2475, mad = 4.1332
2023-11-08 09:51:21,104 - __main__ - INFO - Fold 1 Epoch 59 Batch 0: Train Loss = 1.4746
2023-11-08 09:51:22,742 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.7322 ------------
2023-11-08 09:51:23,092 - __main__ - INFO - ------------ Save best model - MSE: 31.7322 ------------
2023-11-08 09:51:23,094 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9855 ------------
2023-11-08 09:51:23,096 - __main__ - INFO - Fold 1, mse = 31.7322, mad = 4.1446
2023-11-08 09:51:23,358 - __main__ - INFO - Fold 1 Epoch 60 Batch 0: Train Loss = 1.0654
2023-11-08 09:51:24,966 - __main__ - INFO - Fold 1, epoch 60: Loss = 1.2426 Valid loss = 1.5662 MSE = 31.9876 AUROC = 0.9856
2023-11-08 09:51:24,967 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9856 ------------
2023-11-08 09:51:24,969 - __main__ - INFO - Fold 1, mse = 31.9876, mad = 4.1227
2023-11-08 09:51:25,284 - __main__ - INFO - Fold 1 Epoch 61 Batch 0: Train Loss = 1.1746
2023-11-08 09:51:26,996 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9860 ------------
2023-11-08 09:51:27,001 - __main__ - INFO - Fold 1, mse = 31.8178, mad = 4.1261
2023-11-08 09:51:27,290 - __main__ - INFO - Fold 1 Epoch 62 Batch 0: Train Loss = 1.1580
2023-11-08 09:51:29,072 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.5838 ------------
2023-11-08 09:51:29,459 - __main__ - INFO - ------------ Save best model - MSE: 31.5838 ------------
2023-11-08 09:51:29,462 - __main__ - INFO - Fold 1, mse = 31.5838, mad = 4.1682
2023-11-08 09:51:29,790 - __main__ - INFO - Fold 1 Epoch 63 Batch 0: Train Loss = 1.0990
2023-11-08 09:51:31,671 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9861 ------------
2023-11-08 09:51:31,673 - __main__ - INFO - Fold 1, mse = 31.6793, mad = 4.0981
2023-11-08 09:51:31,965 - __main__ - INFO - Fold 1 Epoch 64 Batch 0: Train Loss = 1.2230
2023-11-08 09:51:33,616 - __main__ - INFO - Fold 1, mse = 31.6901, mad = 4.1614
2023-11-08 09:51:33,889 - __main__ - INFO - Fold 1 Epoch 65 Batch 0: Train Loss = 1.1285
2023-11-08 09:51:35,476 - __main__ - INFO - Fold 1, mse = 31.9064, mad = 4.1072
2023-11-08 09:51:35,754 - __main__ - INFO - Fold 1 Epoch 66 Batch 0: Train Loss = 1.3588
2023-11-08 09:51:37,463 - __main__ - INFO - Fold 1, mse = 31.7123, mad = 4.1185
2023-11-08 09:51:37,815 - __main__ - INFO - Fold 1 Epoch 67 Batch 0: Train Loss = 1.1157
2023-11-08 09:51:39,582 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.5092 ------------
2023-11-08 09:51:39,898 - __main__ - INFO - ------------ Save best model - MSE: 31.5092 ------------
2023-11-08 09:51:39,900 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9866 ------------
2023-11-08 09:51:39,901 - __main__ - INFO - Fold 1, mse = 31.5092, mad = 4.1075
2023-11-08 09:51:40,124 - __main__ - INFO - Fold 1 Epoch 68 Batch 0: Train Loss = 1.1648
2023-11-08 09:51:41,809 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.2836 ------------
2023-11-08 09:51:42,096 - __main__ - INFO - ------------ Save best model - MSE: 31.2836 ------------
2023-11-08 09:51:42,098 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9870 ------------
2023-11-08 09:51:42,101 - __main__ - INFO - Fold 1, mse = 31.2836, mad = 4.0906
2023-11-08 09:51:42,389 - __main__ - INFO - Fold 1 Epoch 69 Batch 0: Train Loss = 0.9594
2023-11-08 09:51:44,182 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.0776 ------------
2023-11-08 09:51:44,563 - __main__ - INFO - ------------ Save best model - MSE: 31.0776 ------------
2023-11-08 09:51:44,565 - __main__ - INFO - Fold 1, mse = 31.0776, mad = 4.0928
2023-11-08 09:51:44,827 - __main__ - INFO - Fold 1 Epoch 70 Batch 0: Train Loss = 1.2101
2023-11-08 09:51:46,527 - __main__ - INFO - Fold 1, epoch 70: Loss = 1.2229 Valid loss = 1.5612 MSE = 31.4578 AUROC = 0.9869
2023-11-08 09:51:46,531 - __main__ - INFO - Fold 1, mse = 31.4578, mad = 4.0921
2023-11-08 09:51:46,805 - __main__ - INFO - Fold 1 Epoch 71 Batch 0: Train Loss = 1.3193
2023-11-08 09:51:48,587 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9871 ------------
2023-11-08 09:51:48,589 - __main__ - INFO - Fold 1, mse = 31.6237, mad = 4.1428
2023-11-08 09:51:48,899 - __main__ - INFO - Fold 1 Epoch 72 Batch 0: Train Loss = 1.1222
2023-11-08 09:51:50,505 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9872 ------------
2023-11-08 09:51:50,507 - __main__ - INFO - Fold 1, mse = 31.6341, mad = 4.0928
2023-11-08 09:51:50,845 - __main__ - INFO - Fold 1 Epoch 73 Batch 0: Train Loss = 1.3777
2023-11-08 09:51:52,639 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.9949 ------------
2023-11-08 09:51:52,975 - __main__ - INFO - ------------ Save best model - MSE: 30.9949 ------------
2023-11-08 09:51:52,978 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9872 ------------
2023-11-08 09:51:52,980 - __main__ - INFO - Fold 1, mse = 30.9949, mad = 4.0737
2023-11-08 09:51:53,249 - __main__ - INFO - Fold 1 Epoch 74 Batch 0: Train Loss = 1.1983
2023-11-08 09:51:55,049 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.9390 ------------
2023-11-08 09:51:55,353 - __main__ - INFO - ------------ Save best model - MSE: 30.9390 ------------
2023-11-08 09:51:55,356 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9873 ------------
2023-11-08 09:51:55,358 - __main__ - INFO - Fold 1, mse = 30.9390, mad = 4.0346
2023-11-08 09:51:55,670 - __main__ - INFO - Fold 1 Epoch 75 Batch 0: Train Loss = 1.1712
2023-11-08 09:51:57,452 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9874 ------------
2023-11-08 09:51:57,454 - __main__ - INFO - Fold 1, mse = 31.3221, mad = 4.0442
2023-11-08 09:51:57,733 - __main__ - INFO - Fold 1 Epoch 76 Batch 0: Train Loss = 1.1858
2023-11-08 09:51:59,462 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9875 ------------
2023-11-08 09:51:59,464 - __main__ - INFO - Fold 1, mse = 31.3844, mad = 4.1051
2023-11-08 09:51:59,766 - __main__ - INFO - Fold 1 Epoch 77 Batch 0: Train Loss = 1.0967
2023-11-08 09:52:01,361 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9877 ------------
2023-11-08 09:52:01,365 - __main__ - INFO - Fold 1, mse = 31.7916, mad = 4.0893
2023-11-08 09:52:01,610 - __main__ - INFO - Fold 1 Epoch 78 Batch 0: Train Loss = 1.2462
2023-11-08 09:52:03,210 - __main__ - INFO - Fold 1, mse = 31.4329, mad = 4.0922
2023-11-08 09:52:03,555 - __main__ - INFO - Fold 1 Epoch 79 Batch 0: Train Loss = 1.3154
2023-11-08 09:52:05,131 - __main__ - INFO - Fold 1, mse = 31.1859, mad = 4.0432
2023-11-08 09:52:05,397 - __main__ - INFO - Fold 1 Epoch 80 Batch 0: Train Loss = 1.1716
2023-11-08 09:52:06,743 - __main__ - INFO - Fold 1, epoch 80: Loss = 1.1295 Valid loss = 1.5328 MSE = 31.0855 AUROC = 0.9876
2023-11-08 09:52:06,745 - __main__ - INFO - Fold 1, mse = 31.0855, mad = 4.0079
2023-11-08 09:52:06,957 - __main__ - INFO - Fold 1 Epoch 81 Batch 0: Train Loss = 1.1526
2023-11-08 09:52:08,572 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9886 ------------
2023-11-08 09:52:08,575 - __main__ - INFO - Fold 1, mse = 31.0874, mad = 4.0469
2023-11-08 09:52:08,914 - __main__ - INFO - Fold 1 Epoch 82 Batch 0: Train Loss = 1.1883
2023-11-08 09:52:10,702 - __main__ - INFO - Fold 1, mse = 31.1978, mad = 4.0383
2023-11-08 09:52:11,012 - __main__ - INFO - Fold 1 Epoch 83 Batch 0: Train Loss = 1.1244
2023-11-08 09:52:12,663 - __main__ - INFO - Fold 1, mse = 31.0223, mad = 4.0514
2023-11-08 09:52:12,951 - __main__ - INFO - Fold 1 Epoch 84 Batch 0: Train Loss = 1.1325
2023-11-08 09:52:14,443 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.9120 ------------
2023-11-08 09:52:14,808 - __main__ - INFO - ------------ Save best model - MSE: 30.9120 ------------
2023-11-08 09:52:14,811 - __main__ - INFO - Fold 1, mse = 30.9120, mad = 4.0504
2023-11-08 09:52:15,227 - __main__ - INFO - Fold 1 Epoch 85 Batch 0: Train Loss = 1.0764
2023-11-08 09:52:16,986 - __main__ - INFO - Fold 1, mse = 30.9602, mad = 4.0448
2023-11-08 09:52:17,331 - __main__ - INFO - Fold 1 Epoch 86 Batch 0: Train Loss = 1.1611
2023-11-08 09:52:19,155 - __main__ - INFO - Fold 1, mse = 30.9271, mad = 4.0359
2023-11-08 09:52:19,386 - __main__ - INFO - Fold 1 Epoch 87 Batch 0: Train Loss = 1.0148
2023-11-08 09:52:21,053 - __main__ - INFO - Fold 1, mse = 31.0301, mad = 4.0376
2023-11-08 09:52:21,331 - __main__ - INFO - Fold 1 Epoch 88 Batch 0: Train Loss = 1.0947
2023-11-08 09:52:23,251 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9888 ------------
2023-11-08 09:52:23,256 - __main__ - INFO - Fold 1, mse = 31.2429, mad = 4.0487
2023-11-08 09:52:23,512 - __main__ - INFO - Fold 1 Epoch 89 Batch 0: Train Loss = 1.2360
2023-11-08 09:52:25,513 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9892 ------------
2023-11-08 09:52:25,515 - __main__ - INFO - Fold 1, mse = 31.1551, mad = 4.0552
2023-11-08 09:52:25,899 - __main__ - INFO - Fold 1 Epoch 90 Batch 0: Train Loss = 1.1673
2023-11-08 09:52:27,573 - __main__ - INFO - Fold 1, epoch 90: Loss = 1.0780 Valid loss = 1.5332 MSE = 31.0528 AUROC = 0.9890
2023-11-08 09:52:27,575 - __main__ - INFO - Fold 1, mse = 31.0528, mad = 4.0224
2023-11-08 09:52:27,953 - __main__ - INFO - Fold 1 Epoch 91 Batch 0: Train Loss = 0.9512
2023-11-08 09:52:29,296 - __main__ - INFO - Fold 1, mse = 31.0491, mad = 4.0114
2023-11-08 09:52:29,552 - __main__ - INFO - Fold 1 Epoch 92 Batch 0: Train Loss = 0.9404
2023-11-08 09:52:31,203 - __main__ - INFO - Fold 1, mse = 31.0419, mad = 4.0488
2023-11-08 09:52:31,499 - __main__ - INFO - Fold 1 Epoch 93 Batch 0: Train Loss = 1.0425
2023-11-08 09:52:33,321 - __main__ - INFO - Fold 1, mse = 30.9623, mad = 4.0109
2023-11-08 09:52:33,548 - __main__ - INFO - Fold 1 Epoch 94 Batch 0: Train Loss = 1.0108
2023-11-08 09:52:35,041 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.6682 ------------
2023-11-08 09:52:35,421 - __main__ - INFO - ------------ Save best model - MSE: 30.6682 ------------
2023-11-08 09:52:35,423 - __main__ - INFO - Fold 1, mse = 30.6682, mad = 3.9847
2023-11-08 09:52:35,675 - __main__ - INFO - Fold 1 Epoch 95 Batch 0: Train Loss = 1.0221
2023-11-08 09:52:37,672 - __main__ - INFO - Fold 1, mse = 30.7773, mad = 4.0288
2023-11-08 09:52:38,009 - __main__ - INFO - Fold 1 Epoch 96 Batch 0: Train Loss = 0.9648
2023-11-08 09:52:39,743 - __main__ - INFO - Fold 1, mse = 31.0971, mad = 4.0245
2023-11-08 09:52:40,015 - __main__ - INFO - Fold 1 Epoch 97 Batch 0: Train Loss = 0.9346
2023-11-08 09:52:41,732 - __main__ - INFO - Fold 1, mse = 30.9406, mad = 4.0123
2023-11-08 09:52:41,979 - __main__ - INFO - Fold 1 Epoch 98 Batch 0: Train Loss = 1.3319
2023-11-08 09:52:43,700 - __main__ - INFO - Fold 1, mse = 30.8381, mad = 4.0267
2023-11-08 09:52:44,083 - __main__ - INFO - Fold 1 Epoch 99 Batch 0: Train Loss = 1.0328
2023-11-08 09:52:46,023 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9897 ------------
2023-11-08 09:52:46,026 - __main__ - INFO - Fold 1, mse = 30.7104, mad = 4.0138
2023-11-08 09:52:46,339 - __main__ - INFO - Fold 1 Epoch 100 Batch 0: Train Loss = 1.1649
2023-11-08 09:52:48,067 - __main__ - INFO - Fold 1, epoch 100: Loss = 1.0628 Valid loss = 1.5251 MSE = 30.9272 AUROC = 0.9894
2023-11-08 09:52:48,071 - __main__ - INFO - Fold 1, mse = 30.9272, mad = 4.0114
2023-11-08 09:52:48,371 - __main__ - INFO - Fold 1 Epoch 101 Batch 0: Train Loss = 1.3928
2023-11-08 09:52:49,751 - __main__ - INFO - Fold 1, mse = 31.0688, mad = 4.0558
2023-11-08 09:52:49,959 - __main__ - INFO - Fold 1 Epoch 102 Batch 0: Train Loss = 1.1097
2023-11-08 09:52:51,896 - __main__ - INFO - Fold 1, mse = 30.7960, mad = 4.0192
2023-11-08 09:52:52,274 - __main__ - INFO - Fold 1 Epoch 103 Batch 0: Train Loss = 1.0121
2023-11-08 09:52:54,227 - __main__ - INFO - Fold 1, mse = 30.7332, mad = 4.0615
2023-11-08 09:52:54,512 - __main__ - INFO - Fold 1 Epoch 104 Batch 0: Train Loss = 1.1711
2023-11-08 09:52:56,358 - __main__ - INFO - Fold 1, mse = 31.0123, mad = 4.0100
2023-11-08 09:52:56,648 - __main__ - INFO - Fold 1 Epoch 105 Batch 0: Train Loss = 1.1540
2023-11-08 09:52:58,529 - __main__ - INFO - Fold 1, mse = 30.9378, mad = 4.0576
2023-11-08 09:52:58,808 - __main__ - INFO - Fold 1 Epoch 106 Batch 0: Train Loss = 1.1860
2023-11-08 09:53:00,715 - __main__ - INFO - Fold 1, mse = 31.1317, mad = 4.0365
2023-11-08 09:53:01,010 - __main__ - INFO - Fold 1 Epoch 107 Batch 0: Train Loss = 1.1819
2023-11-08 09:53:02,236 - __main__ - INFO - Fold 1, mse = 30.7419, mad = 4.0130
2023-11-08 09:53:02,454 - __main__ - INFO - Fold 1 Epoch 108 Batch 0: Train Loss = 0.9572
2023-11-08 09:53:04,100 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.6261 ------------
2023-11-08 09:53:04,425 - __main__ - INFO - ------------ Save best model - MSE: 30.6261 ------------
2023-11-08 09:53:04,429 - __main__ - INFO - Fold 1, mse = 30.6261, mad = 3.9721
2023-11-08 09:53:04,679 - __main__ - INFO - Fold 1 Epoch 109 Batch 0: Train Loss = 1.1225
2023-11-08 09:53:06,504 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.5182 ------------
2023-11-08 09:53:06,892 - __main__ - INFO - ------------ Save best model - MSE: 30.5182 ------------
2023-11-08 09:53:06,894 - __main__ - INFO - Fold 1, mse = 30.5182, mad = 3.9575
2023-11-08 09:53:07,214 - __main__ - INFO - Fold 1 Epoch 110 Batch 0: Train Loss = 1.1209
2023-11-08 09:53:08,873 - __main__ - INFO - Fold 1, epoch 110: Loss = 1.0827 Valid loss = 1.5239 MSE = 30.6515 AUROC = 0.9897
2023-11-08 09:53:08,877 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9897 ------------
2023-11-08 09:53:08,881 - __main__ - INFO - Fold 1, mse = 30.6515, mad = 3.9899
2023-11-08 09:53:09,157 - __main__ - INFO - Fold 1 Epoch 111 Batch 0: Train Loss = 0.8524
2023-11-08 09:53:10,960 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9900 ------------
2023-11-08 09:53:10,962 - __main__ - INFO - Fold 1, mse = 30.7718, mad = 4.0328
2023-11-08 09:53:11,249 - __main__ - INFO - Fold 1 Epoch 112 Batch 0: Train Loss = 1.0215
2023-11-08 09:53:12,855 - __main__ - INFO - Fold 1, mse = 30.7916, mad = 4.0118
2023-11-08 09:53:13,146 - __main__ - INFO - Fold 1 Epoch 113 Batch 0: Train Loss = 1.2427
2023-11-08 09:53:14,831 - __main__ - INFO - Fold 1, mse = 30.5578, mad = 4.0032
2023-11-08 09:53:15,134 - __main__ - INFO - Fold 1 Epoch 114 Batch 0: Train Loss = 1.0306
2023-11-08 09:53:16,854 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9902 ------------
2023-11-08 09:53:16,858 - __main__ - INFO - Fold 1, mse = 30.6338, mad = 3.9895
2023-11-08 09:53:17,146 - __main__ - INFO - Fold 1 Epoch 115 Batch 0: Train Loss = 1.0327
2023-11-08 09:53:19,087 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9907 ------------
2023-11-08 09:53:19,088 - __main__ - INFO - Fold 1, mse = 30.7009, mad = 4.0208
2023-11-08 09:53:19,320 - __main__ - INFO - Fold 1 Epoch 116 Batch 0: Train Loss = 1.1231
2023-11-08 09:53:20,938 - __main__ - INFO - Fold 1, mse = 31.0983, mad = 4.0426
2023-11-08 09:53:21,262 - __main__ - INFO - Fold 1 Epoch 117 Batch 0: Train Loss = 1.0172
2023-11-08 09:53:22,797 - __main__ - INFO - Fold 1, mse = 31.0269, mad = 4.0736
2023-11-08 09:53:23,138 - __main__ - INFO - Fold 1 Epoch 118 Batch 0: Train Loss = 1.1174
2023-11-08 09:53:25,105 - __main__ - INFO - Fold 1, mse = 31.0796, mad = 4.0428
2023-11-08 09:53:25,491 - __main__ - INFO - Fold 1 Epoch 119 Batch 0: Train Loss = 1.3271
2023-11-08 09:53:27,391 - __main__ - INFO - Fold 1, mse = 31.1775, mad = 4.0579
2023-11-08 09:53:27,694 - __main__ - INFO - Fold 1 Epoch 120 Batch 0: Train Loss = 1.0735
2023-11-08 09:53:29,499 - __main__ - INFO - Fold 1, epoch 120: Loss = 1.0487 Valid loss = 1.5405 MSE = 31.2268 AUROC = 0.9905
2023-11-08 09:53:29,504 - __main__ - INFO - Fold 1, mse = 31.2268, mad = 4.0341
2023-11-08 09:53:29,783 - __main__ - INFO - Fold 1 Epoch 121 Batch 0: Train Loss = 0.8885
2023-11-08 09:53:31,364 - __main__ - INFO - Fold 1, mse = 30.9181, mad = 4.0178
2023-11-08 09:53:31,762 - __main__ - INFO - Fold 1 Epoch 122 Batch 0: Train Loss = 1.1015
2023-11-08 09:53:33,614 - __main__ - INFO - Fold 1, mse = 30.8883, mad = 4.0277
2023-11-08 09:53:33,906 - __main__ - INFO - Fold 1 Epoch 123 Batch 0: Train Loss = 1.1726
2023-11-08 09:53:35,745 - __main__ - INFO - Fold 1, mse = 30.8305, mad = 3.9911
2023-11-08 09:53:36,039 - __main__ - INFO - Fold 1 Epoch 124 Batch 0: Train Loss = 1.0021
2023-11-08 09:53:37,611 - __main__ - INFO - Fold 1, mse = 30.5191, mad = 4.0083
2023-11-08 09:53:37,926 - __main__ - INFO - Fold 1 Epoch 125 Batch 0: Train Loss = 1.1156
2023-11-08 09:53:39,545 - __main__ - INFO - Fold 1, mse = 30.8391, mad = 4.0104
2023-11-08 09:53:39,869 - __main__ - INFO - Fold 1 Epoch 126 Batch 0: Train Loss = 1.1136
2023-11-08 09:53:41,799 - __main__ - INFO - Fold 1, mse = 30.5484, mad = 4.0396
2023-11-08 09:53:42,132 - __main__ - INFO - Fold 1 Epoch 127 Batch 0: Train Loss = 1.1487
2023-11-08 09:53:43,635 - __main__ - INFO - Fold 1, mse = 30.7566, mad = 4.0207
2023-11-08 09:53:43,942 - __main__ - INFO - Fold 1 Epoch 128 Batch 0: Train Loss = 1.0650
2023-11-08 09:53:45,791 - __main__ - INFO - Fold 1, mse = 30.5597, mad = 4.0427
2023-11-08 09:53:46,125 - __main__ - INFO - Fold 1 Epoch 129 Batch 0: Train Loss = 0.8594
2023-11-08 09:53:47,861 - __main__ - INFO - Fold 1, mse = 30.8736, mad = 4.0238
2023-11-08 09:53:48,160 - __main__ - INFO - Fold 1 Epoch 130 Batch 0: Train Loss = 0.8511
2023-11-08 09:53:49,718 - __main__ - INFO - Fold 1, epoch 130: Loss = 0.9849 Valid loss = 1.4829 MSE = 30.3208 AUROC = 0.9900
2023-11-08 09:53:49,720 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.3208 ------------
2023-11-08 09:53:50,040 - __main__ - INFO - ------------ Save best model - MSE: 30.3208 ------------
2023-11-08 09:53:50,042 - __main__ - INFO - Fold 1, mse = 30.3208, mad = 4.0263
2023-11-08 09:53:50,353 - __main__ - INFO - Fold 1 Epoch 131 Batch 0: Train Loss = 0.9807
2023-11-08 09:53:52,124 - __main__ - INFO - Fold 1, mse = 30.4442, mad = 3.9788
2023-11-08 09:53:52,423 - __main__ - INFO - Fold 1 Epoch 132 Batch 0: Train Loss = 1.0679
2023-11-08 09:53:54,343 - __main__ - INFO - Fold 1, mse = 30.4630, mad = 3.9957
2023-11-08 09:53:54,652 - __main__ - INFO - Fold 1 Epoch 133 Batch 0: Train Loss = 1.1062
2023-11-08 09:53:56,267 - __main__ - INFO - Fold 1, mse = 30.6480, mad = 4.0405
2023-11-08 09:53:56,470 - __main__ - INFO - Fold 1 Epoch 134 Batch 0: Train Loss = 1.0200
2023-11-08 09:53:58,222 - __main__ - INFO - Fold 1, mse = 31.0650, mad = 4.0218
2023-11-08 09:53:58,523 - __main__ - INFO - Fold 1 Epoch 135 Batch 0: Train Loss = 0.9207
2023-11-08 09:54:00,222 - __main__ - INFO - Fold 1, mse = 30.4397, mad = 4.0146
2023-11-08 09:54:00,502 - __main__ - INFO - Fold 1 Epoch 136 Batch 0: Train Loss = 0.8789
2023-11-08 09:54:02,336 - __main__ - INFO - Fold 1, mse = 30.4558, mad = 3.9909
2023-11-08 09:54:02,619 - __main__ - INFO - Fold 1 Epoch 137 Batch 0: Train Loss = 0.9804
2023-11-08 09:54:04,087 - __main__ - INFO - Fold 1, mse = 30.9067, mad = 4.0095
2023-11-08 09:54:04,405 - __main__ - INFO - Fold 1 Epoch 138 Batch 0: Train Loss = 0.9281
2023-11-08 09:54:06,162 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9908 ------------
2023-11-08 09:54:06,164 - __main__ - INFO - Fold 1, mse = 30.9157, mad = 4.0540
2023-11-08 09:54:06,483 - __main__ - INFO - Fold 1 Epoch 139 Batch 0: Train Loss = 1.1285
2023-11-08 09:54:08,005 - __main__ - INFO - Fold 1, mse = 31.2033, mad = 4.0632
2023-11-08 09:54:08,309 - __main__ - INFO - Fold 1 Epoch 140 Batch 0: Train Loss = 0.9249
2023-11-08 09:54:10,096 - __main__ - INFO - Fold 1, epoch 140: Loss = 0.9416 Valid loss = 1.4929 MSE = 30.8251 AUROC = 0.9906
2023-11-08 09:54:10,100 - __main__ - INFO - Fold 1, mse = 30.8251, mad = 4.0744
2023-11-08 09:54:10,375 - __main__ - INFO - Fold 1 Epoch 141 Batch 0: Train Loss = 0.8894
2023-11-08 09:54:12,031 - __main__ - INFO - Fold 1, mse = 31.2969, mad = 4.0337
2023-11-08 09:54:12,337 - __main__ - INFO - Fold 1 Epoch 142 Batch 0: Train Loss = 0.9775
2023-11-08 09:54:13,936 - __main__ - INFO - Fold 1, mse = 30.8082, mad = 4.0381
2023-11-08 09:54:14,175 - __main__ - INFO - Fold 1 Epoch 143 Batch 0: Train Loss = 1.0719
2023-11-08 09:54:15,432 - __main__ - INFO - Fold 1, mse = 30.7977, mad = 4.0440
2023-11-08 09:54:15,708 - __main__ - INFO - Fold 1 Epoch 144 Batch 0: Train Loss = 0.9931
2023-11-08 09:54:17,677 - __main__ - INFO - Fold 1, mse = 30.9505, mad = 4.0153
2023-11-08 09:54:18,078 - __main__ - INFO - Fold 1 Epoch 145 Batch 0: Train Loss = 1.1271
2023-11-08 09:54:19,836 - __main__ - INFO - Fold 1, mse = 30.6537, mad = 4.0298
2023-11-08 09:54:20,083 - __main__ - INFO - Fold 1 Epoch 146 Batch 0: Train Loss = 0.9845
2023-11-08 09:54:21,864 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9908 ------------
2023-11-08 09:54:21,865 - __main__ - INFO - Fold 1, mse = 30.7926, mad = 4.0034
2023-11-08 09:54:22,135 - __main__ - INFO - Fold 1 Epoch 147 Batch 0: Train Loss = 1.0100
2023-11-08 09:54:23,755 - __main__ - INFO - Fold 1, mse = 30.6492, mad = 4.0162
2023-11-08 09:54:24,116 - __main__ - INFO - Fold 1 Epoch 148 Batch 0: Train Loss = 0.7767
2023-11-08 09:54:26,045 - __main__ - INFO - Fold 1, mse = 30.7146, mad = 3.9819
2023-11-08 09:54:26,351 - __main__ - INFO - Fold 1 Epoch 149 Batch 0: Train Loss = 0.9127
2023-11-08 09:54:27,969 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9909 ------------
2023-11-08 09:54:27,973 - __main__ - INFO - Fold 1, mse = 30.7731, mad = 3.9997
2023-11-08 09:54:28,639 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 2.3345
2023-11-08 09:54:30,421 - __main__ - INFO - Fold 2, epoch 0: Loss = 2.4015 Valid loss = 2.0615 MSE = 46.7346 AUROC = 0.6332
2023-11-08 09:54:30,423 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 46.7346 ------------
2023-11-08 09:54:30,624 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.6332 ------------
2023-11-08 09:54:30,625 - __main__ - INFO - Fold 2, mse = 46.7346, mad = 5.0044
2023-11-08 09:54:30,858 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 2.2798
2023-11-08 09:54:32,458 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 46.0287 ------------
2023-11-08 09:54:32,616 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.6498 ------------
2023-11-08 09:54:32,617 - __main__ - INFO - Fold 2, mse = 46.0287, mad = 4.8437
2023-11-08 09:54:32,974 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 2.1634
2023-11-08 09:54:34,980 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.5196 ------------
2023-11-08 09:54:35,169 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.6745 ------------
2023-11-08 09:54:35,172 - __main__ - INFO - Fold 2, mse = 45.5196, mad = 4.8089
2023-11-08 09:54:35,476 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 2.1727
2023-11-08 09:54:37,193 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.3037 ------------
2023-11-08 09:54:37,427 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.7012 ------------
2023-11-08 09:54:37,429 - __main__ - INFO - Fold 2, mse = 45.3037, mad = 4.8447
2023-11-08 09:54:37,687 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 2.0782
2023-11-08 09:54:39,360 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.7578 ------------
2023-11-08 09:54:39,363 - __main__ - INFO - Fold 2, mse = 45.3868, mad = 4.8245
2023-11-08 09:54:39,630 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 1.9942
2023-11-08 09:54:40,940 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8208 ------------
2023-11-08 09:54:40,942 - __main__ - INFO - Fold 2, mse = 45.4335, mad = 4.8026
2023-11-08 09:54:41,218 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 2.2211
2023-11-08 09:54:42,838 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8864 ------------
2023-11-08 09:54:42,840 - __main__ - INFO - Fold 2, mse = 45.3999, mad = 4.7800
2023-11-08 09:54:43,092 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 2.1250
2023-11-08 09:54:44,736 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.2667 ------------
2023-11-08 09:54:44,972 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9352 ------------
2023-11-08 09:54:44,983 - __main__ - INFO - Fold 2, mse = 45.2667, mad = 4.7433
2023-11-08 09:54:45,238 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 1.8575
2023-11-08 09:54:46,955 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.9301 ------------
2023-11-08 09:54:47,120 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9599 ------------
2023-11-08 09:54:47,122 - __main__ - INFO - Fold 2, mse = 44.9301, mad = 4.7628
2023-11-08 09:54:47,412 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 1.9708
2023-11-08 09:54:49,012 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.7756 ------------
2023-11-08 09:54:49,162 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9695 ------------
2023-11-08 09:54:49,165 - __main__ - INFO - Fold 2, mse = 44.7756, mad = 4.7289
2023-11-08 09:54:49,519 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 1.8110
2023-11-08 09:54:51,426 - __main__ - INFO - Fold 2, epoch 10: Loss = 1.8766 Valid loss = 1.8134 MSE = 44.7494 AUROC = 0.9731
2023-11-08 09:54:51,428 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.7494 ------------
2023-11-08 09:54:51,626 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9731 ------------
2023-11-08 09:54:51,628 - __main__ - INFO - Fold 2, mse = 44.7494, mad = 4.6625
2023-11-08 09:54:52,003 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 1.5991
2023-11-08 09:54:53,932 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.4926 ------------
2023-11-08 09:54:54,093 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9746 ------------
2023-11-08 09:54:54,096 - __main__ - INFO - Fold 2, mse = 44.4926, mad = 4.6862
2023-11-08 09:54:54,534 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 1.8293
2023-11-08 09:54:56,107 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.2575 ------------
2023-11-08 09:54:56,359 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9753 ------------
2023-11-08 09:54:56,363 - __main__ - INFO - Fold 2, mse = 44.2575, mad = 4.7169
2023-11-08 09:54:56,661 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 1.6826
2023-11-08 09:54:58,364 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.0316 ------------
2023-11-08 09:54:58,588 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9757 ------------
2023-11-08 09:54:58,591 - __main__ - INFO - Fold 2, mse = 44.0316, mad = 4.6989
2023-11-08 09:54:58,961 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 1.9145
2023-11-08 09:55:00,519 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.0005 ------------
2023-11-08 09:55:00,675 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9757 ------------
2023-11-08 09:55:00,678 - __main__ - INFO - Fold 2, mse = 44.0005, mad = 4.6561
2023-11-08 09:55:00,918 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 1.5806
2023-11-08 09:55:02,940 - __main__ - INFO - Fold 2, mse = 44.0750, mad = 4.6681
2023-11-08 09:55:03,283 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 1.5135
2023-11-08 09:55:04,997 - __main__ - INFO - Fold 2, mse = 44.2242, mad = 4.7643
2023-11-08 09:55:05,310 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 1.8263
2023-11-08 09:55:07,112 - __main__ - INFO - Fold 2, mse = 44.1481, mad = 4.6379
2023-11-08 09:55:07,408 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 1.7912
2023-11-08 09:55:09,197 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.9703 ------------
2023-11-08 09:55:09,451 - __main__ - INFO - Fold 2, mse = 43.9703, mad = 4.7609
2023-11-08 09:55:09,775 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 1.4184
2023-11-08 09:55:11,327 - __main__ - INFO - Fold 2, mse = 44.0364, mad = 4.7641
2023-11-08 09:55:11,562 - __main__ - INFO - Fold 2 Epoch 20 Batch 0: Train Loss = 1.5987
2023-11-08 09:55:13,396 - __main__ - INFO - Fold 2, epoch 20: Loss = 1.5846 Valid loss = 1.6382 MSE = 44.1697 AUROC = 0.9747
2023-11-08 09:55:13,400 - __main__ - INFO - Fold 2, mse = 44.1697, mad = 4.7101
2023-11-08 09:55:13,633 - __main__ - INFO - Fold 2 Epoch 21 Batch 0: Train Loss = 1.4975
2023-11-08 09:55:15,344 - __main__ - INFO - Fold 2, mse = 44.3561, mad = 4.8097
2023-11-08 09:55:15,614 - __main__ - INFO - Fold 2 Epoch 22 Batch 0: Train Loss = 1.6500
2023-11-08 09:55:17,436 - __main__ - INFO - Fold 2, mse = 44.4977, mad = 4.7757
2023-11-08 09:55:17,748 - __main__ - INFO - Fold 2 Epoch 23 Batch 0: Train Loss = 1.5638
2023-11-08 09:55:19,452 - __main__ - INFO - Fold 2, mse = 44.6264, mad = 4.7204
2023-11-08 09:55:19,711 - __main__ - INFO - Fold 2 Epoch 24 Batch 0: Train Loss = 1.5362
2023-11-08 09:55:21,473 - __main__ - INFO - Fold 2, mse = 44.4694, mad = 4.8491
2023-11-08 09:55:21,777 - __main__ - INFO - Fold 2 Epoch 25 Batch 0: Train Loss = 1.3735
2023-11-08 09:55:23,509 - __main__ - INFO - Fold 2, mse = 44.1920, mad = 4.7062
2023-11-08 09:55:23,931 - __main__ - INFO - Fold 2 Epoch 26 Batch 0: Train Loss = 1.3312
2023-11-08 09:55:25,781 - __main__ - INFO - Fold 2, mse = 44.1764, mad = 4.8147
2023-11-08 09:55:26,088 - __main__ - INFO - Fold 2 Epoch 27 Batch 0: Train Loss = 1.2094
2023-11-08 09:55:27,586 - __main__ - INFO - Fold 2, mse = 44.2068, mad = 4.8011
2023-11-08 09:55:27,812 - __main__ - INFO - Fold 2 Epoch 28 Batch 0: Train Loss = 1.4213
2023-11-08 09:55:29,349 - __main__ - INFO - Fold 2, mse = 44.2583, mad = 4.8066
2023-11-08 09:55:29,621 - __main__ - INFO - Fold 2 Epoch 29 Batch 0: Train Loss = 1.5488
2023-11-08 09:55:30,875 - __main__ - INFO - Fold 2, mse = 44.1047, mad = 4.7919
2023-11-08 09:55:31,145 - __main__ - INFO - Fold 2 Epoch 30 Batch 0: Train Loss = 1.1733
2023-11-08 09:55:32,871 - __main__ - INFO - Fold 2, epoch 30: Loss = 1.3516 Valid loss = 1.6215 MSE = 43.9941 AUROC = 0.9747
2023-11-08 09:55:32,875 - __main__ - INFO - Fold 2, mse = 43.9941, mad = 4.7984
2023-11-08 09:55:33,091 - __main__ - INFO - Fold 2 Epoch 31 Batch 0: Train Loss = 1.2234
2023-11-08 09:55:34,646 - __main__ - INFO - Fold 2, mse = 44.2591, mad = 4.8414
2023-11-08 09:55:35,001 - __main__ - INFO - Fold 2 Epoch 32 Batch 0: Train Loss = 1.3152
2023-11-08 09:55:36,694 - __main__ - INFO - Fold 2, mse = 44.2598, mad = 4.7724
2023-11-08 09:55:36,959 - __main__ - INFO - Fold 2 Epoch 33 Batch 0: Train Loss = 1.4056
2023-11-08 09:55:38,612 - __main__ - INFO - Fold 2, mse = 45.0338, mad = 4.9503
2023-11-08 09:55:38,923 - __main__ - INFO - Fold 2 Epoch 34 Batch 0: Train Loss = 1.4372
2023-11-08 09:55:40,733 - __main__ - INFO - Fold 2, mse = 44.6700, mad = 4.8006
2023-11-08 09:55:41,107 - __main__ - INFO - Fold 2 Epoch 35 Batch 0: Train Loss = 1.1132
2023-11-08 09:55:42,762 - __main__ - INFO - Fold 2, mse = 44.7904, mad = 4.9556
2023-11-08 09:55:43,075 - __main__ - INFO - Fold 2 Epoch 36 Batch 0: Train Loss = 1.4883
2023-11-08 09:55:44,993 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.8143 ------------
2023-11-08 09:55:45,174 - __main__ - INFO - Fold 2, mse = 43.8143, mad = 4.7434
2023-11-08 09:55:45,485 - __main__ - INFO - Fold 2 Epoch 37 Batch 0: Train Loss = 1.2529
2023-11-08 09:55:47,280 - __main__ - INFO - Fold 2, mse = 44.4703, mad = 4.8904
2023-11-08 09:55:47,543 - __main__ - INFO - Fold 2 Epoch 38 Batch 0: Train Loss = 1.2749
2023-11-08 09:55:48,950 - __main__ - INFO - Fold 2, mse = 44.1330, mad = 4.7942
2023-11-08 09:55:49,198 - __main__ - INFO - Fold 2 Epoch 39 Batch 0: Train Loss = 1.3215
2023-11-08 09:55:50,829 - __main__ - INFO - Fold 2, mse = 44.0457, mad = 4.8659
2023-11-08 09:55:51,193 - __main__ - INFO - Fold 2 Epoch 40 Batch 0: Train Loss = 1.3993
2023-11-08 09:55:52,888 - __main__ - INFO - Fold 2, epoch 40: Loss = 1.2052 Valid loss = 1.6365 MSE = 43.7442 AUROC = 0.9741
2023-11-08 09:55:52,890 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.7442 ------------
2023-11-08 09:55:53,082 - __main__ - INFO - Fold 2, mse = 43.7442, mad = 4.8428
2023-11-08 09:55:53,360 - __main__ - INFO - Fold 2 Epoch 41 Batch 0: Train Loss = 1.3374
2023-11-08 09:55:54,962 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.4811 ------------
2023-11-08 09:55:55,123 - __main__ - INFO - Fold 2, mse = 43.4811, mad = 4.7843
2023-11-08 09:55:55,529 - __main__ - INFO - Fold 2 Epoch 42 Batch 0: Train Loss = 1.2261
2023-11-08 09:55:57,117 - __main__ - INFO - Fold 2, mse = 43.6490, mad = 4.8535
2023-11-08 09:55:57,505 - __main__ - INFO - Fold 2 Epoch 43 Batch 0: Train Loss = 1.1852
2023-11-08 09:55:59,056 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.3992 ------------
2023-11-08 09:55:59,251 - __main__ - INFO - Fold 2, mse = 43.3992, mad = 4.8177
2023-11-08 09:55:59,505 - __main__ - INFO - Fold 2 Epoch 44 Batch 0: Train Loss = 1.1146
2023-11-08 09:56:01,000 - __main__ - INFO - Fold 2, mse = 43.5897, mad = 4.8705
2023-11-08 09:56:01,276 - __main__ - INFO - Fold 2 Epoch 45 Batch 0: Train Loss = 1.3479
2023-11-08 09:56:02,911 - __main__ - INFO - Fold 2, mse = 43.7231, mad = 4.8605
2023-11-08 09:56:03,163 - __main__ - INFO - Fold 2 Epoch 46 Batch 0: Train Loss = 1.0617
2023-11-08 09:56:04,740 - __main__ - INFO - Fold 2, mse = 43.6291, mad = 4.8218
2023-11-08 09:56:04,961 - __main__ - INFO - Fold 2 Epoch 47 Batch 0: Train Loss = 0.9937
2023-11-08 09:56:06,273 - __main__ - INFO - Fold 2, mse = 43.6342, mad = 4.8123
2023-11-08 09:56:06,572 - __main__ - INFO - Fold 2 Epoch 48 Batch 0: Train Loss = 1.1915
2023-11-08 09:56:08,462 - __main__ - INFO - Fold 2, mse = 44.2223, mad = 4.9277
2023-11-08 09:56:08,716 - __main__ - INFO - Fold 2 Epoch 49 Batch 0: Train Loss = 0.9717
2023-11-08 09:56:10,252 - __main__ - INFO - Fold 2, mse = 43.6941, mad = 4.7908
2023-11-08 09:56:10,521 - __main__ - INFO - Fold 2 Epoch 50 Batch 0: Train Loss = 1.0483
2023-11-08 09:56:12,089 - __main__ - INFO - Fold 2, epoch 50: Loss = 1.1768 Valid loss = 1.6791 MSE = 44.4342 AUROC = 0.9744
2023-11-08 09:56:12,091 - __main__ - INFO - Fold 2, mse = 44.4342, mad = 4.9512
2023-11-08 09:56:12,331 - __main__ - INFO - Fold 2 Epoch 51 Batch 0: Train Loss = 1.2059
2023-11-08 09:56:14,172 - __main__ - INFO - Fold 2, mse = 43.7943, mad = 4.8188
2023-11-08 09:56:14,441 - __main__ - INFO - Fold 2 Epoch 52 Batch 0: Train Loss = 1.1732
2023-11-08 09:56:16,211 - __main__ - INFO - Fold 2, mse = 44.2267, mad = 4.9231
2023-11-08 09:56:16,452 - __main__ - INFO - Fold 2 Epoch 53 Batch 0: Train Loss = 1.2067
2023-11-08 09:56:18,189 - __main__ - INFO - Fold 2, mse = 43.9377, mad = 4.8304
2023-11-08 09:56:18,483 - __main__ - INFO - Fold 2 Epoch 54 Batch 0: Train Loss = 1.2491
2023-11-08 09:56:19,915 - __main__ - INFO - Fold 2, mse = 44.2495, mad = 4.9555
2023-11-08 09:56:20,259 - __main__ - INFO - Fold 2 Epoch 55 Batch 0: Train Loss = 1.1337
2023-11-08 09:56:22,012 - __main__ - INFO - Fold 2, mse = 43.5300, mad = 4.8730
2023-11-08 09:56:22,230 - __main__ - INFO - Fold 2 Epoch 56 Batch 0: Train Loss = 1.1858
2023-11-08 09:56:24,105 - __main__ - INFO - Fold 2, mse = 43.6054, mad = 4.9353
2023-11-08 09:56:24,415 - __main__ - INFO - Fold 2 Epoch 57 Batch 0: Train Loss = 1.1025
2023-11-08 09:56:26,018 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.2812 ------------
2023-11-08 09:56:26,149 - __main__ - INFO - Fold 2, mse = 43.2812, mad = 4.8402
2023-11-08 09:56:26,372 - __main__ - INFO - Fold 2 Epoch 58 Batch 0: Train Loss = 1.0019
2023-11-08 09:56:28,118 - __main__ - INFO - Fold 2, mse = 43.8718, mad = 4.9158
2023-11-08 09:56:28,369 - __main__ - INFO - Fold 2 Epoch 59 Batch 0: Train Loss = 1.1064
2023-11-08 09:56:29,803 - __main__ - INFO - Fold 2, mse = 43.5620, mad = 4.8600
2023-11-08 09:56:30,151 - __main__ - INFO - Fold 2 Epoch 60 Batch 0: Train Loss = 1.1944
2023-11-08 09:56:31,892 - __main__ - INFO - Fold 2, epoch 60: Loss = 1.1235 Valid loss = 1.6383 MSE = 43.5206 AUROC = 0.9755
2023-11-08 09:56:31,896 - __main__ - INFO - Fold 2, mse = 43.5206, mad = 4.8550
2023-11-08 09:56:32,109 - __main__ - INFO - Fold 2 Epoch 61 Batch 0: Train Loss = 1.2014
2023-11-08 09:56:33,480 - __main__ - INFO - Fold 2, mse = 43.6397, mad = 4.9055
2023-11-08 09:56:33,714 - __main__ - INFO - Fold 2 Epoch 62 Batch 0: Train Loss = 1.1393
2023-11-08 09:56:35,159 - __main__ - INFO - Fold 2, mse = 43.4733, mad = 4.8716
2023-11-08 09:56:35,418 - __main__ - INFO - Fold 2 Epoch 63 Batch 0: Train Loss = 1.3282
2023-11-08 09:56:37,194 - __main__ - INFO - Fold 2, mse = 43.3951, mad = 4.8510
2023-11-08 09:56:37,464 - __main__ - INFO - Fold 2 Epoch 64 Batch 0: Train Loss = 1.1034
2023-11-08 09:56:39,175 - __main__ - INFO - Fold 2, mse = 43.4335, mad = 4.8523
2023-11-08 09:56:39,432 - __main__ - INFO - Fold 2 Epoch 65 Batch 0: Train Loss = 1.4003
2023-11-08 09:56:41,424 - __main__ - INFO - Fold 2, mse = 43.7716, mad = 4.8770
2023-11-08 09:56:41,662 - __main__ - INFO - Fold 2 Epoch 66 Batch 0: Train Loss = 1.0096
2023-11-08 09:56:43,383 - __main__ - INFO - Fold 2, mse = 43.4934, mad = 4.7960
2023-11-08 09:56:43,641 - __main__ - INFO - Fold 2 Epoch 67 Batch 0: Train Loss = 1.1001
2023-11-08 09:56:45,525 - __main__ - INFO - Fold 2, mse = 43.9797, mad = 4.8924
2023-11-08 09:56:45,825 - __main__ - INFO - Fold 2 Epoch 68 Batch 0: Train Loss = 1.1244
2023-11-08 09:56:47,283 - __main__ - INFO - Fold 2, mse = 43.9735, mad = 4.8915
2023-11-08 09:56:47,579 - __main__ - INFO - Fold 2 Epoch 69 Batch 0: Train Loss = 1.0480
2023-11-08 09:56:49,401 - __main__ - INFO - Fold 2, mse = 43.5777, mad = 4.8381
2023-11-08 09:56:49,703 - __main__ - INFO - Fold 2 Epoch 70 Batch 0: Train Loss = 0.8920
2023-11-08 09:56:51,511 - __main__ - INFO - Fold 2, epoch 70: Loss = 1.0767 Valid loss = 1.6485 MSE = 43.7155 AUROC = 0.9754
2023-11-08 09:56:51,513 - __main__ - INFO - Fold 2, mse = 43.7155, mad = 4.8928
2023-11-08 09:56:51,800 - __main__ - INFO - Fold 2 Epoch 71 Batch 0: Train Loss = 1.0730
2023-11-08 09:56:53,261 - __main__ - INFO - Fold 2, mse = 43.5806, mad = 4.8450
2023-11-08 09:56:53,490 - __main__ - INFO - Fold 2 Epoch 72 Batch 0: Train Loss = 1.0374
2023-11-08 09:56:55,543 - __main__ - INFO - Fold 2, mse = 43.7221, mad = 4.8344
2023-11-08 09:56:55,863 - __main__ - INFO - Fold 2 Epoch 73 Batch 0: Train Loss = 1.0947
2023-11-08 09:56:57,544 - __main__ - INFO - Fold 2, mse = 43.9799, mad = 4.9014
2023-11-08 09:56:57,806 - __main__ - INFO - Fold 2 Epoch 74 Batch 0: Train Loss = 0.9747
2023-11-08 09:56:59,392 - __main__ - INFO - Fold 2, mse = 43.7751, mad = 4.8735
2023-11-08 09:56:59,681 - __main__ - INFO - Fold 2 Epoch 75 Batch 0: Train Loss = 1.0744
2023-11-08 09:57:01,315 - __main__ - INFO - Fold 2, mse = 43.8594, mad = 4.8910
2023-11-08 09:57:01,619 - __main__ - INFO - Fold 2 Epoch 76 Batch 0: Train Loss = 1.0308
2023-11-08 09:57:03,381 - __main__ - INFO - Fold 2, mse = 44.1959, mad = 4.9415
2023-11-08 09:57:03,661 - __main__ - INFO - Fold 2 Epoch 77 Batch 0: Train Loss = 1.0677
2023-11-08 09:57:05,530 - __main__ - INFO - Fold 2, mse = 43.6082, mad = 4.8109
2023-11-08 09:57:05,786 - __main__ - INFO - Fold 2 Epoch 78 Batch 0: Train Loss = 0.9938
2023-11-08 09:57:07,601 - __main__ - INFO - Fold 2, mse = 43.8061, mad = 4.8889
2023-11-08 09:57:07,885 - __main__ - INFO - Fold 2 Epoch 79 Batch 0: Train Loss = 1.2062
2023-11-08 09:57:09,311 - __main__ - INFO - Fold 2, mse = 43.7250, mad = 4.8475
2023-11-08 09:57:09,568 - __main__ - INFO - Fold 2 Epoch 80 Batch 0: Train Loss = 1.1193
2023-11-08 09:57:11,304 - __main__ - INFO - Fold 2, epoch 80: Loss = 1.0828 Valid loss = 1.6755 MSE = 44.1412 AUROC = 0.9735
2023-11-08 09:57:11,308 - __main__ - INFO - Fold 2, mse = 44.1412, mad = 4.9415
2023-11-08 09:57:11,687 - __main__ - INFO - Fold 2 Epoch 81 Batch 0: Train Loss = 0.9807
2023-11-08 09:57:13,377 - __main__ - INFO - Fold 2, mse = 43.4425, mad = 4.7768
2023-11-08 09:57:13,622 - __main__ - INFO - Fold 2 Epoch 82 Batch 0: Train Loss = 1.1501
2023-11-08 09:57:15,535 - __main__ - INFO - Fold 2, mse = 44.0879, mad = 4.9630
2023-11-08 09:57:15,871 - __main__ - INFO - Fold 2 Epoch 83 Batch 0: Train Loss = 0.9982
2023-11-08 09:57:17,729 - __main__ - INFO - Fold 2, mse = 43.8172, mad = 4.8436
2023-11-08 09:57:18,088 - __main__ - INFO - Fold 2 Epoch 84 Batch 0: Train Loss = 0.9884
2023-11-08 09:57:19,785 - __main__ - INFO - Fold 2, mse = 44.1744, mad = 4.8591
2023-11-08 09:57:20,080 - __main__ - INFO - Fold 2 Epoch 85 Batch 0: Train Loss = 1.0434
2023-11-08 09:57:21,843 - __main__ - INFO - Fold 2, mse = 44.0381, mad = 4.8599
2023-11-08 09:57:22,137 - __main__ - INFO - Fold 2 Epoch 86 Batch 0: Train Loss = 1.0257
2023-11-08 09:57:23,704 - __main__ - INFO - Fold 2, mse = 43.7770, mad = 4.8361
2023-11-08 09:57:23,960 - __main__ - INFO - Fold 2 Epoch 87 Batch 0: Train Loss = 0.9540
2023-11-08 09:57:25,656 - __main__ - INFO - Fold 2, mse = 44.0294, mad = 4.8374
2023-11-08 09:57:25,934 - __main__ - INFO - Fold 2 Epoch 88 Batch 0: Train Loss = 1.1156
2023-11-08 09:57:27,426 - __main__ - INFO - Fold 2, mse = 44.0020, mad = 4.8007
2023-11-08 09:57:27,733 - __main__ - INFO - Fold 2 Epoch 89 Batch 0: Train Loss = 1.0796
2023-11-08 09:57:29,568 - __main__ - INFO - Fold 2, mse = 43.9484, mad = 4.8024
2023-11-08 09:57:29,902 - __main__ - INFO - Fold 2 Epoch 90 Batch 0: Train Loss = 0.9302
2023-11-08 09:57:31,729 - __main__ - INFO - Fold 2, epoch 90: Loss = 1.0010 Valid loss = 1.6485 MSE = 44.1510 AUROC = 0.9757
2023-11-08 09:57:31,731 - __main__ - INFO - Fold 2, mse = 44.1510, mad = 4.8906
2023-11-08 09:57:32,039 - __main__ - INFO - Fold 2 Epoch 91 Batch 0: Train Loss = 1.1337
2023-11-08 09:57:33,629 - __main__ - INFO - Fold 2, mse = 43.7752, mad = 4.8116
2023-11-08 09:57:33,870 - __main__ - INFO - Fold 2 Epoch 92 Batch 0: Train Loss = 1.1503
2023-11-08 09:57:35,534 - __main__ - INFO - Fold 2, mse = 44.1840, mad = 4.9087
2023-11-08 09:57:35,819 - __main__ - INFO - Fold 2 Epoch 93 Batch 0: Train Loss = 0.8600
2023-11-08 09:57:37,689 - __main__ - INFO - Fold 2, mse = 43.8729, mad = 4.8071
2023-11-08 09:57:37,981 - __main__ - INFO - Fold 2 Epoch 94 Batch 0: Train Loss = 0.9699
2023-11-08 09:57:39,762 - __main__ - INFO - Fold 2, mse = 43.9624, mad = 4.8978
2023-11-08 09:57:39,976 - __main__ - INFO - Fold 2 Epoch 95 Batch 0: Train Loss = 0.8623
2023-11-08 09:57:41,724 - __main__ - INFO - Fold 2, mse = 44.2193, mad = 4.9393
2023-11-08 09:57:41,961 - __main__ - INFO - Fold 2 Epoch 96 Batch 0: Train Loss = 0.9827
2023-11-08 09:57:43,736 - __main__ - INFO - Fold 2, mse = 44.0884, mad = 4.8851
2023-11-08 09:57:44,000 - __main__ - INFO - Fold 2 Epoch 97 Batch 0: Train Loss = 1.0466
2023-11-08 09:57:45,592 - __main__ - INFO - Fold 2, mse = 44.4621, mad = 4.9627
2023-11-08 09:57:45,820 - __main__ - INFO - Fold 2 Epoch 98 Batch 0: Train Loss = 1.1394
2023-11-08 09:57:47,554 - __main__ - INFO - Fold 2, mse = 43.9826, mad = 4.8844
2023-11-08 09:57:47,783 - __main__ - INFO - Fold 2 Epoch 99 Batch 0: Train Loss = 0.8528
2023-11-08 09:57:49,161 - __main__ - INFO - Fold 2, mse = 44.2856, mad = 4.9307
2023-11-08 09:57:49,384 - __main__ - INFO - Fold 2 Epoch 100 Batch 0: Train Loss = 0.9272
2023-11-08 09:57:51,041 - __main__ - INFO - Fold 2, epoch 100: Loss = 0.9476 Valid loss = 1.6704 MSE = 44.3839 AUROC = 0.9748
2023-11-08 09:57:51,043 - __main__ - INFO - Fold 2, mse = 44.3839, mad = 4.9239
2023-11-08 09:57:51,375 - __main__ - INFO - Fold 2 Epoch 101 Batch 0: Train Loss = 0.8631
2023-11-08 09:57:53,107 - __main__ - INFO - Fold 2, mse = 44.3860, mad = 4.9241
2023-11-08 09:57:53,388 - __main__ - INFO - Fold 2 Epoch 102 Batch 0: Train Loss = 0.9360
2023-11-08 09:57:55,218 - __main__ - INFO - Fold 2, mse = 44.7301, mad = 4.9810
2023-11-08 09:57:55,448 - __main__ - INFO - Fold 2 Epoch 103 Batch 0: Train Loss = 0.9066
2023-11-08 09:57:57,318 - __main__ - INFO - Fold 2, mse = 44.5636, mad = 4.9310
2023-11-08 09:57:57,646 - __main__ - INFO - Fold 2 Epoch 104 Batch 0: Train Loss = 0.8927
2023-11-08 09:57:59,424 - __main__ - INFO - Fold 2, mse = 44.9406, mad = 5.0164
2023-11-08 09:57:59,803 - __main__ - INFO - Fold 2 Epoch 105 Batch 0: Train Loss = 0.8513
2023-11-08 09:58:01,690 - __main__ - INFO - Fold 2, mse = 44.4854, mad = 4.8620
2023-11-08 09:58:02,102 - __main__ - INFO - Fold 2 Epoch 106 Batch 0: Train Loss = 1.0167
2023-11-08 09:58:03,613 - __main__ - INFO - Fold 2, mse = 44.5813, mad = 4.8951
2023-11-08 09:58:03,925 - __main__ - INFO - Fold 2 Epoch 107 Batch 0: Train Loss = 0.8551
2023-11-08 09:58:05,492 - __main__ - INFO - Fold 2, mse = 44.4459, mad = 4.9102
2023-11-08 09:58:05,795 - __main__ - INFO - Fold 2 Epoch 108 Batch 0: Train Loss = 0.9639
2023-11-08 09:58:07,383 - __main__ - INFO - Fold 2, mse = 44.0902, mad = 4.8672
2023-11-08 09:58:07,744 - __main__ - INFO - Fold 2 Epoch 109 Batch 0: Train Loss = 1.1950
2023-11-08 09:58:09,343 - __main__ - INFO - Fold 2, mse = 44.3083, mad = 4.9231
2023-11-08 09:58:09,689 - __main__ - INFO - Fold 2 Epoch 110 Batch 0: Train Loss = 0.9411
2023-11-08 09:58:11,434 - __main__ - INFO - Fold 2, epoch 110: Loss = 0.9287 Valid loss = 1.6664 MSE = 44.3153 AUROC = 0.9754
2023-11-08 09:58:11,436 - __main__ - INFO - Fold 2, mse = 44.3153, mad = 4.8530
2023-11-08 09:58:11,703 - __main__ - INFO - Fold 2 Epoch 111 Batch 0: Train Loss = 1.2734
2023-11-08 09:58:13,360 - __main__ - INFO - Fold 2, mse = 44.7604, mad = 4.9497
2023-11-08 09:58:13,671 - __main__ - INFO - Fold 2 Epoch 112 Batch 0: Train Loss = 0.9117
2023-11-08 09:58:15,212 - __main__ - INFO - Fold 2, mse = 44.7807, mad = 4.9380
2023-11-08 09:58:15,483 - __main__ - INFO - Fold 2 Epoch 113 Batch 0: Train Loss = 0.9882
2023-11-08 09:58:17,368 - __main__ - INFO - Fold 2, mse = 44.6671, mad = 4.8906
2023-11-08 09:58:17,623 - __main__ - INFO - Fold 2 Epoch 114 Batch 0: Train Loss = 0.9878
2023-11-08 09:58:19,275 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9760 ------------
2023-11-08 09:58:19,277 - __main__ - INFO - Fold 2, mse = 45.1433, mad = 5.0126
2023-11-08 09:58:19,612 - __main__ - INFO - Fold 2 Epoch 115 Batch 0: Train Loss = 0.8377
2023-11-08 09:58:21,213 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9766 ------------
2023-11-08 09:58:21,215 - __main__ - INFO - Fold 2, mse = 44.0707, mad = 4.8196
2023-11-08 09:58:21,466 - __main__ - INFO - Fold 2 Epoch 116 Batch 0: Train Loss = 0.9518
2023-11-08 09:58:23,072 - __main__ - INFO - Fold 2, mse = 44.7762, mad = 5.0016
2023-11-08 09:58:23,365 - __main__ - INFO - Fold 2 Epoch 117 Batch 0: Train Loss = 0.8810
2023-11-08 09:58:24,925 - __main__ - INFO - Fold 2, mse = 44.3789, mad = 4.8528
2023-11-08 09:58:25,200 - __main__ - INFO - Fold 2 Epoch 118 Batch 0: Train Loss = 0.8347
2023-11-08 09:58:27,214 - __main__ - INFO - Fold 2, mse = 45.2513, mad = 4.9984
2023-11-08 09:58:27,537 - __main__ - INFO - Fold 2 Epoch 119 Batch 0: Train Loss = 0.8748
2023-11-08 09:58:29,183 - __main__ - INFO - Fold 2, mse = 45.2241, mad = 4.9578
2023-11-08 09:58:29,570 - __main__ - INFO - Fold 2 Epoch 120 Batch 0: Train Loss = 1.1073
2023-11-08 09:58:30,990 - __main__ - INFO - Fold 2, epoch 120: Loss = 0.9067 Valid loss = 1.7103 MSE = 45.3240 AUROC = 0.9753
2023-11-08 09:58:30,993 - __main__ - INFO - Fold 2, mse = 45.3240, mad = 5.0141
2023-11-08 09:58:31,231 - __main__ - INFO - Fold 2 Epoch 121 Batch 0: Train Loss = 0.9924
2023-11-08 09:58:33,031 - __main__ - INFO - Fold 2, mse = 45.0696, mad = 4.9559
2023-11-08 09:58:33,356 - __main__ - INFO - Fold 2 Epoch 122 Batch 0: Train Loss = 1.3922
2023-11-08 09:58:34,853 - __main__ - INFO - Fold 2, mse = 44.7089, mad = 4.8802
2023-11-08 09:58:35,153 - __main__ - INFO - Fold 2 Epoch 123 Batch 0: Train Loss = 0.8846
2023-11-08 09:58:36,901 - __main__ - INFO - Fold 2, mse = 44.9474, mad = 4.9966
2023-11-08 09:58:37,159 - __main__ - INFO - Fold 2 Epoch 124 Batch 0: Train Loss = 0.9932
2023-11-08 09:58:38,795 - __main__ - INFO - Fold 2, mse = 44.1431, mad = 4.8717
2023-11-08 09:58:39,076 - __main__ - INFO - Fold 2 Epoch 125 Batch 0: Train Loss = 0.9834
2023-11-08 09:58:40,814 - __main__ - INFO - Fold 2, mse = 44.5032, mad = 4.9775
2023-11-08 09:58:41,063 - __main__ - INFO - Fold 2 Epoch 126 Batch 0: Train Loss = 0.9989
2023-11-08 09:58:42,423 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9766 ------------
2023-11-08 09:58:42,425 - __main__ - INFO - Fold 2, mse = 44.3170, mad = 4.8922
2023-11-08 09:58:42,692 - __main__ - INFO - Fold 2 Epoch 127 Batch 0: Train Loss = 0.9979
2023-11-08 09:58:44,329 - __main__ - INFO - Fold 2, mse = 44.8280, mad = 4.9890
2023-11-08 09:58:44,662 - __main__ - INFO - Fold 2 Epoch 128 Batch 0: Train Loss = 0.8443
2023-11-08 09:58:46,136 - __main__ - INFO - Fold 2, mse = 44.0862, mad = 4.8219
2023-11-08 09:58:46,579 - __main__ - INFO - Fold 2 Epoch 129 Batch 0: Train Loss = 0.9652
2023-11-08 09:58:48,192 - __main__ - INFO - Fold 2, mse = 44.4207, mad = 4.9216
2023-11-08 09:58:48,397 - __main__ - INFO - Fold 2 Epoch 130 Batch 0: Train Loss = 0.8036
2023-11-08 09:58:50,078 - __main__ - INFO - Fold 2, epoch 130: Loss = 0.8610 Valid loss = 1.6787 MSE = 44.3134 AUROC = 0.9760
2023-11-08 09:58:50,080 - __main__ - INFO - Fold 2, mse = 44.3134, mad = 4.8816
2023-11-08 09:58:50,358 - __main__ - INFO - Fold 2 Epoch 131 Batch 0: Train Loss = 1.0906
2023-11-08 09:58:52,246 - __main__ - INFO - Fold 2, mse = 44.2535, mad = 4.8693
2023-11-08 09:58:52,549 - __main__ - INFO - Fold 2 Epoch 132 Batch 0: Train Loss = 1.0296
2023-11-08 09:58:54,323 - __main__ - INFO - Fold 2, mse = 44.4542, mad = 4.9046
2023-11-08 09:58:54,580 - __main__ - INFO - Fold 2 Epoch 133 Batch 0: Train Loss = 0.8094
2023-11-08 09:58:56,232 - __main__ - INFO - Fold 2, mse = 44.6917, mad = 4.9414
2023-11-08 09:58:56,564 - __main__ - INFO - Fold 2 Epoch 134 Batch 0: Train Loss = 1.0409
2023-11-08 09:58:58,506 - __main__ - INFO - Fold 2, mse = 44.4416, mad = 4.8664
2023-11-08 09:58:58,823 - __main__ - INFO - Fold 2 Epoch 135 Batch 0: Train Loss = 0.8904
2023-11-08 09:59:00,697 - __main__ - INFO - Fold 2, mse = 44.6769, mad = 4.9139
2023-11-08 09:59:01,025 - __main__ - INFO - Fold 2 Epoch 136 Batch 0: Train Loss = 0.9492
2023-11-08 09:59:02,966 - __main__ - INFO - Fold 2, mse = 44.3490, mad = 4.8574
2023-11-08 09:59:03,308 - __main__ - INFO - Fold 2 Epoch 137 Batch 0: Train Loss = 0.9819
2023-11-08 09:59:05,325 - __main__ - INFO - Fold 2, mse = 44.2133, mad = 4.8871
2023-11-08 09:59:05,672 - __main__ - INFO - Fold 2 Epoch 138 Batch 0: Train Loss = 1.0665
2023-11-08 09:59:07,415 - __main__ - INFO - Fold 2, mse = 43.8233, mad = 4.8345
2023-11-08 09:59:07,746 - __main__ - INFO - Fold 2 Epoch 139 Batch 0: Train Loss = 0.9115
2023-11-08 09:59:09,859 - __main__ - INFO - Fold 2, mse = 44.0038, mad = 4.8951
2023-11-08 09:59:10,300 - __main__ - INFO - Fold 2 Epoch 140 Batch 0: Train Loss = 0.8144
2023-11-08 09:59:12,517 - __main__ - INFO - Fold 2, epoch 140: Loss = 0.9369 Valid loss = 1.6215 MSE = 43.4512 AUROC = 0.9764
2023-11-08 09:59:12,519 - __main__ - INFO - Fold 2, mse = 43.4512, mad = 4.8026
2023-11-08 09:59:12,951 - __main__ - INFO - Fold 2 Epoch 141 Batch 0: Train Loss = 1.0038
2023-11-08 09:59:14,936 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9768 ------------
2023-11-08 09:59:14,940 - __main__ - INFO - Fold 2, mse = 44.1146, mad = 4.8970
2023-11-08 09:59:15,273 - __main__ - INFO - Fold 2 Epoch 142 Batch 0: Train Loss = 0.8624
2023-11-08 09:59:17,164 - __main__ - INFO - Fold 2, mse = 43.9399, mad = 4.8143
2023-11-08 09:59:17,461 - __main__ - INFO - Fold 2 Epoch 143 Batch 0: Train Loss = 0.7774
2023-11-08 09:59:19,544 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9768 ------------
2023-11-08 09:59:19,548 - __main__ - INFO - Fold 2, mse = 44.1434, mad = 4.8806
2023-11-08 09:59:19,845 - __main__ - INFO - Fold 2 Epoch 144 Batch 0: Train Loss = 0.8681
2023-11-08 09:59:21,559 - __main__ - INFO - Fold 2, mse = 43.9904, mad = 4.8567
2023-11-08 09:59:21,880 - __main__ - INFO - Fold 2 Epoch 145 Batch 0: Train Loss = 0.8832
2023-11-08 09:59:23,614 - __main__ - INFO - Fold 2, mse = 44.3559, mad = 4.8935
2023-11-08 09:59:23,892 - __main__ - INFO - Fold 2 Epoch 146 Batch 0: Train Loss = 0.8535
2023-11-08 09:59:26,029 - __main__ - INFO - Fold 2, mse = 44.0661, mad = 4.8397
2023-11-08 09:59:26,356 - __main__ - INFO - Fold 2 Epoch 147 Batch 0: Train Loss = 0.9533
2023-11-08 09:59:28,120 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9773 ------------
2023-11-08 09:59:28,122 - __main__ - INFO - Fold 2, mse = 43.9548, mad = 4.8606
2023-11-08 09:59:28,403 - __main__ - INFO - Fold 2 Epoch 148 Batch 0: Train Loss = 0.7604
2023-11-08 09:59:29,758 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9781 ------------
2023-11-08 09:59:29,760 - __main__ - INFO - Fold 2, mse = 43.7313, mad = 4.8370
2023-11-08 09:59:29,974 - __main__ - INFO - Fold 2 Epoch 149 Batch 0: Train Loss = 1.0405
2023-11-08 09:59:31,756 - __main__ - INFO - Fold 2, mse = 43.8832, mad = 4.8426
2023-11-08 09:59:32,329 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 2.6624
2023-11-08 09:59:34,200 - __main__ - INFO - Fold 3, epoch 0: Loss = 2.5226 Valid loss = 2.3315 MSE = 45.2189 AUROC = 0.6402
2023-11-08 09:59:34,202 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.2189 ------------
2023-11-08 09:59:34,372 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.6402 ------------
2023-11-08 09:59:34,374 - __main__ - INFO - Fold 3, mse = 45.2189, mad = 5.0600
2023-11-08 09:59:34,672 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 2.6602
2023-11-08 09:59:36,448 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.0270 ------------
2023-11-08 09:59:36,640 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.6881 ------------
2023-11-08 09:59:36,644 - __main__ - INFO - Fold 3, mse = 43.0270, mad = 4.8855
2023-11-08 09:59:37,035 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 2.3188
2023-11-08 09:59:38,950 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 41.3003 ------------
2023-11-08 09:59:39,124 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.7103 ------------
2023-11-08 09:59:39,129 - __main__ - INFO - Fold 3, mse = 41.3003, mad = 4.7704
2023-11-08 09:59:39,475 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 2.3440
2023-11-08 09:59:40,739 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 39.9041 ------------
2023-11-08 09:59:40,898 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.7377 ------------
2023-11-08 09:59:40,903 - __main__ - INFO - Fold 3, mse = 39.9041, mad = 4.7668
2023-11-08 09:59:41,122 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 2.0281
2023-11-08 09:59:43,021 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 39.0630 ------------
2023-11-08 09:59:43,192 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.7662 ------------
2023-11-08 09:59:43,196 - __main__ - INFO - Fold 3, mse = 39.0630, mad = 4.6876
2023-11-08 09:59:43,471 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 2.1912
2023-11-08 09:59:45,051 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 38.5864 ------------
2023-11-08 09:59:45,211 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8018 ------------
2023-11-08 09:59:45,214 - __main__ - INFO - Fold 3, mse = 38.5864, mad = 4.6319
2023-11-08 09:59:45,459 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 2.1683
2023-11-08 09:59:47,008 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 38.2083 ------------
2023-11-08 09:59:47,195 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8315 ------------
2023-11-08 09:59:47,197 - __main__ - INFO - Fold 3, mse = 38.2083, mad = 4.6108
2023-11-08 09:59:47,467 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 2.0114
2023-11-08 09:59:49,336 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.6875 ------------
2023-11-08 09:59:49,493 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8562 ------------
2023-11-08 09:59:49,495 - __main__ - INFO - Fold 3, mse = 37.6875, mad = 4.6570
2023-11-08 09:59:49,743 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 1.9770
2023-11-08 09:59:51,741 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.5724 ------------
2023-11-08 09:59:51,895 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8830 ------------
2023-11-08 09:59:51,897 - __main__ - INFO - Fold 3, mse = 37.5724, mad = 4.6276
2023-11-08 09:59:52,154 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 1.9761
2023-11-08 09:59:53,997 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.5561 ------------
2023-11-08 09:59:54,167 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9022 ------------
2023-11-08 09:59:54,172 - __main__ - INFO - Fold 3, mse = 37.5561, mad = 4.6041
2023-11-08 09:59:54,417 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 2.0358
2023-11-08 09:59:55,952 - __main__ - INFO - Fold 3, epoch 10: Loss = 1.9250 Valid loss = 1.9008 MSE = 37.2288 AUROC = 0.9178
2023-11-08 09:59:55,956 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.2288 ------------
2023-11-08 09:59:56,123 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9178 ------------
2023-11-08 09:59:56,127 - __main__ - INFO - Fold 3, mse = 37.2288, mad = 4.6103
2023-11-08 09:59:56,356 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 1.9653
2023-11-08 09:59:58,433 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.0397 ------------
2023-11-08 09:59:58,589 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9292 ------------
2023-11-08 09:59:58,592 - __main__ - INFO - Fold 3, mse = 37.0397, mad = 4.6013
2023-11-08 09:59:58,850 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 1.8894
2023-11-08 10:00:00,534 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.0206 ------------
2023-11-08 10:00:00,696 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9396 ------------
2023-11-08 10:00:00,698 - __main__ - INFO - Fold 3, mse = 37.0206, mad = 4.5736
2023-11-08 10:00:01,015 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 1.8333
2023-11-08 10:00:02,625 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.7740 ------------
2023-11-08 10:00:02,808 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9467 ------------
2023-11-08 10:00:02,810 - __main__ - INFO - Fold 3, mse = 36.7740, mad = 4.5732
2023-11-08 10:00:03,091 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 1.6410
2023-11-08 10:00:05,060 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.5263 ------------
2023-11-08 10:00:05,205 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9517 ------------
2023-11-08 10:00:05,208 - __main__ - INFO - Fold 3, mse = 36.5263, mad = 4.5934
2023-11-08 10:00:05,562 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 1.7822
2023-11-08 10:00:07,074 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9550 ------------
2023-11-08 10:00:07,078 - __main__ - INFO - Fold 3, mse = 36.5940, mad = 4.5450
2023-11-08 10:00:07,351 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 1.6233
2023-11-08 10:00:09,095 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.3875 ------------
2023-11-08 10:00:09,261 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9577 ------------
2023-11-08 10:00:09,264 - __main__ - INFO - Fold 3, mse = 36.3875, mad = 4.5265
2023-11-08 10:00:09,524 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 1.9224
2023-11-08 10:00:11,331 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.0699 ------------
2023-11-08 10:00:11,490 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9589 ------------
2023-11-08 10:00:11,494 - __main__ - INFO - Fold 3, mse = 36.0699, mad = 4.5196
2023-11-08 10:00:11,785 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 1.8466
2023-11-08 10:00:13,338 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9602 ------------
2023-11-08 10:00:13,340 - __main__ - INFO - Fold 3, mse = 36.1262, mad = 4.4978
2023-11-08 10:00:13,576 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 1.7055
2023-11-08 10:00:14,902 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.7491 ------------
2023-11-08 10:00:15,097 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9616 ------------
2023-11-08 10:00:15,099 - __main__ - INFO - Fold 3, mse = 35.7491, mad = 4.5113
2023-11-08 10:00:15,387 - __main__ - INFO - Fold 3 Epoch 20 Batch 0: Train Loss = 1.6665
2023-11-08 10:00:16,839 - __main__ - INFO - Fold 3, epoch 20: Loss = 1.6341 Valid loss = 1.7037 MSE = 36.3336 AUROC = 0.9625
2023-11-08 10:00:16,842 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9625 ------------
2023-11-08 10:00:16,843 - __main__ - INFO - Fold 3, mse = 36.3336, mad = 4.4937
2023-11-08 10:00:17,088 - __main__ - INFO - Fold 3 Epoch 21 Batch 0: Train Loss = 1.6115
2023-11-08 10:00:18,679 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9635 ------------
2023-11-08 10:00:18,684 - __main__ - INFO - Fold 3, mse = 35.7762, mad = 4.5180
2023-11-08 10:00:18,940 - __main__ - INFO - Fold 3 Epoch 22 Batch 0: Train Loss = 1.7764
2023-11-08 10:00:20,571 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9645 ------------
2023-11-08 10:00:20,574 - __main__ - INFO - Fold 3, mse = 35.7531, mad = 4.5065
2023-11-08 10:00:20,851 - __main__ - INFO - Fold 3 Epoch 23 Batch 0: Train Loss = 1.7108
2023-11-08 10:00:22,467 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9650 ------------
2023-11-08 10:00:22,470 - __main__ - INFO - Fold 3, mse = 35.9769, mad = 4.4838
2023-11-08 10:00:22,706 - __main__ - INFO - Fold 3 Epoch 24 Batch 0: Train Loss = 1.7039
2023-11-08 10:00:24,458 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.4192 ------------
2023-11-08 10:00:24,700 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9658 ------------
2023-11-08 10:00:24,703 - __main__ - INFO - Fold 3, mse = 35.4192, mad = 4.4872
2023-11-08 10:00:25,090 - __main__ - INFO - Fold 3 Epoch 25 Batch 0: Train Loss = 1.7329
2023-11-08 10:00:26,931 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9660 ------------
2023-11-08 10:00:26,933 - __main__ - INFO - Fold 3, mse = 35.6790, mad = 4.4787
2023-11-08 10:00:27,309 - __main__ - INFO - Fold 3 Epoch 26 Batch 0: Train Loss = 1.5992
2023-11-08 10:00:29,209 - __main__ - INFO - Fold 3, mse = 35.9189, mad = 4.4881
2023-11-08 10:00:29,561 - __main__ - INFO - Fold 3 Epoch 27 Batch 0: Train Loss = 1.5214
2023-11-08 10:00:31,156 - __main__ - INFO - Fold 3, mse = 35.8693, mad = 4.4815
2023-11-08 10:00:31,523 - __main__ - INFO - Fold 3 Epoch 28 Batch 0: Train Loss = 1.4733
2023-11-08 10:00:33,245 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.3942 ------------
2023-11-08 10:00:33,412 - __main__ - INFO - Fold 3, mse = 35.3942, mad = 4.4464
2023-11-08 10:00:33,648 - __main__ - INFO - Fold 3 Epoch 29 Batch 0: Train Loss = 1.3642
2023-11-08 10:00:35,466 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.8341 ------------
2023-11-08 10:00:35,650 - __main__ - INFO - Fold 3, mse = 34.8341, mad = 4.4477
2023-11-08 10:00:36,013 - __main__ - INFO - Fold 3 Epoch 30 Batch 0: Train Loss = 1.7488
2023-11-08 10:00:37,761 - __main__ - INFO - Fold 3, epoch 30: Loss = 1.4769 Valid loss = 1.6515 MSE = 35.9423 AUROC = 0.9649
2023-11-08 10:00:37,765 - __main__ - INFO - Fold 3, mse = 35.9423, mad = 4.4445
2023-11-08 10:00:38,035 - __main__ - INFO - Fold 3 Epoch 31 Batch 0: Train Loss = 1.4167
2023-11-08 10:00:39,769 - __main__ - INFO - Fold 3, mse = 34.9118, mad = 4.4251
2023-11-08 10:00:40,035 - __main__ - INFO - Fold 3 Epoch 32 Batch 0: Train Loss = 1.2685
2023-11-08 10:00:41,907 - __main__ - INFO - Fold 3, mse = 35.1728, mad = 4.4076
2023-11-08 10:00:42,182 - __main__ - INFO - Fold 3 Epoch 33 Batch 0: Train Loss = 1.1237
2023-11-08 10:00:43,886 - __main__ - INFO - Fold 3, mse = 35.2907, mad = 4.4035
2023-11-08 10:00:44,213 - __main__ - INFO - Fold 3 Epoch 34 Batch 0: Train Loss = 1.3034
2023-11-08 10:00:45,794 - __main__ - INFO - Fold 3, mse = 34.8679, mad = 4.3941
2023-11-08 10:00:46,064 - __main__ - INFO - Fold 3 Epoch 35 Batch 0: Train Loss = 1.4598
2023-11-08 10:00:47,776 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9671 ------------
2023-11-08 10:00:47,778 - __main__ - INFO - Fold 3, mse = 35.3244, mad = 4.3798
2023-11-08 10:00:48,157 - __main__ - INFO - Fold 3 Epoch 36 Batch 0: Train Loss = 1.2860
2023-11-08 10:00:49,885 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.6427 ------------
2023-11-08 10:00:50,097 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9677 ------------
2023-11-08 10:00:50,100 - __main__ - INFO - Fold 3, mse = 34.6427, mad = 4.3547
2023-11-08 10:00:50,490 - __main__ - INFO - Fold 3 Epoch 37 Batch 0: Train Loss = 1.3573
2023-11-08 10:00:52,357 - __main__ - INFO - Fold 3, mse = 34.8218, mad = 4.3418
2023-11-08 10:00:52,745 - __main__ - INFO - Fold 3 Epoch 38 Batch 0: Train Loss = 1.5265
2023-11-08 10:00:54,435 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.4739 ------------
2023-11-08 10:00:54,620 - __main__ - INFO - Fold 3, mse = 34.4739, mad = 4.3445
2023-11-08 10:00:55,032 - __main__ - INFO - Fold 3 Epoch 39 Batch 0: Train Loss = 1.3643
2023-11-08 10:00:56,654 - __main__ - INFO - Fold 3, mse = 34.8691, mad = 4.3465
2023-11-08 10:00:56,905 - __main__ - INFO - Fold 3 Epoch 40 Batch 0: Train Loss = 1.6024
2023-11-08 10:00:58,754 - __main__ - INFO - Fold 3, epoch 40: Loss = 1.3921 Valid loss = 1.5944 MSE = 34.9596 AUROC = 0.9650
2023-11-08 10:00:58,758 - __main__ - INFO - Fold 3, mse = 34.9596, mad = 4.3343
2023-11-08 10:00:59,079 - __main__ - INFO - Fold 3 Epoch 41 Batch 0: Train Loss = 1.2321
2023-11-08 10:01:00,754 - __main__ - INFO - Fold 3, mse = 34.5066, mad = 4.3272
2023-11-08 10:01:01,036 - __main__ - INFO - Fold 3 Epoch 42 Batch 0: Train Loss = 1.2273
2023-11-08 10:01:02,865 - __main__ - INFO - Fold 3, mse = 34.8435, mad = 4.3223
2023-11-08 10:01:03,170 - __main__ - INFO - Fold 3 Epoch 43 Batch 0: Train Loss = 1.2449
2023-11-08 10:01:04,747 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.3205 ------------
2023-11-08 10:01:04,967 - __main__ - INFO - Fold 3, mse = 34.3205, mad = 4.3349
2023-11-08 10:01:05,252 - __main__ - INFO - Fold 3 Epoch 44 Batch 0: Train Loss = 1.1606
2023-11-08 10:01:06,807 - __main__ - INFO - Fold 3, mse = 35.3146, mad = 4.3628
2023-11-08 10:01:07,038 - __main__ - INFO - Fold 3 Epoch 45 Batch 0: Train Loss = 1.2050
2023-11-08 10:01:09,134 - __main__ - INFO - Fold 3, mse = 34.4099, mad = 4.3445
2023-11-08 10:01:09,385 - __main__ - INFO - Fold 3 Epoch 46 Batch 0: Train Loss = 1.3414
2023-11-08 10:01:11,478 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9690 ------------
2023-11-08 10:01:11,480 - __main__ - INFO - Fold 3, mse = 35.0166, mad = 4.3361
2023-11-08 10:01:11,802 - __main__ - INFO - Fold 3 Epoch 47 Batch 0: Train Loss = 1.4310
2023-11-08 10:01:13,743 - __main__ - INFO - Fold 3, mse = 34.7223, mad = 4.3287
2023-11-08 10:01:14,023 - __main__ - INFO - Fold 3 Epoch 48 Batch 0: Train Loss = 1.3110
2023-11-08 10:01:15,594 - __main__ - INFO - Fold 3, mse = 34.5008, mad = 4.3227
2023-11-08 10:01:15,920 - __main__ - INFO - Fold 3 Epoch 49 Batch 0: Train Loss = 1.1674
2023-11-08 10:01:17,503 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9701 ------------
2023-11-08 10:01:17,507 - __main__ - INFO - Fold 3, mse = 34.4861, mad = 4.3111
2023-11-08 10:01:17,735 - __main__ - INFO - Fold 3 Epoch 50 Batch 0: Train Loss = 1.2065
2023-11-08 10:01:19,366 - __main__ - INFO - Fold 3, epoch 50: Loss = 1.2339 Valid loss = 1.5601 MSE = 34.3012 AUROC = 0.9707
2023-11-08 10:01:19,368 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.3012 ------------
2023-11-08 10:01:19,557 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9707 ------------
2023-11-08 10:01:19,560 - __main__ - INFO - Fold 3, mse = 34.3012, mad = 4.2929
2023-11-08 10:01:19,875 - __main__ - INFO - Fold 3 Epoch 51 Batch 0: Train Loss = 1.0639
2023-11-08 10:01:21,437 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9710 ------------
2023-11-08 10:01:21,441 - __main__ - INFO - Fold 3, mse = 35.2859, mad = 4.3340
2023-11-08 10:01:21,682 - __main__ - INFO - Fold 3 Epoch 52 Batch 0: Train Loss = 1.3543
2023-11-08 10:01:23,087 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9720 ------------
2023-11-08 10:01:23,089 - __main__ - INFO - Fold 3, mse = 34.5467, mad = 4.3055
2023-11-08 10:01:23,406 - __main__ - INFO - Fold 3 Epoch 53 Batch 0: Train Loss = 1.1875
2023-11-08 10:01:24,978 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9729 ------------
2023-11-08 10:01:24,981 - __main__ - INFO - Fold 3, mse = 35.0232, mad = 4.3223
2023-11-08 10:01:25,283 - __main__ - INFO - Fold 3 Epoch 54 Batch 0: Train Loss = 1.2502
2023-11-08 10:01:27,092 - __main__ - INFO - Fold 3, mse = 34.4321, mad = 4.3142
2023-11-08 10:01:27,374 - __main__ - INFO - Fold 3 Epoch 55 Batch 0: Train Loss = 1.2074
2023-11-08 10:01:29,388 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9732 ------------
2023-11-08 10:01:29,391 - __main__ - INFO - Fold 3, mse = 35.0063, mad = 4.3336
2023-11-08 10:01:29,660 - __main__ - INFO - Fold 3 Epoch 56 Batch 0: Train Loss = 1.3018
2023-11-08 10:01:31,451 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9734 ------------
2023-11-08 10:01:31,454 - __main__ - INFO - Fold 3, mse = 34.3839, mad = 4.3299
2023-11-08 10:01:31,713 - __main__ - INFO - Fold 3 Epoch 57 Batch 0: Train Loss = 1.0094
2023-11-08 10:01:33,165 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9740 ------------
2023-11-08 10:01:33,167 - __main__ - INFO - Fold 3, mse = 34.7158, mad = 4.3325
2023-11-08 10:01:33,462 - __main__ - INFO - Fold 3 Epoch 58 Batch 0: Train Loss = 1.0931
2023-11-08 10:01:35,363 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9746 ------------
2023-11-08 10:01:35,365 - __main__ - INFO - Fold 3, mse = 35.0002, mad = 4.3544
2023-11-08 10:01:35,621 - __main__ - INFO - Fold 3 Epoch 59 Batch 0: Train Loss = 1.0281
2023-11-08 10:01:37,114 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9749 ------------
2023-11-08 10:01:37,118 - __main__ - INFO - Fold 3, mse = 34.3077, mad = 4.3429
2023-11-08 10:01:37,335 - __main__ - INFO - Fold 3 Epoch 60 Batch 0: Train Loss = 1.1208
2023-11-08 10:01:39,053 - __main__ - INFO - Fold 3, epoch 60: Loss = 1.1948 Valid loss = 1.5969 MSE = 35.2387 AUROC = 0.9747
2023-11-08 10:01:39,057 - __main__ - INFO - Fold 3, mse = 35.2387, mad = 4.3679
2023-11-08 10:01:39,328 - __main__ - INFO - Fold 3 Epoch 61 Batch 0: Train Loss = 1.1103
2023-11-08 10:01:41,071 - __main__ - INFO - Fold 3, mse = 34.7145, mad = 4.3586
2023-11-08 10:01:41,458 - __main__ - INFO - Fold 3 Epoch 62 Batch 0: Train Loss = 1.3434
2023-11-08 10:01:43,396 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9750 ------------
2023-11-08 10:01:43,399 - __main__ - INFO - Fold 3, mse = 34.8007, mad = 4.3513
2023-11-08 10:01:43,762 - __main__ - INFO - Fold 3 Epoch 63 Batch 0: Train Loss = 1.1009
2023-11-08 10:01:45,480 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9751 ------------
2023-11-08 10:01:45,484 - __main__ - INFO - Fold 3, mse = 34.6545, mad = 4.3474
2023-11-08 10:01:45,751 - __main__ - INFO - Fold 3 Epoch 64 Batch 0: Train Loss = 1.1694
2023-11-08 10:01:47,811 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9752 ------------
2023-11-08 10:01:47,812 - __main__ - INFO - Fold 3, mse = 34.3662, mad = 4.3355
2023-11-08 10:01:48,164 - __main__ - INFO - Fold 3 Epoch 65 Batch 0: Train Loss = 1.1310
2023-11-08 10:01:49,791 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9752 ------------
2023-11-08 10:01:49,793 - __main__ - INFO - Fold 3, mse = 34.6776, mad = 4.3270
2023-11-08 10:01:50,104 - __main__ - INFO - Fold 3 Epoch 66 Batch 0: Train Loss = 1.1068
2023-11-08 10:01:51,862 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.8802 ------------
2023-11-08 10:01:52,037 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9756 ------------
2023-11-08 10:01:52,041 - __main__ - INFO - Fold 3, mse = 33.8802, mad = 4.3096
2023-11-08 10:01:52,281 - __main__ - INFO - Fold 3 Epoch 67 Batch 0: Train Loss = 1.3033
2023-11-08 10:01:53,995 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9757 ------------
2023-11-08 10:01:54,003 - __main__ - INFO - Fold 3, mse = 34.6611, mad = 4.3317
2023-11-08 10:01:54,260 - __main__ - INFO - Fold 3 Epoch 68 Batch 0: Train Loss = 1.2191
2023-11-08 10:01:55,705 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9760 ------------
2023-11-08 10:01:55,708 - __main__ - INFO - Fold 3, mse = 34.2035, mad = 4.3418
2023-11-08 10:01:55,968 - __main__ - INFO - Fold 3 Epoch 69 Batch 0: Train Loss = 0.9157
2023-11-08 10:01:57,504 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9762 ------------
2023-11-08 10:01:57,508 - __main__ - INFO - Fold 3, mse = 34.4439, mad = 4.3410
2023-11-08 10:01:57,712 - __main__ - INFO - Fold 3 Epoch 70 Batch 0: Train Loss = 1.0813
2023-11-08 10:01:59,238 - __main__ - INFO - Fold 3, epoch 70: Loss = 1.0992 Valid loss = 1.5847 MSE = 35.0715 AUROC = 0.9763
2023-11-08 10:01:59,242 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9763 ------------
2023-11-08 10:01:59,244 - __main__ - INFO - Fold 3, mse = 35.0715, mad = 4.3393
2023-11-08 10:01:59,499 - __main__ - INFO - Fold 3 Epoch 71 Batch 0: Train Loss = 0.8523
2023-11-08 10:02:01,258 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9764 ------------
2023-11-08 10:02:01,260 - __main__ - INFO - Fold 3, mse = 34.3042, mad = 4.3267
2023-11-08 10:02:01,618 - __main__ - INFO - Fold 3 Epoch 72 Batch 0: Train Loss = 1.3888
2023-11-08 10:02:03,803 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9767 ------------
2023-11-08 10:02:03,805 - __main__ - INFO - Fold 3, mse = 35.3711, mad = 4.3540
2023-11-08 10:02:04,091 - __main__ - INFO - Fold 3 Epoch 73 Batch 0: Train Loss = 0.9022
2023-11-08 10:02:06,147 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9773 ------------
2023-11-08 10:02:06,149 - __main__ - INFO - Fold 3, mse = 34.8199, mad = 4.3414
2023-11-08 10:02:06,499 - __main__ - INFO - Fold 3 Epoch 74 Batch 0: Train Loss = 0.9932
2023-11-08 10:02:08,408 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9777 ------------
2023-11-08 10:02:08,410 - __main__ - INFO - Fold 3, mse = 34.5151, mad = 4.3502
2023-11-08 10:02:08,715 - __main__ - INFO - Fold 3 Epoch 75 Batch 0: Train Loss = 1.1326
2023-11-08 10:02:10,579 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9778 ------------
2023-11-08 10:02:10,581 - __main__ - INFO - Fold 3, mse = 35.3233, mad = 4.3941
2023-11-08 10:02:10,979 - __main__ - INFO - Fold 3 Epoch 76 Batch 0: Train Loss = 1.0964
2023-11-08 10:02:12,534 - __main__ - INFO - Fold 3, mse = 34.7097, mad = 4.3858
2023-11-08 10:02:12,815 - __main__ - INFO - Fold 3 Epoch 77 Batch 0: Train Loss = 1.4475
2023-11-08 10:02:14,576 - __main__ - INFO - Fold 3, mse = 35.1714, mad = 4.3904
2023-11-08 10:02:14,883 - __main__ - INFO - Fold 3 Epoch 78 Batch 0: Train Loss = 1.0552
2023-11-08 10:02:16,617 - __main__ - INFO - Fold 3, mse = 34.7157, mad = 4.3927
2023-11-08 10:02:16,977 - __main__ - INFO - Fold 3 Epoch 79 Batch 0: Train Loss = 1.1079
2023-11-08 10:02:18,932 - __main__ - INFO - Fold 3, mse = 35.2020, mad = 4.4196
2023-11-08 10:02:19,375 - __main__ - INFO - Fold 3 Epoch 80 Batch 0: Train Loss = 1.2845
2023-11-08 10:02:20,938 - __main__ - INFO - Fold 3, epoch 80: Loss = 1.1051 Valid loss = 1.5498 MSE = 34.4022 AUROC = 0.9766
2023-11-08 10:02:20,939 - __main__ - INFO - Fold 3, mse = 34.4022, mad = 4.3856
2023-11-08 10:02:21,196 - __main__ - INFO - Fold 3 Epoch 81 Batch 0: Train Loss = 1.0754
2023-11-08 10:02:22,666 - __main__ - INFO - Fold 3, mse = 35.4053, mad = 4.4092
2023-11-08 10:02:22,969 - __main__ - INFO - Fold 3 Epoch 82 Batch 0: Train Loss = 1.0864
2023-11-08 10:02:24,786 - __main__ - INFO - Fold 3, mse = 35.0074, mad = 4.3810
2023-11-08 10:02:25,048 - __main__ - INFO - Fold 3 Epoch 83 Batch 0: Train Loss = 0.9500
2023-11-08 10:02:26,724 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9780 ------------
2023-11-08 10:02:26,726 - __main__ - INFO - Fold 3, mse = 34.7442, mad = 4.3664
2023-11-08 10:02:27,019 - __main__ - INFO - Fold 3 Epoch 84 Batch 0: Train Loss = 1.1122
2023-11-08 10:02:28,667 - __main__ - INFO - Fold 3, mse = 34.8326, mad = 4.3669
2023-11-08 10:02:29,051 - __main__ - INFO - Fold 3 Epoch 85 Batch 0: Train Loss = 1.3410
2023-11-08 10:02:30,805 - __main__ - INFO - Fold 3, mse = 35.1618, mad = 4.3823
2023-11-08 10:02:31,042 - __main__ - INFO - Fold 3 Epoch 86 Batch 0: Train Loss = 1.0720
2023-11-08 10:02:32,871 - __main__ - INFO - Fold 3, mse = 35.0300, mad = 4.3816
2023-11-08 10:02:33,155 - __main__ - INFO - Fold 3 Epoch 87 Batch 0: Train Loss = 0.9881
2023-11-08 10:02:34,702 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9784 ------------
2023-11-08 10:02:34,704 - __main__ - INFO - Fold 3, mse = 34.2570, mad = 4.3478
2023-11-08 10:02:34,959 - __main__ - INFO - Fold 3 Epoch 88 Batch 0: Train Loss = 0.9979
2023-11-08 10:02:36,572 - __main__ - INFO - Fold 3, mse = 34.7901, mad = 4.3577
2023-11-08 10:02:36,854 - __main__ - INFO - Fold 3 Epoch 89 Batch 0: Train Loss = 1.0585
2023-11-08 10:02:38,736 - __main__ - INFO - Fold 3, mse = 34.7020, mad = 4.3408
2023-11-08 10:02:39,148 - __main__ - INFO - Fold 3 Epoch 90 Batch 0: Train Loss = 1.1490
2023-11-08 10:02:41,036 - __main__ - INFO - Fold 3, epoch 90: Loss = 1.0724 Valid loss = 1.5543 MSE = 34.4937 AUROC = 0.9787
2023-11-08 10:02:41,038 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9787 ------------
2023-11-08 10:02:41,040 - __main__ - INFO - Fold 3, mse = 34.4937, mad = 4.3240
2023-11-08 10:02:41,325 - __main__ - INFO - Fold 3 Epoch 91 Batch 0: Train Loss = 0.9549
2023-11-08 10:02:43,115 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9791 ------------
2023-11-08 10:02:43,118 - __main__ - INFO - Fold 3, mse = 34.4526, mad = 4.3082
2023-11-08 10:02:43,439 - __main__ - INFO - Fold 3 Epoch 92 Batch 0: Train Loss = 1.0458
2023-11-08 10:02:45,036 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9793 ------------
2023-11-08 10:02:45,038 - __main__ - INFO - Fold 3, mse = 34.4848, mad = 4.3189
2023-11-08 10:02:45,386 - __main__ - INFO - Fold 3 Epoch 93 Batch 0: Train Loss = 0.9955
2023-11-08 10:02:47,066 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9794 ------------
2023-11-08 10:02:47,070 - __main__ - INFO - Fold 3, mse = 34.4689, mad = 4.3160
2023-11-08 10:02:47,455 - __main__ - INFO - Fold 3 Epoch 94 Batch 0: Train Loss = 1.1241
2023-11-08 10:02:49,130 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9799 ------------
2023-11-08 10:02:49,132 - __main__ - INFO - Fold 3, mse = 34.5297, mad = 4.3147
2023-11-08 10:02:49,417 - __main__ - INFO - Fold 3 Epoch 95 Batch 0: Train Loss = 1.1774
2023-11-08 10:02:51,199 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.7888 ------------
2023-11-08 10:02:51,440 - __main__ - INFO - Fold 3, mse = 33.7888, mad = 4.2901
2023-11-08 10:02:51,742 - __main__ - INFO - Fold 3 Epoch 96 Batch 0: Train Loss = 0.9525
2023-11-08 10:02:53,446 - __main__ - INFO - Fold 3, mse = 35.0096, mad = 4.3415
2023-11-08 10:02:53,688 - __main__ - INFO - Fold 3 Epoch 97 Batch 0: Train Loss = 0.9944
2023-11-08 10:02:55,514 - __main__ - INFO - Fold 3, mse = 33.8568, mad = 4.3156
2023-11-08 10:02:55,834 - __main__ - INFO - Fold 3 Epoch 98 Batch 0: Train Loss = 0.9382
2023-11-08 10:02:57,219 - __main__ - INFO - Fold 3, mse = 35.0485, mad = 4.3528
2023-11-08 10:02:57,463 - __main__ - INFO - Fold 3 Epoch 99 Batch 0: Train Loss = 1.1760
2023-11-08 10:02:58,913 - __main__ - INFO - Fold 3, mse = 34.4848, mad = 4.3287
2023-11-08 10:02:59,185 - __main__ - INFO - Fold 3 Epoch 100 Batch 0: Train Loss = 0.9944
2023-11-08 10:03:00,445 - __main__ - INFO - Fold 3, epoch 100: Loss = 1.0157 Valid loss = 1.5740 MSE = 34.8029 AUROC = 0.9789
2023-11-08 10:03:00,448 - __main__ - INFO - Fold 3, mse = 34.8029, mad = 4.3425
2023-11-08 10:03:00,657 - __main__ - INFO - Fold 3 Epoch 101 Batch 0: Train Loss = 1.0748
2023-11-08 10:03:01,979 - __main__ - INFO - Fold 3, mse = 34.9716, mad = 4.3674
2023-11-08 10:03:02,225 - __main__ - INFO - Fold 3 Epoch 102 Batch 0: Train Loss = 1.0203
2023-11-08 10:03:03,955 - __main__ - INFO - Fold 3, mse = 34.7090, mad = 4.3751
2023-11-08 10:03:04,212 - __main__ - INFO - Fold 3 Epoch 103 Batch 0: Train Loss = 1.1410
2023-11-08 10:03:05,659 - __main__ - INFO - Fold 3, mse = 34.8550, mad = 4.3610
2023-11-08 10:03:05,885 - __main__ - INFO - Fold 3 Epoch 104 Batch 0: Train Loss = 0.9770
2023-11-08 10:03:07,471 - __main__ - INFO - Fold 3, mse = 34.4157, mad = 4.3418
2023-11-08 10:03:07,761 - __main__ - INFO - Fold 3 Epoch 105 Batch 0: Train Loss = 1.1175
2023-11-08 10:03:09,492 - __main__ - INFO - Fold 3, mse = 34.6990, mad = 4.3438
2023-11-08 10:03:09,790 - __main__ - INFO - Fold 3 Epoch 106 Batch 0: Train Loss = 1.0160
2023-11-08 10:03:11,590 - __main__ - INFO - Fold 3, mse = 34.2310, mad = 4.3324
2023-11-08 10:03:11,892 - __main__ - INFO - Fold 3 Epoch 107 Batch 0: Train Loss = 0.8695
2023-11-08 10:03:13,414 - __main__ - INFO - Fold 3, mse = 34.7873, mad = 4.3530
2023-11-08 10:03:13,681 - __main__ - INFO - Fold 3 Epoch 108 Batch 0: Train Loss = 1.1614
2023-11-08 10:03:15,179 - __main__ - INFO - Fold 3, mse = 33.8958, mad = 4.3368
2023-11-08 10:03:15,428 - __main__ - INFO - Fold 3 Epoch 109 Batch 0: Train Loss = 1.0965
2023-11-08 10:03:16,824 - __main__ - INFO - Fold 3, mse = 35.2079, mad = 4.3864
2023-11-08 10:03:17,099 - __main__ - INFO - Fold 3 Epoch 110 Batch 0: Train Loss = 0.8830
2023-11-08 10:03:18,593 - __main__ - INFO - Fold 3, epoch 110: Loss = 1.0024 Valid loss = 1.5977 MSE = 34.4541 AUROC = 0.9782
2023-11-08 10:03:18,594 - __main__ - INFO - Fold 3, mse = 34.4541, mad = 4.3696
2023-11-08 10:03:18,858 - __main__ - INFO - Fold 3 Epoch 111 Batch 0: Train Loss = 1.0105
2023-11-08 10:03:20,587 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.5016 ------------
2023-11-08 10:03:20,717 - __main__ - INFO - Fold 3, mse = 33.5016, mad = 4.3300
2023-11-08 10:03:21,051 - __main__ - INFO - Fold 3 Epoch 112 Batch 0: Train Loss = 0.9866
2023-11-08 10:03:22,652 - __main__ - INFO - Fold 3, mse = 34.3508, mad = 4.3524
2023-11-08 10:03:22,937 - __main__ - INFO - Fold 3 Epoch 113 Batch 0: Train Loss = 0.8679
2023-11-08 10:03:24,739 - __main__ - INFO - Fold 3, mse = 33.9004, mad = 4.3429
2023-11-08 10:03:25,032 - __main__ - INFO - Fold 3 Epoch 114 Batch 0: Train Loss = 0.9281
2023-11-08 10:03:26,420 - __main__ - INFO - Fold 3, mse = 34.2227, mad = 4.3522
2023-11-08 10:03:26,717 - __main__ - INFO - Fold 3 Epoch 115 Batch 0: Train Loss = 0.8237
2023-11-08 10:03:28,327 - __main__ - INFO - Fold 3, mse = 34.2910, mad = 4.3561
2023-11-08 10:03:28,580 - __main__ - INFO - Fold 3 Epoch 116 Batch 0: Train Loss = 1.0147
2023-11-08 10:03:30,052 - __main__ - INFO - Fold 3, mse = 34.1412, mad = 4.3605
2023-11-08 10:03:30,298 - __main__ - INFO - Fold 3 Epoch 117 Batch 0: Train Loss = 1.1191
2023-11-08 10:03:31,900 - __main__ - INFO - Fold 3, mse = 34.5316, mad = 4.3764
2023-11-08 10:03:32,147 - __main__ - INFO - Fold 3 Epoch 118 Batch 0: Train Loss = 0.9760
2023-11-08 10:03:33,409 - __main__ - INFO - Fold 3, mse = 34.4531, mad = 4.3739
2023-11-08 10:03:33,618 - __main__ - INFO - Fold 3 Epoch 119 Batch 0: Train Loss = 0.9473
2023-11-08 10:03:35,111 - __main__ - INFO - Fold 3, mse = 34.5578, mad = 4.3700
2023-11-08 10:03:35,357 - __main__ - INFO - Fold 3 Epoch 120 Batch 0: Train Loss = 1.1266
2023-11-08 10:03:36,908 - __main__ - INFO - Fold 3, epoch 120: Loss = 1.0182 Valid loss = 1.5741 MSE = 34.5122 AUROC = 0.9780
2023-11-08 10:03:36,910 - __main__ - INFO - Fold 3, mse = 34.5122, mad = 4.3641
2023-11-08 10:03:37,155 - __main__ - INFO - Fold 3 Epoch 121 Batch 0: Train Loss = 1.0222
2023-11-08 10:03:38,548 - __main__ - INFO - Fold 3, mse = 34.3371, mad = 4.3588
2023-11-08 10:03:38,809 - __main__ - INFO - Fold 3 Epoch 122 Batch 0: Train Loss = 0.9667
2023-11-08 10:03:40,558 - __main__ - INFO - Fold 3, mse = 34.4138, mad = 4.3515
2023-11-08 10:03:40,801 - __main__ - INFO - Fold 3 Epoch 123 Batch 0: Train Loss = 0.9115
2023-11-08 10:03:42,625 - __main__ - INFO - Fold 3, mse = 34.4997, mad = 4.3363
2023-11-08 10:03:42,876 - __main__ - INFO - Fold 3 Epoch 124 Batch 0: Train Loss = 0.9127
2023-11-08 10:03:44,381 - __main__ - INFO - Fold 3, mse = 34.8394, mad = 4.3344
2023-11-08 10:03:44,650 - __main__ - INFO - Fold 3 Epoch 125 Batch 0: Train Loss = 1.2934
2023-11-08 10:03:46,077 - __main__ - INFO - Fold 3, mse = 34.1787, mad = 4.3267
2023-11-08 10:03:46,361 - __main__ - INFO - Fold 3 Epoch 126 Batch 0: Train Loss = 0.7399
2023-11-08 10:03:48,174 - __main__ - INFO - Fold 3, mse = 34.0695, mad = 4.3317
2023-11-08 10:03:48,443 - __main__ - INFO - Fold 3 Epoch 127 Batch 0: Train Loss = 0.8303
2023-11-08 10:03:50,001 - __main__ - INFO - Fold 3, mse = 33.9106, mad = 4.3291
2023-11-08 10:03:50,299 - __main__ - INFO - Fold 3 Epoch 128 Batch 0: Train Loss = 1.1260
2023-11-08 10:03:51,923 - __main__ - INFO - Fold 3, mse = 34.5999, mad = 4.3391
2023-11-08 10:03:52,196 - __main__ - INFO - Fold 3 Epoch 129 Batch 0: Train Loss = 0.8083
2023-11-08 10:03:53,771 - __main__ - INFO - Fold 3, mse = 34.2433, mad = 4.3186
2023-11-08 10:03:54,018 - __main__ - INFO - Fold 3 Epoch 130 Batch 0: Train Loss = 0.7386
2023-11-08 10:03:55,504 - __main__ - INFO - Fold 3, epoch 130: Loss = 0.9714 Valid loss = 1.5616 MSE = 33.8256 AUROC = 0.9767
2023-11-08 10:03:55,506 - __main__ - INFO - Fold 3, mse = 33.8256, mad = 4.3064
2023-11-08 10:03:55,815 - __main__ - INFO - Fold 3 Epoch 131 Batch 0: Train Loss = 1.1100
2023-11-08 10:03:57,215 - __main__ - INFO - Fold 3, mse = 34.9013, mad = 4.3403
2023-11-08 10:03:57,494 - __main__ - INFO - Fold 3 Epoch 132 Batch 0: Train Loss = 1.0232
2023-11-08 10:03:58,986 - __main__ - INFO - Fold 3, mse = 33.9121, mad = 4.3164
2023-11-08 10:03:59,256 - __main__ - INFO - Fold 3 Epoch 133 Batch 0: Train Loss = 0.9423
2023-11-08 10:04:00,455 - __main__ - INFO - Fold 3, mse = 34.8342, mad = 4.3378
2023-11-08 10:04:00,662 - __main__ - INFO - Fold 3 Epoch 134 Batch 0: Train Loss = 0.8208
2023-11-08 10:04:02,070 - __main__ - INFO - Fold 3, mse = 34.6530, mad = 4.3203
2023-11-08 10:04:02,352 - __main__ - INFO - Fold 3 Epoch 135 Batch 0: Train Loss = 1.0904
2023-11-08 10:04:03,971 - __main__ - INFO - Fold 3, mse = 33.9384, mad = 4.3090
2023-11-08 10:04:04,243 - __main__ - INFO - Fold 3 Epoch 136 Batch 0: Train Loss = 1.0255
2023-11-08 10:04:06,007 - __main__ - INFO - Fold 3, mse = 35.5316, mad = 4.3652
2023-11-08 10:04:06,299 - __main__ - INFO - Fold 3 Epoch 137 Batch 0: Train Loss = 1.0132
2023-11-08 10:04:07,773 - __main__ - INFO - Fold 3, mse = 34.2029, mad = 4.3212
2023-11-08 10:04:08,015 - __main__ - INFO - Fold 3 Epoch 138 Batch 0: Train Loss = 1.0579
2023-11-08 10:04:09,439 - __main__ - INFO - Fold 3, mse = 34.5985, mad = 4.3377
2023-11-08 10:04:09,739 - __main__ - INFO - Fold 3 Epoch 139 Batch 0: Train Loss = 0.9101
2023-11-08 10:04:11,260 - __main__ - INFO - Fold 3, mse = 33.9983, mad = 4.3165
2023-11-08 10:04:11,568 - __main__ - INFO - Fold 3 Epoch 140 Batch 0: Train Loss = 0.8760
2023-11-08 10:04:13,111 - __main__ - INFO - Fold 3, epoch 140: Loss = 0.9562 Valid loss = 1.5891 MSE = 34.3966 AUROC = 0.9779
2023-11-08 10:04:13,113 - __main__ - INFO - Fold 3, mse = 34.3966, mad = 4.3334
2023-11-08 10:04:13,374 - __main__ - INFO - Fold 3 Epoch 141 Batch 0: Train Loss = 1.0969
2023-11-08 10:04:14,848 - __main__ - INFO - Fold 3, mse = 33.9222, mad = 4.3245
2023-11-08 10:04:15,069 - __main__ - INFO - Fold 3 Epoch 142 Batch 0: Train Loss = 0.8886
2023-11-08 10:04:16,651 - __main__ - INFO - Fold 3, mse = 34.2339, mad = 4.3432
2023-11-08 10:04:16,894 - __main__ - INFO - Fold 3 Epoch 143 Batch 0: Train Loss = 1.1024
2023-11-08 10:04:18,640 - __main__ - INFO - Fold 3, mse = 34.1683, mad = 4.3458
2023-11-08 10:04:18,870 - __main__ - INFO - Fold 3 Epoch 144 Batch 0: Train Loss = 1.0308
2023-11-08 10:04:20,492 - __main__ - INFO - Fold 3, mse = 34.6178, mad = 4.3660
2023-11-08 10:04:20,761 - __main__ - INFO - Fold 3 Epoch 145 Batch 0: Train Loss = 0.9962
2023-11-08 10:04:22,303 - __main__ - INFO - Fold 3, mse = 33.8745, mad = 4.3370
2023-11-08 10:04:22,569 - __main__ - INFO - Fold 3 Epoch 146 Batch 0: Train Loss = 0.9435
2023-11-08 10:04:23,899 - __main__ - INFO - Fold 3, mse = 34.1395, mad = 4.3355
2023-11-08 10:04:24,136 - __main__ - INFO - Fold 3 Epoch 147 Batch 0: Train Loss = 0.9698
2023-11-08 10:04:25,678 - __main__ - INFO - Fold 3, mse = 34.4056, mad = 4.3414
2023-11-08 10:04:25,964 - __main__ - INFO - Fold 3 Epoch 148 Batch 0: Train Loss = 1.1530
2023-11-08 10:04:27,691 - __main__ - INFO - Fold 3, mse = 33.9725, mad = 4.3352
2023-11-08 10:04:27,924 - __main__ - INFO - Fold 3 Epoch 149 Batch 0: Train Loss = 0.8978
2023-11-08 10:04:29,218 - __main__ - INFO - Fold 3, mse = 34.6786, mad = 4.3622
2023-11-08 10:04:29,996 - __main__ - INFO - Fold 4 Epoch 0 Batch 0: Train Loss = 2.6207
2023-11-08 10:04:31,392 - __main__ - INFO - Fold 4, epoch 0: Loss = 2.5392 Valid loss = 2.2759 MSE = 41.6373 AUROC = 0.6686
2023-11-08 10:04:31,396 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 41.6373 ------------
2023-11-08 10:04:31,558 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.6686 ------------
2023-11-08 10:04:31,559 - __main__ - INFO - Fold 4, mse = 41.6373, mad = 4.8470
2023-11-08 10:04:31,820 - __main__ - INFO - Fold 4 Epoch 1 Batch 0: Train Loss = 2.6614
2023-11-08 10:04:33,332 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 40.2371 ------------
2023-11-08 10:04:33,489 - __main__ - INFO - Fold 4, mse = 40.2371, mad = 4.7788
2023-11-08 10:04:33,717 - __main__ - INFO - Fold 4 Epoch 2 Batch 0: Train Loss = 2.6179
2023-11-08 10:04:34,967 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 39.1389 ------------
2023-11-08 10:04:35,177 - __main__ - INFO - Fold 4, mse = 39.1389, mad = 4.7444
2023-11-08 10:04:35,478 - __main__ - INFO - Fold 4 Epoch 3 Batch 0: Train Loss = 2.4107
2023-11-08 10:04:37,020 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 38.2862 ------------
2023-11-08 10:04:37,172 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.6849 ------------
2023-11-08 10:04:37,174 - __main__ - INFO - Fold 4, mse = 38.2862, mad = 4.6993
2023-11-08 10:04:37,434 - __main__ - INFO - Fold 4 Epoch 4 Batch 0: Train Loss = 2.4595
2023-11-08 10:04:38,845 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.7003 ------------
2023-11-08 10:04:39,013 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.7156 ------------
2023-11-08 10:04:39,015 - __main__ - INFO - Fold 4, mse = 37.7003, mad = 4.6842
2023-11-08 10:04:39,279 - __main__ - INFO - Fold 4 Epoch 5 Batch 0: Train Loss = 2.0152
2023-11-08 10:04:40,588 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.3064 ------------
2023-11-08 10:04:40,755 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.7548 ------------
2023-11-08 10:04:40,757 - __main__ - INFO - Fold 4, mse = 37.3064, mad = 4.6838
2023-11-08 10:04:40,997 - __main__ - INFO - Fold 4 Epoch 6 Batch 0: Train Loss = 1.8798
2023-11-08 10:04:42,498 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.0905 ------------
2023-11-08 10:04:42,653 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.7996 ------------
2023-11-08 10:04:42,655 - __main__ - INFO - Fold 4, mse = 37.0905, mad = 4.7420
2023-11-08 10:04:42,905 - __main__ - INFO - Fold 4 Epoch 7 Batch 0: Train Loss = 2.0082
2023-11-08 10:04:44,481 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.7466 ------------
2023-11-08 10:04:44,655 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8506 ------------
2023-11-08 10:04:44,657 - __main__ - INFO - Fold 4, mse = 36.7466, mad = 4.6904
2023-11-08 10:04:44,931 - __main__ - INFO - Fold 4 Epoch 8 Batch 0: Train Loss = 2.0960
2023-11-08 10:04:46,757 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.6125 ------------
2023-11-08 10:04:46,873 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8961 ------------
2023-11-08 10:04:46,875 - __main__ - INFO - Fold 4, mse = 36.6125, mad = 4.6727
2023-11-08 10:04:47,193 - __main__ - INFO - Fold 4 Epoch 9 Batch 0: Train Loss = 2.1534
2023-11-08 10:04:48,699 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.4948 ------------
2023-11-08 10:04:48,841 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9261 ------------
2023-11-08 10:04:48,843 - __main__ - INFO - Fold 4, mse = 36.4948, mad = 4.7134
2023-11-08 10:04:49,114 - __main__ - INFO - Fold 4 Epoch 10 Batch 0: Train Loss = 1.9515
2023-11-08 10:04:50,690 - __main__ - INFO - Fold 4, epoch 10: Loss = 1.9034 Valid loss = 1.9437 MSE = 36.5154 AUROC = 0.9497
2023-11-08 10:04:50,692 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9497 ------------
2023-11-08 10:04:50,694 - __main__ - INFO - Fold 4, mse = 36.5154, mad = 4.7087
2023-11-08 10:04:50,944 - __main__ - INFO - Fold 4 Epoch 11 Batch 0: Train Loss = 2.1001
2023-11-08 10:04:52,343 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9634 ------------
2023-11-08 10:04:52,345 - __main__ - INFO - Fold 4, mse = 36.7508, mad = 4.6922
2023-11-08 10:04:52,631 - __main__ - INFO - Fold 4 Epoch 12 Batch 0: Train Loss = 1.7543
2023-11-08 10:04:54,130 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9680 ------------
2023-11-08 10:04:54,132 - __main__ - INFO - Fold 4, mse = 36.7008, mad = 4.7184
2023-11-08 10:04:54,419 - __main__ - INFO - Fold 4 Epoch 13 Batch 0: Train Loss = 1.9203
2023-11-08 10:04:55,909 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9712 ------------
2023-11-08 10:04:55,914 - __main__ - INFO - Fold 4, mse = 36.7656, mad = 4.7284
2023-11-08 10:04:56,182 - __main__ - INFO - Fold 4 Epoch 14 Batch 0: Train Loss = 1.6957
2023-11-08 10:04:57,815 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9730 ------------
2023-11-08 10:04:57,817 - __main__ - INFO - Fold 4, mse = 36.7015, mad = 4.7153
2023-11-08 10:04:58,060 - __main__ - INFO - Fold 4 Epoch 15 Batch 0: Train Loss = 1.6359
2023-11-08 10:04:59,793 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.4327 ------------
2023-11-08 10:04:59,948 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9756 ------------
2023-11-08 10:04:59,950 - __main__ - INFO - Fold 4, mse = 36.4327, mad = 4.6341
2023-11-08 10:05:00,217 - __main__ - INFO - Fold 4 Epoch 16 Batch 0: Train Loss = 1.5693
2023-11-08 10:05:01,742 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.2489 ------------
2023-11-08 10:05:01,896 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9763 ------------
2023-11-08 10:05:01,898 - __main__ - INFO - Fold 4, mse = 36.2489, mad = 4.6845
2023-11-08 10:05:02,163 - __main__ - INFO - Fold 4 Epoch 17 Batch 0: Train Loss = 1.9986
2023-11-08 10:05:03,534 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9765 ------------
2023-11-08 10:05:03,537 - __main__ - INFO - Fold 4, mse = 36.2969, mad = 4.6861
2023-11-08 10:05:03,799 - __main__ - INFO - Fold 4 Epoch 18 Batch 0: Train Loss = 1.8110
2023-11-08 10:05:05,434 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.2026 ------------
2023-11-08 10:05:05,614 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9766 ------------
2023-11-08 10:05:05,616 - __main__ - INFO - Fold 4, mse = 36.2026, mad = 4.5940
2023-11-08 10:05:05,903 - __main__ - INFO - Fold 4 Epoch 19 Batch 0: Train Loss = 1.8539
2023-11-08 10:05:07,505 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.8375 ------------
2023-11-08 10:05:07,624 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9777 ------------
2023-11-08 10:05:07,625 - __main__ - INFO - Fold 4, mse = 35.8375, mad = 4.6230
2023-11-08 10:05:07,878 - __main__ - INFO - Fold 4 Epoch 20 Batch 0: Train Loss = 1.6977
2023-11-08 10:05:09,259 - __main__ - INFO - Fold 4, epoch 20: Loss = 1.6036 Valid loss = 1.7509 MSE = 35.6608 AUROC = 0.9784
2023-11-08 10:05:09,262 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.6608 ------------
2023-11-08 10:05:09,411 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9784 ------------
2023-11-08 10:05:09,413 - __main__ - INFO - Fold 4, mse = 35.6608, mad = 4.6067
2023-11-08 10:05:09,669 - __main__ - INFO - Fold 4 Epoch 21 Batch 0: Train Loss = 1.6638
2023-11-08 10:05:11,237 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9790 ------------
2023-11-08 10:05:11,239 - __main__ - INFO - Fold 4, mse = 35.7406, mad = 4.5907
2023-11-08 10:05:11,528 - __main__ - INFO - Fold 4 Epoch 22 Batch 0: Train Loss = 1.6579
2023-11-08 10:05:12,873 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.4795 ------------
2023-11-08 10:05:13,033 - __main__ - INFO - Fold 4, mse = 35.4795, mad = 4.5939
2023-11-08 10:05:13,329 - __main__ - INFO - Fold 4 Epoch 23 Batch 0: Train Loss = 1.3189
2023-11-08 10:05:14,949 - __main__ - INFO - Fold 4, mse = 35.5998, mad = 4.6216
2023-11-08 10:05:15,190 - __main__ - INFO - Fold 4 Epoch 24 Batch 0: Train Loss = 1.4099
2023-11-08 10:05:16,982 - __main__ - INFO - Fold 4, mse = 36.0918, mad = 4.6438
2023-11-08 10:05:17,270 - __main__ - INFO - Fold 4 Epoch 25 Batch 0: Train Loss = 1.5315
2023-11-08 10:05:18,875 - __main__ - INFO - Fold 4, mse = 36.2457, mad = 4.5777
2023-11-08 10:05:19,183 - __main__ - INFO - Fold 4 Epoch 26 Batch 0: Train Loss = 1.4621
2023-11-08 10:05:20,930 - __main__ - INFO - Fold 4, mse = 36.1083, mad = 4.6166
2023-11-08 10:05:21,231 - __main__ - INFO - Fold 4 Epoch 27 Batch 0: Train Loss = 1.6712
2023-11-08 10:05:22,737 - __main__ - INFO - Fold 4, mse = 36.0791, mad = 4.5861
2023-11-08 10:05:23,012 - __main__ - INFO - Fold 4 Epoch 28 Batch 0: Train Loss = 1.5603
2023-11-08 10:05:24,541 - __main__ - INFO - Fold 4, mse = 36.3759, mad = 4.5936
2023-11-08 10:05:24,806 - __main__ - INFO - Fold 4 Epoch 29 Batch 0: Train Loss = 1.4766
2023-11-08 10:05:26,420 - __main__ - INFO - Fold 4, mse = 36.5884, mad = 4.6640
2023-11-08 10:05:26,671 - __main__ - INFO - Fold 4 Epoch 30 Batch 0: Train Loss = 1.4518
2023-11-08 10:05:28,293 - __main__ - INFO - Fold 4, epoch 30: Loss = 1.4075 Valid loss = 1.8126 MSE = 36.5673 AUROC = 0.9771
2023-11-08 10:05:28,295 - __main__ - INFO - Fold 4, mse = 36.5673, mad = 4.5956
2023-11-08 10:05:28,516 - __main__ - INFO - Fold 4 Epoch 31 Batch 0: Train Loss = 1.3204
2023-11-08 10:05:29,744 - __main__ - INFO - Fold 4, mse = 36.3852, mad = 4.5946
2023-11-08 10:05:29,958 - __main__ - INFO - Fold 4 Epoch 32 Batch 0: Train Loss = 1.5880
2023-11-08 10:05:31,317 - __main__ - INFO - Fold 4, mse = 36.1154, mad = 4.5156
2023-11-08 10:05:31,528 - __main__ - INFO - Fold 4 Epoch 33 Batch 0: Train Loss = 1.3281
2023-11-08 10:05:32,665 - __main__ - INFO - Fold 4, mse = 36.1826, mad = 4.6091
2023-11-08 10:05:32,869 - __main__ - INFO - Fold 4 Epoch 34 Batch 0: Train Loss = 1.5237
2023-11-08 10:05:34,503 - __main__ - INFO - Fold 4, mse = 36.1886, mad = 4.5717
2023-11-08 10:05:34,865 - __main__ - INFO - Fold 4 Epoch 35 Batch 0: Train Loss = 1.3880
2023-11-08 10:05:36,502 - __main__ - INFO - Fold 4, mse = 35.8686, mad = 4.5731
2023-11-08 10:05:36,820 - __main__ - INFO - Fold 4 Epoch 36 Batch 0: Train Loss = 1.3961
2023-11-08 10:05:38,382 - __main__ - INFO - Fold 4, mse = 35.5780, mad = 4.5767
2023-11-08 10:05:38,659 - __main__ - INFO - Fold 4 Epoch 37 Batch 0: Train Loss = 1.4724
2023-11-08 10:05:40,388 - __main__ - INFO - Fold 4, mse = 35.8945, mad = 4.4688
2023-11-08 10:05:40,688 - __main__ - INFO - Fold 4 Epoch 38 Batch 0: Train Loss = 1.5180
2023-11-08 10:05:42,319 - __main__ - INFO - Fold 4, mse = 35.8597, mad = 4.6091
2023-11-08 10:05:42,597 - __main__ - INFO - Fold 4 Epoch 39 Batch 0: Train Loss = 1.3078
2023-11-08 10:05:44,136 - __main__ - INFO - Fold 4, mse = 35.6791, mad = 4.5715
2023-11-08 10:05:44,439 - __main__ - INFO - Fold 4 Epoch 40 Batch 0: Train Loss = 1.3783
2023-11-08 10:05:45,975 - __main__ - INFO - Fold 4, epoch 40: Loss = 1.3331 Valid loss = 1.8136 MSE = 35.6086 AUROC = 0.9771
2023-11-08 10:05:45,977 - __main__ - INFO - Fold 4, mse = 35.6086, mad = 4.4987
2023-11-08 10:05:46,229 - __main__ - INFO - Fold 4 Epoch 41 Batch 0: Train Loss = 1.5549
2023-11-08 10:05:47,788 - __main__ - INFO - Fold 4, mse = 35.8295, mad = 4.5630
2023-11-08 10:05:48,021 - __main__ - INFO - Fold 4 Epoch 42 Batch 0: Train Loss = 1.1910
2023-11-08 10:05:49,306 - __main__ - INFO - Fold 4, mse = 36.0208, mad = 4.5985
2023-11-08 10:05:49,550 - __main__ - INFO - Fold 4 Epoch 43 Batch 0: Train Loss = 1.1476
2023-11-08 10:05:51,053 - __main__ - INFO - Fold 4, mse = 36.1276, mad = 4.6258
2023-11-08 10:05:51,362 - __main__ - INFO - Fold 4 Epoch 44 Batch 0: Train Loss = 1.2064
2023-11-08 10:05:52,790 - __main__ - INFO - Fold 4, mse = 35.9762, mad = 4.5956
2023-11-08 10:05:52,995 - __main__ - INFO - Fold 4 Epoch 45 Batch 0: Train Loss = 1.2953
2023-11-08 10:05:54,628 - __main__ - INFO - Fold 4, mse = 36.0727, mad = 4.5707
2023-11-08 10:05:54,874 - __main__ - INFO - Fold 4 Epoch 46 Batch 0: Train Loss = 1.2130
2023-11-08 10:05:56,142 - __main__ - INFO - Fold 4, mse = 36.0983, mad = 4.6518
2023-11-08 10:05:56,378 - __main__ - INFO - Fold 4 Epoch 47 Batch 0: Train Loss = 1.4689
2023-11-08 10:05:57,850 - __main__ - INFO - Fold 4, mse = 35.9478, mad = 4.5626
2023-11-08 10:05:58,072 - __main__ - INFO - Fold 4 Epoch 48 Batch 0: Train Loss = 1.4949
2023-11-08 10:05:59,340 - __main__ - INFO - Fold 4, mse = 35.7343, mad = 4.6095
2023-11-08 10:05:59,593 - __main__ - INFO - Fold 4 Epoch 49 Batch 0: Train Loss = 1.1084
2023-11-08 10:06:01,131 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.4093 ------------
2023-11-08 10:06:01,250 - __main__ - INFO - Fold 4, mse = 35.4093, mad = 4.4707
2023-11-08 10:06:01,495 - __main__ - INFO - Fold 4 Epoch 50 Batch 0: Train Loss = 1.2175
2023-11-08 10:06:02,984 - __main__ - INFO - Fold 4, epoch 50: Loss = 1.1889 Valid loss = 1.7455 MSE = 35.4281 AUROC = 0.9780
2023-11-08 10:06:02,987 - __main__ - INFO - Fold 4, mse = 35.4281, mad = 4.5432
2023-11-08 10:06:03,257 - __main__ - INFO - Fold 4 Epoch 51 Batch 0: Train Loss = 1.1452
2023-11-08 10:06:04,675 - __main__ - INFO - Fold 4, mse = 35.9736, mad = 4.5915
2023-11-08 10:06:04,900 - __main__ - INFO - Fold 4 Epoch 52 Batch 0: Train Loss = 1.0642
2023-11-08 10:06:06,657 - __main__ - INFO - Fold 4, mse = 35.8818, mad = 4.5352
2023-11-08 10:06:06,952 - __main__ - INFO - Fold 4 Epoch 53 Batch 0: Train Loss = 1.2706
2023-11-08 10:06:08,553 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.2326 ------------
2023-11-08 10:06:08,720 - __main__ - INFO - Fold 4, mse = 35.2326, mad = 4.5340
2023-11-08 10:06:08,992 - __main__ - INFO - Fold 4 Epoch 54 Batch 0: Train Loss = 1.2369
2023-11-08 10:06:10,426 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.1175 ------------
2023-11-08 10:06:10,599 - __main__ - INFO - Fold 4, mse = 35.1175, mad = 4.5512
2023-11-08 10:06:10,893 - __main__ - INFO - Fold 4 Epoch 55 Batch 0: Train Loss = 1.1939
2023-11-08 10:06:12,363 - __main__ - INFO - Fold 4, mse = 35.6070, mad = 4.4803
2023-11-08 10:06:12,622 - __main__ - INFO - Fold 4 Epoch 56 Batch 0: Train Loss = 1.0901
2023-11-08 10:06:14,357 - __main__ - INFO - Fold 4, mse = 36.1607, mad = 4.6584
2023-11-08 10:06:14,645 - __main__ - INFO - Fold 4 Epoch 57 Batch 0: Train Loss = 1.2541
2023-11-08 10:06:16,369 - __main__ - INFO - Fold 4, mse = 35.8720, mad = 4.5598
2023-11-08 10:06:16,673 - __main__ - INFO - Fold 4 Epoch 58 Batch 0: Train Loss = 1.2902
2023-11-08 10:06:18,077 - __main__ - INFO - Fold 4, mse = 35.5153, mad = 4.5636
2023-11-08 10:06:18,304 - __main__ - INFO - Fold 4 Epoch 59 Batch 0: Train Loss = 1.1931
2023-11-08 10:06:19,745 - __main__ - INFO - Fold 4, mse = 35.3614, mad = 4.5161
2023-11-08 10:06:19,967 - __main__ - INFO - Fold 4 Epoch 60 Batch 0: Train Loss = 1.2274
2023-11-08 10:06:21,460 - __main__ - INFO - Fold 4, epoch 60: Loss = 1.1505 Valid loss = 1.7536 MSE = 35.4452 AUROC = 0.9773
2023-11-08 10:06:21,462 - __main__ - INFO - Fold 4, mse = 35.4452, mad = 4.5331
2023-11-08 10:06:21,734 - __main__ - INFO - Fold 4 Epoch 61 Batch 0: Train Loss = 0.9463
2023-11-08 10:06:23,330 - __main__ - INFO - Fold 4, mse = 35.5374, mad = 4.5633
2023-11-08 10:06:23,640 - __main__ - INFO - Fold 4 Epoch 62 Batch 0: Train Loss = 1.1451
2023-11-08 10:06:24,988 - __main__ - INFO - Fold 4, mse = 35.7981, mad = 4.5547
2023-11-08 10:06:25,278 - __main__ - INFO - Fold 4 Epoch 63 Batch 0: Train Loss = 1.2570
2023-11-08 10:06:26,538 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9792 ------------
2023-11-08 10:06:26,540 - __main__ - INFO - Fold 4, mse = 35.8137, mad = 4.5893
2023-11-08 10:06:26,766 - __main__ - INFO - Fold 4 Epoch 64 Batch 0: Train Loss = 1.0059
2023-11-08 10:06:28,147 - __main__ - INFO - Fold 4, mse = 35.5561, mad = 4.5454
2023-11-08 10:06:28,430 - __main__ - INFO - Fold 4 Epoch 65 Batch 0: Train Loss = 1.2125
2023-11-08 10:06:29,936 - __main__ - INFO - Fold 4, mse = 35.4726, mad = 4.5455
2023-11-08 10:06:30,221 - __main__ - INFO - Fold 4 Epoch 66 Batch 0: Train Loss = 1.0401
2023-11-08 10:06:31,587 - __main__ - INFO - Fold 4, mse = 35.3920, mad = 4.5553
2023-11-08 10:06:31,796 - __main__ - INFO - Fold 4 Epoch 67 Batch 0: Train Loss = 1.2063
2023-11-08 10:06:33,429 - __main__ - INFO - Fold 4, mse = 35.1321, mad = 4.4887
2023-11-08 10:06:33,717 - __main__ - INFO - Fold 4 Epoch 68 Batch 0: Train Loss = 1.2108
2023-11-08 10:06:35,227 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.1112 ------------
2023-11-08 10:06:35,383 - __main__ - INFO - Fold 4, mse = 35.1112, mad = 4.5598
2023-11-08 10:06:35,637 - __main__ - INFO - Fold 4 Epoch 69 Batch 0: Train Loss = 1.2819
2023-11-08 10:06:36,933 - __main__ - INFO - Fold 4, mse = 35.2710, mad = 4.5453
2023-11-08 10:06:37,172 - __main__ - INFO - Fold 4 Epoch 70 Batch 0: Train Loss = 1.1773
2023-11-08 10:06:38,903 - __main__ - INFO - Fold 4, epoch 70: Loss = 1.1328 Valid loss = 1.7929 MSE = 36.0141 AUROC = 0.9785
2023-11-08 10:06:38,905 - __main__ - INFO - Fold 4, mse = 36.0141, mad = 4.6233
2023-11-08 10:06:39,170 - __main__ - INFO - Fold 4 Epoch 71 Batch 0: Train Loss = 1.2255
2023-11-08 10:06:40,623 - __main__ - INFO - Fold 4, mse = 35.7430, mad = 4.6142
2023-11-08 10:06:40,858 - __main__ - INFO - Fold 4 Epoch 72 Batch 0: Train Loss = 1.1666
2023-11-08 10:06:42,486 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.1042 ------------
2023-11-08 10:06:42,692 - __main__ - INFO - Fold 4, mse = 35.1042, mad = 4.4810
2023-11-08 10:06:42,970 - __main__ - INFO - Fold 4 Epoch 73 Batch 0: Train Loss = 1.0275
2023-11-08 10:06:44,787 - __main__ - INFO - Fold 4, mse = 35.1283, mad = 4.5538
2023-11-08 10:06:45,123 - __main__ - INFO - Fold 4 Epoch 74 Batch 0: Train Loss = 1.2038
2023-11-08 10:06:46,846 - __main__ - INFO - Fold 4, mse = 35.4578, mad = 4.5385
2023-11-08 10:06:47,087 - __main__ - INFO - Fold 4 Epoch 75 Batch 0: Train Loss = 1.1006
2023-11-08 10:06:48,463 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9794 ------------
2023-11-08 10:06:48,465 - __main__ - INFO - Fold 4, mse = 35.4704, mad = 4.4600
2023-11-08 10:06:48,723 - __main__ - INFO - Fold 4 Epoch 76 Batch 0: Train Loss = 1.2303
2023-11-08 10:06:50,130 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9796 ------------
2023-11-08 10:06:50,132 - __main__ - INFO - Fold 4, mse = 35.6093, mad = 4.5601
2023-11-08 10:06:50,414 - __main__ - INFO - Fold 4 Epoch 77 Batch 0: Train Loss = 1.2838
2023-11-08 10:06:51,861 - __main__ - INFO - Fold 4, mse = 35.9452, mad = 4.5859
2023-11-08 10:06:52,083 - __main__ - INFO - Fold 4 Epoch 78 Batch 0: Train Loss = 1.2295
2023-11-08 10:06:53,547 - __main__ - INFO - Fold 4, mse = 36.1192, mad = 4.6041
2023-11-08 10:06:53,782 - __main__ - INFO - Fold 4 Epoch 79 Batch 0: Train Loss = 0.9663
2023-11-08 10:06:55,446 - __main__ - INFO - Fold 4, mse = 35.7029, mad = 4.5348
2023-11-08 10:06:55,797 - __main__ - INFO - Fold 4 Epoch 80 Batch 0: Train Loss = 1.0957
2023-11-08 10:06:57,195 - __main__ - INFO - Fold 4, epoch 80: Loss = 1.0818 Valid loss = 1.7204 MSE = 35.3293 AUROC = 0.9797
2023-11-08 10:06:57,197 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9797 ------------
2023-11-08 10:06:57,200 - __main__ - INFO - Fold 4, mse = 35.3293, mad = 4.5483
2023-11-08 10:06:57,409 - __main__ - INFO - Fold 4 Epoch 81 Batch 0: Train Loss = 1.3288
2023-11-08 10:06:58,588 - __main__ - INFO - Fold 4, mse = 35.1397, mad = 4.4744
2023-11-08 10:06:58,789 - __main__ - INFO - Fold 4 Epoch 82 Batch 0: Train Loss = 1.4083
2023-11-08 10:07:00,407 - __main__ - INFO - Fold 4, mse = 35.3440, mad = 4.5274
2023-11-08 10:07:00,675 - __main__ - INFO - Fold 4 Epoch 83 Batch 0: Train Loss = 1.0238
2023-11-08 10:07:02,095 - __main__ - INFO - Fold 4, mse = 35.5265, mad = 4.5750
2023-11-08 10:07:02,370 - __main__ - INFO - Fold 4 Epoch 84 Batch 0: Train Loss = 0.9587
2023-11-08 10:07:03,854 - __main__ - INFO - Fold 4, mse = 35.4219, mad = 4.5326
2023-11-08 10:07:04,107 - __main__ - INFO - Fold 4 Epoch 85 Batch 0: Train Loss = 1.3284
2023-11-08 10:07:05,618 - __main__ - INFO - Fold 4, mse = 35.3396, mad = 4.4948
2023-11-08 10:07:05,855 - __main__ - INFO - Fold 4 Epoch 86 Batch 0: Train Loss = 1.2011
2023-11-08 10:07:07,568 - __main__ - INFO - Fold 4, mse = 35.6297, mad = 4.5696
2023-11-08 10:07:07,881 - __main__ - INFO - Fold 4 Epoch 87 Batch 0: Train Loss = 1.2192
2023-11-08 10:07:09,350 - __main__ - INFO - Fold 4, mse = 35.4487, mad = 4.4646
2023-11-08 10:07:09,632 - __main__ - INFO - Fold 4 Epoch 88 Batch 0: Train Loss = 1.0800
2023-11-08 10:07:10,990 - __main__ - INFO - Fold 4, mse = 35.4893, mad = 4.5393
2023-11-08 10:07:11,301 - __main__ - INFO - Fold 4 Epoch 89 Batch 0: Train Loss = 0.8967
2023-11-08 10:07:12,991 - __main__ - INFO - Fold 4, mse = 35.5914, mad = 4.5779
2023-11-08 10:07:13,243 - __main__ - INFO - Fold 4 Epoch 90 Batch 0: Train Loss = 1.1003
2023-11-08 10:07:14,859 - __main__ - INFO - Fold 4, epoch 90: Loss = 1.1199 Valid loss = 1.7439 MSE = 35.4157 AUROC = 0.9800
2023-11-08 10:07:14,863 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9800 ------------
2023-11-08 10:07:14,867 - __main__ - INFO - Fold 4, mse = 35.4157, mad = 4.5139
2023-11-08 10:07:15,156 - __main__ - INFO - Fold 4 Epoch 91 Batch 0: Train Loss = 1.0813
2023-11-08 10:07:16,684 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9802 ------------
2023-11-08 10:07:16,686 - __main__ - INFO - Fold 4, mse = 35.7402, mad = 4.5721
2023-11-08 10:07:16,906 - __main__ - INFO - Fold 4 Epoch 92 Batch 0: Train Loss = 1.0410
2023-11-08 10:07:18,292 - __main__ - INFO - Fold 4, mse = 35.6226, mad = 4.4938
2023-11-08 10:07:18,602 - __main__ - INFO - Fold 4 Epoch 93 Batch 0: Train Loss = 1.1537
2023-11-08 10:07:20,223 - __main__ - INFO - Fold 4, mse = 35.5854, mad = 4.5012
2023-11-08 10:07:20,453 - __main__ - INFO - Fold 4 Epoch 94 Batch 0: Train Loss = 0.9409
2023-11-08 10:07:21,805 - __main__ - INFO - Fold 4, mse = 35.9516, mad = 4.5649
2023-11-08 10:07:22,085 - __main__ - INFO - Fold 4 Epoch 95 Batch 0: Train Loss = 0.8483
2023-11-08 10:07:23,728 - __main__ - INFO - Fold 4, mse = 35.6749, mad = 4.4757
2023-11-08 10:07:23,998 - __main__ - INFO - Fold 4 Epoch 96 Batch 0: Train Loss = 1.0417
2023-11-08 10:07:25,468 - __main__ - INFO - Fold 4, mse = 35.8882, mad = 4.6006
2023-11-08 10:07:25,784 - __main__ - INFO - Fold 4 Epoch 97 Batch 0: Train Loss = 0.9072
2023-11-08 10:07:27,225 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9803 ------------
2023-11-08 10:07:27,227 - __main__ - INFO - Fold 4, mse = 35.6022, mad = 4.5588
2023-11-08 10:07:27,465 - __main__ - INFO - Fold 4 Epoch 98 Batch 0: Train Loss = 1.3143
2023-11-08 10:07:28,858 - __main__ - INFO - Fold 4, mse = 35.7146, mad = 4.5229
2023-11-08 10:07:29,146 - __main__ - INFO - Fold 4 Epoch 99 Batch 0: Train Loss = 1.0322
2023-11-08 10:07:30,961 - __main__ - INFO - Fold 4, mse = 36.5632, mad = 4.6663
2023-11-08 10:07:31,265 - __main__ - INFO - Fold 4 Epoch 100 Batch 0: Train Loss = 0.8294
2023-11-08 10:07:32,833 - __main__ - INFO - Fold 4, epoch 100: Loss = 1.0891 Valid loss = 1.7486 MSE = 36.6602 AUROC = 0.9796
2023-11-08 10:07:32,836 - __main__ - INFO - Fold 4, mse = 36.6602, mad = 4.6357
2023-11-08 10:07:33,092 - __main__ - INFO - Fold 4 Epoch 101 Batch 0: Train Loss = 1.1114
2023-11-08 10:07:34,324 - __main__ - INFO - Fold 4, mse = 36.4317, mad = 4.5997
2023-11-08 10:07:34,525 - __main__ - INFO - Fold 4 Epoch 102 Batch 0: Train Loss = 0.9416
2023-11-08 10:07:35,976 - __main__ - INFO - Fold 4, mse = 36.4384, mad = 4.6488
2023-11-08 10:07:36,248 - __main__ - INFO - Fold 4 Epoch 103 Batch 0: Train Loss = 0.9177
2023-11-08 10:07:37,810 - __main__ - INFO - Fold 4, mse = 36.3831, mad = 4.6255
2023-11-08 10:07:38,045 - __main__ - INFO - Fold 4 Epoch 104 Batch 0: Train Loss = 0.8626
2023-11-08 10:07:39,471 - __main__ - INFO - Fold 4, mse = 36.1589, mad = 4.5685
2023-11-08 10:07:39,728 - __main__ - INFO - Fold 4 Epoch 105 Batch 0: Train Loss = 1.0733
2023-11-08 10:07:41,228 - __main__ - INFO - Fold 4, mse = 35.8210, mad = 4.5571
2023-11-08 10:07:41,472 - __main__ - INFO - Fold 4 Epoch 106 Batch 0: Train Loss = 0.8717
2023-11-08 10:07:42,943 - __main__ - INFO - Fold 4, mse = 35.2933, mad = 4.5175
2023-11-08 10:07:43,280 - __main__ - INFO - Fold 4 Epoch 107 Batch 0: Train Loss = 0.9470
2023-11-08 10:07:44,819 - __main__ - INFO - Fold 4, mse = 35.2163, mad = 4.4830
2023-11-08 10:07:45,058 - __main__ - INFO - Fold 4 Epoch 108 Batch 0: Train Loss = 0.8947
2023-11-08 10:07:46,755 - __main__ - INFO - Fold 4, mse = 35.4282, mad = 4.5350
2023-11-08 10:07:47,039 - __main__ - INFO - Fold 4 Epoch 109 Batch 0: Train Loss = 1.1091
2023-11-08 10:07:48,554 - __main__ - INFO - Fold 4, mse = 35.8756, mad = 4.5726
2023-11-08 10:07:48,814 - __main__ - INFO - Fold 4 Epoch 110 Batch 0: Train Loss = 0.8702
2023-11-08 10:07:50,243 - __main__ - INFO - Fold 4, epoch 110: Loss = 1.0327 Valid loss = 1.7603 MSE = 36.0266 AUROC = 0.9777
2023-11-08 10:07:50,245 - __main__ - INFO - Fold 4, mse = 36.0266, mad = 4.5922
2023-11-08 10:07:50,521 - __main__ - INFO - Fold 4 Epoch 111 Batch 0: Train Loss = 1.1081
2023-11-08 10:07:52,137 - __main__ - INFO - Fold 4, mse = 35.8428, mad = 4.5682
2023-11-08 10:07:52,391 - __main__ - INFO - Fold 4 Epoch 112 Batch 0: Train Loss = 1.0165
2023-11-08 10:07:54,191 - __main__ - INFO - Fold 4, mse = 35.6009, mad = 4.4538
2023-11-08 10:07:54,476 - __main__ - INFO - Fold 4 Epoch 113 Batch 0: Train Loss = 0.9702
2023-11-08 10:07:56,092 - __main__ - INFO - Fold 4, mse = 36.0197, mad = 4.5678
2023-11-08 10:07:56,312 - __main__ - INFO - Fold 4 Epoch 114 Batch 0: Train Loss = 0.9163
2023-11-08 10:07:57,682 - __main__ - INFO - Fold 4, mse = 36.2645, mad = 4.5851
2023-11-08 10:07:57,917 - __main__ - INFO - Fold 4 Epoch 115 Batch 0: Train Loss = 1.1059
2023-11-08 10:07:59,490 - __main__ - INFO - Fold 4, mse = 36.4351, mad = 4.6065
2023-11-08 10:07:59,751 - __main__ - INFO - Fold 4 Epoch 116 Batch 0: Train Loss = 0.9419
2023-11-08 10:08:01,102 - __main__ - INFO - Fold 4, mse = 35.9933, mad = 4.5510
2023-11-08 10:08:01,348 - __main__ - INFO - Fold 4 Epoch 117 Batch 0: Train Loss = 1.0553
2023-11-08 10:08:02,786 - __main__ - INFO - Fold 4, mse = 35.9394, mad = 4.5569
2023-11-08 10:08:02,998 - __main__ - INFO - Fold 4 Epoch 118 Batch 0: Train Loss = 0.8635
2023-11-08 10:08:04,452 - __main__ - INFO - Fold 4, mse = 36.1466, mad = 4.5763
2023-11-08 10:08:04,705 - __main__ - INFO - Fold 4 Epoch 119 Batch 0: Train Loss = 0.8295
2023-11-08 10:08:06,139 - __main__ - INFO - Fold 4, mse = 36.8267, mad = 4.6895
2023-11-08 10:08:06,386 - __main__ - INFO - Fold 4 Epoch 120 Batch 0: Train Loss = 0.9825
2023-11-08 10:08:07,918 - __main__ - INFO - Fold 4, epoch 120: Loss = 1.0137 Valid loss = 1.7577 MSE = 36.2240 AUROC = 0.9783
2023-11-08 10:08:07,920 - __main__ - INFO - Fold 4, mse = 36.2240, mad = 4.5574
2023-11-08 10:08:08,172 - __main__ - INFO - Fold 4 Epoch 121 Batch 0: Train Loss = 0.9666
2023-11-08 10:08:09,946 - __main__ - INFO - Fold 4, mse = 36.5777, mad = 4.6641
2023-11-08 10:08:10,189 - __main__ - INFO - Fold 4 Epoch 122 Batch 0: Train Loss = 1.0137
2023-11-08 10:08:11,852 - __main__ - INFO - Fold 4, mse = 36.5100, mad = 4.6175
2023-11-08 10:08:12,074 - __main__ - INFO - Fold 4 Epoch 123 Batch 0: Train Loss = 1.0221
2023-11-08 10:08:13,747 - __main__ - INFO - Fold 4, mse = 36.6141, mad = 4.5879
2023-11-08 10:08:14,035 - __main__ - INFO - Fold 4 Epoch 124 Batch 0: Train Loss = 0.8017
2023-11-08 10:08:15,419 - __main__ - INFO - Fold 4, mse = 36.9103, mad = 4.6716
2023-11-08 10:08:15,715 - __main__ - INFO - Fold 4 Epoch 125 Batch 0: Train Loss = 0.9460
2023-11-08 10:08:17,297 - __main__ - INFO - Fold 4, mse = 36.5997, mad = 4.5984
2023-11-08 10:08:17,630 - __main__ - INFO - Fold 4 Epoch 126 Batch 0: Train Loss = 0.9471
2023-11-08 10:08:19,165 - __main__ - INFO - Fold 4, mse = 36.3294, mad = 4.6039
2023-11-08 10:08:19,379 - __main__ - INFO - Fold 4 Epoch 127 Batch 0: Train Loss = 0.9823
2023-11-08 10:08:20,867 - __main__ - INFO - Fold 4, mse = 36.0601, mad = 4.5518
2023-11-08 10:08:21,113 - __main__ - INFO - Fold 4 Epoch 128 Batch 0: Train Loss = 0.9920
2023-11-08 10:08:22,531 - __main__ - INFO - Fold 4, mse = 36.4423, mad = 4.6298
2023-11-08 10:08:22,848 - __main__ - INFO - Fold 4 Epoch 129 Batch 0: Train Loss = 0.9869
2023-11-08 10:08:24,660 - __main__ - INFO - Fold 4, mse = 36.3667, mad = 4.5964
2023-11-08 10:08:24,964 - __main__ - INFO - Fold 4 Epoch 130 Batch 0: Train Loss = 0.8427
2023-11-08 10:08:26,570 - __main__ - INFO - Fold 4, epoch 130: Loss = 0.9260 Valid loss = 1.7920 MSE = 36.3953 AUROC = 0.9770
2023-11-08 10:08:26,574 - __main__ - INFO - Fold 4, mse = 36.3953, mad = 4.5959
2023-11-08 10:08:26,796 - __main__ - INFO - Fold 4 Epoch 131 Batch 0: Train Loss = 0.8565
2023-11-08 10:08:28,294 - __main__ - INFO - Fold 4, mse = 36.1413, mad = 4.5947
2023-11-08 10:08:28,598 - __main__ - INFO - Fold 4 Epoch 132 Batch 0: Train Loss = 1.0000
2023-11-08 10:08:30,345 - __main__ - INFO - Fold 4, mse = 35.8423, mad = 4.5518
2023-11-08 10:08:30,613 - __main__ - INFO - Fold 4 Epoch 133 Batch 0: Train Loss = 0.9119
2023-11-08 10:08:32,312 - __main__ - INFO - Fold 4, mse = 35.8637, mad = 4.5880
2023-11-08 10:08:32,536 - __main__ - INFO - Fold 4 Epoch 134 Batch 0: Train Loss = 1.1050
2023-11-08 10:08:34,188 - __main__ - INFO - Fold 4, mse = 35.8603, mad = 4.5834
2023-11-08 10:08:34,451 - __main__ - INFO - Fold 4 Epoch 135 Batch 0: Train Loss = 0.8976
2023-11-08 10:08:36,079 - __main__ - INFO - Fold 4, mse = 36.1268, mad = 4.5951
2023-11-08 10:08:36,286 - __main__ - INFO - Fold 4 Epoch 136 Batch 0: Train Loss = 0.8778
2023-11-08 10:08:37,944 - __main__ - INFO - Fold 4, mse = 36.3523, mad = 4.6530
2023-11-08 10:08:38,230 - __main__ - INFO - Fold 4 Epoch 137 Batch 0: Train Loss = 1.0589
2023-11-08 10:08:39,709 - __main__ - INFO - Fold 4, mse = 35.7184, mad = 4.5359
2023-11-08 10:08:39,983 - __main__ - INFO - Fold 4 Epoch 138 Batch 0: Train Loss = 0.9471
2023-11-08 10:08:41,666 - __main__ - INFO - Fold 4, mse = 35.7401, mad = 4.5469
2023-11-08 10:08:41,992 - __main__ - INFO - Fold 4 Epoch 139 Batch 0: Train Loss = 1.0736
2023-11-08 10:08:43,697 - __main__ - INFO - Fold 4, mse = 36.5251, mad = 4.6606
2023-11-08 10:08:44,010 - __main__ - INFO - Fold 4 Epoch 140 Batch 0: Train Loss = 0.7724
2023-11-08 10:08:45,671 - __main__ - INFO - Fold 4, epoch 140: Loss = 0.9496 Valid loss = 1.7647 MSE = 36.1641 AUROC = 0.9776
2023-11-08 10:08:45,673 - __main__ - INFO - Fold 4, mse = 36.1641, mad = 4.5614
2023-11-08 10:08:45,917 - __main__ - INFO - Fold 4 Epoch 141 Batch 0: Train Loss = 0.9778
2023-11-08 10:08:47,348 - __main__ - INFO - Fold 4, mse = 35.8517, mad = 4.5418
2023-11-08 10:08:47,558 - __main__ - INFO - Fold 4 Epoch 142 Batch 0: Train Loss = 0.9540
2023-11-08 10:08:49,109 - __main__ - INFO - Fold 4, mse = 35.7943, mad = 4.5362
2023-11-08 10:08:49,353 - __main__ - INFO - Fold 4 Epoch 143 Batch 0: Train Loss = 0.9686
2023-11-08 10:08:50,945 - __main__ - INFO - Fold 4, mse = 36.0841, mad = 4.5674
2023-11-08 10:08:51,224 - __main__ - INFO - Fold 4 Epoch 144 Batch 0: Train Loss = 0.8622
2023-11-08 10:08:52,701 - __main__ - INFO - Fold 4, mse = 36.3332, mad = 4.5676
2023-11-08 10:08:52,997 - __main__ - INFO - Fold 4 Epoch 145 Batch 0: Train Loss = 0.8357
2023-11-08 10:08:54,740 - __main__ - INFO - Fold 4, mse = 36.4268, mad = 4.6119
2023-11-08 10:08:54,996 - __main__ - INFO - Fold 4 Epoch 146 Batch 0: Train Loss = 0.9574
2023-11-08 10:08:56,526 - __main__ - INFO - Fold 4, mse = 36.3268, mad = 4.5397
2023-11-08 10:08:56,792 - __main__ - INFO - Fold 4 Epoch 147 Batch 0: Train Loss = 0.9562
2023-11-08 10:08:58,475 - __main__ - INFO - Fold 4, mse = 36.3172, mad = 4.5352
2023-11-08 10:08:58,756 - __main__ - INFO - Fold 4 Epoch 148 Batch 0: Train Loss = 0.9092
2023-11-08 10:09:00,382 - __main__ - INFO - Fold 4, mse = 36.8049, mad = 4.6512
2023-11-08 10:09:00,738 - __main__ - INFO - Fold 4 Epoch 149 Batch 0: Train Loss = 1.0999
2023-11-08 10:09:02,341 - __main__ - INFO - Fold 4, mse = 36.8548, mad = 4.5692
2023-11-08 10:09:02,769 - __main__ - INFO - Fold 5 Epoch 0 Batch 0: Train Loss = 2.2823
2023-11-08 10:09:04,011 - __main__ - INFO - Fold 5, epoch 0: Loss = 2.3382 Valid loss = 3.0719 MSE = 54.1724 AUROC = 0.8061
2023-11-08 10:09:04,014 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 54.1724 ------------
2023-11-08 10:09:04,182 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8061 ------------
2023-11-08 10:09:04,184 - __main__ - INFO - Fold 5, mse = 54.1724, mad = 5.2438
2023-11-08 10:09:04,425 - __main__ - INFO - Fold 5 Epoch 1 Batch 0: Train Loss = 2.5534
2023-11-08 10:09:05,827 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 53.0567 ------------
2023-11-08 10:09:05,984 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8903 ------------
2023-11-08 10:09:05,986 - __main__ - INFO - Fold 5, mse = 53.0567, mad = 5.1575
2023-11-08 10:09:06,252 - __main__ - INFO - Fold 5 Epoch 2 Batch 0: Train Loss = 2.4046
2023-11-08 10:09:07,670 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 52.1106 ------------
2023-11-08 10:09:07,837 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9112 ------------
2023-11-08 10:09:07,839 - __main__ - INFO - Fold 5, mse = 52.1106, mad = 5.0979
2023-11-08 10:09:08,117 - __main__ - INFO - Fold 5 Epoch 3 Batch 0: Train Loss = 2.3530
2023-11-08 10:09:09,649 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 51.3492 ------------
2023-11-08 10:09:09,849 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9155 ------------
2023-11-08 10:09:09,851 - __main__ - INFO - Fold 5, mse = 51.3492, mad = 5.0956
2023-11-08 10:09:10,144 - __main__ - INFO - Fold 5 Epoch 4 Batch 0: Train Loss = 1.8967
2023-11-08 10:09:11,629 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 50.8117 ------------
2023-11-08 10:09:11,791 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9206 ------------
2023-11-08 10:09:11,794 - __main__ - INFO - Fold 5, mse = 50.8117, mad = 5.1315
2023-11-08 10:09:12,077 - __main__ - INFO - Fold 5 Epoch 5 Batch 0: Train Loss = 2.1602
2023-11-08 10:09:13,646 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 50.3666 ------------
2023-11-08 10:09:13,817 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9296 ------------
2023-11-08 10:09:13,820 - __main__ - INFO - Fold 5, mse = 50.3666, mad = 5.1305
2023-11-08 10:09:14,081 - __main__ - INFO - Fold 5 Epoch 6 Batch 0: Train Loss = 2.0854
2023-11-08 10:09:15,481 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 49.7660 ------------
2023-11-08 10:09:15,644 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9411 ------------
2023-11-08 10:09:15,646 - __main__ - INFO - Fold 5, mse = 49.7660, mad = 5.0345
2023-11-08 10:09:15,886 - __main__ - INFO - Fold 5 Epoch 7 Batch 0: Train Loss = 2.0002
2023-11-08 10:09:17,227 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 49.3929 ------------
2023-11-08 10:09:17,423 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9535 ------------
2023-11-08 10:09:17,425 - __main__ - INFO - Fold 5, mse = 49.3929, mad = 5.0004
2023-11-08 10:09:17,700 - __main__ - INFO - Fold 5 Epoch 8 Batch 0: Train Loss = 1.5341
2023-11-08 10:09:19,120 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 49.0401 ------------
2023-11-08 10:09:19,290 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9670 ------------
2023-11-08 10:09:19,292 - __main__ - INFO - Fold 5, mse = 49.0401, mad = 4.9826
2023-11-08 10:09:19,591 - __main__ - INFO - Fold 5 Epoch 9 Batch 0: Train Loss = 1.9068
2023-11-08 10:09:21,175 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 48.7113 ------------
2023-11-08 10:09:21,334 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9741 ------------
2023-11-08 10:09:21,337 - __main__ - INFO - Fold 5, mse = 48.7113, mad = 4.9643
2023-11-08 10:09:21,618 - __main__ - INFO - Fold 5 Epoch 10 Batch 0: Train Loss = 1.7682
2023-11-08 10:09:23,207 - __main__ - INFO - Fold 5, epoch 10: Loss = 1.7817 Valid loss = 2.5801 MSE = 48.3432 AUROC = 0.9795
2023-11-08 10:09:23,209 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 48.3432 ------------
2023-11-08 10:09:23,350 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9795 ------------
2023-11-08 10:09:23,352 - __main__ - INFO - Fold 5, mse = 48.3432, mad = 4.8458
2023-11-08 10:09:23,554 - __main__ - INFO - Fold 5 Epoch 11 Batch 0: Train Loss = 1.9295
2023-11-08 10:09:25,126 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 48.0243 ------------
2023-11-08 10:09:25,290 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9816 ------------
2023-11-08 10:09:25,291 - __main__ - INFO - Fold 5, mse = 48.0243, mad = 4.7853
2023-11-08 10:09:25,542 - __main__ - INFO - Fold 5 Epoch 12 Batch 0: Train Loss = 1.6003
2023-11-08 10:09:27,135 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 47.9156 ------------
2023-11-08 10:09:27,334 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9823 ------------
2023-11-08 10:09:27,338 - __main__ - INFO - Fold 5, mse = 47.9156, mad = 4.8369
2023-11-08 10:09:27,646 - __main__ - INFO - Fold 5 Epoch 13 Batch 0: Train Loss = 1.8047
2023-11-08 10:09:28,954 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 47.8949 ------------
2023-11-08 10:09:29,120 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9834 ------------
2023-11-08 10:09:29,122 - __main__ - INFO - Fold 5, mse = 47.8949, mad = 4.8657
2023-11-08 10:09:29,346 - __main__ - INFO - Fold 5 Epoch 14 Batch 0: Train Loss = 1.7984
2023-11-08 10:09:30,637 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 47.7933 ------------
2023-11-08 10:09:30,801 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9843 ------------
2023-11-08 10:09:30,803 - __main__ - INFO - Fold 5, mse = 47.7933, mad = 4.7347
2023-11-08 10:09:31,033 - __main__ - INFO - Fold 5 Epoch 15 Batch 0: Train Loss = 1.4087
2023-11-08 10:09:32,805 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 47.7135 ------------
2023-11-08 10:09:32,978 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9850 ------------
2023-11-08 10:09:32,980 - __main__ - INFO - Fold 5, mse = 47.7135, mad = 4.7424
2023-11-08 10:09:33,262 - __main__ - INFO - Fold 5 Epoch 16 Batch 0: Train Loss = 1.4791
2023-11-08 10:09:34,794 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 47.5191 ------------
2023-11-08 10:09:34,953 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9852 ------------
2023-11-08 10:09:34,955 - __main__ - INFO - Fold 5, mse = 47.5191, mad = 4.7904
2023-11-08 10:09:35,183 - __main__ - INFO - Fold 5 Epoch 17 Batch 0: Train Loss = 1.3639
2023-11-08 10:09:36,521 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 47.3017 ------------
2023-11-08 10:09:36,749 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9855 ------------
2023-11-08 10:09:36,752 - __main__ - INFO - Fold 5, mse = 47.3017, mad = 4.7213
2023-11-08 10:09:37,031 - __main__ - INFO - Fold 5 Epoch 18 Batch 0: Train Loss = 1.6313
2023-11-08 10:09:38,593 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 47.1989 ------------
2023-11-08 10:09:38,747 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9858 ------------
2023-11-08 10:09:38,750 - __main__ - INFO - Fold 5, mse = 47.1989, mad = 4.7069
2023-11-08 10:09:39,035 - __main__ - INFO - Fold 5 Epoch 19 Batch 0: Train Loss = 1.3567
2023-11-08 10:09:40,474 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 46.9749 ------------
2023-11-08 10:09:40,628 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9862 ------------
2023-11-08 10:09:40,630 - __main__ - INFO - Fold 5, mse = 46.9749, mad = 4.6440
2023-11-08 10:09:40,869 - __main__ - INFO - Fold 5 Epoch 20 Batch 0: Train Loss = 1.2293
2023-11-08 10:09:42,514 - __main__ - INFO - Fold 5, epoch 20: Loss = 1.5048 Valid loss = 2.4380 MSE = 46.8553 AUROC = 0.9862
2023-11-08 10:09:42,516 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 46.8553 ------------
2023-11-08 10:09:42,706 - __main__ - INFO - Fold 5, mse = 46.8553, mad = 4.6534
2023-11-08 10:09:43,028 - __main__ - INFO - Fold 5 Epoch 21 Batch 0: Train Loss = 1.3278
2023-11-08 10:09:44,562 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 46.8176 ------------
2023-11-08 10:09:44,739 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9863 ------------
2023-11-08 10:09:44,742 - __main__ - INFO - Fold 5, mse = 46.8176, mad = 4.6672
2023-11-08 10:09:44,989 - __main__ - INFO - Fold 5 Epoch 22 Batch 0: Train Loss = 1.4633
2023-11-08 10:09:46,415 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 46.6669 ------------
2023-11-08 10:09:46,565 - __main__ - INFO - Fold 5, mse = 46.6669, mad = 4.6513
2023-11-08 10:09:46,776 - __main__ - INFO - Fold 5 Epoch 23 Batch 0: Train Loss = 1.0842
2023-11-08 10:09:47,938 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 46.5433 ------------
2023-11-08 10:09:48,093 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9864 ------------
2023-11-08 10:09:48,095 - __main__ - INFO - Fold 5, mse = 46.5433, mad = 4.6740
2023-11-08 10:09:48,417 - __main__ - INFO - Fold 5 Epoch 24 Batch 0: Train Loss = 1.3963
2023-11-08 10:09:50,032 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 46.1241 ------------
2023-11-08 10:09:50,174 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9864 ------------
2023-11-08 10:09:50,176 - __main__ - INFO - Fold 5, mse = 46.1241, mad = 4.6304
2023-11-08 10:09:50,390 - __main__ - INFO - Fold 5 Epoch 25 Batch 0: Train Loss = 1.3164
2023-11-08 10:09:51,686 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.9702 ------------
2023-11-08 10:09:51,839 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9866 ------------
2023-11-08 10:09:51,841 - __main__ - INFO - Fold 5, mse = 45.9702, mad = 4.6357
2023-11-08 10:09:52,159 - __main__ - INFO - Fold 5 Epoch 26 Batch 0: Train Loss = 1.4184
2023-11-08 10:09:53,568 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9869 ------------
2023-11-08 10:09:53,570 - __main__ - INFO - Fold 5, mse = 46.1307, mad = 4.6786
2023-11-08 10:09:53,780 - __main__ - INFO - Fold 5 Epoch 27 Batch 0: Train Loss = 1.2548
2023-11-08 10:09:55,371 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9870 ------------
2023-11-08 10:09:55,373 - __main__ - INFO - Fold 5, mse = 46.3770, mad = 4.5777
2023-11-08 10:09:55,635 - __main__ - INFO - Fold 5 Epoch 28 Batch 0: Train Loss = 1.2955
2023-11-08 10:09:57,238 - __main__ - INFO - Fold 5, mse = 46.4998, mad = 4.6777
2023-11-08 10:09:57,513 - __main__ - INFO - Fold 5 Epoch 29 Batch 0: Train Loss = 1.2521
2023-11-08 10:09:59,012 - __main__ - INFO - Fold 5, mse = 46.2464, mad = 4.5661
2023-11-08 10:09:59,316 - __main__ - INFO - Fold 5 Epoch 30 Batch 0: Train Loss = 1.2103
2023-11-08 10:10:00,834 - __main__ - INFO - Fold 5, epoch 30: Loss = 1.3489 Valid loss = 2.3901 MSE = 46.0955 AUROC = 0.9868
2023-11-08 10:10:00,838 - __main__ - INFO - Fold 5, mse = 46.0955, mad = 4.5908
2023-11-08 10:10:01,032 - __main__ - INFO - Fold 5 Epoch 31 Batch 0: Train Loss = 0.9767
2023-11-08 10:10:02,647 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9872 ------------
2023-11-08 10:10:02,663 - __main__ - INFO - Fold 5, mse = 46.2802, mad = 4.7538
2023-11-08 10:10:03,021 - __main__ - INFO - Fold 5 Epoch 32 Batch 0: Train Loss = 1.1250
2023-11-08 10:10:04,761 - __main__ - INFO - Fold 5, mse = 46.1798, mad = 4.5314
2023-11-08 10:10:05,045 - __main__ - INFO - Fold 5 Epoch 33 Batch 0: Train Loss = 1.4168
2023-11-08 10:10:06,675 - __main__ - INFO - Fold 5, mse = 46.0138, mad = 4.6635
2023-11-08 10:10:06,905 - __main__ - INFO - Fold 5 Epoch 34 Batch 0: Train Loss = 1.1446
2023-11-08 10:10:08,348 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.8400 ------------
2023-11-08 10:10:08,547 - __main__ - INFO - Fold 5, mse = 45.8400, mad = 4.6023
2023-11-08 10:10:08,789 - __main__ - INFO - Fold 5 Epoch 35 Batch 0: Train Loss = 1.1481
2023-11-08 10:10:10,475 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.8022 ------------
2023-11-08 10:10:10,606 - __main__ - INFO - Fold 5, mse = 45.8022, mad = 4.5533
2023-11-08 10:10:10,857 - __main__ - INFO - Fold 5 Epoch 36 Batch 0: Train Loss = 1.5404
2023-11-08 10:10:12,475 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.7062 ------------
2023-11-08 10:10:12,666 - __main__ - INFO - Fold 5, mse = 45.7062, mad = 4.6140
2023-11-08 10:10:12,948 - __main__ - INFO - Fold 5 Epoch 37 Batch 0: Train Loss = 0.9629
2023-11-08 10:10:14,495 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.6130 ------------
2023-11-08 10:10:14,637 - __main__ - INFO - Fold 5, mse = 45.6130, mad = 4.6164
2023-11-08 10:10:14,859 - __main__ - INFO - Fold 5 Epoch 38 Batch 0: Train Loss = 1.2171
2023-11-08 10:10:16,672 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.5461 ------------
2023-11-08 10:10:16,844 - __main__ - INFO - Fold 5, mse = 45.5461, mad = 4.5871
2023-11-08 10:10:17,142 - __main__ - INFO - Fold 5 Epoch 39 Batch 0: Train Loss = 1.1428
2023-11-08 10:10:18,877 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.4321 ------------
2023-11-08 10:10:19,016 - __main__ - INFO - Fold 5, mse = 45.4321, mad = 4.5878
2023-11-08 10:10:19,307 - __main__ - INFO - Fold 5 Epoch 40 Batch 0: Train Loss = 1.0953
2023-11-08 10:10:21,027 - __main__ - INFO - Fold 5, epoch 40: Loss = 1.2141 Valid loss = 2.3608 MSE = 45.4417 AUROC = 0.9867
2023-11-08 10:10:21,029 - __main__ - INFO - Fold 5, mse = 45.4417, mad = 4.6268
2023-11-08 10:10:21,321 - __main__ - INFO - Fold 5 Epoch 41 Batch 0: Train Loss = 1.1033
2023-11-08 10:10:23,086 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.1495 ------------
2023-11-08 10:10:23,256 - __main__ - INFO - Fold 5, mse = 45.1495, mad = 4.5464
2023-11-08 10:10:23,588 - __main__ - INFO - Fold 5 Epoch 42 Batch 0: Train Loss = 1.1655
2023-11-08 10:10:25,191 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.0990 ------------
2023-11-08 10:10:25,354 - __main__ - INFO - Fold 5, mse = 45.0990, mad = 4.5808
2023-11-08 10:10:25,738 - __main__ - INFO - Fold 5 Epoch 43 Batch 0: Train Loss = 1.1656
2023-11-08 10:10:27,058 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.0665 ------------
2023-11-08 10:10:27,214 - __main__ - INFO - Fold 5, mse = 45.0665, mad = 4.6006
2023-11-08 10:10:27,440 - __main__ - INFO - Fold 5 Epoch 44 Batch 0: Train Loss = 1.2865
2023-11-08 10:10:28,934 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.0534 ------------
2023-11-08 10:10:29,124 - __main__ - INFO - Fold 5, mse = 45.0534, mad = 4.5080
2023-11-08 10:10:29,408 - __main__ - INFO - Fold 5 Epoch 45 Batch 0: Train Loss = 1.0368
2023-11-08 10:10:30,729 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.0147 ------------
2023-11-08 10:10:30,885 - __main__ - INFO - Fold 5, mse = 45.0147, mad = 4.6016
2023-11-08 10:10:31,128 - __main__ - INFO - Fold 5 Epoch 46 Batch 0: Train Loss = 1.2026
2023-11-08 10:10:32,681 - __main__ - INFO - Fold 5, mse = 45.1390, mad = 4.5961
2023-11-08 10:10:32,921 - __main__ - INFO - Fold 5 Epoch 47 Batch 0: Train Loss = 1.1743
2023-11-08 10:10:34,370 - __main__ - INFO - Fold 5, mse = 45.0486, mad = 4.5532
2023-11-08 10:10:34,663 - __main__ - INFO - Fold 5 Epoch 48 Batch 0: Train Loss = 1.1071
2023-11-08 10:10:36,429 - __main__ - INFO - Fold 5, mse = 45.1035, mad = 4.5539
2023-11-08 10:10:36,680 - __main__ - INFO - Fold 5 Epoch 49 Batch 0: Train Loss = 1.1550
2023-11-08 10:10:38,312 - __main__ - INFO - Fold 5, mse = 45.2474, mad = 4.5843
2023-11-08 10:10:38,578 - __main__ - INFO - Fold 5 Epoch 50 Batch 0: Train Loss = 1.2755
2023-11-08 10:10:40,279 - __main__ - INFO - Fold 5, epoch 50: Loss = 1.1627 Valid loss = 2.4114 MSE = 45.2845 AUROC = 0.9855
2023-11-08 10:10:40,281 - __main__ - INFO - Fold 5, mse = 45.2845, mad = 4.6038
2023-11-08 10:10:40,623 - __main__ - INFO - Fold 5 Epoch 51 Batch 0: Train Loss = 1.1769
2023-11-08 10:10:42,367 - __main__ - INFO - Fold 5, mse = 45.2264, mad = 4.5519
2023-11-08 10:10:42,607 - __main__ - INFO - Fold 5 Epoch 52 Batch 0: Train Loss = 1.1468
2023-11-08 10:10:43,965 - __main__ - INFO - Fold 5, mse = 45.2790, mad = 4.6307
2023-11-08 10:10:44,242 - __main__ - INFO - Fold 5 Epoch 53 Batch 0: Train Loss = 1.1514
2023-11-08 10:10:45,556 - __main__ - INFO - Fold 5, mse = 45.4270, mad = 4.5994
2023-11-08 10:10:45,808 - __main__ - INFO - Fold 5 Epoch 54 Batch 0: Train Loss = 0.9840
2023-11-08 10:10:47,358 - __main__ - INFO - Fold 5, mse = 45.4387, mad = 4.5844
2023-11-08 10:10:47,653 - __main__ - INFO - Fold 5 Epoch 55 Batch 0: Train Loss = 1.2306
2023-11-08 10:10:49,338 - __main__ - INFO - Fold 5, mse = 45.4518, mad = 4.6276
2023-11-08 10:10:49,618 - __main__ - INFO - Fold 5 Epoch 56 Batch 0: Train Loss = 1.0472
2023-11-08 10:10:51,321 - __main__ - INFO - Fold 5, mse = 45.3097, mad = 4.5737
2023-11-08 10:10:51,632 - __main__ - INFO - Fold 5 Epoch 57 Batch 0: Train Loss = 1.1244
2023-11-08 10:10:53,405 - __main__ - INFO - Fold 5, mse = 45.4125, mad = 4.6479
2023-11-08 10:10:53,680 - __main__ - INFO - Fold 5 Epoch 58 Batch 0: Train Loss = 1.3071
2023-11-08 10:10:55,475 - __main__ - INFO - Fold 5, mse = 45.2663, mad = 4.5566
2023-11-08 10:10:55,748 - __main__ - INFO - Fold 5 Epoch 59 Batch 0: Train Loss = 1.3265
2023-11-08 10:10:57,351 - __main__ - INFO - Fold 5, mse = 45.4735, mad = 4.6324
2023-11-08 10:10:57,618 - __main__ - INFO - Fold 5 Epoch 60 Batch 0: Train Loss = 1.2660
2023-11-08 10:10:59,388 - __main__ - INFO - Fold 5, epoch 60: Loss = 1.0962 Valid loss = 2.4348 MSE = 45.3401 AUROC = 0.9849
2023-11-08 10:10:59,389 - __main__ - INFO - Fold 5, mse = 45.3401, mad = 4.5515
2023-11-08 10:10:59,638 - __main__ - INFO - Fold 5 Epoch 61 Batch 0: Train Loss = 1.0693
2023-11-08 10:11:01,304 - __main__ - INFO - Fold 5, mse = 45.3278, mad = 4.6080
2023-11-08 10:11:01,538 - __main__ - INFO - Fold 5 Epoch 62 Batch 0: Train Loss = 1.1597
2023-11-08 10:11:02,911 - __main__ - INFO - Fold 5, mse = 45.2974, mad = 4.5768
2023-11-08 10:11:03,129 - __main__ - INFO - Fold 5 Epoch 63 Batch 0: Train Loss = 1.0805
2023-11-08 10:11:04,361 - __main__ - INFO - Fold 5, mse = 45.3747, mad = 4.5926
2023-11-08 10:11:04,646 - __main__ - INFO - Fold 5 Epoch 64 Batch 0: Train Loss = 1.0113
2023-11-08 10:11:06,129 - __main__ - INFO - Fold 5, mse = 45.2400, mad = 4.5554
2023-11-08 10:11:06,391 - __main__ - INFO - Fold 5 Epoch 65 Batch 0: Train Loss = 1.0374
2023-11-08 10:11:08,174 - __main__ - INFO - Fold 5, mse = 45.2073, mad = 4.5970
2023-11-08 10:11:08,477 - __main__ - INFO - Fold 5 Epoch 66 Batch 0: Train Loss = 0.9925
2023-11-08 10:11:10,004 - __main__ - INFO - Fold 5, mse = 45.1656, mad = 4.6113
2023-11-08 10:11:10,224 - __main__ - INFO - Fold 5 Epoch 67 Batch 0: Train Loss = 1.0952
2023-11-08 10:11:11,658 - __main__ - INFO - Fold 5, mse = 45.1063, mad = 4.5202
2023-11-08 10:11:11,944 - __main__ - INFO - Fold 5 Epoch 68 Batch 0: Train Loss = 0.9551
2023-11-08 10:11:13,355 - __main__ - INFO - Fold 5, mse = 45.2448, mad = 4.6375
2023-11-08 10:11:13,641 - __main__ - INFO - Fold 5 Epoch 69 Batch 0: Train Loss = 0.9924
2023-11-08 10:11:15,237 - __main__ - INFO - Fold 5, mse = 45.0779, mad = 4.5477
2023-11-08 10:11:15,488 - __main__ - INFO - Fold 5 Epoch 70 Batch 0: Train Loss = 1.0896
2023-11-08 10:11:16,914 - __main__ - INFO - Fold 5, epoch 70: Loss = 1.0629 Valid loss = 2.4532 MSE = 45.1605 AUROC = 0.9847
2023-11-08 10:11:16,916 - __main__ - INFO - Fold 5, mse = 45.1605, mad = 4.6633
2023-11-08 10:11:17,166 - __main__ - INFO - Fold 5 Epoch 71 Batch 0: Train Loss = 0.9235
2023-11-08 10:11:18,474 - __main__ - INFO - Fold 5, mse = 45.0208, mad = 4.6140
2023-11-08 10:11:18,740 - __main__ - INFO - Fold 5 Epoch 72 Batch 0: Train Loss = 1.0780
2023-11-08 10:11:20,394 - __main__ - INFO - Fold 5, mse = 45.0899, mad = 4.6406
2023-11-08 10:11:20,615 - __main__ - INFO - Fold 5 Epoch 73 Batch 0: Train Loss = 1.0483
2023-11-08 10:11:22,132 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.8889 ------------
2023-11-08 10:11:22,290 - __main__ - INFO - Fold 5, mse = 44.8889, mad = 4.5999
2023-11-08 10:11:22,524 - __main__ - INFO - Fold 5 Epoch 74 Batch 0: Train Loss = 0.9615
2023-11-08 10:11:23,981 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.5125 ------------
2023-11-08 10:11:24,135 - __main__ - INFO - Fold 5, mse = 44.5125, mad = 4.5268
2023-11-08 10:11:24,386 - __main__ - INFO - Fold 5 Epoch 75 Batch 0: Train Loss = 1.1035
2023-11-08 10:11:25,768 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.3076 ------------
2023-11-08 10:11:25,923 - __main__ - INFO - Fold 5, mse = 44.3076, mad = 4.5868
2023-11-08 10:11:26,137 - __main__ - INFO - Fold 5 Epoch 76 Batch 0: Train Loss = 1.0360
2023-11-08 10:11:27,510 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.2515 ------------
2023-11-08 10:11:27,710 - __main__ - INFO - Fold 5, mse = 44.2515, mad = 4.5601
2023-11-08 10:11:27,944 - __main__ - INFO - Fold 5 Epoch 77 Batch 0: Train Loss = 1.0732
2023-11-08 10:11:29,556 - __main__ - INFO - Fold 5, mse = 44.2825, mad = 4.5822
2023-11-08 10:11:29,802 - __main__ - INFO - Fold 5 Epoch 78 Batch 0: Train Loss = 1.0543
2023-11-08 10:11:31,495 - __main__ - INFO - Fold 5, mse = 44.2604, mad = 4.5876
2023-11-08 10:11:31,755 - __main__ - INFO - Fold 5 Epoch 79 Batch 0: Train Loss = 0.8773
2023-11-08 10:11:33,197 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.2224 ------------
2023-11-08 10:11:33,345 - __main__ - INFO - Fold 5, mse = 44.2224, mad = 4.5485
2023-11-08 10:11:33,591 - __main__ - INFO - Fold 5 Epoch 80 Batch 0: Train Loss = 1.0182
2023-11-08 10:11:35,303 - __main__ - INFO - Fold 5, epoch 80: Loss = 1.0645 Valid loss = 2.4351 MSE = 44.4095 AUROC = 0.9838
2023-11-08 10:11:35,305 - __main__ - INFO - Fold 5, mse = 44.4095, mad = 4.5788
2023-11-08 10:11:35,647 - __main__ - INFO - Fold 5 Epoch 81 Batch 0: Train Loss = 1.0446
2023-11-08 10:11:37,105 - __main__ - INFO - Fold 5, mse = 44.4776, mad = 4.5739
2023-11-08 10:11:37,348 - __main__ - INFO - Fold 5 Epoch 82 Batch 0: Train Loss = 1.1285
2023-11-08 10:11:39,113 - __main__ - INFO - Fold 5, mse = 44.3946, mad = 4.5746
2023-11-08 10:11:39,361 - __main__ - INFO - Fold 5 Epoch 83 Batch 0: Train Loss = 0.9057
2023-11-08 10:11:40,931 - __main__ - INFO - Fold 5, mse = 44.3254, mad = 4.6103
2023-11-08 10:11:41,186 - __main__ - INFO - Fold 5 Epoch 84 Batch 0: Train Loss = 0.8447
2023-11-08 10:11:42,791 - __main__ - INFO - Fold 5, mse = 44.3044, mad = 4.5830
2023-11-08 10:11:43,032 - __main__ - INFO - Fold 5 Epoch 85 Batch 0: Train Loss = 1.0452
2023-11-08 10:11:44,454 - __main__ - INFO - Fold 5, mse = 44.5601, mad = 4.5900
2023-11-08 10:11:44,753 - __main__ - INFO - Fold 5 Epoch 86 Batch 0: Train Loss = 0.9780
2023-11-08 10:11:46,232 - __main__ - INFO - Fold 5, mse = 44.7484, mad = 4.5972
2023-11-08 10:11:46,493 - __main__ - INFO - Fold 5 Epoch 87 Batch 0: Train Loss = 1.0714
2023-11-08 10:11:48,043 - __main__ - INFO - Fold 5, mse = 44.8854, mad = 4.5515
2023-11-08 10:11:48,272 - __main__ - INFO - Fold 5 Epoch 88 Batch 0: Train Loss = 1.0021
2023-11-08 10:11:49,904 - __main__ - INFO - Fold 5, mse = 45.1732, mad = 4.7243
2023-11-08 10:11:50,160 - __main__ - INFO - Fold 5 Epoch 89 Batch 0: Train Loss = 1.0013
2023-11-08 10:11:51,698 - __main__ - INFO - Fold 5, mse = 44.8654, mad = 4.5375
2023-11-08 10:11:51,992 - __main__ - INFO - Fold 5 Epoch 90 Batch 0: Train Loss = 1.0380
2023-11-08 10:11:53,457 - __main__ - INFO - Fold 5, epoch 90: Loss = 0.9713 Valid loss = 2.4703 MSE = 44.8877 AUROC = 0.9840
2023-11-08 10:11:53,459 - __main__ - INFO - Fold 5, mse = 44.8877, mad = 4.6219
2023-11-08 10:11:53,656 - __main__ - INFO - Fold 5 Epoch 91 Batch 0: Train Loss = 0.9602
2023-11-08 10:11:55,049 - __main__ - INFO - Fold 5, mse = 44.5985, mad = 4.5631
2023-11-08 10:11:55,332 - __main__ - INFO - Fold 5 Epoch 92 Batch 0: Train Loss = 0.8777
2023-11-08 10:11:56,877 - __main__ - INFO - Fold 5, mse = 44.4392, mad = 4.5728
2023-11-08 10:11:57,151 - __main__ - INFO - Fold 5 Epoch 93 Batch 0: Train Loss = 0.9962
2023-11-08 10:11:58,693 - __main__ - INFO - Fold 5, mse = 44.5964, mad = 4.6800
2023-11-08 10:11:59,001 - __main__ - INFO - Fold 5 Epoch 94 Batch 0: Train Loss = 0.9502
2023-11-08 10:12:00,669 - __main__ - INFO - Fold 5, mse = 44.4323, mad = 4.5637
2023-11-08 10:12:00,957 - __main__ - INFO - Fold 5 Epoch 95 Batch 0: Train Loss = 1.0992
2023-11-08 10:12:02,471 - __main__ - INFO - Fold 5, mse = 44.3894, mad = 4.6310
2023-11-08 10:12:02,709 - __main__ - INFO - Fold 5 Epoch 96 Batch 0: Train Loss = 1.1943
2023-11-08 10:12:04,372 - __main__ - INFO - Fold 5, mse = 44.2245, mad = 4.5509
2023-11-08 10:12:04,651 - __main__ - INFO - Fold 5 Epoch 97 Batch 0: Train Loss = 0.9380
2023-11-08 10:12:06,515 - __main__ - INFO - Fold 5, mse = 44.3610, mad = 4.6269
2023-11-08 10:12:06,801 - __main__ - INFO - Fold 5 Epoch 98 Batch 0: Train Loss = 1.1476
2023-11-08 10:12:08,165 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.1893 ------------
2023-11-08 10:12:08,388 - __main__ - INFO - Fold 5, mse = 44.1893, mad = 4.5667
2023-11-08 10:12:08,640 - __main__ - INFO - Fold 5 Epoch 99 Batch 0: Train Loss = 0.9972
2023-11-08 10:12:10,372 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.1234 ------------
2023-11-08 10:12:10,610 - __main__ - INFO - Fold 5, mse = 44.1234, mad = 4.6450
2023-11-08 10:12:10,931 - __main__ - INFO - Fold 5 Epoch 100 Batch 0: Train Loss = 0.9959
2023-11-08 10:12:12,864 - __main__ - INFO - Fold 5, epoch 100: Loss = 1.0288 Valid loss = 2.4295 MSE = 43.9501 AUROC = 0.9854
2023-11-08 10:12:12,866 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.9501 ------------
2023-11-08 10:12:13,074 - __main__ - INFO - Fold 5, mse = 43.9501, mad = 4.5608
2023-11-08 10:12:13,330 - __main__ - INFO - Fold 5 Epoch 101 Batch 0: Train Loss = 0.9411
2023-11-08 10:12:15,073 - __main__ - INFO - Fold 5, mse = 44.3442, mad = 4.6361
2023-11-08 10:12:15,334 - __main__ - INFO - Fold 5 Epoch 102 Batch 0: Train Loss = 0.8945
2023-11-08 10:12:16,925 - __main__ - INFO - Fold 5, mse = 44.4509, mad = 4.5670
2023-11-08 10:12:17,207 - __main__ - INFO - Fold 5 Epoch 103 Batch 0: Train Loss = 1.0599
2023-11-08 10:12:18,835 - __main__ - INFO - Fold 5, mse = 44.3771, mad = 4.5983
2023-11-08 10:12:19,155 - __main__ - INFO - Fold 5 Epoch 104 Batch 0: Train Loss = 1.0528
2023-11-08 10:12:20,878 - __main__ - INFO - Fold 5, mse = 44.3362, mad = 4.5618
2023-11-08 10:12:21,203 - __main__ - INFO - Fold 5 Epoch 105 Batch 0: Train Loss = 1.1459
2023-11-08 10:12:22,599 - __main__ - INFO - Fold 5, mse = 44.3540, mad = 4.6036
2023-11-08 10:12:22,818 - __main__ - INFO - Fold 5 Epoch 106 Batch 0: Train Loss = 1.1897
2023-11-08 10:12:24,269 - __main__ - INFO - Fold 5, mse = 44.3441, mad = 4.6151
2023-11-08 10:12:24,620 - __main__ - INFO - Fold 5 Epoch 107 Batch 0: Train Loss = 1.1837
2023-11-08 10:12:26,264 - __main__ - INFO - Fold 5, mse = 44.3213, mad = 4.5868
2023-11-08 10:12:26,472 - __main__ - INFO - Fold 5 Epoch 108 Batch 0: Train Loss = 0.9087
2023-11-08 10:12:28,148 - __main__ - INFO - Fold 5, mse = 44.2547, mad = 4.6183
2023-11-08 10:12:28,436 - __main__ - INFO - Fold 5 Epoch 109 Batch 0: Train Loss = 1.0993
2023-11-08 10:12:29,953 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.9273 ------------
2023-11-08 10:12:30,203 - __main__ - INFO - Fold 5, mse = 43.9273, mad = 4.5830
2023-11-08 10:12:30,474 - __main__ - INFO - Fold 5 Epoch 110 Batch 0: Train Loss = 1.0888
2023-11-08 10:12:32,235 - __main__ - INFO - Fold 5, epoch 110: Loss = 0.9748 Valid loss = 2.4700 MSE = 43.8317 AUROC = 0.9851
2023-11-08 10:12:32,236 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.8317 ------------
2023-11-08 10:12:32,394 - __main__ - INFO - Fold 5, mse = 43.8317, mad = 4.6113
2023-11-08 10:12:32,775 - __main__ - INFO - Fold 5 Epoch 111 Batch 0: Train Loss = 0.8283
2023-11-08 10:12:34,457 - __main__ - INFO - Fold 5, mse = 43.8385, mad = 4.5759
2023-11-08 10:12:34,743 - __main__ - INFO - Fold 5 Epoch 112 Batch 0: Train Loss = 0.9502
2023-11-08 10:12:36,452 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.8315 ------------
2023-11-08 10:12:36,588 - __main__ - INFO - Fold 5, mse = 43.8315, mad = 4.5784
2023-11-08 10:12:36,876 - __main__ - INFO - Fold 5 Epoch 113 Batch 0: Train Loss = 0.9191
2023-11-08 10:12:38,358 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.7966 ------------
2023-11-08 10:12:38,521 - __main__ - INFO - Fold 5, mse = 43.7966, mad = 4.5892
2023-11-08 10:12:38,816 - __main__ - INFO - Fold 5 Epoch 114 Batch 0: Train Loss = 1.0295
2023-11-08 10:12:40,320 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.7047 ------------
2023-11-08 10:12:40,483 - __main__ - INFO - Fold 5, mse = 43.7047, mad = 4.5617
2023-11-08 10:12:40,784 - __main__ - INFO - Fold 5 Epoch 115 Batch 0: Train Loss = 0.8531
2023-11-08 10:12:42,315 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.5761 ------------
2023-11-08 10:12:42,505 - __main__ - INFO - Fold 5, mse = 43.5761, mad = 4.6044
2023-11-08 10:12:42,761 - __main__ - INFO - Fold 5 Epoch 116 Batch 0: Train Loss = 1.0105
2023-11-08 10:12:44,273 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.5518 ------------
2023-11-08 10:12:44,458 - __main__ - INFO - Fold 5, mse = 43.5518, mad = 4.5748
2023-11-08 10:12:44,753 - __main__ - INFO - Fold 5 Epoch 117 Batch 0: Train Loss = 0.8796
2023-11-08 10:12:46,383 - __main__ - INFO - Fold 5, mse = 43.7801, mad = 4.6197
2023-11-08 10:12:46,678 - __main__ - INFO - Fold 5 Epoch 118 Batch 0: Train Loss = 0.8843
2023-11-08 10:12:48,343 - __main__ - INFO - Fold 5, mse = 43.9278, mad = 4.5468
2023-11-08 10:12:48,602 - __main__ - INFO - Fold 5 Epoch 119 Batch 0: Train Loss = 0.8753
2023-11-08 10:12:50,134 - __main__ - INFO - Fold 5, mse = 43.8923, mad = 4.6017
2023-11-08 10:12:50,425 - __main__ - INFO - Fold 5 Epoch 120 Batch 0: Train Loss = 0.8862
2023-11-08 10:12:51,724 - __main__ - INFO - Fold 5, epoch 120: Loss = 0.9472 Valid loss = 2.5018 MSE = 43.6967 AUROC = 0.9839
2023-11-08 10:12:51,726 - __main__ - INFO - Fold 5, mse = 43.6967, mad = 4.5798
2023-11-08 10:12:51,975 - __main__ - INFO - Fold 5 Epoch 121 Batch 0: Train Loss = 0.9579
2023-11-08 10:12:53,349 - __main__ - INFO - Fold 5, mse = 43.6024, mad = 4.5457
2023-11-08 10:12:53,592 - __main__ - INFO - Fold 5 Epoch 122 Batch 0: Train Loss = 0.8187
2023-11-08 10:12:54,897 - __main__ - INFO - Fold 5, mse = 43.5673, mad = 4.5931
2023-11-08 10:12:55,185 - __main__ - INFO - Fold 5 Epoch 123 Batch 0: Train Loss = 0.9706
2023-11-08 10:12:56,854 - __main__ - INFO - Fold 5, mse = 43.5657, mad = 4.5660
2023-11-08 10:12:57,094 - __main__ - INFO - Fold 5 Epoch 124 Batch 0: Train Loss = 1.1628
2023-11-08 10:12:58,560 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.4631 ------------
2023-11-08 10:12:58,772 - __main__ - INFO - Fold 5, mse = 43.4631, mad = 4.5255
2023-11-08 10:12:59,067 - __main__ - INFO - Fold 5 Epoch 125 Batch 0: Train Loss = 0.9179
2023-11-08 10:13:00,610 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.2589 ------------
2023-11-08 10:13:00,808 - __main__ - INFO - Fold 5, mse = 43.2589, mad = 4.5902
2023-11-08 10:13:01,102 - __main__ - INFO - Fold 5 Epoch 126 Batch 0: Train Loss = 0.8393
2023-11-08 10:13:02,698 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 42.9329 ------------
2023-11-08 10:13:02,851 - __main__ - INFO - Fold 5, mse = 42.9329, mad = 4.5269
2023-11-08 10:13:03,098 - __main__ - INFO - Fold 5 Epoch 127 Batch 0: Train Loss = 0.9180
2023-11-08 10:13:04,601 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 42.7417 ------------
2023-11-08 10:13:04,769 - __main__ - INFO - Fold 5, mse = 42.7417, mad = 4.5077
2023-11-08 10:13:05,041 - __main__ - INFO - Fold 5 Epoch 128 Batch 0: Train Loss = 1.1070
2023-11-08 10:13:06,482 - __main__ - INFO - Fold 5, mse = 42.8556, mad = 4.5834
2023-11-08 10:13:06,749 - __main__ - INFO - Fold 5 Epoch 129 Batch 0: Train Loss = 1.0065
2023-11-08 10:13:08,257 - __main__ - INFO - Fold 5, mse = 42.8912, mad = 4.5047
2023-11-08 10:13:08,462 - __main__ - INFO - Fold 5 Epoch 130 Batch 0: Train Loss = 0.9097
2023-11-08 10:13:09,978 - __main__ - INFO - Fold 5, epoch 130: Loss = 0.9499 Valid loss = 2.4214 MSE = 42.9250 AUROC = 0.9842
2023-11-08 10:13:09,982 - __main__ - INFO - Fold 5, mse = 42.9250, mad = 4.5536
2023-11-08 10:13:10,252 - __main__ - INFO - Fold 5 Epoch 131 Batch 0: Train Loss = 0.9014
2023-11-08 10:13:11,918 - __main__ - INFO - Fold 5, mse = 43.1084, mad = 4.5649
2023-11-08 10:13:12,207 - __main__ - INFO - Fold 5 Epoch 132 Batch 0: Train Loss = 0.8837
2023-11-08 10:13:13,733 - __main__ - INFO - Fold 5, mse = 43.0056, mad = 4.4773
2023-11-08 10:13:13,975 - __main__ - INFO - Fold 5 Epoch 133 Batch 0: Train Loss = 0.9980
2023-11-08 10:13:15,446 - __main__ - INFO - Fold 5, mse = 43.0001, mad = 4.5563
2023-11-08 10:13:15,718 - __main__ - INFO - Fold 5 Epoch 134 Batch 0: Train Loss = 0.7366
2023-11-08 10:13:17,441 - __main__ - INFO - Fold 5, mse = 42.8248, mad = 4.5110
2023-11-08 10:13:17,733 - __main__ - INFO - Fold 5 Epoch 135 Batch 0: Train Loss = 1.0476
2023-11-08 10:13:19,036 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 42.6766 ------------
2023-11-08 10:13:19,203 - __main__ - INFO - Fold 5, mse = 42.6766, mad = 4.4897
2023-11-08 10:13:19,432 - __main__ - INFO - Fold 5 Epoch 136 Batch 0: Train Loss = 0.6869
2023-11-08 10:13:20,989 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 42.6522 ------------
2023-11-08 10:13:21,186 - __main__ - INFO - Fold 5, mse = 42.6522, mad = 4.5111
2023-11-08 10:13:21,409 - __main__ - INFO - Fold 5 Epoch 137 Batch 0: Train Loss = 1.1368
2023-11-08 10:13:23,065 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 42.5691 ------------
2023-11-08 10:13:23,249 - __main__ - INFO - Fold 5, mse = 42.5691, mad = 4.5116
2023-11-08 10:13:23,541 - __main__ - INFO - Fold 5 Epoch 138 Batch 0: Train Loss = 0.9492
2023-11-08 10:13:25,346 - __main__ - INFO - Fold 5, mse = 42.7992, mad = 4.4659
2023-11-08 10:13:25,627 - __main__ - INFO - Fold 5 Epoch 139 Batch 0: Train Loss = 1.1107
2023-11-08 10:13:27,234 - __main__ - INFO - Fold 5, mse = 43.0426, mad = 4.5901
2023-11-08 10:13:27,525 - __main__ - INFO - Fold 5 Epoch 140 Batch 0: Train Loss = 0.9228
2023-11-08 10:13:29,127 - __main__ - INFO - Fold 5, epoch 140: Loss = 0.8887 Valid loss = 2.4852 MSE = 43.0347 AUROC = 0.9840
2023-11-08 10:13:29,130 - __main__ - INFO - Fold 5, mse = 43.0347, mad = 4.5290
2023-11-08 10:13:29,339 - __main__ - INFO - Fold 5 Epoch 141 Batch 0: Train Loss = 0.8598
2023-11-08 10:13:30,631 - __main__ - INFO - Fold 5, mse = 43.0901, mad = 4.5818
2023-11-08 10:13:30,839 - __main__ - INFO - Fold 5 Epoch 142 Batch 0: Train Loss = 0.8455
2023-11-08 10:13:32,374 - __main__ - INFO - Fold 5, mse = 42.8690, mad = 4.5343
2023-11-08 10:13:32,634 - __main__ - INFO - Fold 5 Epoch 143 Batch 0: Train Loss = 0.8657
2023-11-08 10:13:34,250 - __main__ - INFO - Fold 5, mse = 42.6610, mad = 4.5521
2023-11-08 10:13:34,508 - __main__ - INFO - Fold 5 Epoch 144 Batch 0: Train Loss = 0.8833
2023-11-08 10:13:35,842 - __main__ - INFO - Fold 5, mse = 42.8112, mad = 4.5786
2023-11-08 10:13:36,035 - __main__ - INFO - Fold 5 Epoch 145 Batch 0: Train Loss = 1.0059
2023-11-08 10:13:37,328 - __main__ - INFO - Fold 5, mse = 42.8955, mad = 4.5491
2023-11-08 10:13:37,587 - __main__ - INFO - Fold 5 Epoch 146 Batch 0: Train Loss = 0.7887
2023-11-08 10:13:39,060 - __main__ - INFO - Fold 5, mse = 43.1511, mad = 4.5858
2023-11-08 10:13:39,344 - __main__ - INFO - Fold 5 Epoch 147 Batch 0: Train Loss = 1.0000
2023-11-08 10:13:40,799 - __main__ - INFO - Fold 5, mse = 43.5432, mad = 4.5491
2023-11-08 10:13:41,045 - __main__ - INFO - Fold 5 Epoch 148 Batch 0: Train Loss = 0.8781
2023-11-08 10:13:42,595 - __main__ - INFO - Fold 5, mse = 43.6759, mad = 4.6380
2023-11-08 10:13:42,911 - __main__ - INFO - Fold 5 Epoch 149 Batch 0: Train Loss = 0.9637
2023-11-08 10:13:44,324 - __main__ - INFO - Fold 5, mse = 43.6521, mad = 4.5244
2023-11-08 10:13:44,327 - __main__ - INFO - mse 36.9554(5.1167)
2023-11-08 10:13:44,330 - __main__ - INFO - mad 4.4378(0.2647)
2023-11-08 10:13:44,331 - __main__ - INFO - auroc 0.9833(0.0049)
2023-11-08 10:13:44,333 - __main__ - INFO - auprc 0.9755(0.0059)
2023-11-08 10:13:44,358 - __main__ - INFO - mse 36.9554(5.1167)
2023-11-08 10:13:44,361 - __main__ - INFO - mad 4.4378(0.2647)
2023-11-08 10:13:44,362 - __main__ - INFO - auroc 0.9833(0.0049)
2023-11-08 10:13:44,364 - __main__ - INFO - auprc 0.9755(0.0059)
