2023-11-08 01:30:09,743 - __main__ - INFO - 这是希望输出的info内容
2023-11-08 01:30:09,744 - __main__ - WARNING - 这是希望输出的warning内容
2023-11-08 01:30:40,362 - __main__ - INFO - [[-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 1.2605692444279462, 0.585371321489137, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.9546043520029015, 0.2987563180527675, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 1.0769903089729194, 0.4420638197709522, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.566534136852991, 1.803485086093707, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, 0.46506052412282983, -0.05951243624269434, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, 0.281481588667803, -0.2744736888199714, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.8934113735178925, 0.9436400757845987, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592]]
2023-11-08 01:30:40,370 - __main__ - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
2023-11-08 01:30:40,372 - __main__ - INFO - 110609
2023-11-08 01:30:40,373 - __main__ - INFO - [[-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8962118618028821, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8617355345389544, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8272592072750267, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7927828800110989, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7583065527471711, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7238302254832434, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6893538982193156, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6548775709553878, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.62040124369146, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5859249164275323, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5514485891636045, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5169722618996767, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.48249593463574897, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.4480196073718212, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.41354328010789343, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3790669528439657, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3445906255800379, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.31011429831611015, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.27563797105218235, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.2411616437882546, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.20668531652432684, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.17220898926039907, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.1377326619964713, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.10325633473254354, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.06878000746861578, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.034303680204688006, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.0001726470592397616, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.034648974323167527, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.06912530158709529, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.10360162885102306, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.13807795611495083, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.1725542833788786, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.20703061064280637, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.24150693790673414, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.2759832651706619, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.31045959243458965, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.34493591969851745, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.3794122469624452, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.413888574226373, '0']]
2023-11-08 01:31:05,538 - __main__ - INFO - [[-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.38817852813712, 0.585371321489137, 0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.2605692444279462, 0.6420908338073934, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.043819195154697, 0.2987563180527675, 0.6590707727034506, -0.4065324680479176, -1.009369603802189, 0.9546043520029015, 0.6420908338073934, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.38817852813712, 0.4420638197709522, 0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.0769903089729194, 0.8382388140070928, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.4742683613827257, 1.803485086093707, 1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.566534136852991, 0.8382388140070928, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.44119036243545645, -0.05951243624269434, 0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.46506052412282983, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.6564149455494709, -0.2744736888199714, 0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.281481588667803, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.09683102945303342, 0.9436400757845987, 0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.8934113735178925, 1.4266827546061909, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.3981454458126536, -0.05951243624269434, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.3981454458126536, -0.05951243624269434, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.37666305339779826, -0.5610886922563408, 0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.26925521769727756, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.37666305339779826, -0.5610886922563408, 0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.26925521769727756, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.1299090284003026, 0.585371321489137, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.0157973304879104, 0.44594285360769403, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.1299090284003026, 0.585371321489137, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.0157973304879104, 0.44594285360769403, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.613370028926668, 1.6601775843755224, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 1.1993762659429372, -0.9270930077902015, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.613370028926668, 1.6601775843755224, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 1.1993762659429372, -0.9270930077902015, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327]]
2023-11-08 01:31:05,544 - __main__ - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
2023-11-08 01:31:05,546 - __main__ - INFO - 34
2023-11-08 01:31:08,502 - __main__ - INFO - 32269
2023-11-08 01:31:08,510 - __main__ - INFO - 4034
2023-11-08 01:31:08,512 - __main__ - INFO - 4033
2023-11-08 01:31:21,345 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 0.6240
2023-11-08 01:31:38,879 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 0.2997
2023-11-08 01:31:56,127 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 0.2477
2023-11-08 01:32:13,291 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 0.2417
2023-11-08 01:32:30,282 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 0.2860
2023-11-08 01:32:48,070 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 0.3025
2023-11-08 01:33:05,323 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 0.2834
2023-11-08 01:33:19,941 - __main__ - INFO - Epoch 0: Loss = 0.2925 Valid loss = 0.2394 roc = 0.7744
2023-11-08 01:33:21,138 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 0.1633
2023-11-08 01:33:38,538 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 0.2541
2023-11-08 01:33:57,019 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 0.2760
2023-11-08 01:34:13,351 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 0.2415
2023-11-08 01:34:30,427 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 0.2587
2023-11-08 01:34:49,117 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 0.2654
2023-11-08 01:35:06,981 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 0.2270
2023-11-08 01:35:20,500 - __main__ - INFO - Epoch 1: Loss = 0.2501 Valid loss = 0.2315 roc = 0.7890
2023-11-08 01:35:21,604 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 0.1742
2023-11-08 01:35:38,113 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 0.2440
2023-11-08 01:35:55,174 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 0.3357
2023-11-08 01:36:12,337 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 0.2128
2023-11-08 01:36:29,878 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 0.2596
2023-11-08 01:36:47,952 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 0.2857
2023-11-08 01:37:04,254 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 0.2312
2023-11-08 01:37:17,424 - __main__ - INFO - Epoch 2: Loss = 0.2465 Valid loss = 0.2318 roc = 0.8021
2023-11-08 01:37:18,609 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 0.2581
2023-11-08 01:37:35,139 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 0.3068
2023-11-08 01:37:52,305 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 0.2437
2023-11-08 01:38:08,565 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 0.2802
2023-11-08 01:38:26,855 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 0.2823
2023-11-08 01:38:45,402 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 0.2785
2023-11-08 01:39:01,648 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 0.1832
2023-11-08 01:39:15,120 - __main__ - INFO - Epoch 3: Loss = 0.2511 Valid loss = 0.2246 roc = 0.8120
2023-11-08 01:39:16,207 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 0.2244
2023-11-08 01:39:34,503 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 0.2577
2023-11-08 01:39:50,099 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 0.2460
2023-11-08 01:40:07,002 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 0.2906
2023-11-08 01:40:22,778 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 0.2761
2023-11-08 01:40:40,048 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 0.2505
2023-11-08 01:40:56,413 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 0.2842
2023-11-08 01:41:08,809 - __main__ - INFO - Epoch 4: Loss = 0.2478 Valid loss = 0.2270 roc = 0.8097
2023-11-08 01:41:09,857 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 0.2891
2023-11-08 01:41:27,579 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 0.2717
2023-11-08 01:41:45,794 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 0.2726
2023-11-08 01:42:02,079 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 0.2422
2023-11-08 01:42:20,239 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 0.2374
2023-11-08 01:42:37,133 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 0.2257
2023-11-08 01:42:52,861 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 0.2068
2023-11-08 01:43:07,135 - __main__ - INFO - Epoch 5: Loss = 0.2443 Valid loss = 0.2246 roc = 0.8254
2023-11-08 01:43:08,015 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 0.2807
2023-11-08 01:43:24,719 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 0.2383
2023-11-08 01:43:40,448 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 0.2614
2023-11-08 01:43:57,339 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 0.2449
2023-11-08 01:44:14,675 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 0.3706
2023-11-08 01:44:32,167 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 0.2659
2023-11-08 01:44:50,604 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 0.2214
2023-11-08 01:45:04,728 - __main__ - INFO - Epoch 6: Loss = 0.2440 Valid loss = 0.2239 roc = 0.8178
2023-11-08 01:45:05,653 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 0.2507
2023-11-08 01:45:22,761 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 0.2114
2023-11-08 01:45:39,403 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 0.2707
2023-11-08 01:45:56,052 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 0.2272
2023-11-08 01:46:14,294 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 0.1943
2023-11-08 01:46:32,095 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 0.2698
2023-11-08 01:46:48,993 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 0.2844
2023-11-08 01:47:03,533 - __main__ - INFO - Epoch 7: Loss = 0.2385 Valid loss = 0.2236 roc = 0.8333
2023-11-08 01:47:04,449 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 0.2663
2023-11-08 01:47:21,636 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 0.1544
2023-11-08 01:47:38,757 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 0.2058
2023-11-08 01:47:54,878 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 0.1920
2023-11-08 01:48:10,951 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 0.2310
2023-11-08 01:48:29,616 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 0.2370
2023-11-08 01:48:47,450 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 0.2733
2023-11-08 01:49:00,819 - __main__ - INFO - Epoch 8: Loss = 0.2427 Valid loss = 0.2228 roc = 0.8258
2023-11-08 01:49:01,594 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 0.2887
2023-11-08 01:49:19,584 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 0.2789
2023-11-08 01:49:35,293 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 0.2601
2023-11-08 01:49:54,953 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 0.2733
2023-11-08 01:50:13,124 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 0.2249
2023-11-08 01:50:29,343 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 0.2073
2023-11-08 01:50:47,000 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 0.1684
2023-11-08 01:51:01,064 - __main__ - INFO - Epoch 9: Loss = 0.2409 Valid loss = 0.2203 roc = 0.8268
2023-11-08 01:51:02,174 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 0.1756
2023-11-08 01:51:19,944 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 0.3462
2023-11-08 01:51:37,376 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 0.1699
2023-11-08 01:51:54,916 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 0.2885
2023-11-08 01:52:11,902 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 0.2891
2023-11-08 01:52:27,803 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 0.1918
2023-11-08 01:52:46,970 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 0.2438
2023-11-08 01:52:59,717 - __main__ - INFO - Epoch 10: Loss = 0.2402 Valid loss = 0.2202 roc = 0.8373
2023-11-08 01:53:00,587 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 0.1864
2023-11-08 01:53:16,911 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 0.2647
2023-11-08 01:53:32,633 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 0.2990
2023-11-08 01:53:50,449 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 0.1987
2023-11-08 01:54:07,043 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 0.2648
2023-11-08 01:54:25,063 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 0.2583
2023-11-08 01:54:43,685 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 0.2184
2023-11-08 01:54:56,802 - __main__ - INFO - Epoch 11: Loss = 0.2422 Valid loss = 0.2194 roc = 0.8228
2023-11-08 01:54:57,612 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 0.3178
2023-11-08 01:55:16,036 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 0.1944
2023-11-08 01:55:34,029 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 0.2461
2023-11-08 01:55:50,796 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 0.2549
2023-11-08 01:56:07,011 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 0.1813
2023-11-08 01:56:23,651 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 0.2685
2023-11-08 01:56:39,381 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 0.2302
2023-11-08 01:56:52,976 - __main__ - INFO - Epoch 12: Loss = 0.2483 Valid loss = 0.2228 roc = 0.8264
2023-11-08 01:56:53,743 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 0.2142
2023-11-08 01:57:10,930 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 0.2494
2023-11-08 01:57:27,135 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 0.2522
2023-11-08 01:57:45,067 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 0.2556
2023-11-08 01:58:01,740 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 0.2539
2023-11-08 01:58:19,710 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 0.2846
2023-11-08 01:58:36,653 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 0.2653
2023-11-08 01:58:49,164 - __main__ - INFO - Epoch 13: Loss = 0.2437 Valid loss = 0.2200 roc = 0.8237
2023-11-08 01:58:50,031 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 0.2267
2023-11-08 01:59:07,634 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 0.2967
2023-11-08 01:59:23,633 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 0.2177
2023-11-08 01:59:41,180 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 0.2601
2023-11-08 01:59:58,513 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 0.2361
2023-11-08 02:00:17,141 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 0.2402
2023-11-08 02:00:34,273 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 0.1530
2023-11-08 02:00:48,076 - __main__ - INFO - Epoch 14: Loss = 0.2408 Valid loss = 0.2185 roc = 0.8269
2023-11-08 02:00:48,839 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 0.3095
2023-11-08 02:01:07,306 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 0.2813
2023-11-08 02:01:23,616 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 0.2692
2023-11-08 02:01:39,704 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 0.2395
2023-11-08 02:01:55,948 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 0.2178
2023-11-08 02:02:13,148 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 0.2691
2023-11-08 02:02:31,100 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 0.1924
2023-11-08 02:02:43,605 - __main__ - INFO - Epoch 15: Loss = 0.2407 Valid loss = 0.2149 roc = 0.8354
2023-11-08 02:02:44,655 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 0.2910
2023-11-08 02:03:02,667 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 0.2634
2023-11-08 02:03:19,639 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 0.2154
2023-11-08 02:03:35,554 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 0.1870
2023-11-08 02:03:53,286 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 0.2704
2023-11-08 02:04:09,482 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 0.1745
2023-11-08 02:04:26,549 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 0.2232
2023-11-08 02:04:41,295 - __main__ - INFO - Epoch 16: Loss = 0.2367 Valid loss = 0.2095 roc = 0.8285
2023-11-08 02:04:42,376 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 0.2236
2023-11-08 02:05:01,168 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 0.2480
2023-11-08 02:05:18,810 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 0.2014
2023-11-08 02:05:36,295 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 0.1988
2023-11-08 02:05:52,785 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 0.1775
2023-11-08 02:06:10,153 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 0.2310
2023-11-08 02:06:28,127 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 0.2309
2023-11-08 02:06:40,381 - __main__ - INFO - Epoch 17: Loss = 0.2320 Valid loss = 0.2030 roc = 0.8523
2023-11-08 02:06:41,307 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 0.2985
2023-11-08 02:06:58,492 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 0.3220
2023-11-08 02:07:17,224 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 0.1600
2023-11-08 02:07:33,942 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 0.2280
2023-11-08 02:07:50,724 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 0.1689
2023-11-08 02:08:08,224 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 0.1982
2023-11-08 02:08:25,367 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 0.2258
2023-11-08 02:08:39,078 - __main__ - INFO - Epoch 18: Loss = 0.2244 Valid loss = 0.1971 roc = 0.8701
2023-11-08 02:08:40,122 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 0.2141
2023-11-08 02:08:57,331 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 0.2390
2023-11-08 02:09:13,612 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 0.2497
2023-11-08 02:09:31,426 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 0.2335
2023-11-08 02:09:48,981 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 0.2146
2023-11-08 02:10:07,538 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 0.2486
2023-11-08 02:10:25,640 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 0.2750
2023-11-08 02:10:38,078 - __main__ - INFO - Epoch 19: Loss = 0.2214 Valid loss = 0.2001 roc = 0.8458
2023-11-08 02:10:39,170 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 0.2423
2023-11-08 02:10:56,057 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 0.1999
2023-11-08 02:11:12,951 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 0.1783
2023-11-08 02:11:31,151 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 0.3051
2023-11-08 02:11:48,361 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 0.2575
2023-11-08 02:12:06,433 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 0.2317
2023-11-08 02:12:23,819 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 0.1677
2023-11-08 02:12:37,636 - __main__ - INFO - Epoch 20: Loss = 0.2275 Valid loss = 0.2046 roc = 0.8492
2023-11-08 02:12:38,538 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 0.2310
2023-11-08 02:12:56,227 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 0.2733
2023-11-08 02:13:14,248 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 0.1980
2023-11-08 02:13:31,613 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 0.2095
2023-11-08 02:13:48,231 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 0.2307
2023-11-08 02:14:05,229 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 0.2214
2023-11-08 02:14:22,029 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 0.3134
2023-11-08 02:14:36,175 - __main__ - INFO - Epoch 21: Loss = 0.2291 Valid loss = 0.2050 roc = 0.8492
2023-11-08 02:14:37,016 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 0.2694
2023-11-08 02:14:54,899 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 0.2364
2023-11-08 02:15:12,295 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 0.1624
2023-11-08 02:15:29,652 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 0.2280
2023-11-08 02:15:47,961 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 0.2263
2023-11-08 02:16:04,605 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 0.1881
2023-11-08 02:16:21,715 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 0.2231
2023-11-08 02:16:35,173 - __main__ - INFO - Epoch 22: Loss = 0.2261 Valid loss = 0.2031 roc = 0.8519
2023-11-08 02:16:35,964 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 0.2161
2023-11-08 02:16:54,280 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 0.2397
2023-11-08 02:17:11,870 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 0.1957
2023-11-08 02:17:29,043 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 0.2767
2023-11-08 02:17:46,925 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 0.2929
2023-11-08 02:18:03,651 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 0.2503
2023-11-08 02:18:21,009 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 0.2198
2023-11-08 02:18:35,254 - __main__ - INFO - Epoch 23: Loss = 0.2251 Valid loss = 0.2034 roc = 0.8537
2023-11-08 02:18:36,025 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 0.2189
2023-11-08 02:18:53,113 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 0.2023
2023-11-08 02:19:09,879 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 0.2174
2023-11-08 02:19:27,321 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 0.2820
2023-11-08 02:19:46,141 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 0.2959
2023-11-08 02:20:02,742 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 0.2403
2023-11-08 02:20:18,577 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 0.2591
2023-11-08 02:20:31,126 - __main__ - INFO - Epoch 24: Loss = 0.2238 Valid loss = 0.2029 roc = 0.8561
2023-11-08 02:20:32,154 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 0.2001
2023-11-08 02:20:48,695 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 0.2304
2023-11-08 02:21:06,308 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 0.2473
2023-11-08 02:21:23,391 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 0.2119
2023-11-08 02:21:40,942 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 0.2264
2023-11-08 02:21:59,019 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 0.2470
2023-11-08 02:22:15,001 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 0.1696
2023-11-08 02:22:28,350 - __main__ - INFO - Epoch 25: Loss = 0.2274 Valid loss = 0.2029 roc = 0.8569
2023-11-08 02:22:29,493 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 0.2020
2023-11-08 02:22:46,267 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 0.1721
2023-11-08 02:23:05,206 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 0.1911
2023-11-08 02:23:21,410 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 0.1812
2023-11-08 02:23:38,375 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 0.2498
2023-11-08 02:23:55,698 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 0.2016
2023-11-08 02:24:12,984 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 0.2615
2023-11-08 02:24:26,624 - __main__ - INFO - Epoch 26: Loss = 0.2279 Valid loss = 0.2052 roc = 0.8458
2023-11-08 02:24:27,800 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 0.2181
2023-11-08 02:24:45,344 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 0.2044
2023-11-08 02:25:03,306 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 0.1899
2023-11-08 02:25:20,459 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 0.1704
2023-11-08 02:25:37,513 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 0.2433
2023-11-08 02:25:55,090 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 0.2240
2023-11-08 02:26:12,569 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 0.2616
2023-11-08 02:26:24,610 - __main__ - INFO - Epoch 27: Loss = 0.2273 Valid loss = 0.2097 roc = 0.8412
2023-11-08 02:26:25,583 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 0.2384
2023-11-08 02:26:43,928 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 0.1228
2023-11-08 02:27:00,623 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 0.2411
2023-11-08 02:27:17,616 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 0.1778
2023-11-08 02:27:34,743 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 0.3295
2023-11-08 02:27:51,045 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 0.2392
2023-11-08 02:28:09,208 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 0.2599
2023-11-08 02:28:22,284 - __main__ - INFO - Epoch 28: Loss = 0.2292 Valid loss = 0.2030 roc = 0.8513
2023-11-08 02:28:23,191 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 0.2223
2023-11-08 02:28:40,608 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 0.2948
2023-11-08 02:28:58,354 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 0.2834
2023-11-08 02:29:15,279 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 0.2341
2023-11-08 02:29:32,531 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 0.2410
2023-11-08 02:29:49,352 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 0.1579
2023-11-08 02:30:06,543 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 0.1972
2023-11-08 02:30:19,661 - __main__ - INFO - Epoch 29: Loss = 0.2283 Valid loss = 0.2025 roc = 0.8587
2023-11-08 02:30:20,491 - __main__ - INFO - Epoch 30 Batch 0: Train Loss = 0.2305
2023-11-08 02:30:37,859 - __main__ - INFO - Epoch 30 Batch 20: Train Loss = 0.2686
2023-11-08 02:30:55,565 - __main__ - INFO - Epoch 30 Batch 40: Train Loss = 0.2034
2023-11-08 02:31:12,904 - __main__ - INFO - Epoch 30 Batch 60: Train Loss = 0.1937
2023-11-08 02:31:30,888 - __main__ - INFO - Epoch 30 Batch 80: Train Loss = 0.2664
2023-11-08 02:31:48,029 - __main__ - INFO - Epoch 30 Batch 100: Train Loss = 0.2346
2023-11-08 02:32:04,839 - __main__ - INFO - Epoch 30 Batch 120: Train Loss = 0.2560
2023-11-08 02:32:17,333 - __main__ - INFO - Epoch 30: Loss = 0.2254 Valid loss = 0.2064 roc = 0.8580
2023-11-08 02:32:18,220 - __main__ - INFO - Epoch 31 Batch 0: Train Loss = 0.2682
2023-11-08 02:32:35,951 - __main__ - INFO - Epoch 31 Batch 20: Train Loss = 0.2484
2023-11-08 02:32:53,861 - __main__ - INFO - Epoch 31 Batch 40: Train Loss = 0.2593
2023-11-08 02:33:09,991 - __main__ - INFO - Epoch 31 Batch 60: Train Loss = 0.1992
2023-11-08 02:33:26,999 - __main__ - INFO - Epoch 31 Batch 80: Train Loss = 0.1821
2023-11-08 02:33:43,619 - __main__ - INFO - Epoch 31 Batch 100: Train Loss = 0.2709
2023-11-08 02:34:00,989 - __main__ - INFO - Epoch 31 Batch 120: Train Loss = 0.1547
2023-11-08 02:34:14,947 - __main__ - INFO - Epoch 31: Loss = 0.2251 Valid loss = 0.2026 roc = 0.8490
2023-11-08 02:34:16,043 - __main__ - INFO - Epoch 32 Batch 0: Train Loss = 0.1767
2023-11-08 02:34:33,093 - __main__ - INFO - Epoch 32 Batch 20: Train Loss = 0.2288
2023-11-08 02:34:49,291 - __main__ - INFO - Epoch 32 Batch 40: Train Loss = 0.2267
2023-11-08 02:35:05,908 - __main__ - INFO - Epoch 32 Batch 60: Train Loss = 0.2583
2023-11-08 02:35:22,775 - __main__ - INFO - Epoch 32 Batch 80: Train Loss = 0.2332
2023-11-08 02:35:40,262 - __main__ - INFO - Epoch 32 Batch 100: Train Loss = 0.2253
2023-11-08 02:35:57,338 - __main__ - INFO - Epoch 32 Batch 120: Train Loss = 0.2185
2023-11-08 02:36:10,854 - __main__ - INFO - Epoch 32: Loss = 0.2257 Valid loss = 0.2043 roc = 0.8603
2023-11-08 02:36:11,824 - __main__ - INFO - Epoch 33 Batch 0: Train Loss = 0.1453
2023-11-08 02:36:29,200 - __main__ - INFO - Epoch 33 Batch 20: Train Loss = 0.2346
2023-11-08 02:36:46,577 - __main__ - INFO - Epoch 33 Batch 40: Train Loss = 0.2187
2023-11-08 02:37:05,025 - __main__ - INFO - Epoch 33 Batch 60: Train Loss = 0.2851
2023-11-08 02:37:23,273 - __main__ - INFO - Epoch 33 Batch 80: Train Loss = 0.2215
2023-11-08 02:37:41,113 - __main__ - INFO - Epoch 33 Batch 100: Train Loss = 0.2011
2023-11-08 02:37:56,968 - __main__ - INFO - Epoch 33 Batch 120: Train Loss = 0.2261
2023-11-08 02:38:09,517 - __main__ - INFO - Epoch 33: Loss = 0.2269 Valid loss = 0.2054 roc = 0.8616
2023-11-08 02:38:10,516 - __main__ - INFO - Epoch 34 Batch 0: Train Loss = 0.1635
2023-11-08 02:38:28,262 - __main__ - INFO - Epoch 34 Batch 20: Train Loss = 0.3256
2023-11-08 02:38:44,928 - __main__ - INFO - Epoch 34 Batch 40: Train Loss = 0.1627
2023-11-08 02:39:03,300 - __main__ - INFO - Epoch 34 Batch 60: Train Loss = 0.2229
2023-11-08 02:39:20,891 - __main__ - INFO - Epoch 34 Batch 80: Train Loss = 0.2109
2023-11-08 02:39:38,060 - __main__ - INFO - Epoch 34 Batch 100: Train Loss = 0.2179
2023-11-08 02:39:55,656 - __main__ - INFO - Epoch 34 Batch 120: Train Loss = 0.1997
2023-11-08 02:40:08,614 - __main__ - INFO - Epoch 34: Loss = 0.2234 Valid loss = 0.2009 roc = 0.8554
2023-11-08 02:40:09,280 - __main__ - INFO - Epoch 35 Batch 0: Train Loss = 0.2730
2023-11-08 02:40:25,750 - __main__ - INFO - Epoch 35 Batch 20: Train Loss = 0.2211
2023-11-08 02:40:43,423 - __main__ - INFO - Epoch 35 Batch 40: Train Loss = 0.2695
2023-11-08 02:41:01,825 - __main__ - INFO - Epoch 35 Batch 60: Train Loss = 0.1886
2023-11-08 02:41:19,729 - __main__ - INFO - Epoch 35 Batch 80: Train Loss = 0.1858
2023-11-08 02:41:37,162 - __main__ - INFO - Epoch 35 Batch 100: Train Loss = 0.2244
2023-11-08 02:41:54,422 - __main__ - INFO - Epoch 35 Batch 120: Train Loss = 0.2280
2023-11-08 02:42:07,363 - __main__ - INFO - Epoch 35: Loss = 0.2220 Valid loss = 0.1991 roc = 0.8560
2023-11-08 02:42:08,255 - __main__ - INFO - Epoch 36 Batch 0: Train Loss = 0.1475
2023-11-08 02:42:25,267 - __main__ - INFO - Epoch 36 Batch 20: Train Loss = 0.3186
2023-11-08 02:42:41,427 - __main__ - INFO - Epoch 36 Batch 40: Train Loss = 0.2148
2023-11-08 02:42:57,896 - __main__ - INFO - Epoch 36 Batch 60: Train Loss = 0.2472
2023-11-08 02:43:15,351 - __main__ - INFO - Epoch 36 Batch 80: Train Loss = 0.2516
2023-11-08 02:43:33,137 - __main__ - INFO - Epoch 36 Batch 100: Train Loss = 0.2106
2023-11-08 02:43:50,863 - __main__ - INFO - Epoch 36 Batch 120: Train Loss = 0.2647
2023-11-08 02:44:03,645 - __main__ - INFO - Epoch 36: Loss = 0.2219 Valid loss = 0.1995 roc = 0.8575
2023-11-08 02:44:04,592 - __main__ - INFO - Epoch 37 Batch 0: Train Loss = 0.2044
2023-11-08 02:44:23,272 - __main__ - INFO - Epoch 37 Batch 20: Train Loss = 0.1719
2023-11-08 02:44:40,238 - __main__ - INFO - Epoch 37 Batch 40: Train Loss = 0.1586
2023-11-08 02:44:57,843 - __main__ - INFO - Epoch 37 Batch 60: Train Loss = 0.2123
2023-11-08 02:45:14,872 - __main__ - INFO - Epoch 37 Batch 80: Train Loss = 0.2545
2023-11-08 02:45:32,584 - __main__ - INFO - Epoch 37 Batch 100: Train Loss = 0.1647
2023-11-08 02:45:50,203 - __main__ - INFO - Epoch 37 Batch 120: Train Loss = 0.2335
2023-11-08 02:46:03,641 - __main__ - INFO - Epoch 37: Loss = 0.2225 Valid loss = 0.1985 roc = 0.8683
2023-11-08 02:46:04,547 - __main__ - INFO - Epoch 38 Batch 0: Train Loss = 0.2435
2023-11-08 02:46:21,468 - __main__ - INFO - Epoch 38 Batch 20: Train Loss = 0.2898
2023-11-08 02:46:37,586 - __main__ - INFO - Epoch 38 Batch 40: Train Loss = 0.1922
2023-11-08 02:46:57,259 - __main__ - INFO - Epoch 38 Batch 60: Train Loss = 0.2017
2023-11-08 02:47:15,168 - __main__ - INFO - Epoch 38 Batch 80: Train Loss = 0.2229
2023-11-08 02:47:31,964 - __main__ - INFO - Epoch 38 Batch 100: Train Loss = 0.2293
2023-11-08 02:47:49,561 - __main__ - INFO - Epoch 38 Batch 120: Train Loss = 0.2866
2023-11-08 02:48:02,435 - __main__ - INFO - Epoch 38: Loss = 0.2277 Valid loss = 0.2011 roc = 0.8533
2023-11-08 02:48:03,275 - __main__ - INFO - Epoch 39 Batch 0: Train Loss = 0.2451
2023-11-08 02:48:21,127 - __main__ - INFO - Epoch 39 Batch 20: Train Loss = 0.2087
2023-11-08 02:48:39,116 - __main__ - INFO - Epoch 39 Batch 40: Train Loss = 0.2237
2023-11-08 02:48:57,738 - __main__ - INFO - Epoch 39 Batch 60: Train Loss = 0.1787
2023-11-08 02:49:15,622 - __main__ - INFO - Epoch 39 Batch 80: Train Loss = 0.2358
2023-11-08 02:49:32,529 - __main__ - INFO - Epoch 39 Batch 100: Train Loss = 0.2338
2023-11-08 02:49:50,969 - __main__ - INFO - Epoch 39 Batch 120: Train Loss = 0.2447
2023-11-08 02:50:03,572 - __main__ - INFO - Epoch 39: Loss = 0.2226 Valid loss = 0.2019 roc = 0.8500
2023-11-08 02:50:04,686 - __main__ - INFO - Epoch 40 Batch 0: Train Loss = 0.2325
2023-11-08 02:50:21,582 - __main__ - INFO - Epoch 40 Batch 20: Train Loss = 0.2472
2023-11-08 02:50:39,249 - __main__ - INFO - Epoch 40 Batch 40: Train Loss = 0.1514
2023-11-08 02:50:56,571 - __main__ - INFO - Epoch 40 Batch 60: Train Loss = 0.2232
2023-11-08 02:51:13,228 - __main__ - INFO - Epoch 40 Batch 80: Train Loss = 0.1857
2023-11-08 02:51:29,667 - __main__ - INFO - Epoch 40 Batch 100: Train Loss = 0.2192
2023-11-08 02:51:46,621 - __main__ - INFO - Epoch 40 Batch 120: Train Loss = 0.2255
2023-11-08 02:52:00,331 - __main__ - INFO - Epoch 40: Loss = 0.2266 Valid loss = 0.2045 roc = 0.8574
2023-11-08 02:52:01,177 - __main__ - INFO - Epoch 41 Batch 0: Train Loss = 0.3143
2023-11-08 02:52:18,205 - __main__ - INFO - Epoch 41 Batch 20: Train Loss = 0.2753
2023-11-08 02:52:36,184 - __main__ - INFO - Epoch 41 Batch 40: Train Loss = 0.1175
2023-11-08 02:52:54,248 - __main__ - INFO - Epoch 41 Batch 60: Train Loss = 0.2458
2023-11-08 02:53:11,280 - __main__ - INFO - Epoch 41 Batch 80: Train Loss = 0.2714
2023-11-08 02:53:27,531 - __main__ - INFO - Epoch 41 Batch 100: Train Loss = 0.1900
2023-11-08 02:53:45,782 - __main__ - INFO - Epoch 41 Batch 120: Train Loss = 0.2354
2023-11-08 02:53:58,659 - __main__ - INFO - Epoch 41: Loss = 0.2227 Valid loss = 0.2017 roc = 0.8613
2023-11-08 02:53:59,563 - __main__ - INFO - Epoch 42 Batch 0: Train Loss = 0.2522
2023-11-08 02:54:17,328 - __main__ - INFO - Epoch 42 Batch 20: Train Loss = 0.2442
2023-11-08 02:54:35,464 - __main__ - INFO - Epoch 42 Batch 40: Train Loss = 0.2003
2023-11-08 02:54:53,502 - __main__ - INFO - Epoch 42 Batch 60: Train Loss = 0.2140
2023-11-08 02:55:10,634 - __main__ - INFO - Epoch 42 Batch 80: Train Loss = 0.2185
2023-11-08 02:55:28,159 - __main__ - INFO - Epoch 42 Batch 100: Train Loss = 0.2211
2023-11-08 02:55:46,134 - __main__ - INFO - Epoch 42 Batch 120: Train Loss = 0.2638
2023-11-08 02:55:59,341 - __main__ - INFO - Epoch 42: Loss = 0.2222 Valid loss = 0.2058 roc = 0.8433
2023-11-08 02:56:00,083 - __main__ - INFO - Epoch 43 Batch 0: Train Loss = 0.2068
2023-11-08 02:56:16,595 - __main__ - INFO - Epoch 43 Batch 20: Train Loss = 0.2069
2023-11-08 02:56:33,776 - __main__ - INFO - Epoch 43 Batch 40: Train Loss = 0.2109
2023-11-08 02:56:52,670 - __main__ - INFO - Epoch 43 Batch 60: Train Loss = 0.1812
2023-11-08 02:57:09,617 - __main__ - INFO - Epoch 43 Batch 80: Train Loss = 0.1414
2023-11-08 02:57:26,966 - __main__ - INFO - Epoch 43 Batch 100: Train Loss = 0.1804
2023-11-08 02:57:44,950 - __main__ - INFO - Epoch 43 Batch 120: Train Loss = 0.2349
2023-11-08 02:57:57,945 - __main__ - INFO - Epoch 43: Loss = 0.2252 Valid loss = 0.2013 roc = 0.8617
2023-11-08 02:57:58,760 - __main__ - INFO - Epoch 44 Batch 0: Train Loss = 0.2464
2023-11-08 02:58:15,921 - __main__ - INFO - Epoch 44 Batch 20: Train Loss = 0.2101
2023-11-08 02:58:33,094 - __main__ - INFO - Epoch 44 Batch 40: Train Loss = 0.1782
2023-11-08 02:58:50,578 - __main__ - INFO - Epoch 44 Batch 60: Train Loss = 0.2511
2023-11-08 02:59:07,235 - __main__ - INFO - Epoch 44 Batch 80: Train Loss = 0.1996
2023-11-08 02:59:25,019 - __main__ - INFO - Epoch 44 Batch 100: Train Loss = 0.1969
2023-11-08 02:59:42,799 - __main__ - INFO - Epoch 44 Batch 120: Train Loss = 0.2634
2023-11-08 02:59:56,668 - __main__ - INFO - Epoch 44: Loss = 0.2220 Valid loss = 0.2011 roc = 0.8562
2023-11-08 02:59:57,487 - __main__ - INFO - Epoch 45 Batch 0: Train Loss = 0.2315
2023-11-08 03:00:15,621 - __main__ - INFO - Epoch 45 Batch 20: Train Loss = 0.2292
2023-11-08 03:00:32,481 - __main__ - INFO - Epoch 45 Batch 40: Train Loss = 0.1764
2023-11-08 03:00:48,648 - __main__ - INFO - Epoch 45 Batch 60: Train Loss = 0.2144
2023-11-08 03:01:06,350 - __main__ - INFO - Epoch 45 Batch 80: Train Loss = 0.2400
2023-11-08 03:01:23,342 - __main__ - INFO - Epoch 45 Batch 100: Train Loss = 0.2145
2023-11-08 03:01:39,662 - __main__ - INFO - Epoch 45 Batch 120: Train Loss = 0.2118
2023-11-08 03:01:51,682 - __main__ - INFO - Epoch 45: Loss = 0.2218 Valid loss = 0.2011 roc = 0.8584
2023-11-08 03:01:52,566 - __main__ - INFO - Epoch 46 Batch 0: Train Loss = 0.2559
2023-11-08 03:02:09,804 - __main__ - INFO - Epoch 46 Batch 20: Train Loss = 0.1758
2023-11-08 03:02:27,021 - __main__ - INFO - Epoch 46 Batch 40: Train Loss = 0.2572
2023-11-08 03:02:43,593 - __main__ - INFO - Epoch 46 Batch 60: Train Loss = 0.1919
2023-11-08 03:03:01,221 - __main__ - INFO - Epoch 46 Batch 80: Train Loss = 0.1533
2023-11-08 03:03:17,614 - __main__ - INFO - Epoch 46 Batch 100: Train Loss = 0.1974
2023-11-08 03:03:35,408 - __main__ - INFO - Epoch 46 Batch 120: Train Loss = 0.1972
2023-11-08 03:03:49,739 - __main__ - INFO - Epoch 46: Loss = 0.2163 Valid loss = 0.1986 roc = 0.8580
2023-11-08 03:03:50,734 - __main__ - INFO - Epoch 47 Batch 0: Train Loss = 0.2332
2023-11-08 03:04:07,753 - __main__ - INFO - Epoch 47 Batch 20: Train Loss = 0.1662
2023-11-08 03:04:24,392 - __main__ - INFO - Epoch 47 Batch 40: Train Loss = 0.2088
2023-11-08 03:04:42,081 - __main__ - INFO - Epoch 47 Batch 60: Train Loss = 0.1582
2023-11-08 03:04:58,782 - __main__ - INFO - Epoch 47 Batch 80: Train Loss = 0.2359
2023-11-08 03:05:16,121 - __main__ - INFO - Epoch 47 Batch 100: Train Loss = 0.1747
2023-11-08 03:05:33,515 - __main__ - INFO - Epoch 47 Batch 120: Train Loss = 0.1960
2023-11-08 03:05:46,913 - __main__ - INFO - Epoch 47: Loss = 0.2214 Valid loss = 0.2001 roc = 0.8579
2023-11-08 03:05:47,896 - __main__ - INFO - Epoch 48 Batch 0: Train Loss = 0.1877
2023-11-08 03:06:04,913 - __main__ - INFO - Epoch 48 Batch 20: Train Loss = 0.1839
2023-11-08 03:06:23,284 - __main__ - INFO - Epoch 48 Batch 40: Train Loss = 0.2139
2023-11-08 03:06:39,892 - __main__ - INFO - Epoch 48 Batch 60: Train Loss = 0.1926
2023-11-08 03:06:56,830 - __main__ - INFO - Epoch 48 Batch 80: Train Loss = 0.2262
2023-11-08 03:07:14,110 - __main__ - INFO - Epoch 48 Batch 100: Train Loss = 0.2094
2023-11-08 03:07:32,783 - __main__ - INFO - Epoch 48 Batch 120: Train Loss = 0.2119
2023-11-08 03:07:46,537 - __main__ - INFO - Epoch 48: Loss = 0.2233 Valid loss = 0.2030 roc = 0.8575
2023-11-08 03:07:47,243 - __main__ - INFO - Epoch 49 Batch 0: Train Loss = 0.1597
2023-11-08 03:08:05,890 - __main__ - INFO - Epoch 49 Batch 20: Train Loss = 0.2713
2023-11-08 03:08:22,783 - __main__ - INFO - Epoch 49 Batch 40: Train Loss = 0.2415
2023-11-08 03:08:40,026 - __main__ - INFO - Epoch 49 Batch 60: Train Loss = 0.1832
2023-11-08 03:08:57,979 - __main__ - INFO - Epoch 49 Batch 80: Train Loss = 0.2097
2023-11-08 03:09:16,043 - __main__ - INFO - Epoch 49 Batch 100: Train Loss = 0.1477
2023-11-08 03:09:32,065 - __main__ - INFO - Epoch 49 Batch 120: Train Loss = 0.2035
2023-11-08 03:09:45,625 - __main__ - INFO - Epoch 49: Loss = 0.2232 Valid loss = 0.2000 roc = 0.8601
2023-11-08 03:09:46,743 - __main__ - INFO - Epoch 50 Batch 0: Train Loss = 0.2762
2023-11-08 03:10:04,785 - __main__ - INFO - Epoch 50 Batch 20: Train Loss = 0.1760
2023-11-08 03:10:22,771 - __main__ - INFO - Epoch 50 Batch 40: Train Loss = 0.2195
2023-11-08 03:10:40,052 - __main__ - INFO - Epoch 50 Batch 60: Train Loss = 0.2666
2023-11-08 03:10:58,433 - __main__ - INFO - Epoch 50 Batch 80: Train Loss = 0.1843
2023-11-08 03:11:15,695 - __main__ - INFO - Epoch 50 Batch 100: Train Loss = 0.2226
2023-11-08 03:11:30,978 - __main__ - INFO - Epoch 50 Batch 120: Train Loss = 0.2242
2023-11-08 03:11:44,307 - __main__ - INFO - Epoch 50: Loss = 0.2187 Valid loss = 0.2005 roc = 0.8668
2023-11-08 03:11:45,178 - __main__ - INFO - Epoch 51 Batch 0: Train Loss = 0.2020
2023-11-08 03:12:03,016 - __main__ - INFO - Epoch 51 Batch 20: Train Loss = 0.2226
2023-11-08 03:12:20,561 - __main__ - INFO - Epoch 51 Batch 40: Train Loss = 0.1999
2023-11-08 03:12:38,105 - __main__ - INFO - Epoch 51 Batch 60: Train Loss = 0.2344
2023-11-08 03:12:55,208 - __main__ - INFO - Epoch 51 Batch 80: Train Loss = 0.1918
2023-11-08 03:13:11,664 - __main__ - INFO - Epoch 51 Batch 100: Train Loss = 0.2728
2023-11-08 03:13:29,009 - __main__ - INFO - Epoch 51 Batch 120: Train Loss = 0.1528
2023-11-08 03:13:42,023 - __main__ - INFO - Epoch 51: Loss = 0.2186 Valid loss = 0.2006 roc = 0.8597
2023-11-08 03:13:42,803 - __main__ - INFO - Epoch 52 Batch 0: Train Loss = 0.2075
2023-11-08 03:13:59,416 - __main__ - INFO - Epoch 52 Batch 20: Train Loss = 0.2139
2023-11-08 03:14:17,812 - __main__ - INFO - Epoch 52 Batch 40: Train Loss = 0.1791
2023-11-08 03:14:34,716 - __main__ - INFO - Epoch 52 Batch 60: Train Loss = 0.2655
2023-11-08 03:14:53,176 - __main__ - INFO - Epoch 52 Batch 80: Train Loss = 0.1866
2023-11-08 03:15:09,786 - __main__ - INFO - Epoch 52 Batch 100: Train Loss = 0.1860
2023-11-08 03:15:26,210 - __main__ - INFO - Epoch 52 Batch 120: Train Loss = 0.1917
2023-11-08 03:15:39,112 - __main__ - INFO - Epoch 52: Loss = 0.2235 Valid loss = 0.2010 roc = 0.8620
2023-11-08 03:15:39,890 - __main__ - INFO - Epoch 53 Batch 0: Train Loss = 0.2109
2023-11-08 03:15:57,089 - __main__ - INFO - Epoch 53 Batch 20: Train Loss = 0.2546
2023-11-08 03:16:14,345 - __main__ - INFO - Epoch 53 Batch 40: Train Loss = 0.2004
2023-11-08 03:16:31,220 - __main__ - INFO - Epoch 53 Batch 60: Train Loss = 0.2069
2023-11-08 03:16:48,003 - __main__ - INFO - Epoch 53 Batch 80: Train Loss = 0.2059
2023-11-08 03:17:04,998 - __main__ - INFO - Epoch 53 Batch 100: Train Loss = 0.2126
2023-11-08 03:17:24,109 - __main__ - INFO - Epoch 53 Batch 120: Train Loss = 0.2126
2023-11-08 03:17:37,766 - __main__ - INFO - Epoch 53: Loss = 0.2206 Valid loss = 0.1991 roc = 0.8601
2023-11-08 03:17:38,561 - __main__ - INFO - Epoch 54 Batch 0: Train Loss = 0.1869
2023-11-08 03:17:55,830 - __main__ - INFO - Epoch 54 Batch 20: Train Loss = 0.2740
2023-11-08 03:18:12,340 - __main__ - INFO - Epoch 54 Batch 40: Train Loss = 0.2506
2023-11-08 03:18:31,088 - __main__ - INFO - Epoch 54 Batch 60: Train Loss = 0.1599
2023-11-08 03:18:48,906 - __main__ - INFO - Epoch 54 Batch 80: Train Loss = 0.1800
2023-11-08 03:19:06,692 - __main__ - INFO - Epoch 54 Batch 100: Train Loss = 0.2416
2023-11-08 03:19:23,418 - __main__ - INFO - Epoch 54 Batch 120: Train Loss = 0.1276
2023-11-08 03:19:36,799 - __main__ - INFO - Epoch 54: Loss = 0.2207 Valid loss = 0.1990 roc = 0.8659
2023-11-08 03:19:37,708 - __main__ - INFO - Epoch 55 Batch 0: Train Loss = 0.1972
2023-11-08 03:19:54,728 - __main__ - INFO - Epoch 55 Batch 20: Train Loss = 0.1989
2023-11-08 03:20:11,437 - __main__ - INFO - Epoch 55 Batch 40: Train Loss = 0.2279
2023-11-08 03:20:28,297 - __main__ - INFO - Epoch 55 Batch 60: Train Loss = 0.2319
2023-11-08 03:20:45,009 - __main__ - INFO - Epoch 55 Batch 80: Train Loss = 0.1966
2023-11-08 03:21:03,867 - __main__ - INFO - Epoch 55 Batch 100: Train Loss = 0.1533
2023-11-08 03:21:20,592 - __main__ - INFO - Epoch 55 Batch 120: Train Loss = 0.2202
2023-11-08 03:21:33,524 - __main__ - INFO - Epoch 55: Loss = 0.2194 Valid loss = 0.1982 roc = 0.8575
2023-11-08 03:21:34,363 - __main__ - INFO - Epoch 56 Batch 0: Train Loss = 0.2170
2023-11-08 03:21:51,350 - __main__ - INFO - Epoch 56 Batch 20: Train Loss = 0.2191
2023-11-08 03:22:09,268 - __main__ - INFO - Epoch 56 Batch 40: Train Loss = 0.2364
2023-11-08 03:22:28,315 - __main__ - INFO - Epoch 56 Batch 60: Train Loss = 0.1887
2023-11-08 03:22:45,785 - __main__ - INFO - Epoch 56 Batch 80: Train Loss = 0.1290
2023-11-08 03:23:03,177 - __main__ - INFO - Epoch 56 Batch 100: Train Loss = 0.2112
2023-11-08 03:23:20,667 - __main__ - INFO - Epoch 56 Batch 120: Train Loss = 0.1489
2023-11-08 03:23:33,640 - __main__ - INFO - Epoch 56: Loss = 0.2200 Valid loss = 0.1992 roc = 0.8642
2023-11-08 03:23:34,327 - __main__ - INFO - Epoch 57 Batch 0: Train Loss = 0.1846
2023-11-08 03:23:51,982 - __main__ - INFO - Epoch 57 Batch 20: Train Loss = 0.2371
2023-11-08 03:24:09,774 - __main__ - INFO - Epoch 57 Batch 40: Train Loss = 0.2245
2023-11-08 03:24:27,548 - __main__ - INFO - Epoch 57 Batch 60: Train Loss = 0.1891
2023-11-08 03:24:45,565 - __main__ - INFO - Epoch 57 Batch 80: Train Loss = 0.2492
2023-11-08 03:25:03,692 - __main__ - INFO - Epoch 57 Batch 100: Train Loss = 0.2398
2023-11-08 03:25:19,553 - __main__ - INFO - Epoch 57 Batch 120: Train Loss = 0.2874
2023-11-08 03:25:32,014 - __main__ - INFO - Epoch 57: Loss = 0.2190 Valid loss = 0.1974 roc = 0.8578
2023-11-08 03:25:32,929 - __main__ - INFO - Epoch 58 Batch 0: Train Loss = 0.2621
2023-11-08 03:25:50,773 - __main__ - INFO - Epoch 58 Batch 20: Train Loss = 0.1913
2023-11-08 03:26:07,158 - __main__ - INFO - Epoch 58 Batch 40: Train Loss = 0.1982
2023-11-08 03:26:24,563 - __main__ - INFO - Epoch 58 Batch 60: Train Loss = 0.2732
2023-11-08 03:26:40,727 - __main__ - INFO - Epoch 58 Batch 80: Train Loss = 0.2166
2023-11-08 03:26:57,963 - __main__ - INFO - Epoch 58 Batch 100: Train Loss = 0.1128
2023-11-08 03:27:14,950 - __main__ - INFO - Epoch 58 Batch 120: Train Loss = 0.2428
2023-11-08 03:27:28,194 - __main__ - INFO - Epoch 58: Loss = 0.2177 Valid loss = 0.1959 roc = 0.8584
2023-11-08 03:27:28,897 - __main__ - INFO - Epoch 59 Batch 0: Train Loss = 0.2279
2023-11-08 03:27:46,284 - __main__ - INFO - Epoch 59 Batch 20: Train Loss = 0.1874
2023-11-08 03:28:02,606 - __main__ - INFO - Epoch 59 Batch 40: Train Loss = 0.2033
2023-11-08 03:28:19,577 - __main__ - INFO - Epoch 59 Batch 60: Train Loss = 0.1901
2023-11-08 03:28:36,496 - __main__ - INFO - Epoch 59 Batch 80: Train Loss = 0.1724
2023-11-08 03:28:55,107 - __main__ - INFO - Epoch 59 Batch 100: Train Loss = 0.2254
2023-11-08 03:29:12,135 - __main__ - INFO - Epoch 59 Batch 120: Train Loss = 0.1618
2023-11-08 03:29:24,585 - __main__ - INFO - Epoch 59: Loss = 0.2184 Valid loss = 0.1978 roc = 0.8569
2023-11-08 03:29:25,500 - __main__ - INFO - Epoch 60 Batch 0: Train Loss = 0.1725
2023-11-08 03:29:43,176 - __main__ - INFO - Epoch 60 Batch 20: Train Loss = 0.2171
2023-11-08 03:30:00,578 - __main__ - INFO - Epoch 60 Batch 40: Train Loss = 0.1767
2023-11-08 03:30:18,493 - __main__ - INFO - Epoch 60 Batch 60: Train Loss = 0.2195
2023-11-08 03:30:36,321 - __main__ - INFO - Epoch 60 Batch 80: Train Loss = 0.1761
2023-11-08 03:30:54,764 - __main__ - INFO - Epoch 60 Batch 100: Train Loss = 0.2526
2023-11-08 03:31:12,159 - __main__ - INFO - Epoch 60 Batch 120: Train Loss = 0.2381
2023-11-08 03:31:25,586 - __main__ - INFO - Epoch 60: Loss = 0.2177 Valid loss = 0.2000 roc = 0.8605
2023-11-08 03:31:26,834 - __main__ - INFO - Epoch 61 Batch 0: Train Loss = 0.2334
2023-11-08 03:31:44,644 - __main__ - INFO - Epoch 61 Batch 20: Train Loss = 0.1728
2023-11-08 03:32:02,094 - __main__ - INFO - Epoch 61 Batch 40: Train Loss = 0.2102
2023-11-08 03:32:18,636 - __main__ - INFO - Epoch 61 Batch 60: Train Loss = 0.2009
2023-11-08 03:32:36,578 - __main__ - INFO - Epoch 61 Batch 80: Train Loss = 0.2365
2023-11-08 03:32:55,558 - __main__ - INFO - Epoch 61 Batch 100: Train Loss = 0.2222
2023-11-08 03:33:11,252 - __main__ - INFO - Epoch 61 Batch 120: Train Loss = 0.2468
2023-11-08 03:33:25,155 - __main__ - INFO - Epoch 61: Loss = 0.2161 Valid loss = 0.1965 roc = 0.8574
2023-11-08 03:33:26,033 - __main__ - INFO - Epoch 62 Batch 0: Train Loss = 0.1847
2023-11-08 03:33:42,173 - __main__ - INFO - Epoch 62 Batch 20: Train Loss = 0.2137
2023-11-08 03:33:58,787 - __main__ - INFO - Epoch 62 Batch 40: Train Loss = 0.2756
2023-11-08 03:34:16,009 - __main__ - INFO - Epoch 62 Batch 60: Train Loss = 0.2463
2023-11-08 03:34:33,085 - __main__ - INFO - Epoch 62 Batch 80: Train Loss = 0.1979
2023-11-08 03:34:50,628 - __main__ - INFO - Epoch 62 Batch 100: Train Loss = 0.2388
2023-11-08 03:35:08,015 - __main__ - INFO - Epoch 62 Batch 120: Train Loss = 0.1415
2023-11-08 03:35:21,358 - __main__ - INFO - Epoch 62: Loss = 0.2165 Valid loss = 0.1946 roc = 0.8566
2023-11-08 03:35:22,243 - __main__ - INFO - Epoch 63 Batch 0: Train Loss = 0.1652
2023-11-08 03:35:38,971 - __main__ - INFO - Epoch 63 Batch 20: Train Loss = 0.2481
2023-11-08 03:35:55,228 - __main__ - INFO - Epoch 63 Batch 40: Train Loss = 0.1820
2023-11-08 03:36:12,976 - __main__ - INFO - Epoch 63 Batch 60: Train Loss = 0.2129
2023-11-08 03:36:28,689 - __main__ - INFO - Epoch 63 Batch 80: Train Loss = 0.2323
2023-11-08 03:36:46,563 - __main__ - INFO - Epoch 63 Batch 100: Train Loss = 0.2546
2023-11-08 03:37:04,055 - __main__ - INFO - Epoch 63 Batch 120: Train Loss = 0.1267
2023-11-08 03:37:17,152 - __main__ - INFO - Epoch 63: Loss = 0.2170 Valid loss = 0.1953 roc = 0.8663
2023-11-08 03:37:17,903 - __main__ - INFO - Epoch 64 Batch 0: Train Loss = 0.2579
2023-11-08 03:37:35,189 - __main__ - INFO - Epoch 64 Batch 20: Train Loss = 0.2461
2023-11-08 03:37:51,990 - __main__ - INFO - Epoch 64 Batch 40: Train Loss = 0.1781
2023-11-08 03:38:11,000 - __main__ - INFO - Epoch 64 Batch 60: Train Loss = 0.2354
2023-11-08 03:38:28,477 - __main__ - INFO - Epoch 64 Batch 80: Train Loss = 0.2351
2023-11-08 03:38:44,706 - __main__ - INFO - Epoch 64 Batch 100: Train Loss = 0.1977
2023-11-08 03:39:03,221 - __main__ - INFO - Epoch 64 Batch 120: Train Loss = 0.2402
2023-11-08 03:39:17,467 - __main__ - INFO - Epoch 64: Loss = 0.2164 Valid loss = 0.1963 roc = 0.8631
2023-11-08 03:39:18,624 - __main__ - INFO - Epoch 65 Batch 0: Train Loss = 0.2684
2023-11-08 03:39:36,156 - __main__ - INFO - Epoch 65 Batch 20: Train Loss = 0.1978
2023-11-08 03:39:52,144 - __main__ - INFO - Epoch 65 Batch 40: Train Loss = 0.1995
2023-11-08 03:40:08,663 - __main__ - INFO - Epoch 65 Batch 60: Train Loss = 0.1685
2023-11-08 03:40:25,852 - __main__ - INFO - Epoch 65 Batch 80: Train Loss = 0.2627
2023-11-08 03:40:43,347 - __main__ - INFO - Epoch 65 Batch 100: Train Loss = 0.2489
2023-11-08 03:41:00,697 - __main__ - INFO - Epoch 65 Batch 120: Train Loss = 0.2087
2023-11-08 03:41:14,277 - __main__ - INFO - Epoch 65: Loss = 0.2211 Valid loss = 0.1994 roc = 0.8618
2023-11-08 03:41:15,117 - __main__ - INFO - Epoch 66 Batch 0: Train Loss = 0.2644
2023-11-08 03:41:31,412 - __main__ - INFO - Epoch 66 Batch 20: Train Loss = 0.2518
2023-11-08 03:41:47,778 - __main__ - INFO - Epoch 66 Batch 40: Train Loss = 0.2098
2023-11-08 03:42:05,474 - __main__ - INFO - Epoch 66 Batch 60: Train Loss = 0.2230
2023-11-08 03:42:23,147 - __main__ - INFO - Epoch 66 Batch 80: Train Loss = 0.2602
2023-11-08 03:42:40,853 - __main__ - INFO - Epoch 66 Batch 100: Train Loss = 0.2507
2023-11-08 03:42:58,504 - __main__ - INFO - Epoch 66 Batch 120: Train Loss = 0.2154
2023-11-08 03:43:10,783 - __main__ - INFO - Epoch 66: Loss = 0.2194 Valid loss = 0.2008 roc = 0.8431
2023-11-08 03:43:11,769 - __main__ - INFO - Epoch 67 Batch 0: Train Loss = 0.2032
2023-11-08 03:43:28,243 - __main__ - INFO - Epoch 67 Batch 20: Train Loss = 0.2146
2023-11-08 03:43:45,013 - __main__ - INFO - Epoch 67 Batch 40: Train Loss = 0.2066
2023-11-08 03:44:01,637 - __main__ - INFO - Epoch 67 Batch 60: Train Loss = 0.2776
2023-11-08 03:44:19,443 - __main__ - INFO - Epoch 67 Batch 80: Train Loss = 0.2034
2023-11-08 03:44:36,439 - __main__ - INFO - Epoch 67 Batch 100: Train Loss = 0.2223
2023-11-08 03:44:53,575 - __main__ - INFO - Epoch 67 Batch 120: Train Loss = 0.2942
2023-11-08 03:45:08,019 - __main__ - INFO - Epoch 67: Loss = 0.2195 Valid loss = 0.2023 roc = 0.8482
2023-11-08 03:45:08,792 - __main__ - INFO - Epoch 68 Batch 0: Train Loss = 0.2267
2023-11-08 03:45:24,961 - __main__ - INFO - Epoch 68 Batch 20: Train Loss = 0.2349
2023-11-08 03:45:42,407 - __main__ - INFO - Epoch 68 Batch 40: Train Loss = 0.2463
2023-11-08 03:46:00,149 - __main__ - INFO - Epoch 68 Batch 60: Train Loss = 0.1786
2023-11-08 03:46:17,586 - __main__ - INFO - Epoch 68 Batch 80: Train Loss = 0.2628
2023-11-08 03:46:35,726 - __main__ - INFO - Epoch 68 Batch 100: Train Loss = 0.2590
2023-11-08 03:46:52,953 - __main__ - INFO - Epoch 68 Batch 120: Train Loss = 0.2618
2023-11-08 03:47:08,681 - __main__ - INFO - Epoch 68: Loss = 0.2209 Valid loss = 0.2006 roc = 0.8576
2023-11-08 03:47:09,887 - __main__ - INFO - Epoch 69 Batch 0: Train Loss = 0.2460
2023-11-08 03:47:25,931 - __main__ - INFO - Epoch 69 Batch 20: Train Loss = 0.2128
2023-11-08 03:47:44,136 - __main__ - INFO - Epoch 69 Batch 40: Train Loss = 0.2844
2023-11-08 03:48:00,137 - __main__ - INFO - Epoch 69 Batch 60: Train Loss = 0.2050
2023-11-08 03:48:17,886 - __main__ - INFO - Epoch 69 Batch 80: Train Loss = 0.2973
2023-11-08 03:48:35,776 - __main__ - INFO - Epoch 69 Batch 100: Train Loss = 0.2695
2023-11-08 03:48:53,527 - __main__ - INFO - Epoch 69 Batch 120: Train Loss = 0.2039
2023-11-08 03:49:06,931 - __main__ - INFO - Epoch 69: Loss = 0.2175 Valid loss = 0.1997 roc = 0.8508
2023-11-08 03:49:07,874 - __main__ - INFO - Epoch 70 Batch 0: Train Loss = 0.2435
2023-11-08 03:49:23,745 - __main__ - INFO - Epoch 70 Batch 20: Train Loss = 0.2029
2023-11-08 03:49:41,119 - __main__ - INFO - Epoch 70 Batch 40: Train Loss = 0.2163
2023-11-08 03:49:59,056 - __main__ - INFO - Epoch 70 Batch 60: Train Loss = 0.2329
2023-11-08 03:50:15,039 - __main__ - INFO - Epoch 70 Batch 80: Train Loss = 0.2289
2023-11-08 03:50:32,205 - __main__ - INFO - Epoch 70 Batch 100: Train Loss = 0.2067
2023-11-08 03:50:49,840 - __main__ - INFO - Epoch 70 Batch 120: Train Loss = 0.1919
2023-11-08 03:51:02,874 - __main__ - INFO - Epoch 70: Loss = 0.2170 Valid loss = 0.1977 roc = 0.8617
2023-11-08 03:51:03,585 - __main__ - INFO - Epoch 71 Batch 0: Train Loss = 0.2072
2023-11-08 03:51:21,426 - __main__ - INFO - Epoch 71 Batch 20: Train Loss = 0.1932
2023-11-08 03:51:37,951 - __main__ - INFO - Epoch 71 Batch 40: Train Loss = 0.2253
2023-11-08 03:51:54,723 - __main__ - INFO - Epoch 71 Batch 60: Train Loss = 0.2881
2023-11-08 03:52:10,959 - __main__ - INFO - Epoch 71 Batch 80: Train Loss = 0.2313
2023-11-08 03:52:27,988 - __main__ - INFO - Epoch 71 Batch 100: Train Loss = 0.2152
2023-11-08 03:52:46,349 - __main__ - INFO - Epoch 71 Batch 120: Train Loss = 0.2545
2023-11-08 03:53:00,015 - __main__ - INFO - Epoch 71: Loss = 0.2222 Valid loss = 0.2000 roc = 0.8626
2023-11-08 03:53:00,862 - __main__ - INFO - Epoch 72 Batch 0: Train Loss = 0.2581
2023-11-08 03:53:17,689 - __main__ - INFO - Epoch 72 Batch 20: Train Loss = 0.2683
2023-11-08 03:53:34,388 - __main__ - INFO - Epoch 72 Batch 40: Train Loss = 0.2252
2023-11-08 03:53:49,741 - __main__ - INFO - Epoch 72 Batch 60: Train Loss = 0.1579
2023-11-08 03:54:06,667 - __main__ - INFO - Epoch 72 Batch 80: Train Loss = 0.1688
2023-11-08 03:54:25,543 - __main__ - INFO - Epoch 72 Batch 100: Train Loss = 0.2322
2023-11-08 03:54:42,533 - __main__ - INFO - Epoch 72 Batch 120: Train Loss = 0.2825
2023-11-08 03:54:55,257 - __main__ - INFO - Epoch 72: Loss = 0.2218 Valid loss = 0.2045 roc = 0.8464
2023-11-08 03:54:56,071 - __main__ - INFO - Epoch 73 Batch 0: Train Loss = 0.1853
2023-11-08 03:55:13,816 - __main__ - INFO - Epoch 73 Batch 20: Train Loss = 0.2394
2023-11-08 03:55:30,024 - __main__ - INFO - Epoch 73 Batch 40: Train Loss = 0.2887
2023-11-08 03:55:47,814 - __main__ - INFO - Epoch 73 Batch 60: Train Loss = 0.2466
2023-11-08 03:56:05,250 - __main__ - INFO - Epoch 73 Batch 80: Train Loss = 0.2367
2023-11-08 03:56:23,300 - __main__ - INFO - Epoch 73 Batch 100: Train Loss = 0.1363
2023-11-08 03:56:39,897 - __main__ - INFO - Epoch 73 Batch 120: Train Loss = 0.2411
2023-11-08 03:56:53,749 - __main__ - INFO - Epoch 73: Loss = 0.2207 Valid loss = 0.1991 roc = 0.8564
2023-11-08 03:56:54,593 - __main__ - INFO - Epoch 74 Batch 0: Train Loss = 0.1952
2023-11-08 03:57:11,760 - __main__ - INFO - Epoch 74 Batch 20: Train Loss = 0.2456
2023-11-08 03:57:28,989 - __main__ - INFO - Epoch 74 Batch 40: Train Loss = 0.2405
2023-11-08 03:57:47,184 - __main__ - INFO - Epoch 74 Batch 60: Train Loss = 0.2904
2023-11-08 03:58:04,247 - __main__ - INFO - Epoch 74 Batch 80: Train Loss = 0.2029
2023-11-08 03:58:21,603 - __main__ - INFO - Epoch 74 Batch 100: Train Loss = 0.1794
2023-11-08 03:58:39,678 - __main__ - INFO - Epoch 74 Batch 120: Train Loss = 0.3052
2023-11-08 03:58:53,238 - __main__ - INFO - Epoch 74: Loss = 0.2167 Valid loss = 0.1986 roc = 0.8580
2023-11-08 03:58:54,016 - __main__ - INFO - Epoch 75 Batch 0: Train Loss = 0.2440
2023-11-08 03:59:11,554 - __main__ - INFO - Epoch 75 Batch 20: Train Loss = 0.2369
2023-11-08 03:59:28,637 - __main__ - INFO - Epoch 75 Batch 40: Train Loss = 0.2470
2023-11-08 03:59:44,634 - __main__ - INFO - Epoch 75 Batch 60: Train Loss = 0.1626
2023-11-08 04:00:01,371 - __main__ - INFO - Epoch 75 Batch 80: Train Loss = 0.1894
2023-11-08 04:00:17,717 - __main__ - INFO - Epoch 75 Batch 100: Train Loss = 0.1953
2023-11-08 04:00:36,043 - __main__ - INFO - Epoch 75 Batch 120: Train Loss = 0.1833
2023-11-08 04:00:49,211 - __main__ - INFO - Epoch 75: Loss = 0.2202 Valid loss = 0.1983 roc = 0.8603
2023-11-08 04:00:50,066 - __main__ - INFO - Epoch 76 Batch 0: Train Loss = 0.1893
2023-11-08 04:01:08,349 - __main__ - INFO - Epoch 76 Batch 20: Train Loss = 0.2197
2023-11-08 04:01:24,753 - __main__ - INFO - Epoch 76 Batch 40: Train Loss = 0.2062
2023-11-08 04:01:41,102 - __main__ - INFO - Epoch 76 Batch 60: Train Loss = 0.2078
2023-11-08 04:01:58,157 - __main__ - INFO - Epoch 76 Batch 80: Train Loss = 0.1850
2023-11-08 04:02:16,701 - __main__ - INFO - Epoch 76 Batch 100: Train Loss = 0.1683
2023-11-08 04:02:33,942 - __main__ - INFO - Epoch 76 Batch 120: Train Loss = 0.1910
2023-11-08 04:02:47,330 - __main__ - INFO - Epoch 76: Loss = 0.2188 Valid loss = 0.1995 roc = 0.8604
2023-11-08 04:02:48,078 - __main__ - INFO - Epoch 77 Batch 0: Train Loss = 0.1720
2023-11-08 04:03:05,628 - __main__ - INFO - Epoch 77 Batch 20: Train Loss = 0.2681
2023-11-08 04:03:22,809 - __main__ - INFO - Epoch 77 Batch 40: Train Loss = 0.2712
2023-11-08 04:03:38,817 - __main__ - INFO - Epoch 77 Batch 60: Train Loss = 0.2497
2023-11-08 04:03:55,336 - __main__ - INFO - Epoch 77 Batch 80: Train Loss = 0.2534
2023-11-08 04:04:11,549 - __main__ - INFO - Epoch 77 Batch 100: Train Loss = 0.1669
2023-11-08 04:04:29,153 - __main__ - INFO - Epoch 77 Batch 120: Train Loss = 0.1264
2023-11-08 04:04:43,578 - __main__ - INFO - Epoch 77: Loss = 0.2171 Valid loss = 0.1981 roc = 0.8591
2023-11-08 04:04:44,361 - __main__ - INFO - Epoch 78 Batch 0: Train Loss = 0.1429
2023-11-08 04:05:02,183 - __main__ - INFO - Epoch 78 Batch 20: Train Loss = 0.2328
2023-11-08 04:05:21,045 - __main__ - INFO - Epoch 78 Batch 40: Train Loss = 0.2730
2023-11-08 04:05:38,337 - __main__ - INFO - Epoch 78 Batch 60: Train Loss = 0.1817
2023-11-08 04:05:55,690 - __main__ - INFO - Epoch 78 Batch 80: Train Loss = 0.2301
2023-11-08 04:06:13,179 - __main__ - INFO - Epoch 78 Batch 100: Train Loss = 0.1556
2023-11-08 04:06:30,186 - __main__ - INFO - Epoch 78 Batch 120: Train Loss = 0.2682
2023-11-08 04:06:44,028 - __main__ - INFO - Epoch 78: Loss = 0.2170 Valid loss = 0.1983 roc = 0.8551
2023-11-08 04:06:44,918 - __main__ - INFO - Epoch 79 Batch 0: Train Loss = 0.2011
2023-11-08 04:07:01,652 - __main__ - INFO - Epoch 79 Batch 20: Train Loss = 0.1558
2023-11-08 04:07:19,150 - __main__ - INFO - Epoch 79 Batch 40: Train Loss = 0.1732
2023-11-08 04:07:34,991 - __main__ - INFO - Epoch 79 Batch 60: Train Loss = 0.1273
2023-11-08 04:07:52,009 - __main__ - INFO - Epoch 79 Batch 80: Train Loss = 0.2570
2023-11-08 04:08:09,502 - __main__ - INFO - Epoch 79 Batch 100: Train Loss = 0.2175
2023-11-08 04:08:28,036 - __main__ - INFO - Epoch 79 Batch 120: Train Loss = 0.1597
2023-11-08 04:08:40,996 - __main__ - INFO - Epoch 79: Loss = 0.2167 Valid loss = 0.1969 roc = 0.8662
2023-11-08 04:08:41,942 - __main__ - INFO - Epoch 80 Batch 0: Train Loss = 0.1914
2023-11-08 04:08:58,879 - __main__ - INFO - Epoch 80 Batch 20: Train Loss = 0.3011
2023-11-08 04:09:15,454 - __main__ - INFO - Epoch 80 Batch 40: Train Loss = 0.1916
2023-11-08 04:09:32,466 - __main__ - INFO - Epoch 80 Batch 60: Train Loss = 0.2723
2023-11-08 04:09:50,060 - __main__ - INFO - Epoch 80 Batch 80: Train Loss = 0.2588
2023-11-08 04:10:06,918 - __main__ - INFO - Epoch 80 Batch 100: Train Loss = 0.2045
2023-11-08 04:10:25,302 - __main__ - INFO - Epoch 80 Batch 120: Train Loss = 0.1930
2023-11-08 04:10:39,824 - __main__ - INFO - Epoch 80: Loss = 0.2178 Valid loss = 0.1989 roc = 0.8654
2023-11-08 04:10:40,915 - __main__ - INFO - Epoch 81 Batch 0: Train Loss = 0.1799
2023-11-08 04:10:58,264 - __main__ - INFO - Epoch 81 Batch 20: Train Loss = 0.2601
2023-11-08 04:11:16,621 - __main__ - INFO - Epoch 81 Batch 40: Train Loss = 0.2760
2023-11-08 04:11:32,850 - __main__ - INFO - Epoch 81 Batch 60: Train Loss = 0.2451
2023-11-08 04:11:49,853 - __main__ - INFO - Epoch 81 Batch 80: Train Loss = 0.2352
2023-11-08 04:12:06,168 - __main__ - INFO - Epoch 81 Batch 100: Train Loss = 0.2108
2023-11-08 04:12:22,978 - __main__ - INFO - Epoch 81 Batch 120: Train Loss = 0.2578
2023-11-08 04:12:36,059 - __main__ - INFO - Epoch 81: Loss = 0.2155 Valid loss = 0.1961 roc = 0.8666
2023-11-08 04:12:36,786 - __main__ - INFO - Epoch 82 Batch 0: Train Loss = 0.1529
2023-11-08 04:12:55,371 - __main__ - INFO - Epoch 82 Batch 20: Train Loss = 0.1907
2023-11-08 04:13:12,564 - __main__ - INFO - Epoch 82 Batch 40: Train Loss = 0.1772
2023-11-08 04:13:29,114 - __main__ - INFO - Epoch 82 Batch 60: Train Loss = 0.2212
2023-11-08 04:13:46,065 - __main__ - INFO - Epoch 82 Batch 80: Train Loss = 0.1976
2023-11-08 04:14:02,849 - __main__ - INFO - Epoch 82 Batch 100: Train Loss = 0.2232
2023-11-08 04:14:20,318 - __main__ - INFO - Epoch 82 Batch 120: Train Loss = 0.2446
2023-11-08 04:14:35,276 - __main__ - INFO - Epoch 82: Loss = 0.2151 Valid loss = 0.1967 roc = 0.8667
2023-11-08 04:14:36,258 - __main__ - INFO - Epoch 83 Batch 0: Train Loss = 0.2353
2023-11-08 04:14:53,667 - __main__ - INFO - Epoch 83 Batch 20: Train Loss = 0.2035
2023-11-08 04:15:10,885 - __main__ - INFO - Epoch 83 Batch 40: Train Loss = 0.2127
2023-11-08 04:15:27,759 - __main__ - INFO - Epoch 83 Batch 60: Train Loss = 0.2065
2023-11-08 04:15:45,133 - __main__ - INFO - Epoch 83 Batch 80: Train Loss = 0.2150
2023-11-08 04:16:02,862 - __main__ - INFO - Epoch 83 Batch 100: Train Loss = 0.1870
2023-11-08 04:16:21,299 - __main__ - INFO - Epoch 83 Batch 120: Train Loss = 0.2095
2023-11-08 04:16:35,259 - __main__ - INFO - Epoch 83: Loss = 0.2180 Valid loss = 0.1951 roc = 0.8659
2023-11-08 04:16:36,151 - __main__ - INFO - Epoch 84 Batch 0: Train Loss = 0.1638
2023-11-08 04:16:53,265 - __main__ - INFO - Epoch 84 Batch 20: Train Loss = 0.2322
2023-11-08 04:17:10,247 - __main__ - INFO - Epoch 84 Batch 40: Train Loss = 0.2146
2023-11-08 04:17:26,913 - __main__ - INFO - Epoch 84 Batch 60: Train Loss = 0.1427
2023-11-08 04:17:44,007 - __main__ - INFO - Epoch 84 Batch 80: Train Loss = 0.1707
2023-11-08 04:18:00,680 - __main__ - INFO - Epoch 84 Batch 100: Train Loss = 0.2252
2023-11-08 04:18:17,853 - __main__ - INFO - Epoch 84 Batch 120: Train Loss = 0.2639
2023-11-08 04:18:31,080 - __main__ - INFO - Epoch 84: Loss = 0.2158 Valid loss = 0.1990 roc = 0.8599
2023-11-08 04:18:31,635 - __main__ - INFO - Epoch 85 Batch 0: Train Loss = 0.1943
2023-11-08 04:18:49,201 - __main__ - INFO - Epoch 85 Batch 20: Train Loss = 0.2259
2023-11-08 04:19:06,679 - __main__ - INFO - Epoch 85 Batch 40: Train Loss = 0.2551
2023-11-08 04:19:23,764 - __main__ - INFO - Epoch 85 Batch 60: Train Loss = 0.2313
2023-11-08 04:19:39,520 - __main__ - INFO - Epoch 85 Batch 80: Train Loss = 0.2072
2023-11-08 04:19:56,379 - __main__ - INFO - Epoch 85 Batch 100: Train Loss = 0.2282
2023-11-08 04:20:13,553 - __main__ - INFO - Epoch 85 Batch 120: Train Loss = 0.2641
2023-11-08 04:20:26,820 - __main__ - INFO - Epoch 85: Loss = 0.2188 Valid loss = 0.1988 roc = 0.8645
2023-11-08 04:20:27,578 - __main__ - INFO - Epoch 86 Batch 0: Train Loss = 0.2116
2023-11-08 04:20:45,113 - __main__ - INFO - Epoch 86 Batch 20: Train Loss = 0.2163
2023-11-08 04:21:03,105 - __main__ - INFO - Epoch 86 Batch 40: Train Loss = 0.2508
2023-11-08 04:21:20,918 - __main__ - INFO - Epoch 86 Batch 60: Train Loss = 0.2874
2023-11-08 04:21:37,966 - __main__ - INFO - Epoch 86 Batch 80: Train Loss = 0.1647
2023-11-08 04:21:54,897 - __main__ - INFO - Epoch 86 Batch 100: Train Loss = 0.1862
2023-11-08 04:22:11,817 - __main__ - INFO - Epoch 86 Batch 120: Train Loss = 0.1982
2023-11-08 04:22:25,995 - __main__ - INFO - Epoch 86: Loss = 0.2169 Valid loss = 0.1946 roc = 0.8694
2023-11-08 04:22:26,890 - __main__ - INFO - Epoch 87 Batch 0: Train Loss = 0.1526
2023-11-08 04:22:44,179 - __main__ - INFO - Epoch 87 Batch 20: Train Loss = 0.2032
2023-11-08 04:23:02,025 - __main__ - INFO - Epoch 87 Batch 40: Train Loss = 0.1751
2023-11-08 04:23:20,270 - __main__ - INFO - Epoch 87 Batch 60: Train Loss = 0.2126
2023-11-08 04:23:37,118 - __main__ - INFO - Epoch 87 Batch 80: Train Loss = 0.2481
2023-11-08 04:23:54,551 - __main__ - INFO - Epoch 87 Batch 100: Train Loss = 0.2579
2023-11-08 04:24:11,907 - __main__ - INFO - Epoch 87 Batch 120: Train Loss = 0.2477
2023-11-08 04:24:25,291 - __main__ - INFO - Epoch 87: Loss = 0.2153 Valid loss = 0.1963 roc = 0.8568
2023-11-08 04:24:26,279 - __main__ - INFO - Epoch 88 Batch 0: Train Loss = 0.2167
2023-11-08 04:24:43,452 - __main__ - INFO - Epoch 88 Batch 20: Train Loss = 0.2176
2023-11-08 04:24:59,943 - __main__ - INFO - Epoch 88 Batch 40: Train Loss = 0.2345
2023-11-08 04:25:17,385 - __main__ - INFO - Epoch 88 Batch 60: Train Loss = 0.2179
2023-11-08 04:25:35,021 - __main__ - INFO - Epoch 88 Batch 80: Train Loss = 0.2031
2023-11-08 04:25:50,637 - __main__ - INFO - Epoch 88 Batch 100: Train Loss = 0.1490
2023-11-08 04:26:07,862 - __main__ - INFO - Epoch 88 Batch 120: Train Loss = 0.2231
2023-11-08 04:26:20,242 - __main__ - INFO - Epoch 88: Loss = 0.2219 Valid loss = 0.1996 roc = 0.8587
2023-11-08 04:26:21,395 - __main__ - INFO - Epoch 89 Batch 0: Train Loss = 0.1865
2023-11-08 04:26:38,693 - __main__ - INFO - Epoch 89 Batch 20: Train Loss = 0.2470
2023-11-08 04:26:55,260 - __main__ - INFO - Epoch 89 Batch 40: Train Loss = 0.2146
2023-11-08 04:27:12,065 - __main__ - INFO - Epoch 89 Batch 60: Train Loss = 0.1974
2023-11-08 04:27:28,764 - __main__ - INFO - Epoch 89 Batch 80: Train Loss = 0.2806
2023-11-08 04:27:45,480 - __main__ - INFO - Epoch 89 Batch 100: Train Loss = 0.2289
2023-11-08 04:28:03,162 - __main__ - INFO - Epoch 89 Batch 120: Train Loss = 0.2651
2023-11-08 04:28:16,724 - __main__ - INFO - Epoch 89: Loss = 0.2196 Valid loss = 0.1982 roc = 0.8542
2023-11-08 04:28:17,395 - __main__ - INFO - Epoch 90 Batch 0: Train Loss = 0.1883
2023-11-08 04:28:36,260 - __main__ - INFO - Epoch 90 Batch 20: Train Loss = 0.1598
2023-11-08 04:28:54,479 - __main__ - INFO - Epoch 90 Batch 40: Train Loss = 0.2213
2023-11-08 04:29:11,902 - __main__ - INFO - Epoch 90 Batch 60: Train Loss = 0.1884
2023-11-08 04:29:29,465 - __main__ - INFO - Epoch 90 Batch 80: Train Loss = 0.2342
2023-11-08 04:29:46,023 - __main__ - INFO - Epoch 90 Batch 100: Train Loss = 0.1813
2023-11-08 04:30:02,866 - __main__ - INFO - Epoch 90 Batch 120: Train Loss = 0.2472
2023-11-08 04:30:15,842 - __main__ - INFO - Epoch 90: Loss = 0.2197 Valid loss = 0.1997 roc = 0.8646
2023-11-08 04:30:16,666 - __main__ - INFO - Epoch 91 Batch 0: Train Loss = 0.2086
2023-11-08 04:30:34,679 - __main__ - INFO - Epoch 91 Batch 20: Train Loss = 0.1783
2023-11-08 04:30:52,820 - __main__ - INFO - Epoch 91 Batch 40: Train Loss = 0.2478
2023-11-08 04:31:09,626 - __main__ - INFO - Epoch 91 Batch 60: Train Loss = 0.1663
2023-11-08 04:31:26,566 - __main__ - INFO - Epoch 91 Batch 80: Train Loss = 0.2156
2023-11-08 04:31:42,822 - __main__ - INFO - Epoch 91 Batch 100: Train Loss = 0.2301
2023-11-08 04:32:00,749 - __main__ - INFO - Epoch 91 Batch 120: Train Loss = 0.1649
2023-11-08 04:32:14,800 - __main__ - INFO - Epoch 91: Loss = 0.2144 Valid loss = 0.1958 roc = 0.8644
2023-11-08 04:32:15,930 - __main__ - INFO - Epoch 92 Batch 0: Train Loss = 0.2232
2023-11-08 04:32:33,995 - __main__ - INFO - Epoch 92 Batch 20: Train Loss = 0.1949
2023-11-08 04:32:51,154 - __main__ - INFO - Epoch 92 Batch 40: Train Loss = 0.2027
2023-11-08 04:33:08,080 - __main__ - INFO - Epoch 92 Batch 60: Train Loss = 0.1658
2023-11-08 04:33:25,992 - __main__ - INFO - Epoch 92 Batch 80: Train Loss = 0.2206
2023-11-08 04:33:43,378 - __main__ - INFO - Epoch 92 Batch 100: Train Loss = 0.2517
2023-11-08 04:34:00,428 - __main__ - INFO - Epoch 92 Batch 120: Train Loss = 0.2244
2023-11-08 04:34:14,542 - __main__ - INFO - Epoch 92: Loss = 0.2156 Valid loss = 0.1952 roc = 0.8650
2023-11-08 04:34:15,640 - __main__ - INFO - Epoch 93 Batch 0: Train Loss = 0.2331
2023-11-08 04:34:33,274 - __main__ - INFO - Epoch 93 Batch 20: Train Loss = 0.2393
2023-11-08 04:34:52,590 - __main__ - INFO - Epoch 93 Batch 40: Train Loss = 0.2539
2023-11-08 04:35:09,470 - __main__ - INFO - Epoch 93 Batch 60: Train Loss = 0.1321
2023-11-08 04:35:26,819 - __main__ - INFO - Epoch 93 Batch 80: Train Loss = 0.1812
2023-11-08 04:35:45,793 - __main__ - INFO - Epoch 93 Batch 100: Train Loss = 0.2149
2023-11-08 04:36:02,307 - __main__ - INFO - Epoch 93 Batch 120: Train Loss = 0.2303
2023-11-08 04:36:15,883 - __main__ - INFO - Epoch 93: Loss = 0.2192 Valid loss = 0.1970 roc = 0.8654
2023-11-08 04:36:16,647 - __main__ - INFO - Epoch 94 Batch 0: Train Loss = 0.1525
2023-11-08 04:36:34,187 - __main__ - INFO - Epoch 94 Batch 20: Train Loss = 0.2084
2023-11-08 04:36:50,374 - __main__ - INFO - Epoch 94 Batch 40: Train Loss = 0.1749
2023-11-08 04:37:07,695 - __main__ - INFO - Epoch 94 Batch 60: Train Loss = 0.2151
2023-11-08 04:37:22,853 - __main__ - INFO - Epoch 94 Batch 80: Train Loss = 0.2546
2023-11-08 04:37:40,163 - __main__ - INFO - Epoch 94 Batch 100: Train Loss = 0.2064
2023-11-08 04:37:57,673 - __main__ - INFO - Epoch 94 Batch 120: Train Loss = 0.2021
2023-11-08 04:38:11,880 - __main__ - INFO - Epoch 94: Loss = 0.2177 Valid loss = 0.1990 roc = 0.8589
2023-11-08 04:38:12,927 - __main__ - INFO - Epoch 95 Batch 0: Train Loss = 0.2294
2023-11-08 04:38:30,577 - __main__ - INFO - Epoch 95 Batch 20: Train Loss = 0.2212
2023-11-08 04:38:48,267 - __main__ - INFO - Epoch 95 Batch 40: Train Loss = 0.2381
2023-11-08 04:39:05,299 - __main__ - INFO - Epoch 95 Batch 60: Train Loss = 0.2391
2023-11-08 04:39:23,429 - __main__ - INFO - Epoch 95 Batch 80: Train Loss = 0.2815
2023-11-08 04:39:40,535 - __main__ - INFO - Epoch 95 Batch 100: Train Loss = 0.1679
2023-11-08 04:39:58,104 - __main__ - INFO - Epoch 95 Batch 120: Train Loss = 0.2348
2023-11-08 04:40:12,670 - __main__ - INFO - Epoch 95: Loss = 0.2148 Valid loss = 0.1970 roc = 0.8555
2023-11-08 04:40:13,450 - __main__ - INFO - Epoch 96 Batch 0: Train Loss = 0.2403
2023-11-08 04:40:31,710 - __main__ - INFO - Epoch 96 Batch 20: Train Loss = 0.2407
2023-11-08 04:40:48,669 - __main__ - INFO - Epoch 96 Batch 40: Train Loss = 0.2502
2023-11-08 04:41:05,896 - __main__ - INFO - Epoch 96 Batch 60: Train Loss = 0.2201
2023-11-08 04:41:22,472 - __main__ - INFO - Epoch 96 Batch 80: Train Loss = 0.2476
2023-11-08 04:41:40,765 - __main__ - INFO - Epoch 96 Batch 100: Train Loss = 0.2155
2023-11-08 04:41:58,208 - __main__ - INFO - Epoch 96 Batch 120: Train Loss = 0.2220
2023-11-08 04:42:11,532 - __main__ - INFO - Epoch 96: Loss = 0.2164 Valid loss = 0.1947 roc = 0.8557
2023-11-08 04:42:12,524 - __main__ - INFO - Epoch 97 Batch 0: Train Loss = 0.2106
2023-11-08 04:42:30,457 - __main__ - INFO - Epoch 97 Batch 20: Train Loss = 0.2001
2023-11-08 04:42:46,646 - __main__ - INFO - Epoch 97 Batch 40: Train Loss = 0.1821
2023-11-08 04:43:03,121 - __main__ - INFO - Epoch 97 Batch 60: Train Loss = 0.2088
2023-11-08 04:43:21,111 - __main__ - INFO - Epoch 97 Batch 80: Train Loss = 0.2292
2023-11-08 04:43:38,379 - __main__ - INFO - Epoch 97 Batch 100: Train Loss = 0.1573
2023-11-08 04:43:55,974 - __main__ - INFO - Epoch 97 Batch 120: Train Loss = 0.1592
2023-11-08 04:44:09,976 - __main__ - INFO - Epoch 97: Loss = 0.2155 Valid loss = 0.1954 roc = 0.8602
2023-11-08 04:44:10,875 - __main__ - INFO - Epoch 98 Batch 0: Train Loss = 0.1781
2023-11-08 04:44:26,595 - __main__ - INFO - Epoch 98 Batch 20: Train Loss = 0.2220
2023-11-08 04:44:44,337 - __main__ - INFO - Epoch 98 Batch 40: Train Loss = 0.1991
2023-11-08 04:45:00,080 - __main__ - INFO - Epoch 98 Batch 60: Train Loss = 0.2069
2023-11-08 04:45:17,761 - __main__ - INFO - Epoch 98 Batch 80: Train Loss = 0.1525
2023-11-08 04:45:36,358 - __main__ - INFO - Epoch 98 Batch 100: Train Loss = 0.2333
2023-11-08 04:45:52,950 - __main__ - INFO - Epoch 98 Batch 120: Train Loss = 0.2579
2023-11-08 04:46:07,127 - __main__ - INFO - Epoch 98: Loss = 0.2152 Valid loss = 0.1955 roc = 0.8594
2023-11-08 04:46:08,023 - __main__ - INFO - Epoch 99 Batch 0: Train Loss = 0.2291
2023-11-08 04:46:25,607 - __main__ - INFO - Epoch 99 Batch 20: Train Loss = 0.2319
2023-11-08 04:46:43,730 - __main__ - INFO - Epoch 99 Batch 40: Train Loss = 0.2107
2023-11-08 04:47:00,958 - __main__ - INFO - Epoch 99 Batch 60: Train Loss = 0.2049
2023-11-08 04:47:18,146 - __main__ - INFO - Epoch 99 Batch 80: Train Loss = 0.1861
2023-11-08 04:47:36,356 - __main__ - INFO - Epoch 99 Batch 100: Train Loss = 0.2120
2023-11-08 04:47:53,935 - __main__ - INFO - Epoch 99 Batch 120: Train Loss = 0.2880
2023-11-08 04:48:06,808 - __main__ - INFO - Epoch 99: Loss = 0.2167 Valid loss = 0.1966 roc = 0.8641
2023-11-08 04:48:07,803 - __main__ - INFO - Epoch 100 Batch 0: Train Loss = 0.2623
2023-11-08 04:48:26,383 - __main__ - INFO - Epoch 100 Batch 20: Train Loss = 0.1353
2023-11-08 04:48:43,338 - __main__ - INFO - Epoch 100 Batch 40: Train Loss = 0.2472
2023-11-08 04:49:00,864 - __main__ - INFO - Epoch 100 Batch 60: Train Loss = 0.2105
2023-11-08 04:49:17,887 - __main__ - INFO - Epoch 100 Batch 80: Train Loss = 0.2560
2023-11-08 04:49:33,160 - __main__ - INFO - Epoch 100 Batch 100: Train Loss = 0.2618
2023-11-08 04:49:50,802 - __main__ - INFO - Epoch 100 Batch 120: Train Loss = 0.2513
2023-11-08 04:50:05,293 - __main__ - INFO - Epoch 100: Loss = 0.2152 Valid loss = 0.1974 roc = 0.8642
2023-11-08 04:50:06,156 - __main__ - INFO - Epoch 101 Batch 0: Train Loss = 0.2226
2023-11-08 04:50:22,224 - __main__ - INFO - Epoch 101 Batch 20: Train Loss = 0.1992
2023-11-08 04:50:41,040 - __main__ - INFO - Epoch 101 Batch 40: Train Loss = 0.2624
2023-11-08 04:50:58,053 - __main__ - INFO - Epoch 101 Batch 60: Train Loss = 0.2462
2023-11-08 04:51:14,283 - __main__ - INFO - Epoch 101 Batch 80: Train Loss = 0.2247
2023-11-08 04:51:30,603 - __main__ - INFO - Epoch 101 Batch 100: Train Loss = 0.1815
2023-11-08 04:51:47,577 - __main__ - INFO - Epoch 101 Batch 120: Train Loss = 0.1999
2023-11-08 04:52:02,181 - __main__ - INFO - Epoch 101: Loss = 0.2157 Valid loss = 0.1946 roc = 0.8647
2023-11-08 04:52:03,257 - __main__ - INFO - Epoch 102 Batch 0: Train Loss = 0.2121
2023-11-08 04:52:19,823 - __main__ - INFO - Epoch 102 Batch 20: Train Loss = 0.2205
2023-11-08 04:52:40,118 - __main__ - INFO - Epoch 102 Batch 40: Train Loss = 0.1773
2023-11-08 04:52:57,015 - __main__ - INFO - Epoch 102 Batch 60: Train Loss = 0.2146
2023-11-08 04:53:14,602 - __main__ - INFO - Epoch 102 Batch 80: Train Loss = 0.1815
2023-11-08 04:53:30,579 - __main__ - INFO - Epoch 102 Batch 100: Train Loss = 0.2919
2023-11-08 04:53:47,505 - __main__ - INFO - Epoch 102 Batch 120: Train Loss = 0.1812
2023-11-08 04:54:00,458 - __main__ - INFO - Epoch 102: Loss = 0.2148 Valid loss = 0.1975 roc = 0.8590
2023-11-08 04:54:01,316 - __main__ - INFO - Epoch 103 Batch 0: Train Loss = 0.2321
2023-11-08 04:54:17,840 - __main__ - INFO - Epoch 103 Batch 20: Train Loss = 0.3018
2023-11-08 04:54:34,375 - __main__ - INFO - Epoch 103 Batch 40: Train Loss = 0.2174
2023-11-08 04:54:51,077 - __main__ - INFO - Epoch 103 Batch 60: Train Loss = 0.1703
2023-11-08 04:55:09,746 - __main__ - INFO - Epoch 103 Batch 80: Train Loss = 0.1393
2023-11-08 04:55:26,255 - __main__ - INFO - Epoch 103 Batch 100: Train Loss = 0.2154
2023-11-08 04:55:42,530 - __main__ - INFO - Epoch 103 Batch 120: Train Loss = 0.1548
2023-11-08 04:55:55,667 - __main__ - INFO - Epoch 103: Loss = 0.2154 Valid loss = 0.1983 roc = 0.8603
2023-11-08 04:55:56,467 - __main__ - INFO - Epoch 104 Batch 0: Train Loss = 0.2153
2023-11-08 04:56:12,483 - __main__ - INFO - Epoch 104 Batch 20: Train Loss = 0.2421
2023-11-08 04:56:30,609 - __main__ - INFO - Epoch 104 Batch 40: Train Loss = 0.2000
2023-11-08 04:56:49,157 - __main__ - INFO - Epoch 104 Batch 60: Train Loss = 0.2165
2023-11-08 04:57:06,638 - __main__ - INFO - Epoch 104 Batch 80: Train Loss = 0.1633
2023-11-08 04:57:22,713 - __main__ - INFO - Epoch 104 Batch 100: Train Loss = 0.2090
2023-11-08 04:57:40,465 - __main__ - INFO - Epoch 104 Batch 120: Train Loss = 0.2810
2023-11-08 04:57:54,175 - __main__ - INFO - Epoch 104: Loss = 0.2204 Valid loss = 0.1961 roc = 0.8665
2023-11-08 04:57:54,824 - __main__ - INFO - Epoch 105 Batch 0: Train Loss = 0.2484
2023-11-08 04:58:12,153 - __main__ - INFO - Epoch 105 Batch 20: Train Loss = 0.1846
2023-11-08 04:58:30,774 - __main__ - INFO - Epoch 105 Batch 40: Train Loss = 0.1730
2023-11-08 04:58:48,499 - __main__ - INFO - Epoch 105 Batch 60: Train Loss = 0.2225
2023-11-08 04:59:06,033 - __main__ - INFO - Epoch 105 Batch 80: Train Loss = 0.2096
2023-11-08 04:59:21,995 - __main__ - INFO - Epoch 105 Batch 100: Train Loss = 0.2076
2023-11-08 04:59:39,095 - __main__ - INFO - Epoch 105 Batch 120: Train Loss = 0.2672
2023-11-08 04:59:52,919 - __main__ - INFO - Epoch 105: Loss = 0.2143 Valid loss = 0.1988 roc = 0.8646
2023-11-08 04:59:53,832 - __main__ - INFO - Epoch 106 Batch 0: Train Loss = 0.2250
2023-11-08 05:00:13,155 - __main__ - INFO - Epoch 106 Batch 20: Train Loss = 0.2534
2023-11-08 05:00:31,340 - __main__ - INFO - Epoch 106 Batch 40: Train Loss = 0.1881
2023-11-08 05:00:49,336 - __main__ - INFO - Epoch 106 Batch 60: Train Loss = 0.2622
2023-11-08 05:01:05,555 - __main__ - INFO - Epoch 106 Batch 80: Train Loss = 0.1877
2023-11-08 05:01:23,389 - __main__ - INFO - Epoch 106 Batch 100: Train Loss = 0.1873
2023-11-08 05:01:40,507 - __main__ - INFO - Epoch 106 Batch 120: Train Loss = 0.1879
2023-11-08 05:01:54,582 - __main__ - INFO - Epoch 106: Loss = 0.2161 Valid loss = 0.1934 roc = 0.8686
2023-11-08 05:01:55,291 - __main__ - INFO - Epoch 107 Batch 0: Train Loss = 0.2167
2023-11-08 05:02:12,919 - __main__ - INFO - Epoch 107 Batch 20: Train Loss = 0.2102
2023-11-08 05:02:30,477 - __main__ - INFO - Epoch 107 Batch 40: Train Loss = 0.2333
2023-11-08 05:02:48,330 - __main__ - INFO - Epoch 107 Batch 60: Train Loss = 0.2515
2023-11-08 05:03:05,388 - __main__ - INFO - Epoch 107 Batch 80: Train Loss = 0.1757
2023-11-08 05:03:22,325 - __main__ - INFO - Epoch 107 Batch 100: Train Loss = 0.2096
2023-11-08 05:03:38,128 - __main__ - INFO - Epoch 107 Batch 120: Train Loss = 0.1369
2023-11-08 05:03:51,609 - __main__ - INFO - Epoch 107: Loss = 0.2139 Valid loss = 0.1937 roc = 0.8687
2023-11-08 05:03:52,831 - __main__ - INFO - Epoch 108 Batch 0: Train Loss = 0.3082
2023-11-08 05:04:10,065 - __main__ - INFO - Epoch 108 Batch 20: Train Loss = 0.2334
2023-11-08 05:04:26,908 - __main__ - INFO - Epoch 108 Batch 40: Train Loss = 0.2361
2023-11-08 05:04:44,301 - __main__ - INFO - Epoch 108 Batch 60: Train Loss = 0.1922
2023-11-08 05:05:00,833 - __main__ - INFO - Epoch 108 Batch 80: Train Loss = 0.2416
2023-11-08 05:05:18,539 - __main__ - INFO - Epoch 108 Batch 100: Train Loss = 0.2115
2023-11-08 05:05:36,373 - __main__ - INFO - Epoch 108 Batch 120: Train Loss = 0.2638
2023-11-08 05:05:48,783 - __main__ - INFO - Epoch 108: Loss = 0.2162 Valid loss = 0.1958 roc = 0.8675
2023-11-08 05:05:49,494 - __main__ - INFO - Epoch 109 Batch 0: Train Loss = 0.2035
2023-11-08 05:06:07,023 - __main__ - INFO - Epoch 109 Batch 20: Train Loss = 0.2182
2023-11-08 05:06:24,753 - __main__ - INFO - Epoch 109 Batch 40: Train Loss = 0.2782
2023-11-08 05:06:44,530 - __main__ - INFO - Epoch 109 Batch 60: Train Loss = 0.2154
2023-11-08 05:07:02,516 - __main__ - INFO - Epoch 109 Batch 80: Train Loss = 0.2436
2023-11-08 05:07:20,817 - __main__ - INFO - Epoch 109 Batch 100: Train Loss = 0.1586
2023-11-08 05:07:38,045 - __main__ - INFO - Epoch 109 Batch 120: Train Loss = 0.1925
2023-11-08 05:07:51,127 - __main__ - INFO - Epoch 109: Loss = 0.2147 Valid loss = 0.1975 roc = 0.8650
2023-11-08 05:07:52,206 - __main__ - INFO - Epoch 110 Batch 0: Train Loss = 0.2501
2023-11-08 05:08:08,454 - __main__ - INFO - Epoch 110 Batch 20: Train Loss = 0.1977
2023-11-08 05:08:25,907 - __main__ - INFO - Epoch 110 Batch 40: Train Loss = 0.2706
2023-11-08 05:08:42,024 - __main__ - INFO - Epoch 110 Batch 60: Train Loss = 0.1242
2023-11-08 05:08:59,160 - __main__ - INFO - Epoch 110 Batch 80: Train Loss = 0.2042
2023-11-08 05:09:16,144 - __main__ - INFO - Epoch 110 Batch 100: Train Loss = 0.1845
2023-11-08 05:09:33,788 - __main__ - INFO - Epoch 110 Batch 120: Train Loss = 0.2235
2023-11-08 05:09:48,496 - __main__ - INFO - Epoch 110: Loss = 0.2117 Valid loss = 0.1979 roc = 0.8680
2023-11-08 05:09:49,602 - __main__ - INFO - Epoch 111 Batch 0: Train Loss = 0.2099
2023-11-08 05:10:06,919 - __main__ - INFO - Epoch 111 Batch 20: Train Loss = 0.2636
2023-11-08 05:10:23,667 - __main__ - INFO - Epoch 111 Batch 40: Train Loss = 0.2253
2023-11-08 05:10:39,787 - __main__ - INFO - Epoch 111 Batch 60: Train Loss = 0.1879
2023-11-08 05:10:57,245 - __main__ - INFO - Epoch 111 Batch 80: Train Loss = 0.2765
2023-11-08 05:11:15,318 - __main__ - INFO - Epoch 111 Batch 100: Train Loss = 0.1787
2023-11-08 05:11:32,613 - __main__ - INFO - Epoch 111 Batch 120: Train Loss = 0.2198
2023-11-08 05:11:46,129 - __main__ - INFO - Epoch 111: Loss = 0.2169 Valid loss = 0.1981 roc = 0.8504
2023-11-08 05:11:47,158 - __main__ - INFO - Epoch 112 Batch 0: Train Loss = 0.2042
2023-11-08 05:12:04,516 - __main__ - INFO - Epoch 112 Batch 20: Train Loss = 0.2372
2023-11-08 05:12:22,247 - __main__ - INFO - Epoch 112 Batch 40: Train Loss = 0.2069
2023-11-08 05:12:39,961 - __main__ - INFO - Epoch 112 Batch 60: Train Loss = 0.2156
2023-11-08 05:12:58,229 - __main__ - INFO - Epoch 112 Batch 80: Train Loss = 0.1475
2023-11-08 05:13:16,103 - __main__ - INFO - Epoch 112 Batch 100: Train Loss = 0.2244
2023-11-08 05:13:32,569 - __main__ - INFO - Epoch 112 Batch 120: Train Loss = 0.1801
2023-11-08 05:13:45,423 - __main__ - INFO - Epoch 112: Loss = 0.2185 Valid loss = 0.1987 roc = 0.8576
2023-11-08 05:13:46,248 - __main__ - INFO - Epoch 113 Batch 0: Train Loss = 0.1993
2023-11-08 05:14:03,666 - __main__ - INFO - Epoch 113 Batch 20: Train Loss = 0.2736
2023-11-08 05:14:20,823 - __main__ - INFO - Epoch 113 Batch 40: Train Loss = 0.1929
2023-11-08 05:14:38,013 - __main__ - INFO - Epoch 113 Batch 60: Train Loss = 0.1687
2023-11-08 05:14:54,342 - __main__ - INFO - Epoch 113 Batch 80: Train Loss = 0.2258
2023-11-08 05:15:11,231 - __main__ - INFO - Epoch 113 Batch 100: Train Loss = 0.1661
2023-11-08 05:15:29,074 - __main__ - INFO - Epoch 113 Batch 120: Train Loss = 0.1808
2023-11-08 05:15:42,955 - __main__ - INFO - Epoch 113: Loss = 0.2168 Valid loss = 0.1956 roc = 0.8673
2023-11-08 05:15:43,981 - __main__ - INFO - Epoch 114 Batch 0: Train Loss = 0.1820
2023-11-08 05:16:00,946 - __main__ - INFO - Epoch 114 Batch 20: Train Loss = 0.2154
2023-11-08 05:16:18,098 - __main__ - INFO - Epoch 114 Batch 40: Train Loss = 0.1732
2023-11-08 05:16:35,925 - __main__ - INFO - Epoch 114 Batch 60: Train Loss = 0.2373
2023-11-08 05:16:52,043 - __main__ - INFO - Epoch 114 Batch 80: Train Loss = 0.2315
2023-11-08 05:17:09,172 - __main__ - INFO - Epoch 114 Batch 100: Train Loss = 0.2211
2023-11-08 05:17:26,343 - __main__ - INFO - Epoch 114 Batch 120: Train Loss = 0.3326
2023-11-08 05:17:39,662 - __main__ - INFO - Epoch 114: Loss = 0.2175 Valid loss = 0.1950 roc = 0.8703
2023-11-08 05:17:40,647 - __main__ - INFO - Epoch 115 Batch 0: Train Loss = 0.2252
2023-11-08 05:17:57,414 - __main__ - INFO - Epoch 115 Batch 20: Train Loss = 0.1831
2023-11-08 05:18:15,706 - __main__ - INFO - Epoch 115 Batch 40: Train Loss = 0.2314
2023-11-08 05:18:33,011 - __main__ - INFO - Epoch 115 Batch 60: Train Loss = 0.2232
2023-11-08 05:18:49,750 - __main__ - INFO - Epoch 115 Batch 80: Train Loss = 0.2508
2023-11-08 05:19:06,457 - __main__ - INFO - Epoch 115 Batch 100: Train Loss = 0.2423
2023-11-08 05:19:23,831 - __main__ - INFO - Epoch 115 Batch 120: Train Loss = 0.2390
2023-11-08 05:19:37,528 - __main__ - INFO - Epoch 115: Loss = 0.2183 Valid loss = 0.1939 roc = 0.8625
2023-11-08 05:19:38,430 - __main__ - INFO - Epoch 116 Batch 0: Train Loss = 0.1690
2023-11-08 05:19:54,906 - __main__ - INFO - Epoch 116 Batch 20: Train Loss = 0.1498
2023-11-08 05:20:12,562 - __main__ - INFO - Epoch 116 Batch 40: Train Loss = 0.2379
2023-11-08 05:20:29,787 - __main__ - INFO - Epoch 116 Batch 60: Train Loss = 0.2501
2023-11-08 05:20:48,640 - __main__ - INFO - Epoch 116 Batch 80: Train Loss = 0.1963
2023-11-08 05:21:06,068 - __main__ - INFO - Epoch 116 Batch 100: Train Loss = 0.2661
2023-11-08 05:21:22,632 - __main__ - INFO - Epoch 116 Batch 120: Train Loss = 0.2335
2023-11-08 05:21:36,901 - __main__ - INFO - Epoch 116: Loss = 0.2171 Valid loss = 0.1933 roc = 0.8618
2023-11-08 05:21:37,447 - __main__ - INFO - Epoch 117 Batch 0: Train Loss = 0.1903
2023-11-08 05:21:54,706 - __main__ - INFO - Epoch 117 Batch 20: Train Loss = 0.2094
2023-11-08 05:22:11,792 - __main__ - INFO - Epoch 117 Batch 40: Train Loss = 0.2822
2023-11-08 05:22:28,168 - __main__ - INFO - Epoch 117 Batch 60: Train Loss = 0.1844
2023-11-08 05:22:46,413 - __main__ - INFO - Epoch 117 Batch 80: Train Loss = 0.1744
2023-11-08 05:23:04,593 - __main__ - INFO - Epoch 117 Batch 100: Train Loss = 0.2461
2023-11-08 05:23:22,011 - __main__ - INFO - Epoch 117 Batch 120: Train Loss = 0.1733
2023-11-08 05:23:36,824 - __main__ - INFO - Epoch 117: Loss = 0.2117 Valid loss = 0.1966 roc = 0.8697
2023-11-08 05:23:37,762 - __main__ - INFO - Epoch 118 Batch 0: Train Loss = 0.1657
2023-11-08 05:23:54,659 - __main__ - INFO - Epoch 118 Batch 20: Train Loss = 0.2208
2023-11-08 05:24:11,637 - __main__ - INFO - Epoch 118 Batch 40: Train Loss = 0.1607
2023-11-08 05:24:28,671 - __main__ - INFO - Epoch 118 Batch 60: Train Loss = 0.2537
2023-11-08 05:24:46,962 - __main__ - INFO - Epoch 118 Batch 80: Train Loss = 0.2638
2023-11-08 05:25:03,435 - __main__ - INFO - Epoch 118 Batch 100: Train Loss = 0.1852
2023-11-08 05:25:21,577 - __main__ - INFO - Epoch 118 Batch 120: Train Loss = 0.1506
2023-11-08 05:25:35,206 - __main__ - INFO - Epoch 118: Loss = 0.2140 Valid loss = 0.1987 roc = 0.8628
2023-11-08 05:25:36,149 - __main__ - INFO - Epoch 119 Batch 0: Train Loss = 0.1782
2023-11-08 05:25:53,794 - __main__ - INFO - Epoch 119 Batch 20: Train Loss = 0.2253
2023-11-08 05:26:09,943 - __main__ - INFO - Epoch 119 Batch 40: Train Loss = 0.2330
2023-11-08 05:26:28,025 - __main__ - INFO - Epoch 119 Batch 60: Train Loss = 0.2336
2023-11-08 05:26:45,357 - __main__ - INFO - Epoch 119 Batch 80: Train Loss = 0.1854
2023-11-08 05:27:02,201 - __main__ - INFO - Epoch 119 Batch 100: Train Loss = 0.2135
2023-11-08 05:27:20,217 - __main__ - INFO - Epoch 119 Batch 120: Train Loss = 0.1987
2023-11-08 05:27:35,414 - __main__ - INFO - Epoch 119: Loss = 0.2145 Valid loss = 0.1938 roc = 0.8732
2023-11-08 05:27:36,446 - __main__ - INFO - Epoch 120 Batch 0: Train Loss = 0.1654
2023-11-08 05:27:53,619 - __main__ - INFO - Epoch 120 Batch 20: Train Loss = 0.2997
2023-11-08 05:28:09,847 - __main__ - INFO - Epoch 120 Batch 40: Train Loss = 0.2022
2023-11-08 05:28:26,329 - __main__ - INFO - Epoch 120 Batch 60: Train Loss = 0.1660
2023-11-08 05:28:44,588 - __main__ - INFO - Epoch 120 Batch 80: Train Loss = 0.2040
2023-11-08 05:29:02,343 - __main__ - INFO - Epoch 120 Batch 100: Train Loss = 0.2379
2023-11-08 05:29:19,455 - __main__ - INFO - Epoch 120 Batch 120: Train Loss = 0.2026
2023-11-08 05:29:33,051 - __main__ - INFO - Epoch 120: Loss = 0.2157 Valid loss = 0.1924 roc = 0.8661
2023-11-08 05:29:34,128 - __main__ - INFO - Epoch 121 Batch 0: Train Loss = 0.2204
2023-11-08 05:29:51,193 - __main__ - INFO - Epoch 121 Batch 20: Train Loss = 0.1998
2023-11-08 05:30:08,351 - __main__ - INFO - Epoch 121 Batch 40: Train Loss = 0.2793
2023-11-08 05:30:24,665 - __main__ - INFO - Epoch 121 Batch 60: Train Loss = 0.1474
2023-11-08 05:30:42,190 - __main__ - INFO - Epoch 121 Batch 80: Train Loss = 0.2285
2023-11-08 05:30:59,131 - __main__ - INFO - Epoch 121 Batch 100: Train Loss = 0.2236
2023-11-08 05:31:17,014 - __main__ - INFO - Epoch 121 Batch 120: Train Loss = 0.2023
2023-11-08 05:31:30,899 - __main__ - INFO - Epoch 121: Loss = 0.2152 Valid loss = 0.1952 roc = 0.8719
2023-11-08 05:31:31,850 - __main__ - INFO - Epoch 122 Batch 0: Train Loss = 0.2321
2023-11-08 05:31:48,953 - __main__ - INFO - Epoch 122 Batch 20: Train Loss = 0.1753
2023-11-08 05:32:04,887 - __main__ - INFO - Epoch 122 Batch 40: Train Loss = 0.2383
2023-11-08 05:32:23,139 - __main__ - INFO - Epoch 122 Batch 60: Train Loss = 0.2543
2023-11-08 05:32:40,794 - __main__ - INFO - Epoch 122 Batch 80: Train Loss = 0.2606
2023-11-08 05:32:59,214 - __main__ - INFO - Epoch 122 Batch 100: Train Loss = 0.2693
2023-11-08 05:33:15,596 - __main__ - INFO - Epoch 122 Batch 120: Train Loss = 0.2026
2023-11-08 05:33:29,691 - __main__ - INFO - Epoch 122: Loss = 0.2138 Valid loss = 0.1921 roc = 0.8714
2023-11-08 05:33:30,806 - __main__ - INFO - Epoch 123 Batch 0: Train Loss = 0.1679
2023-11-08 05:33:47,612 - __main__ - INFO - Epoch 123 Batch 20: Train Loss = 0.1724
2023-11-08 05:34:03,584 - __main__ - INFO - Epoch 123 Batch 40: Train Loss = 0.2608
2023-11-08 05:34:20,177 - __main__ - INFO - Epoch 123 Batch 60: Train Loss = 0.1710
2023-11-08 05:34:36,824 - __main__ - INFO - Epoch 123 Batch 80: Train Loss = 0.2550
2023-11-08 05:34:52,769 - __main__ - INFO - Epoch 123 Batch 100: Train Loss = 0.1939
2023-11-08 05:35:10,767 - __main__ - INFO - Epoch 123 Batch 120: Train Loss = 0.1905
2023-11-08 05:35:23,763 - __main__ - INFO - Epoch 123: Loss = 0.2141 Valid loss = 0.1922 roc = 0.8710
2023-11-08 05:35:24,709 - __main__ - INFO - Epoch 124 Batch 0: Train Loss = 0.3144
2023-11-08 05:35:40,576 - __main__ - INFO - Epoch 124 Batch 20: Train Loss = 0.2361
2023-11-08 05:35:57,851 - __main__ - INFO - Epoch 124 Batch 40: Train Loss = 0.3079
2023-11-08 05:36:15,112 - __main__ - INFO - Epoch 124 Batch 60: Train Loss = 0.1645
2023-11-08 05:36:33,210 - __main__ - INFO - Epoch 124 Batch 80: Train Loss = 0.2138
2023-11-08 05:36:49,466 - __main__ - INFO - Epoch 124 Batch 100: Train Loss = 0.1434
2023-11-08 05:37:05,075 - __main__ - INFO - Epoch 124 Batch 120: Train Loss = 0.2072
2023-11-08 05:37:17,827 - __main__ - INFO - Epoch 124: Loss = 0.2111 Valid loss = 0.1932 roc = 0.8688
2023-11-08 05:37:18,755 - __main__ - INFO - Epoch 125 Batch 0: Train Loss = 0.1569
2023-11-08 05:37:34,852 - __main__ - INFO - Epoch 125 Batch 20: Train Loss = 0.2232
2023-11-08 05:37:49,565 - __main__ - INFO - Epoch 125 Batch 40: Train Loss = 0.1576
2023-11-08 05:38:04,488 - __main__ - INFO - Epoch 125 Batch 60: Train Loss = 0.2296
2023-11-08 05:38:21,068 - __main__ - INFO - Epoch 125 Batch 80: Train Loss = 0.2023
2023-11-08 05:38:36,212 - __main__ - INFO - Epoch 125 Batch 100: Train Loss = 0.2662
2023-11-08 05:38:52,258 - __main__ - INFO - Epoch 125 Batch 120: Train Loss = 0.2565
2023-11-08 05:39:05,981 - __main__ - INFO - Epoch 125: Loss = 0.2121 Valid loss = 0.1915 roc = 0.8735
2023-11-08 05:39:06,646 - __main__ - INFO - Epoch 126 Batch 0: Train Loss = 0.1628
2023-11-08 05:39:21,322 - __main__ - INFO - Epoch 126 Batch 20: Train Loss = 0.2242
2023-11-08 05:39:36,919 - __main__ - INFO - Epoch 126 Batch 40: Train Loss = 0.2027
2023-11-08 05:39:52,943 - __main__ - INFO - Epoch 126 Batch 60: Train Loss = 0.1707
2023-11-08 05:40:09,531 - __main__ - INFO - Epoch 126 Batch 80: Train Loss = 0.1619
2023-11-08 05:40:25,321 - __main__ - INFO - Epoch 126 Batch 100: Train Loss = 0.1961
2023-11-08 05:40:40,633 - __main__ - INFO - Epoch 126 Batch 120: Train Loss = 0.2374
2023-11-08 05:40:52,953 - __main__ - INFO - Epoch 126: Loss = 0.2138 Valid loss = 0.1921 roc = 0.8743
2023-11-08 05:40:53,783 - __main__ - INFO - Epoch 127 Batch 0: Train Loss = 0.1847
2023-11-08 05:41:09,899 - __main__ - INFO - Epoch 127 Batch 20: Train Loss = 0.2302
2023-11-08 05:41:25,087 - __main__ - INFO - Epoch 127 Batch 40: Train Loss = 0.1787
2023-11-08 05:41:39,995 - __main__ - INFO - Epoch 127 Batch 60: Train Loss = 0.1816
2023-11-08 05:41:55,520 - __main__ - INFO - Epoch 127 Batch 80: Train Loss = 0.1318
2023-11-08 05:42:09,314 - __main__ - INFO - Epoch 127 Batch 100: Train Loss = 0.1980
2023-11-08 05:42:25,408 - __main__ - INFO - Epoch 127 Batch 120: Train Loss = 0.2755
2023-11-08 05:42:37,368 - __main__ - INFO - Epoch 127: Loss = 0.2159 Valid loss = 0.1925 roc = 0.8719
2023-11-08 05:42:38,380 - __main__ - INFO - Epoch 128 Batch 0: Train Loss = 0.3141
2023-11-08 05:42:54,086 - __main__ - INFO - Epoch 128 Batch 20: Train Loss = 0.1977
2023-11-08 05:43:09,849 - __main__ - INFO - Epoch 128 Batch 40: Train Loss = 0.2346
2023-11-08 05:43:25,435 - __main__ - INFO - Epoch 128 Batch 60: Train Loss = 0.2132
2023-11-08 05:43:40,432 - __main__ - INFO - Epoch 128 Batch 80: Train Loss = 0.2551
2023-11-08 05:43:56,708 - __main__ - INFO - Epoch 128 Batch 100: Train Loss = 0.2441
2023-11-08 05:44:10,961 - __main__ - INFO - Epoch 128 Batch 120: Train Loss = 0.2067
2023-11-08 05:44:23,435 - __main__ - INFO - Epoch 128: Loss = 0.2156 Valid loss = 0.1946 roc = 0.8604
2023-11-08 05:44:24,209 - __main__ - INFO - Epoch 129 Batch 0: Train Loss = 0.2392
2023-11-08 05:44:40,295 - __main__ - INFO - Epoch 129 Batch 20: Train Loss = 0.2521
2023-11-08 05:44:55,669 - __main__ - INFO - Epoch 129 Batch 40: Train Loss = 0.2572
2023-11-08 05:45:11,642 - __main__ - INFO - Epoch 129 Batch 60: Train Loss = 0.1954
2023-11-08 05:45:27,299 - __main__ - INFO - Epoch 129 Batch 80: Train Loss = 0.1708
2023-11-08 05:45:43,189 - __main__ - INFO - Epoch 129 Batch 100: Train Loss = 0.1651
2023-11-08 05:45:58,800 - __main__ - INFO - Epoch 129 Batch 120: Train Loss = 0.2093
2023-11-08 05:46:09,500 - __main__ - INFO - Epoch 129: Loss = 0.2128 Valid loss = 0.1939 roc = 0.8626
2023-11-08 05:46:10,290 - __main__ - INFO - Epoch 130 Batch 0: Train Loss = 0.1524
2023-11-08 05:46:27,236 - __main__ - INFO - Epoch 130 Batch 20: Train Loss = 0.2168
2023-11-08 05:46:42,779 - __main__ - INFO - Epoch 130 Batch 40: Train Loss = 0.1949
2023-11-08 05:46:58,231 - __main__ - INFO - Epoch 130 Batch 60: Train Loss = 0.1797
2023-11-08 05:47:12,854 - __main__ - INFO - Epoch 130 Batch 80: Train Loss = 0.2292
2023-11-08 05:47:29,097 - __main__ - INFO - Epoch 130 Batch 100: Train Loss = 0.2332
2023-11-08 05:47:45,302 - __main__ - INFO - Epoch 130 Batch 120: Train Loss = 0.2282
2023-11-08 05:47:57,856 - __main__ - INFO - Epoch 130: Loss = 0.2182 Valid loss = 0.2013 roc = 0.8592
2023-11-08 05:47:58,527 - __main__ - INFO - Epoch 131 Batch 0: Train Loss = 0.2579
2023-11-08 05:48:15,519 - __main__ - INFO - Epoch 131 Batch 20: Train Loss = 0.1964
2023-11-08 05:48:30,465 - __main__ - INFO - Epoch 131 Batch 40: Train Loss = 0.2235
2023-11-08 05:48:46,938 - __main__ - INFO - Epoch 131 Batch 60: Train Loss = 0.2550
2023-11-08 05:49:03,954 - __main__ - INFO - Epoch 131 Batch 80: Train Loss = 0.1308
2023-11-08 05:49:19,870 - __main__ - INFO - Epoch 131 Batch 100: Train Loss = 0.2171
2023-11-08 05:49:35,577 - __main__ - INFO - Epoch 131 Batch 120: Train Loss = 0.1774
2023-11-08 05:49:47,090 - __main__ - INFO - Epoch 131: Loss = 0.2188 Valid loss = 0.1974 roc = 0.8615
2023-11-08 05:49:47,906 - __main__ - INFO - Epoch 132 Batch 0: Train Loss = 0.2144
2023-11-08 05:50:03,178 - __main__ - INFO - Epoch 132 Batch 20: Train Loss = 0.2198
2023-11-08 05:50:18,924 - __main__ - INFO - Epoch 132 Batch 40: Train Loss = 0.2172
2023-11-08 05:50:34,408 - __main__ - INFO - Epoch 132 Batch 60: Train Loss = 0.2074
2023-11-08 05:50:50,044 - __main__ - INFO - Epoch 132 Batch 80: Train Loss = 0.2051
2023-11-08 05:51:05,294 - __main__ - INFO - Epoch 132 Batch 100: Train Loss = 0.2074
2023-11-08 05:51:22,465 - __main__ - INFO - Epoch 132 Batch 120: Train Loss = 0.1818
2023-11-08 05:51:35,377 - __main__ - INFO - Epoch 132: Loss = 0.2156 Valid loss = 0.1933 roc = 0.8672
2023-11-08 05:51:36,094 - __main__ - INFO - Epoch 133 Batch 0: Train Loss = 0.2187
2023-11-08 05:51:52,276 - __main__ - INFO - Epoch 133 Batch 20: Train Loss = 0.2475
2023-11-08 05:52:06,971 - __main__ - INFO - Epoch 133 Batch 40: Train Loss = 0.2370
2023-11-08 05:52:23,353 - __main__ - INFO - Epoch 133 Batch 60: Train Loss = 0.2472
2023-11-08 05:52:38,727 - __main__ - INFO - Epoch 133 Batch 80: Train Loss = 0.2740
2023-11-08 05:52:55,165 - __main__ - INFO - Epoch 133 Batch 100: Train Loss = 0.1925
2023-11-08 05:53:11,594 - __main__ - INFO - Epoch 133 Batch 120: Train Loss = 0.2194
2023-11-08 05:53:22,312 - __main__ - INFO - Epoch 133: Loss = 0.2134 Valid loss = 0.1946 roc = 0.8640
2023-11-08 05:53:22,922 - __main__ - INFO - Epoch 134 Batch 0: Train Loss = 0.2072
2023-11-08 05:53:38,144 - __main__ - INFO - Epoch 134 Batch 20: Train Loss = 0.1863
2023-11-08 05:53:54,627 - __main__ - INFO - Epoch 134 Batch 40: Train Loss = 0.1971
2023-11-08 05:54:09,980 - __main__ - INFO - Epoch 134 Batch 60: Train Loss = 0.2445
2023-11-08 05:54:25,376 - __main__ - INFO - Epoch 134 Batch 80: Train Loss = 0.2317
2023-11-08 05:54:41,901 - __main__ - INFO - Epoch 134 Batch 100: Train Loss = 0.2492
2023-11-08 05:54:58,377 - __main__ - INFO - Epoch 134 Batch 120: Train Loss = 0.2025
2023-11-08 05:55:10,616 - __main__ - INFO - Epoch 134: Loss = 0.2149 Valid loss = 0.1919 roc = 0.8708
2023-11-08 05:55:11,366 - __main__ - INFO - Epoch 135 Batch 0: Train Loss = 0.2350
2023-11-08 05:55:26,895 - __main__ - INFO - Epoch 135 Batch 20: Train Loss = 0.1692
2023-11-08 05:55:42,658 - __main__ - INFO - Epoch 135 Batch 40: Train Loss = 0.1360
2023-11-08 05:55:58,288 - __main__ - INFO - Epoch 135 Batch 60: Train Loss = 0.1832
2023-11-08 05:56:12,763 - __main__ - INFO - Epoch 135 Batch 80: Train Loss = 0.2052
2023-11-08 05:56:28,919 - __main__ - INFO - Epoch 135 Batch 100: Train Loss = 0.1902
2023-11-08 05:56:44,475 - __main__ - INFO - Epoch 135 Batch 120: Train Loss = 0.2043
2023-11-08 05:56:56,886 - __main__ - INFO - Epoch 135: Loss = 0.2168 Valid loss = 0.1908 roc = 0.8667
2023-11-08 05:56:57,751 - __main__ - INFO - Epoch 136 Batch 0: Train Loss = 0.2480
2023-11-08 05:57:13,292 - __main__ - INFO - Epoch 136 Batch 20: Train Loss = 0.1321
2023-11-08 05:57:28,312 - __main__ - INFO - Epoch 136 Batch 40: Train Loss = 0.2144
2023-11-08 05:57:43,751 - __main__ - INFO - Epoch 136 Batch 60: Train Loss = 0.2707
2023-11-08 05:57:59,252 - __main__ - INFO - Epoch 136 Batch 80: Train Loss = 0.2354
2023-11-08 05:58:14,266 - __main__ - INFO - Epoch 136 Batch 100: Train Loss = 0.2192
2023-11-08 05:58:29,913 - __main__ - INFO - Epoch 136 Batch 120: Train Loss = 0.1688
2023-11-08 05:58:42,315 - __main__ - INFO - Epoch 136: Loss = 0.2126 Valid loss = 0.1924 roc = 0.8643
2023-11-08 05:58:42,981 - __main__ - INFO - Epoch 137 Batch 0: Train Loss = 0.1740
2023-11-08 05:58:58,634 - __main__ - INFO - Epoch 137 Batch 20: Train Loss = 0.2453
2023-11-08 05:59:13,587 - __main__ - INFO - Epoch 137 Batch 40: Train Loss = 0.2631
2023-11-08 05:59:30,282 - __main__ - INFO - Epoch 137 Batch 60: Train Loss = 0.1605
2023-11-08 05:59:45,977 - __main__ - INFO - Epoch 137 Batch 80: Train Loss = 0.1806
2023-11-08 06:00:01,545 - __main__ - INFO - Epoch 137 Batch 100: Train Loss = 0.1783
2023-11-08 06:00:17,198 - __main__ - INFO - Epoch 137 Batch 120: Train Loss = 0.2458
2023-11-08 06:00:28,953 - __main__ - INFO - Epoch 137: Loss = 0.2104 Valid loss = 0.1910 roc = 0.8707
2023-11-08 06:00:29,826 - __main__ - INFO - Epoch 138 Batch 0: Train Loss = 0.3241
2023-11-08 06:00:44,481 - __main__ - INFO - Epoch 138 Batch 20: Train Loss = 0.1963
2023-11-08 06:01:01,403 - __main__ - INFO - Epoch 138 Batch 40: Train Loss = 0.1666
2023-11-08 06:01:17,186 - __main__ - INFO - Epoch 138 Batch 60: Train Loss = 0.1962
2023-11-08 06:01:33,834 - __main__ - INFO - Epoch 138 Batch 80: Train Loss = 0.1623
2023-11-08 06:01:49,349 - __main__ - INFO - Epoch 138 Batch 100: Train Loss = 0.2140
2023-11-08 06:02:06,118 - __main__ - INFO - Epoch 138 Batch 120: Train Loss = 0.2050
2023-11-08 06:02:18,291 - __main__ - INFO - Epoch 138: Loss = 0.2111 Valid loss = 0.1933 roc = 0.8757
2023-11-08 06:02:19,160 - __main__ - INFO - Epoch 139 Batch 0: Train Loss = 0.2019
2023-11-08 06:02:34,795 - __main__ - INFO - Epoch 139 Batch 20: Train Loss = 0.2547
2023-11-08 06:02:51,060 - __main__ - INFO - Epoch 139 Batch 40: Train Loss = 0.2584
2023-11-08 06:03:06,711 - __main__ - INFO - Epoch 139 Batch 60: Train Loss = 0.1775
2023-11-08 06:03:21,042 - __main__ - INFO - Epoch 139 Batch 80: Train Loss = 0.2290
2023-11-08 06:03:36,608 - __main__ - INFO - Epoch 139 Batch 100: Train Loss = 0.1900
2023-11-08 06:03:51,811 - __main__ - INFO - Epoch 139 Batch 120: Train Loss = 0.2108
2023-11-08 06:04:04,511 - __main__ - INFO - Epoch 139: Loss = 0.2105 Valid loss = 0.1958 roc = 0.8727
2023-11-08 06:04:05,566 - __main__ - INFO - Epoch 140 Batch 0: Train Loss = 0.1491
2023-11-08 06:04:21,233 - __main__ - INFO - Epoch 140 Batch 20: Train Loss = 0.1504
2023-11-08 06:04:36,713 - __main__ - INFO - Epoch 140 Batch 40: Train Loss = 0.2699
2023-11-08 06:04:53,051 - __main__ - INFO - Epoch 140 Batch 60: Train Loss = 0.2156
2023-11-08 06:05:07,703 - __main__ - INFO - Epoch 140 Batch 80: Train Loss = 0.2104
2023-11-08 06:05:23,348 - __main__ - INFO - Epoch 140 Batch 100: Train Loss = 0.2430
2023-11-08 06:05:39,789 - __main__ - INFO - Epoch 140 Batch 120: Train Loss = 0.1945
2023-11-08 06:05:51,372 - __main__ - INFO - Epoch 140: Loss = 0.2174 Valid loss = 0.1927 roc = 0.8710
2023-11-08 06:05:51,958 - __main__ - INFO - Epoch 141 Batch 0: Train Loss = 0.2083
2023-11-08 06:06:06,328 - __main__ - INFO - Epoch 141 Batch 20: Train Loss = 0.2069
2023-11-08 06:06:22,221 - __main__ - INFO - Epoch 141 Batch 40: Train Loss = 0.1889
2023-11-08 06:06:38,705 - __main__ - INFO - Epoch 141 Batch 60: Train Loss = 0.1886
2023-11-08 06:06:54,616 - __main__ - INFO - Epoch 141 Batch 80: Train Loss = 0.2273
2023-11-08 06:07:09,184 - __main__ - INFO - Epoch 141 Batch 100: Train Loss = 0.1853
2023-11-08 06:07:25,125 - __main__ - INFO - Epoch 141 Batch 120: Train Loss = 0.2007
2023-11-08 06:07:36,468 - __main__ - INFO - Epoch 141: Loss = 0.2144 Valid loss = 0.1941 roc = 0.8624
2023-11-08 06:07:37,201 - __main__ - INFO - Epoch 142 Batch 0: Train Loss = 0.2110
2023-11-08 06:07:52,321 - __main__ - INFO - Epoch 142 Batch 20: Train Loss = 0.2235
2023-11-08 06:08:08,057 - __main__ - INFO - Epoch 142 Batch 40: Train Loss = 0.1692
2023-11-08 06:08:23,794 - __main__ - INFO - Epoch 142 Batch 60: Train Loss = 0.1797
2023-11-08 06:08:39,477 - __main__ - INFO - Epoch 142 Batch 80: Train Loss = 0.1799
2023-11-08 06:08:54,972 - __main__ - INFO - Epoch 142 Batch 100: Train Loss = 0.2955
2023-11-08 06:09:12,327 - __main__ - INFO - Epoch 142 Batch 120: Train Loss = 0.1791
2023-11-08 06:09:24,670 - __main__ - INFO - Epoch 142: Loss = 0.2146 Valid loss = 0.1934 roc = 0.8655
2023-11-08 06:09:25,463 - __main__ - INFO - Epoch 143 Batch 0: Train Loss = 0.2671
2023-11-08 06:09:42,051 - __main__ - INFO - Epoch 143 Batch 20: Train Loss = 0.2115
2023-11-08 06:09:58,407 - __main__ - INFO - Epoch 143 Batch 40: Train Loss = 0.1737
2023-11-08 06:10:13,568 - __main__ - INFO - Epoch 143 Batch 60: Train Loss = 0.2033
2023-11-08 06:10:29,414 - __main__ - INFO - Epoch 143 Batch 80: Train Loss = 0.2076
2023-11-08 06:10:44,139 - __main__ - INFO - Epoch 143 Batch 100: Train Loss = 0.1624
2023-11-08 06:11:00,052 - __main__ - INFO - Epoch 143 Batch 120: Train Loss = 0.2078
2023-11-08 06:11:12,512 - __main__ - INFO - Epoch 143: Loss = 0.2135 Valid loss = 0.1941 roc = 0.8614
2023-11-08 06:11:13,347 - __main__ - INFO - Epoch 144 Batch 0: Train Loss = 0.2780
2023-11-08 06:11:27,829 - __main__ - INFO - Epoch 144 Batch 20: Train Loss = 0.2378
2023-11-08 06:11:43,934 - __main__ - INFO - Epoch 144 Batch 40: Train Loss = 0.1742
2023-11-08 06:11:59,755 - __main__ - INFO - Epoch 144 Batch 60: Train Loss = 0.1797
2023-11-08 06:12:16,046 - __main__ - INFO - Epoch 144 Batch 80: Train Loss = 0.2045
2023-11-08 06:12:31,201 - __main__ - INFO - Epoch 144 Batch 100: Train Loss = 0.2785
2023-11-08 06:12:47,450 - __main__ - INFO - Epoch 144 Batch 120: Train Loss = 0.2945
2023-11-08 06:12:59,829 - __main__ - INFO - Epoch 144: Loss = 0.2135 Valid loss = 0.1933 roc = 0.8654
2023-11-08 06:13:00,681 - __main__ - INFO - Epoch 145 Batch 0: Train Loss = 0.2376
2023-11-08 06:13:16,014 - __main__ - INFO - Epoch 145 Batch 20: Train Loss = 0.1936
2023-11-08 06:13:32,559 - __main__ - INFO - Epoch 145 Batch 40: Train Loss = 0.1974
2023-11-08 06:13:47,598 - __main__ - INFO - Epoch 145 Batch 60: Train Loss = 0.2616
2023-11-08 06:14:03,465 - __main__ - INFO - Epoch 145 Batch 80: Train Loss = 0.2071
2023-11-08 06:14:20,308 - __main__ - INFO - Epoch 145 Batch 100: Train Loss = 0.2356
2023-11-08 06:14:36,379 - __main__ - INFO - Epoch 145 Batch 120: Train Loss = 0.2274
2023-11-08 06:14:48,476 - __main__ - INFO - Epoch 145: Loss = 0.2089 Valid loss = 0.1945 roc = 0.8647
2023-11-08 06:14:49,068 - __main__ - INFO - Epoch 146 Batch 0: Train Loss = 0.1509
2023-11-08 06:15:04,391 - __main__ - INFO - Epoch 146 Batch 20: Train Loss = 0.1996
2023-11-08 06:15:20,575 - __main__ - INFO - Epoch 146 Batch 40: Train Loss = 0.2458
2023-11-08 06:15:36,454 - __main__ - INFO - Epoch 146 Batch 60: Train Loss = 0.1861
2023-11-08 06:15:51,126 - __main__ - INFO - Epoch 146 Batch 80: Train Loss = 0.2343
2023-11-08 06:16:07,061 - __main__ - INFO - Epoch 146 Batch 100: Train Loss = 0.1401
2023-11-08 06:16:23,891 - __main__ - INFO - Epoch 146 Batch 120: Train Loss = 0.2002
2023-11-08 06:16:35,483 - __main__ - INFO - Epoch 146: Loss = 0.2136 Valid loss = 0.1937 roc = 0.8647
2023-11-08 06:16:36,295 - __main__ - INFO - Epoch 147 Batch 0: Train Loss = 0.2102
2023-11-08 06:16:52,457 - __main__ - INFO - Epoch 147 Batch 20: Train Loss = 0.2195
2023-11-08 06:17:07,432 - __main__ - INFO - Epoch 147 Batch 40: Train Loss = 0.2040
2023-11-08 06:17:23,768 - __main__ - INFO - Epoch 147 Batch 60: Train Loss = 0.2378
2023-11-08 06:17:39,640 - __main__ - INFO - Epoch 147 Batch 80: Train Loss = 0.1323
2023-11-08 06:17:55,552 - __main__ - INFO - Epoch 147 Batch 100: Train Loss = 0.2315
2023-11-08 06:18:11,027 - __main__ - INFO - Epoch 147 Batch 120: Train Loss = 0.2057
2023-11-08 06:18:23,808 - __main__ - INFO - Epoch 147: Loss = 0.2135 Valid loss = 0.1935 roc = 0.8571
2023-11-08 06:18:24,585 - __main__ - INFO - Epoch 148 Batch 0: Train Loss = 0.2004
2023-11-08 06:18:39,575 - __main__ - INFO - Epoch 148 Batch 20: Train Loss = 0.2027
2023-11-08 06:18:54,529 - __main__ - INFO - Epoch 148 Batch 40: Train Loss = 0.1734
2023-11-08 06:19:11,497 - __main__ - INFO - Epoch 148 Batch 60: Train Loss = 0.2750
2023-11-08 06:19:27,353 - __main__ - INFO - Epoch 148 Batch 80: Train Loss = 0.1880
2023-11-08 06:19:43,870 - __main__ - INFO - Epoch 148 Batch 100: Train Loss = 0.2255
2023-11-08 06:20:00,329 - __main__ - INFO - Epoch 148 Batch 120: Train Loss = 0.1947
2023-11-08 06:20:13,406 - __main__ - INFO - Epoch 148: Loss = 0.2124 Valid loss = 0.1919 roc = 0.8728
2023-11-08 06:20:14,281 - __main__ - INFO - Epoch 149 Batch 0: Train Loss = 0.2301
2023-11-08 06:20:29,487 - __main__ - INFO - Epoch 149 Batch 20: Train Loss = 0.1938
2023-11-08 06:20:45,697 - __main__ - INFO - Epoch 149 Batch 40: Train Loss = 0.1706
2023-11-08 06:21:02,445 - __main__ - INFO - Epoch 149 Batch 60: Train Loss = 0.3065
2023-11-08 06:21:18,017 - __main__ - INFO - Epoch 149 Batch 80: Train Loss = 0.1753
2023-11-08 06:21:34,663 - __main__ - INFO - Epoch 149 Batch 100: Train Loss = 0.2109
2023-11-08 06:21:50,792 - __main__ - INFO - Epoch 149 Batch 120: Train Loss = 0.2247
2023-11-08 06:22:02,719 - __main__ - INFO - Epoch 149: Loss = 0.2116 Valid loss = 0.1926 roc = 0.8717
2023-11-08 06:22:02,751 - __main__ - INFO - auroc 0.8757
2023-11-08 06:22:02,752 - __main__ - INFO - auprc 0.5023
2023-11-08 06:22:02,753 - __main__ - INFO - minpse 0.4744
2023-11-08 06:22:02,904 - __main__ - INFO - last saved model is in epoch 138
2023-11-08 06:22:03,634 - __main__ - INFO - Batch 0: Test Loss = 0.2309
2023-11-08 06:22:10,361 - __main__ - INFO - 
==>Predicting on test
2023-11-08 06:22:10,362 - __main__ - INFO - Test Loss = 0.1962
2023-11-08 06:22:10,457 - __main__ - INFO - load target data
2023-11-08 06:22:10,482 - __main__ - INFO - [[-0.84276482  0.37440202  0.67961237 -1.39897541 -0.48314192 -0.21203008
   1.58875966  0.79457896 -0.86126933 -0.48197292 -0.67452243  0.71372084
  -1.44708995 -0.77101282 -1.42318156 -0.58514053]
 [-0.84276482  0.37440202  0.67961237 -1.39897541 -0.48314192 -0.21203008
   1.58875966  0.79457896 -0.86126933 -0.48197292 -0.67452243  0.71372084
  -1.44708995 -0.77101282 -1.42318156 -0.58514053]
 [-0.53383302  0.43115698  0.89646606 -1.34242175 -0.69460513 -0.21203008
   0.89527972  0.03870413 -0.86126933  0.07263348 -0.50511021 -0.28657272
   0.39888266 -1.37943777 -0.82370897 -0.92211195]
 [-0.71138003  0.09062723  0.55763217 -1.39897541 -0.8637757  -0.21203008
   0.46852283  0.33565496 -0.86126933 -0.57440732 -0.50142734 -0.17115423
  -0.25396131 -0.77101282 -0.67384083 -2.43848334]
 [-0.71138003  0.09062723  0.55763217 -1.39897541 -0.8637757  -0.21203008
   0.46852283  0.33565496 -0.86126933 -0.57440732 -0.50142734 -0.17115423
  -0.25396131  0.64864541  0.0754999  -0.58514053]
 [-0.51252738  0.60142185  0.50341875 -1.45552908 -0.52543456 -0.41638034
   0.7885905   0.2816639  -0.58901728  0.59026612 -0.34306374 -0.74824667
  -0.18642573  0.64864541  0.0754999  -0.58514053]
 [-0.46281422 -0.24990251 -0.37754938 -1.11620709 -1.17392174  0.19667043
   0.68190128  0.4706326  -0.60716742 -0.14920908 -0.335698   -0.44046404
  -1.2444832   0.39513501 -0.52397268 -0.83786909]
 [-0.46281422 -0.24990251 -0.37754938 -1.11620709 -1.17392174  0.19667043
   0.68190128  0.4706326  -0.60716742 -0.14920908 -0.335698   -0.44046404
  -1.2444832   0.69934749 -0.67384083 -0.92211195]
 [-0.64391217  0.03387228 -0.17424904 -1.37069858 -0.55362966  0.40102068
   1.24201969  0.4706326  -0.60716742 -0.87019741 -0.4351356   0.02120991
  -1.06438832  0.69934749 -0.67384083 -0.92211195]
 [-0.64391217  0.03387228 -0.17424904 -1.37069858 -0.55362966  0.40102068
   1.24201969  0.4706326  -0.60716742 -0.87019741 -0.4351356   0.02120991
  -1.06438832  1.05426204 -0.67384083 -0.0796834 ]
 [-1.03806653  0.26089211  0.16458485 -1.45552908 -1.6391408   0.19667043
   1.24201969  0.4706326  -0.66161783 -0.5004598  -0.46091572  0.02120991
  -1.17694762  0.09092253 -1.34824748 -1.25908337]
 [-0.89957987  0.48791194  0.15103149 -1.41311383 -0.65231249  0.19667043
   0.14845517  0.4706326  -0.75236851 -0.94414493 -0.950738    0.17510123
  -1.04187645  0.54724125 -0.67384083 -1.0063548 ]
 [-0.5764443   0.31764706 -0.12003562 -1.3000065  -0.63821494  0.19667043
   0.86860741  0.4706326  -0.80681892 -0.24164348 -1.00229824 -4.7109481
   6.99485797  0.54724125 -0.67384083 -1.0063548 ]
 [-0.5764443   0.31764706 -0.12003562 -1.3000065  -0.63821494  0.19667043
   0.86860741  0.4706326  -0.80681892 -0.24164348 -1.00229824 -4.7109481
   6.99485797  0.24302877  0.82484063 -1.42756908]
 [-0.2320031  -0.64718721 -0.41820945 -1.15862233 -1.32899476 -0.21203008
   0.25514439  1.1995119  -0.75236851 -0.33407788 -0.93600651 -0.09420857
  -0.09637829  0.24302877  0.82484063 -1.42756908]
 [-0.2320031  -0.64718721 -0.41820945 -1.15862233 -1.32899476 -0.21203008
   0.25514439  1.1995119  -0.75236851 -0.33407788 -0.93600651 -0.09420857
  -0.09637829 -1.07522529 -1.04851119  2.27911655]
 [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043
  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097
   0.62400127 -1.07522529 -1.04851119  2.27911655]
 [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043
  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097
   0.62400127 -0.26399202 -0.67384083  2.27911655]
 [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043
  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097
   0.62400127 -0.77101282 -0.67384083 -0.66938338]
 [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043
  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097
   0.62400127 -0.77101282 -0.67384083  0.08880231]]
2023-11-08 06:22:10,491 - __main__ - INFO - 16
2023-11-08 06:22:10,492 - __main__ - INFO - 325
2023-11-08 06:22:11,149 - __main__ - INFO - Batch 0: Test Loss = 0.1885
2023-11-08 06:22:21,623 - __main__ - INFO - Batch 20: Test Loss = 0.2137
2023-11-08 06:22:31,429 - __main__ - INFO - Batch 40: Test Loss = 0.2318
2023-11-08 06:22:41,774 - __main__ - INFO - Batch 60: Test Loss = 0.2223
2023-11-08 06:22:51,136 - __main__ - INFO - Batch 80: Test Loss = 0.1684
2023-11-08 06:23:01,168 - __main__ - INFO - Batch 100: Test Loss = 0.1480
2023-11-08 06:23:11,094 - __main__ - INFO - Batch 120: Test Loss = 0.1871
2023-11-08 06:23:13,987 - __main__ - INFO - Training Student
2023-11-08 06:23:14,925 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 1.0876
2023-11-08 06:23:34,143 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 0.5864
2023-11-08 06:23:51,576 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 0.3508
2023-11-08 06:24:07,997 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 0.4054
2023-11-08 06:24:25,300 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 0.2972
2023-11-08 06:24:41,976 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 0.2671
2023-11-08 06:24:58,272 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 0.2946
2023-11-08 06:25:10,958 - __main__ - INFO - ------------ Save best model - AUROC: 0.7221 ------------
2023-11-08 06:25:11,783 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 0.3489
2023-11-08 06:25:28,516 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 0.3581
2023-11-08 06:25:44,677 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 0.3470
2023-11-08 06:26:01,783 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 0.3783
2023-11-08 06:26:18,506 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 0.2942
2023-11-08 06:26:35,811 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 0.2530
2023-11-08 06:26:53,402 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 0.2963
2023-11-08 06:27:05,236 - __main__ - INFO - ------------ Save best model - AUROC: 0.7445 ------------
2023-11-08 06:27:05,938 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 0.3649
2023-11-08 06:27:22,908 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 0.3304
2023-11-08 06:27:38,548 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 0.3134
2023-11-08 06:27:54,716 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 0.3332
2023-11-08 06:28:10,279 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 0.2927
2023-11-08 06:28:26,846 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 0.2588
2023-11-08 06:28:42,807 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 0.3065
2023-11-08 06:28:54,479 - __main__ - INFO - ------------ Save best model - AUROC: 0.7667 ------------
2023-11-08 06:28:55,140 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 0.3416
2023-11-08 06:29:11,452 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 0.3322
2023-11-08 06:29:28,712 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 0.3235
2023-11-08 06:29:44,767 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 0.3203
2023-11-08 06:30:01,324 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 0.2485
2023-11-08 06:30:18,019 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 0.2352
2023-11-08 06:30:34,000 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 0.2694
2023-11-08 06:30:45,754 - __main__ - INFO - ------------ Save best model - AUROC: 0.7992 ------------
2023-11-08 06:30:46,396 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 0.3106
2023-11-08 06:31:03,601 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 0.3292
2023-11-08 06:31:20,556 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 0.3181
2023-11-08 06:31:36,931 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 0.3456
2023-11-08 06:31:54,160 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 0.2728
2023-11-08 06:32:11,320 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 0.2374
2023-11-08 06:32:27,902 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 0.2769
2023-11-08 06:32:40,146 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 0.3860
2023-11-08 06:32:57,366 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 0.3117
2023-11-08 06:33:13,079 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 0.3032
2023-11-08 06:33:29,132 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 0.3408
2023-11-08 06:33:46,626 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 0.2705
2023-11-08 06:34:03,635 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 0.2348
2023-11-08 06:34:19,324 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 0.2891
2023-11-08 06:34:30,907 - __main__ - INFO - ------------ Save best model - AUROC: 0.8128 ------------
2023-11-08 06:34:31,749 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 0.3330
2023-11-08 06:34:48,629 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 0.3058
2023-11-08 06:35:05,184 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 0.2922
2023-11-08 06:35:22,013 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 0.3173
2023-11-08 06:35:38,589 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 0.2621
2023-11-08 06:35:55,553 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 0.2178
2023-11-08 06:36:12,361 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 0.2768
2023-11-08 06:36:23,222 - __main__ - INFO - ------------ Save best model - AUROC: 0.8156 ------------
2023-11-08 06:36:23,910 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 0.3009
2023-11-08 06:36:41,032 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 0.2842
2023-11-08 06:36:57,828 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 0.3111
2023-11-08 06:37:15,247 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 0.2875
2023-11-08 06:37:32,021 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 0.2577
2023-11-08 06:37:47,391 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 0.2114
2023-11-08 06:38:03,415 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 0.2562
2023-11-08 06:38:15,574 - __main__ - INFO - ------------ Save best model - AUROC: 0.8250 ------------
2023-11-08 06:38:16,257 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 0.3040
2023-11-08 06:38:32,842 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 0.2666
2023-11-08 06:38:48,823 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 0.2859
2023-11-08 06:39:05,375 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 0.2784
2023-11-08 06:39:21,427 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 0.2455
2023-11-08 06:39:38,206 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 0.2227
2023-11-08 06:39:53,653 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 0.2781
2023-11-08 06:40:05,154 - __main__ - INFO - ------------ Save best model - AUROC: 0.8317 ------------
2023-11-08 06:40:05,853 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 0.2924
2023-11-08 06:40:21,816 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 0.2760
2023-11-08 06:40:38,061 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 0.2847
2023-11-08 06:40:53,710 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 0.2922
2023-11-08 06:41:09,946 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 0.2402
2023-11-08 06:41:26,666 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 0.2123
2023-11-08 06:41:41,413 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 0.2566
2023-11-08 06:41:52,744 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 0.3468
2023-11-08 06:42:09,456 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 0.2541
2023-11-08 06:42:25,192 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 0.2855
2023-11-08 06:42:40,569 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 0.2888
2023-11-08 06:42:57,810 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 0.2521
2023-11-08 06:43:14,635 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 0.2049
2023-11-08 06:43:29,702 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 0.2340
2023-11-08 06:43:41,009 - __main__ - INFO - ------------ Save best model - AUROC: 0.8374 ------------
2023-11-08 06:43:41,621 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 0.3065
2023-11-08 06:43:58,506 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 0.2799
2023-11-08 06:44:14,351 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 0.2902
2023-11-08 06:44:30,435 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 0.2708
2023-11-08 06:44:46,122 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 0.2403
2023-11-08 06:45:02,575 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 0.1969
2023-11-08 06:45:19,409 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 0.2515
2023-11-08 06:45:31,493 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 0.2994
2023-11-08 06:45:47,216 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 0.2593
2023-11-08 06:46:03,024 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 0.2939
2023-11-08 06:46:19,276 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 0.2733
2023-11-08 06:46:36,214 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 0.2515
2023-11-08 06:46:52,553 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 0.2087
2023-11-08 06:47:08,261 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 0.2389
2023-11-08 06:47:19,640 - __main__ - INFO - ------------ Save best model - AUROC: 0.8378 ------------
2023-11-08 06:47:20,444 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 0.2899
2023-11-08 06:47:37,276 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 0.2843
2023-11-08 06:47:53,347 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 0.2965
2023-11-08 06:48:08,338 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 0.2849
2023-11-08 06:48:24,120 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 0.2334
2023-11-08 06:48:40,792 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 0.2232
2023-11-08 06:48:57,272 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 0.2525
2023-11-08 06:49:08,550 - __main__ - INFO - ------------ Save best model - AUROC: 0.8393 ------------
2023-11-08 06:49:09,380 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 0.2678
2023-11-08 06:49:26,550 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 0.2665
2023-11-08 06:49:42,686 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 0.2664
2023-11-08 06:49:57,935 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 0.2843
2023-11-08 06:50:14,091 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 0.2435
2023-11-08 06:50:30,318 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 0.1921
2023-11-08 06:50:46,062 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 0.2492
2023-11-08 06:50:57,814 - __main__ - INFO - ------------ Save best model - AUROC: 0.8400 ------------
2023-11-08 06:50:58,580 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 0.2642
2023-11-08 06:51:15,383 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 0.2821
2023-11-08 06:51:31,562 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 0.2786
2023-11-08 06:51:47,231 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 0.2841
2023-11-08 06:52:04,577 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 0.2352
2023-11-08 06:52:20,564 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 0.2113
2023-11-08 06:52:36,515 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 0.2483
2023-11-08 06:52:48,480 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 0.2703
2023-11-08 06:53:05,264 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 0.2762
2023-11-08 06:53:21,356 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 0.3136
2023-11-08 06:53:37,617 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 0.2766
2023-11-08 06:53:54,168 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 0.2284
2023-11-08 06:54:10,715 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 0.2092
2023-11-08 06:54:27,095 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 0.2451
2023-11-08 06:54:39,034 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 0.2834
2023-11-08 06:54:55,148 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 0.2844
2023-11-08 06:55:11,680 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 0.2890
2023-11-08 06:55:27,750 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 0.2737
2023-11-08 06:55:43,840 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 0.2314
2023-11-08 06:56:00,664 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 0.2005
2023-11-08 06:56:16,590 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 0.2339
2023-11-08 06:56:27,240 - __main__ - INFO - ------------ Save best model - AUROC: 0.8428 ------------
2023-11-08 06:56:27,886 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 0.2862
2023-11-08 06:56:44,847 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 0.2699
2023-11-08 06:57:01,053 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 0.2787
2023-11-08 06:57:16,674 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 0.2746
2023-11-08 06:57:32,952 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 0.2275
2023-11-08 06:57:48,289 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 0.2073
2023-11-08 06:58:04,546 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 0.2551
2023-11-08 06:58:16,101 - __main__ - INFO - ------------ Save best model - AUROC: 0.8432 ------------
2023-11-08 06:58:16,847 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 0.2735
2023-11-08 06:58:32,962 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 0.2635
2023-11-08 06:58:48,162 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 0.2973
2023-11-08 06:59:05,068 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 0.3025
2023-11-08 06:59:21,094 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 0.2220
2023-11-08 06:59:37,502 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 0.2051
2023-11-08 06:59:53,316 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 0.2553
2023-11-08 07:00:05,066 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 0.2657
2023-11-08 07:00:21,917 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 0.2565
2023-11-08 07:00:37,642 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 0.3012
2023-11-08 07:00:54,497 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 0.2855
2023-11-08 07:01:10,958 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 0.2594
2023-11-08 07:01:28,181 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 0.1935
2023-11-08 07:01:44,111 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 0.2370
2023-11-08 07:01:57,060 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 0.2786
2023-11-08 07:02:13,462 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 0.2753
2023-11-08 07:02:29,812 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 0.2638
2023-11-08 07:02:45,423 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 0.2695
2023-11-08 07:03:02,192 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 0.2408
2023-11-08 07:03:18,849 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 0.1811
2023-11-08 07:03:35,400 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 0.2441
2023-11-08 07:03:47,647 - __main__ - INFO - ------------ Save best model - AUROC: 0.8451 ------------
2023-11-08 07:03:48,367 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 0.2578
2023-11-08 07:04:05,086 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 0.2637
2023-11-08 07:04:20,802 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 0.2768
2023-11-08 07:04:36,961 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 0.2767
2023-11-08 07:04:52,778 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 0.2480
2023-11-08 07:05:09,289 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 0.1951
2023-11-08 07:05:25,105 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 0.2550
2023-11-08 07:05:37,386 - __main__ - INFO - ------------ Save best model - AUROC: 0.8471 ------------
2023-11-08 07:05:38,146 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 0.2663
2023-11-08 07:05:55,835 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 0.2703
2023-11-08 07:06:11,078 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 0.2960
2023-11-08 07:06:26,681 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 0.2909
2023-11-08 07:06:43,149 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 0.2259
2023-11-08 07:06:59,564 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 0.1958
2023-11-08 07:07:15,379 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 0.2508
2023-11-08 07:07:26,418 - __main__ - INFO - ------------ Save best model - AUROC: 0.8491 ------------
2023-11-08 07:07:27,246 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 0.2450
2023-11-08 07:07:43,298 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 0.2654
2023-11-08 07:07:59,116 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 0.3076
2023-11-08 07:08:15,472 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 0.2623
2023-11-08 07:08:32,778 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 0.2483
2023-11-08 07:08:49,238 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 0.1865
2023-11-08 07:09:04,982 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 0.2250
2023-11-08 07:09:16,762 - __main__ - INFO - ------------ Save best model - AUROC: 0.8494 ------------
2023-11-08 07:09:17,477 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 0.2876
2023-11-08 07:09:33,833 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 0.2694
2023-11-08 07:09:50,249 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 0.2887
2023-11-08 07:10:05,219 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 0.2757
2023-11-08 07:10:22,221 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 0.2269
2023-11-08 07:10:39,056 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 0.1964
2023-11-08 07:10:53,984 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 0.2328
2023-11-08 07:11:05,596 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 0.2285
2023-11-08 07:11:22,356 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 0.2639
2023-11-08 07:11:37,299 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 0.2892
2023-11-08 07:11:53,126 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 0.2546
2023-11-08 07:12:09,742 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 0.2300
2023-11-08 07:12:25,041 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 0.1870
2023-11-08 07:12:41,585 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 0.2343
2023-11-08 07:12:55,733 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 0.2540
2023-11-08 07:13:12,534 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 0.2730
2023-11-08 07:13:28,366 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 0.2984
2023-11-08 07:13:43,745 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 0.2731
2023-11-08 07:14:00,213 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 0.2366
2023-11-08 07:14:17,395 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 0.2014
2023-11-08 07:14:32,878 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 0.2773
2023-11-08 07:14:44,727 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 0.2381
2023-11-08 07:15:01,012 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 0.2840
2023-11-08 07:15:17,322 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 0.3006
2023-11-08 07:15:33,281 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 0.2850
2023-11-08 07:15:49,900 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 0.2234
2023-11-08 07:16:06,302 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 0.1804
2023-11-08 07:16:21,776 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 0.2493
2023-11-08 07:16:33,077 - __main__ - INFO - ------------ Save best model - AUROC: 0.8494 ------------
2023-11-08 07:16:33,898 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 0.2645
2023-11-08 07:16:50,347 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 0.2736
2023-11-08 07:17:05,392 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 0.2754
2023-11-08 07:17:21,603 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 0.2696
2023-11-08 07:17:38,595 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 0.2363
2023-11-08 07:17:54,921 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 0.1937
2023-11-08 07:18:11,046 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 0.2448
2023-11-08 07:18:22,846 - __main__ - INFO - Epoch 30 Batch 0: Train Loss = 0.2482
2023-11-08 07:18:39,548 - __main__ - INFO - Epoch 30 Batch 20: Train Loss = 0.2618
2023-11-08 07:18:54,712 - __main__ - INFO - Epoch 30 Batch 40: Train Loss = 0.2907
2023-11-08 07:19:11,360 - __main__ - INFO - Epoch 30 Batch 60: Train Loss = 0.2719
2023-11-08 07:19:28,038 - __main__ - INFO - Epoch 30 Batch 80: Train Loss = 0.2232
2023-11-08 07:19:44,739 - __main__ - INFO - Epoch 30 Batch 100: Train Loss = 0.1821
2023-11-08 07:20:01,173 - __main__ - INFO - Epoch 30 Batch 120: Train Loss = 0.2319
2023-11-08 07:20:12,265 - __main__ - INFO - Epoch 31 Batch 0: Train Loss = 0.2530
2023-11-08 07:20:27,944 - __main__ - INFO - Epoch 31 Batch 20: Train Loss = 0.2680
2023-11-08 07:20:43,579 - __main__ - INFO - Epoch 31 Batch 40: Train Loss = 0.2863
2023-11-08 07:20:59,616 - __main__ - INFO - Epoch 31 Batch 60: Train Loss = 0.2740
2023-11-08 07:21:15,312 - __main__ - INFO - Epoch 31 Batch 80: Train Loss = 0.2363
2023-11-08 07:21:31,225 - __main__ - INFO - Epoch 31 Batch 100: Train Loss = 0.1989
2023-11-08 07:21:47,710 - __main__ - INFO - Epoch 31 Batch 120: Train Loss = 0.2324
2023-11-08 07:21:59,879 - __main__ - INFO - ------------ Save best model - AUROC: 0.8496 ------------
2023-11-08 07:22:00,752 - __main__ - INFO - Epoch 32 Batch 0: Train Loss = 0.2569
2023-11-08 07:22:17,910 - __main__ - INFO - Epoch 32 Batch 20: Train Loss = 0.2828
2023-11-08 07:22:33,879 - __main__ - INFO - Epoch 32 Batch 40: Train Loss = 0.2860
2023-11-08 07:22:50,011 - __main__ - INFO - Epoch 32 Batch 60: Train Loss = 0.2696
2023-11-08 07:23:06,742 - __main__ - INFO - Epoch 32 Batch 80: Train Loss = 0.2350
2023-11-08 07:23:24,171 - __main__ - INFO - Epoch 32 Batch 100: Train Loss = 0.2152
2023-11-08 07:23:39,311 - __main__ - INFO - Epoch 32 Batch 120: Train Loss = 0.2343
2023-11-08 07:23:50,673 - __main__ - INFO - ------------ Save best model - AUROC: 0.8508 ------------
2023-11-08 07:23:51,341 - __main__ - INFO - Epoch 33 Batch 0: Train Loss = 0.2586
2023-11-08 07:24:08,375 - __main__ - INFO - Epoch 33 Batch 20: Train Loss = 0.2716
2023-11-08 07:24:25,103 - __main__ - INFO - Epoch 33 Batch 40: Train Loss = 0.2963
2023-11-08 07:24:41,984 - __main__ - INFO - Epoch 33 Batch 60: Train Loss = 0.2709
2023-11-08 07:24:58,500 - __main__ - INFO - Epoch 33 Batch 80: Train Loss = 0.2245
2023-11-08 07:25:14,480 - __main__ - INFO - Epoch 33 Batch 100: Train Loss = 0.1927
2023-11-08 07:25:30,170 - __main__ - INFO - Epoch 33 Batch 120: Train Loss = 0.2319
2023-11-08 07:25:42,845 - __main__ - INFO - Epoch 34 Batch 0: Train Loss = 0.2608
2023-11-08 07:25:59,672 - __main__ - INFO - Epoch 34 Batch 20: Train Loss = 0.2467
2023-11-08 07:26:15,270 - __main__ - INFO - Epoch 34 Batch 40: Train Loss = 0.2728
2023-11-08 07:26:32,059 - __main__ - INFO - Epoch 34 Batch 60: Train Loss = 0.2690
2023-11-08 07:26:48,878 - __main__ - INFO - Epoch 34 Batch 80: Train Loss = 0.2211
2023-11-08 07:27:05,322 - __main__ - INFO - Epoch 34 Batch 100: Train Loss = 0.1891
2023-11-08 07:27:21,455 - __main__ - INFO - Epoch 34 Batch 120: Train Loss = 0.2299
2023-11-08 07:27:34,243 - __main__ - INFO - Epoch 35 Batch 0: Train Loss = 0.2652
2023-11-08 07:27:51,310 - __main__ - INFO - Epoch 35 Batch 20: Train Loss = 0.2677
2023-11-08 07:28:07,087 - __main__ - INFO - Epoch 35 Batch 40: Train Loss = 0.2957
2023-11-08 07:28:22,942 - __main__ - INFO - Epoch 35 Batch 60: Train Loss = 0.2563
2023-11-08 07:28:39,131 - __main__ - INFO - Epoch 35 Batch 80: Train Loss = 0.2316
2023-11-08 07:28:56,886 - __main__ - INFO - Epoch 35 Batch 100: Train Loss = 0.2076
2023-11-08 07:29:13,496 - __main__ - INFO - Epoch 35 Batch 120: Train Loss = 0.2521
2023-11-08 07:29:25,228 - __main__ - INFO - Epoch 36 Batch 0: Train Loss = 0.2369
2023-11-08 07:29:41,278 - __main__ - INFO - Epoch 36 Batch 20: Train Loss = 0.2674
2023-11-08 07:29:56,847 - __main__ - INFO - Epoch 36 Batch 40: Train Loss = 0.2949
2023-11-08 07:30:13,046 - __main__ - INFO - Epoch 36 Batch 60: Train Loss = 0.2839
2023-11-08 07:30:29,657 - __main__ - INFO - Epoch 36 Batch 80: Train Loss = 0.2349
2023-11-08 07:30:44,600 - __main__ - INFO - Epoch 36 Batch 100: Train Loss = 0.1958
2023-11-08 07:31:00,671 - __main__ - INFO - Epoch 36 Batch 120: Train Loss = 0.2159
2023-11-08 07:31:14,202 - __main__ - INFO - Epoch 37 Batch 0: Train Loss = 0.2462
2023-11-08 07:31:30,159 - __main__ - INFO - Epoch 37 Batch 20: Train Loss = 0.2755
2023-11-08 07:31:45,154 - __main__ - INFO - Epoch 37 Batch 40: Train Loss = 0.2664
2023-11-08 07:32:01,853 - __main__ - INFO - Epoch 37 Batch 60: Train Loss = 0.2691
2023-11-08 07:32:17,909 - __main__ - INFO - Epoch 37 Batch 80: Train Loss = 0.2199
2023-11-08 07:32:33,587 - __main__ - INFO - Epoch 37 Batch 100: Train Loss = 0.1882
2023-11-08 07:32:49,758 - __main__ - INFO - Epoch 37 Batch 120: Train Loss = 0.2244
2023-11-08 07:33:00,540 - __main__ - INFO - ------------ Save best model - AUROC: 0.8512 ------------
2023-11-08 07:33:01,486 - __main__ - INFO - Epoch 38 Batch 0: Train Loss = 0.2450
2023-11-08 07:33:17,489 - __main__ - INFO - Epoch 38 Batch 20: Train Loss = 0.2607
2023-11-08 07:33:34,071 - __main__ - INFO - Epoch 38 Batch 40: Train Loss = 0.2783
2023-11-08 07:33:51,143 - __main__ - INFO - Epoch 38 Batch 60: Train Loss = 0.2609
2023-11-08 07:34:06,806 - __main__ - INFO - Epoch 38 Batch 80: Train Loss = 0.2158
2023-11-08 07:34:23,478 - __main__ - INFO - Epoch 38 Batch 100: Train Loss = 0.1861
2023-11-08 07:34:38,043 - __main__ - INFO - Epoch 38 Batch 120: Train Loss = 0.2324
2023-11-08 07:34:50,760 - __main__ - INFO - Epoch 39 Batch 0: Train Loss = 0.2526
2023-11-08 07:35:07,335 - __main__ - INFO - Epoch 39 Batch 20: Train Loss = 0.2653
2023-11-08 07:35:23,637 - __main__ - INFO - Epoch 39 Batch 40: Train Loss = 0.2869
2023-11-08 07:35:39,144 - __main__ - INFO - Epoch 39 Batch 60: Train Loss = 0.2463
2023-11-08 07:35:55,988 - __main__ - INFO - Epoch 39 Batch 80: Train Loss = 0.2306
2023-11-08 07:36:12,485 - __main__ - INFO - Epoch 39 Batch 100: Train Loss = 0.1766
2023-11-08 07:36:27,967 - __main__ - INFO - Epoch 39 Batch 120: Train Loss = 0.2434
2023-11-08 07:36:39,282 - __main__ - INFO - ------------ Save best model - AUROC: 0.8524 ------------
2023-11-08 07:36:40,163 - __main__ - INFO - Epoch 40 Batch 0: Train Loss = 0.2260
2023-11-08 07:36:57,155 - __main__ - INFO - Epoch 40 Batch 20: Train Loss = 0.2614
2023-11-08 07:37:14,156 - __main__ - INFO - Epoch 40 Batch 40: Train Loss = 0.2829
2023-11-08 07:37:30,380 - __main__ - INFO - Epoch 40 Batch 60: Train Loss = 0.2690
2023-11-08 07:37:46,963 - __main__ - INFO - Epoch 40 Batch 80: Train Loss = 0.2283
2023-11-08 07:38:04,010 - __main__ - INFO - Epoch 40 Batch 100: Train Loss = 0.2068
2023-11-08 07:38:20,548 - __main__ - INFO - Epoch 40 Batch 120: Train Loss = 0.2406
2023-11-08 07:38:32,807 - __main__ - INFO - Epoch 41 Batch 0: Train Loss = 0.2390
2023-11-08 07:38:49,643 - __main__ - INFO - Epoch 41 Batch 20: Train Loss = 0.2681
2023-11-08 07:39:05,241 - __main__ - INFO - Epoch 41 Batch 40: Train Loss = 0.2897
2023-11-08 07:39:21,584 - __main__ - INFO - Epoch 41 Batch 60: Train Loss = 0.2361
2023-11-08 07:39:37,055 - __main__ - INFO - Epoch 41 Batch 80: Train Loss = 0.2340
2023-11-08 07:39:53,627 - __main__ - INFO - Epoch 41 Batch 100: Train Loss = 0.1862
2023-11-08 07:40:10,198 - __main__ - INFO - Epoch 41 Batch 120: Train Loss = 0.2537
2023-11-08 07:40:21,745 - __main__ - INFO - ------------ Save best model - AUROC: 0.8535 ------------
2023-11-08 07:40:22,554 - __main__ - INFO - Epoch 42 Batch 0: Train Loss = 0.2637
2023-11-08 07:40:39,508 - __main__ - INFO - Epoch 42 Batch 20: Train Loss = 0.2577
2023-11-08 07:40:55,398 - __main__ - INFO - Epoch 42 Batch 40: Train Loss = 0.2825
2023-11-08 07:41:11,107 - __main__ - INFO - Epoch 42 Batch 60: Train Loss = 0.2683
2023-11-08 07:41:26,824 - __main__ - INFO - Epoch 42 Batch 80: Train Loss = 0.2474
2023-11-08 07:41:44,288 - __main__ - INFO - Epoch 42 Batch 100: Train Loss = 0.1941
2023-11-08 07:41:59,641 - __main__ - INFO - Epoch 42 Batch 120: Train Loss = 0.2244
2023-11-08 07:42:11,060 - __main__ - INFO - ------------ Save best model - AUROC: 0.8537 ------------
2023-11-08 07:42:11,812 - __main__ - INFO - Epoch 43 Batch 0: Train Loss = 0.2429
2023-11-08 07:42:28,534 - __main__ - INFO - Epoch 43 Batch 20: Train Loss = 0.2876
2023-11-08 07:42:45,359 - __main__ - INFO - Epoch 43 Batch 40: Train Loss = 0.2933
2023-11-08 07:43:01,957 - __main__ - INFO - Epoch 43 Batch 60: Train Loss = 0.2679
2023-11-08 07:43:18,427 - __main__ - INFO - Epoch 43 Batch 80: Train Loss = 0.2284
2023-11-08 07:43:35,201 - __main__ - INFO - Epoch 43 Batch 100: Train Loss = 0.1944
2023-11-08 07:43:51,209 - __main__ - INFO - Epoch 43 Batch 120: Train Loss = 0.2416
2023-11-08 07:44:03,770 - __main__ - INFO - Epoch 44 Batch 0: Train Loss = 0.2594
2023-11-08 07:44:20,567 - __main__ - INFO - Epoch 44 Batch 20: Train Loss = 0.2600
2023-11-08 07:44:36,286 - __main__ - INFO - Epoch 44 Batch 40: Train Loss = 0.2847
2023-11-08 07:44:52,827 - __main__ - INFO - Epoch 44 Batch 60: Train Loss = 0.2576
2023-11-08 07:45:10,666 - __main__ - INFO - Epoch 44 Batch 80: Train Loss = 0.2214
2023-11-08 07:45:26,958 - __main__ - INFO - Epoch 44 Batch 100: Train Loss = 0.1995
2023-11-08 07:45:43,375 - __main__ - INFO - Epoch 44 Batch 120: Train Loss = 0.2232
2023-11-08 07:45:55,258 - __main__ - INFO - Epoch 45 Batch 0: Train Loss = 0.2503
2023-11-08 07:46:11,253 - __main__ - INFO - Epoch 45 Batch 20: Train Loss = 0.2462
2023-11-08 07:46:27,051 - __main__ - INFO - Epoch 45 Batch 40: Train Loss = 0.2539
2023-11-08 07:46:43,786 - __main__ - INFO - Epoch 45 Batch 60: Train Loss = 0.2545
2023-11-08 07:46:59,271 - __main__ - INFO - Epoch 45 Batch 80: Train Loss = 0.2285
2023-11-08 07:47:15,520 - __main__ - INFO - Epoch 45 Batch 100: Train Loss = 0.2183
2023-11-08 07:47:31,224 - __main__ - INFO - Epoch 45 Batch 120: Train Loss = 0.2499
2023-11-08 07:47:42,884 - __main__ - INFO - Epoch 46 Batch 0: Train Loss = 0.2439
2023-11-08 07:47:59,501 - __main__ - INFO - Epoch 46 Batch 20: Train Loss = 0.2755
2023-11-08 07:48:15,367 - __main__ - INFO - Epoch 46 Batch 40: Train Loss = 0.2689
2023-11-08 07:48:30,679 - __main__ - INFO - Epoch 46 Batch 60: Train Loss = 0.2693
2023-11-08 07:48:46,521 - __main__ - INFO - Epoch 46 Batch 80: Train Loss = 0.2012
2023-11-08 07:49:03,043 - __main__ - INFO - Epoch 46 Batch 100: Train Loss = 0.1731
2023-11-08 07:49:19,049 - __main__ - INFO - Epoch 46 Batch 120: Train Loss = 0.2485
2023-11-08 07:49:31,162 - __main__ - INFO - Epoch 47 Batch 0: Train Loss = 0.2442
2023-11-08 07:49:47,493 - __main__ - INFO - Epoch 47 Batch 20: Train Loss = 0.2513
2023-11-08 07:50:02,857 - __main__ - INFO - Epoch 47 Batch 40: Train Loss = 0.2884
2023-11-08 07:50:18,561 - __main__ - INFO - Epoch 47 Batch 60: Train Loss = 0.2807
2023-11-08 07:50:34,529 - __main__ - INFO - Epoch 47 Batch 80: Train Loss = 0.2183
2023-11-08 07:50:51,012 - __main__ - INFO - Epoch 47 Batch 100: Train Loss = 0.1964
2023-11-08 07:51:07,616 - __main__ - INFO - Epoch 47 Batch 120: Train Loss = 0.2373
2023-11-08 07:51:20,035 - __main__ - INFO - Epoch 48 Batch 0: Train Loss = 0.2373
2023-11-08 07:51:35,681 - __main__ - INFO - Epoch 48 Batch 20: Train Loss = 0.2637
2023-11-08 07:51:52,604 - __main__ - INFO - Epoch 48 Batch 40: Train Loss = 0.2858
2023-11-08 07:52:08,402 - __main__ - INFO - Epoch 48 Batch 60: Train Loss = 0.2848
2023-11-08 07:52:24,447 - __main__ - INFO - Epoch 48 Batch 80: Train Loss = 0.2233
2023-11-08 07:52:40,371 - __main__ - INFO - Epoch 48 Batch 100: Train Loss = 0.1764
2023-11-08 07:52:56,004 - __main__ - INFO - Epoch 48 Batch 120: Train Loss = 0.2607
2023-11-08 07:53:06,713 - __main__ - INFO - ------------ Save best model - AUROC: 0.8540 ------------
2023-11-08 07:53:07,567 - __main__ - INFO - Epoch 49 Batch 0: Train Loss = 0.2613
2023-11-08 07:53:24,435 - __main__ - INFO - Epoch 49 Batch 20: Train Loss = 0.2426
2023-11-08 07:53:40,188 - __main__ - INFO - Epoch 49 Batch 40: Train Loss = 0.2755
2023-11-08 07:53:57,055 - __main__ - INFO - Epoch 49 Batch 60: Train Loss = 0.2576
2023-11-08 07:54:12,969 - __main__ - INFO - Epoch 49 Batch 80: Train Loss = 0.2228
2023-11-08 07:54:29,452 - __main__ - INFO - Epoch 49 Batch 100: Train Loss = 0.1952
2023-11-08 07:54:45,710 - __main__ - INFO - Epoch 49 Batch 120: Train Loss = 0.2557
2023-11-08 07:54:57,943 - __main__ - INFO - ------------ Save best model - AUROC: 0.8544 ------------
2023-11-08 07:54:58,638 - __main__ - INFO - Epoch 50 Batch 0: Train Loss = 0.2566
2023-11-08 07:55:15,133 - __main__ - INFO - Epoch 50 Batch 20: Train Loss = 0.2733
2023-11-08 07:55:31,253 - __main__ - INFO - Epoch 50 Batch 40: Train Loss = 0.3045
2023-11-08 07:55:47,154 - __main__ - INFO - Epoch 50 Batch 60: Train Loss = 0.2708
2023-11-08 07:56:04,304 - __main__ - INFO - Epoch 50 Batch 80: Train Loss = 0.2364
2023-11-08 07:56:20,324 - __main__ - INFO - Epoch 50 Batch 100: Train Loss = 0.1767
2023-11-08 07:56:37,358 - __main__ - INFO - Epoch 50 Batch 120: Train Loss = 0.2300
2023-11-08 07:56:49,429 - __main__ - INFO - Epoch 51 Batch 0: Train Loss = 0.2506
2023-11-08 07:57:06,572 - __main__ - INFO - Epoch 51 Batch 20: Train Loss = 0.2507
2023-11-08 07:57:22,109 - __main__ - INFO - Epoch 51 Batch 40: Train Loss = 0.2975
2023-11-08 07:57:38,427 - __main__ - INFO - Epoch 51 Batch 60: Train Loss = 0.2584
2023-11-08 07:57:54,458 - __main__ - INFO - Epoch 51 Batch 80: Train Loss = 0.2056
2023-11-08 07:58:10,490 - __main__ - INFO - Epoch 51 Batch 100: Train Loss = 0.2016
2023-11-08 07:58:27,225 - __main__ - INFO - Epoch 51 Batch 120: Train Loss = 0.2300
2023-11-08 07:58:38,160 - __main__ - INFO - Epoch 52 Batch 0: Train Loss = 0.2626
2023-11-08 07:58:54,560 - __main__ - INFO - Epoch 52 Batch 20: Train Loss = 0.2731
2023-11-08 07:59:10,249 - __main__ - INFO - Epoch 52 Batch 40: Train Loss = 0.2891
2023-11-08 07:59:26,615 - __main__ - INFO - Epoch 52 Batch 60: Train Loss = 0.2482
2023-11-08 07:59:42,338 - __main__ - INFO - Epoch 52 Batch 80: Train Loss = 0.2212
2023-11-08 07:59:59,704 - __main__ - INFO - Epoch 52 Batch 100: Train Loss = 0.1877
2023-11-08 08:00:15,221 - __main__ - INFO - Epoch 52 Batch 120: Train Loss = 0.2597
2023-11-08 08:00:26,560 - __main__ - INFO - ------------ Save best model - AUROC: 0.8551 ------------
2023-11-08 08:00:27,331 - __main__ - INFO - Epoch 53 Batch 0: Train Loss = 0.2635
2023-11-08 08:00:44,903 - __main__ - INFO - Epoch 53 Batch 20: Train Loss = 0.2584
2023-11-08 08:01:01,255 - __main__ - INFO - Epoch 53 Batch 40: Train Loss = 0.2884
2023-11-08 08:01:16,701 - __main__ - INFO - Epoch 53 Batch 60: Train Loss = 0.2515
2023-11-08 08:01:33,485 - __main__ - INFO - Epoch 53 Batch 80: Train Loss = 0.2082
2023-11-08 08:01:49,451 - __main__ - INFO - Epoch 53 Batch 100: Train Loss = 0.1821
2023-11-08 08:02:04,436 - __main__ - INFO - Epoch 53 Batch 120: Train Loss = 0.2300
2023-11-08 08:02:15,612 - __main__ - INFO - Epoch 54 Batch 0: Train Loss = 0.2479
2023-11-08 08:02:32,790 - __main__ - INFO - Epoch 54 Batch 20: Train Loss = 0.2744
2023-11-08 08:02:48,797 - __main__ - INFO - Epoch 54 Batch 40: Train Loss = 0.2685
2023-11-08 08:03:04,598 - __main__ - INFO - Epoch 54 Batch 60: Train Loss = 0.2624
2023-11-08 08:03:22,758 - __main__ - INFO - Epoch 54 Batch 80: Train Loss = 0.2094
2023-11-08 08:03:39,334 - __main__ - INFO - Epoch 54 Batch 100: Train Loss = 0.1787
2023-11-08 08:03:54,537 - __main__ - INFO - Epoch 54 Batch 120: Train Loss = 0.2344
2023-11-08 08:04:06,291 - __main__ - INFO - Epoch 55 Batch 0: Train Loss = 0.2631
2023-11-08 08:04:22,271 - __main__ - INFO - Epoch 55 Batch 20: Train Loss = 0.2598
2023-11-08 08:04:37,339 - __main__ - INFO - Epoch 55 Batch 40: Train Loss = 0.2881
2023-11-08 08:04:53,203 - __main__ - INFO - Epoch 55 Batch 60: Train Loss = 0.2576
2023-11-08 08:05:09,539 - __main__ - INFO - Epoch 55 Batch 80: Train Loss = 0.2258
2023-11-08 08:05:25,586 - __main__ - INFO - Epoch 55 Batch 100: Train Loss = 0.1969
2023-11-08 08:05:41,841 - __main__ - INFO - Epoch 55 Batch 120: Train Loss = 0.2295
2023-11-08 08:05:52,613 - __main__ - INFO - ------------ Save best model - AUROC: 0.8556 ------------
2023-11-08 08:05:53,343 - __main__ - INFO - Epoch 56 Batch 0: Train Loss = 0.2515
2023-11-08 08:06:10,019 - __main__ - INFO - Epoch 56 Batch 20: Train Loss = 0.2536
2023-11-08 08:06:25,526 - __main__ - INFO - Epoch 56 Batch 40: Train Loss = 0.2780
2023-11-08 08:06:41,868 - __main__ - INFO - Epoch 56 Batch 60: Train Loss = 0.2842
2023-11-08 08:06:57,640 - __main__ - INFO - Epoch 56 Batch 80: Train Loss = 0.2140
2023-11-08 08:07:14,417 - __main__ - INFO - Epoch 56 Batch 100: Train Loss = 0.1859
2023-11-08 08:07:30,416 - __main__ - INFO - Epoch 56 Batch 120: Train Loss = 0.2285
2023-11-08 08:07:40,869 - __main__ - INFO - ------------ Save best model - AUROC: 0.8563 ------------
2023-11-08 08:07:41,492 - __main__ - INFO - Epoch 57 Batch 0: Train Loss = 0.2529
2023-11-08 08:07:57,397 - __main__ - INFO - Epoch 57 Batch 20: Train Loss = 0.2639
2023-11-08 08:08:13,013 - __main__ - INFO - Epoch 57 Batch 40: Train Loss = 0.2751
2023-11-08 08:08:28,752 - __main__ - INFO - Epoch 57 Batch 60: Train Loss = 0.2691
2023-11-08 08:08:45,734 - __main__ - INFO - Epoch 57 Batch 80: Train Loss = 0.2006
2023-11-08 08:09:01,727 - __main__ - INFO - Epoch 57 Batch 100: Train Loss = 0.2003
2023-11-08 08:09:16,956 - __main__ - INFO - Epoch 57 Batch 120: Train Loss = 0.2266
2023-11-08 08:09:29,180 - __main__ - INFO - Epoch 58 Batch 0: Train Loss = 0.2033
2023-11-08 08:09:46,063 - __main__ - INFO - Epoch 58 Batch 20: Train Loss = 0.2646
2023-11-08 08:10:01,657 - __main__ - INFO - Epoch 58 Batch 40: Train Loss = 0.2672
2023-11-08 08:10:18,966 - __main__ - INFO - Epoch 58 Batch 60: Train Loss = 0.2456
2023-11-08 08:10:35,307 - __main__ - INFO - Epoch 58 Batch 80: Train Loss = 0.2345
2023-11-08 08:10:51,161 - __main__ - INFO - Epoch 58 Batch 100: Train Loss = 0.1977
2023-11-08 08:11:07,307 - __main__ - INFO - Epoch 58 Batch 120: Train Loss = 0.2345
2023-11-08 08:11:19,721 - __main__ - INFO - Epoch 59 Batch 0: Train Loss = 0.2372
2023-11-08 08:11:35,652 - __main__ - INFO - Epoch 59 Batch 20: Train Loss = 0.2712
2023-11-08 08:11:51,231 - __main__ - INFO - Epoch 59 Batch 40: Train Loss = 0.2576
2023-11-08 08:12:07,434 - __main__ - INFO - Epoch 59 Batch 60: Train Loss = 0.2659
2023-11-08 08:12:23,294 - __main__ - INFO - Epoch 59 Batch 80: Train Loss = 0.2251
2023-11-08 08:12:40,789 - __main__ - INFO - Epoch 59 Batch 100: Train Loss = 0.2032
2023-11-08 08:12:56,569 - __main__ - INFO - Epoch 59 Batch 120: Train Loss = 0.2285
2023-11-08 08:13:08,406 - __main__ - INFO - Epoch 60 Batch 0: Train Loss = 0.2396
2023-11-08 08:13:25,139 - __main__ - INFO - Epoch 60 Batch 20: Train Loss = 0.2608
2023-11-08 08:13:41,340 - __main__ - INFO - Epoch 60 Batch 40: Train Loss = 0.2821
2023-11-08 08:13:56,371 - __main__ - INFO - Epoch 60 Batch 60: Train Loss = 0.2631
2023-11-08 08:14:12,709 - __main__ - INFO - Epoch 60 Batch 80: Train Loss = 0.2340
2023-11-08 08:14:28,821 - __main__ - INFO - Epoch 60 Batch 100: Train Loss = 0.1903
2023-11-08 08:14:45,344 - __main__ - INFO - Epoch 60 Batch 120: Train Loss = 0.2237
2023-11-08 08:14:58,683 - __main__ - INFO - Epoch 61 Batch 0: Train Loss = 0.2368
2023-11-08 08:15:15,302 - __main__ - INFO - Epoch 61 Batch 20: Train Loss = 0.2535
2023-11-08 08:15:31,709 - __main__ - INFO - Epoch 61 Batch 40: Train Loss = 0.2694
2023-11-08 08:15:46,979 - __main__ - INFO - Epoch 61 Batch 60: Train Loss = 0.2561
2023-11-08 08:16:04,009 - __main__ - INFO - Epoch 61 Batch 80: Train Loss = 0.2146
2023-11-08 08:16:19,705 - __main__ - INFO - Epoch 61 Batch 100: Train Loss = 0.1880
2023-11-08 08:16:35,859 - __main__ - INFO - Epoch 61 Batch 120: Train Loss = 0.2269
2023-11-08 08:16:47,363 - __main__ - INFO - ------------ Save best model - AUROC: 0.8571 ------------
2023-11-08 08:16:48,272 - __main__ - INFO - Epoch 62 Batch 0: Train Loss = 0.2337
2023-11-08 08:17:04,742 - __main__ - INFO - Epoch 62 Batch 20: Train Loss = 0.2746
2023-11-08 08:17:20,407 - __main__ - INFO - Epoch 62 Batch 40: Train Loss = 0.2690
2023-11-08 08:17:36,541 - __main__ - INFO - Epoch 62 Batch 60: Train Loss = 0.2475
2023-11-08 08:17:52,279 - __main__ - INFO - Epoch 62 Batch 80: Train Loss = 0.2221
2023-11-08 08:18:08,191 - __main__ - INFO - Epoch 62 Batch 100: Train Loss = 0.1965
2023-11-08 08:18:24,673 - __main__ - INFO - Epoch 62 Batch 120: Train Loss = 0.2414
2023-11-08 08:18:37,481 - __main__ - INFO - Epoch 63 Batch 0: Train Loss = 0.2851
2023-11-08 08:18:54,062 - __main__ - INFO - Epoch 63 Batch 20: Train Loss = 0.2458
2023-11-08 08:19:10,094 - __main__ - INFO - Epoch 63 Batch 40: Train Loss = 0.2946
2023-11-08 08:19:26,414 - __main__ - INFO - Epoch 63 Batch 60: Train Loss = 0.2493
2023-11-08 08:19:41,947 - __main__ - INFO - Epoch 63 Batch 80: Train Loss = 0.2125
2023-11-08 08:19:58,062 - __main__ - INFO - Epoch 63 Batch 100: Train Loss = 0.2159
2023-11-08 08:20:13,485 - __main__ - INFO - Epoch 63 Batch 120: Train Loss = 0.2283
2023-11-08 08:20:25,151 - __main__ - INFO - Epoch 64 Batch 0: Train Loss = 0.2566
2023-11-08 08:20:41,300 - __main__ - INFO - Epoch 64 Batch 20: Train Loss = 0.2470
2023-11-08 08:20:57,455 - __main__ - INFO - Epoch 64 Batch 40: Train Loss = 0.2671
2023-11-08 08:21:13,424 - __main__ - INFO - Epoch 64 Batch 60: Train Loss = 0.2712
2023-11-08 08:21:30,000 - __main__ - INFO - Epoch 64 Batch 80: Train Loss = 0.2255
2023-11-08 08:21:47,617 - __main__ - INFO - Epoch 64 Batch 100: Train Loss = 0.1927
2023-11-08 08:22:03,204 - __main__ - INFO - Epoch 64 Batch 120: Train Loss = 0.2008
2023-11-08 08:22:15,313 - __main__ - INFO - Epoch 65 Batch 0: Train Loss = 0.2570
2023-11-08 08:22:32,199 - __main__ - INFO - Epoch 65 Batch 20: Train Loss = 0.2812
2023-11-08 08:22:47,461 - __main__ - INFO - Epoch 65 Batch 40: Train Loss = 0.2779
2023-11-08 08:23:03,310 - __main__ - INFO - Epoch 65 Batch 60: Train Loss = 0.2425
2023-11-08 08:23:20,495 - __main__ - INFO - Epoch 65 Batch 80: Train Loss = 0.2044
2023-11-08 08:23:36,180 - __main__ - INFO - Epoch 65 Batch 100: Train Loss = 0.2003
2023-11-08 08:23:52,001 - __main__ - INFO - Epoch 65 Batch 120: Train Loss = 0.2416
2023-11-08 08:24:04,068 - __main__ - INFO - ------------ Save best model - AUROC: 0.8585 ------------
2023-11-08 08:24:04,901 - __main__ - INFO - Epoch 66 Batch 0: Train Loss = 0.2503
2023-11-08 08:24:20,846 - __main__ - INFO - Epoch 66 Batch 20: Train Loss = 0.2584
2023-11-08 08:24:36,503 - __main__ - INFO - Epoch 66 Batch 40: Train Loss = 0.2617
2023-11-08 08:24:52,718 - __main__ - INFO - Epoch 66 Batch 60: Train Loss = 0.2525
2023-11-08 08:25:09,898 - __main__ - INFO - Epoch 66 Batch 80: Train Loss = 0.2125
2023-11-08 08:25:26,445 - __main__ - INFO - Epoch 66 Batch 100: Train Loss = 0.1868
2023-11-08 08:25:42,583 - __main__ - INFO - Epoch 66 Batch 120: Train Loss = 0.2114
2023-11-08 08:25:53,864 - __main__ - INFO - Epoch 67 Batch 0: Train Loss = 0.2576
2023-11-08 08:26:09,756 - __main__ - INFO - Epoch 67 Batch 20: Train Loss = 0.2573
2023-11-08 08:26:26,572 - __main__ - INFO - Epoch 67 Batch 40: Train Loss = 0.2651
2023-11-08 08:26:42,718 - __main__ - INFO - Epoch 67 Batch 60: Train Loss = 0.2728
2023-11-08 08:26:58,421 - __main__ - INFO - Epoch 67 Batch 80: Train Loss = 0.2215
2023-11-08 08:27:14,737 - __main__ - INFO - Epoch 67 Batch 100: Train Loss = 0.1889
2023-11-08 08:27:31,422 - __main__ - INFO - Epoch 67 Batch 120: Train Loss = 0.2321
2023-11-08 08:27:43,613 - __main__ - INFO - Epoch 68 Batch 0: Train Loss = 0.2612
2023-11-08 08:28:00,258 - __main__ - INFO - Epoch 68 Batch 20: Train Loss = 0.2568
2023-11-08 08:28:16,511 - __main__ - INFO - Epoch 68 Batch 40: Train Loss = 0.2940
2023-11-08 08:28:32,429 - __main__ - INFO - Epoch 68 Batch 60: Train Loss = 0.2664
2023-11-08 08:28:50,060 - __main__ - INFO - Epoch 68 Batch 80: Train Loss = 0.2098
2023-11-08 08:29:06,198 - __main__ - INFO - Epoch 68 Batch 100: Train Loss = 0.1878
2023-11-08 08:29:22,539 - __main__ - INFO - Epoch 68 Batch 120: Train Loss = 0.2435
2023-11-08 08:29:35,316 - __main__ - INFO - Epoch 69 Batch 0: Train Loss = 0.2669
2023-11-08 08:29:51,975 - __main__ - INFO - Epoch 69 Batch 20: Train Loss = 0.2720
2023-11-08 08:30:08,182 - __main__ - INFO - Epoch 69 Batch 40: Train Loss = 0.2641
2023-11-08 08:30:24,142 - __main__ - INFO - Epoch 69 Batch 60: Train Loss = 0.2608
2023-11-08 08:30:40,403 - __main__ - INFO - Epoch 69 Batch 80: Train Loss = 0.2391
2023-11-08 08:30:56,237 - __main__ - INFO - Epoch 69 Batch 100: Train Loss = 0.1828
2023-11-08 08:31:12,308 - __main__ - INFO - Epoch 69 Batch 120: Train Loss = 0.2495
2023-11-08 08:31:24,147 - __main__ - INFO - Epoch 70 Batch 0: Train Loss = 0.2528
2023-11-08 08:31:41,341 - __main__ - INFO - Epoch 70 Batch 20: Train Loss = 0.2678
2023-11-08 08:31:57,130 - __main__ - INFO - Epoch 70 Batch 40: Train Loss = 0.2832
2023-11-08 08:32:12,588 - __main__ - INFO - Epoch 70 Batch 60: Train Loss = 0.2746
2023-11-08 08:32:29,326 - __main__ - INFO - Epoch 70 Batch 80: Train Loss = 0.2319
2023-11-08 08:32:45,853 - __main__ - INFO - Epoch 70 Batch 100: Train Loss = 0.1895
2023-11-08 08:33:01,925 - __main__ - INFO - Epoch 70 Batch 120: Train Loss = 0.2331
2023-11-08 08:33:14,421 - __main__ - INFO - Epoch 71 Batch 0: Train Loss = 0.2283
2023-11-08 08:33:31,363 - __main__ - INFO - Epoch 71 Batch 20: Train Loss = 0.2650
2023-11-08 08:33:46,724 - __main__ - INFO - Epoch 71 Batch 40: Train Loss = 0.2745
2023-11-08 08:34:02,031 - __main__ - INFO - Epoch 71 Batch 60: Train Loss = 0.2598
2023-11-08 08:34:18,368 - __main__ - INFO - Epoch 71 Batch 80: Train Loss = 0.2044
2023-11-08 08:34:34,696 - __main__ - INFO - Epoch 71 Batch 100: Train Loss = 0.1769
2023-11-08 08:34:50,042 - __main__ - INFO - Epoch 71 Batch 120: Train Loss = 0.2351
2023-11-08 08:35:01,665 - __main__ - INFO - Epoch 72 Batch 0: Train Loss = 0.2475
2023-11-08 08:35:18,110 - __main__ - INFO - Epoch 72 Batch 20: Train Loss = 0.2673
2023-11-08 08:35:34,512 - __main__ - INFO - Epoch 72 Batch 40: Train Loss = 0.2636
2023-11-08 08:35:50,795 - __main__ - INFO - Epoch 72 Batch 60: Train Loss = 0.2512
2023-11-08 08:36:07,211 - __main__ - INFO - Epoch 72 Batch 80: Train Loss = 0.2075
2023-11-08 08:36:23,688 - __main__ - INFO - Epoch 72 Batch 100: Train Loss = 0.2033
2023-11-08 08:36:39,914 - __main__ - INFO - Epoch 72 Batch 120: Train Loss = 0.2144
2023-11-08 08:36:51,420 - __main__ - INFO - Epoch 73 Batch 0: Train Loss = 0.2261
2023-11-08 08:37:07,459 - __main__ - INFO - Epoch 73 Batch 20: Train Loss = 0.2555
2023-11-08 08:37:23,580 - __main__ - INFO - Epoch 73 Batch 40: Train Loss = 0.2990
2023-11-08 08:37:40,474 - __main__ - INFO - Epoch 73 Batch 60: Train Loss = 0.2649
2023-11-08 08:37:56,764 - __main__ - INFO - Epoch 73 Batch 80: Train Loss = 0.2048
2023-11-08 08:38:13,178 - __main__ - INFO - Epoch 73 Batch 100: Train Loss = 0.1873
2023-11-08 08:38:28,914 - __main__ - INFO - Epoch 73 Batch 120: Train Loss = 0.2138
2023-11-08 08:38:41,166 - __main__ - INFO - Epoch 74 Batch 0: Train Loss = 0.2632
2023-11-08 08:38:57,077 - __main__ - INFO - Epoch 74 Batch 20: Train Loss = 0.2744
2023-11-08 08:39:12,544 - __main__ - INFO - Epoch 74 Batch 40: Train Loss = 0.2840
2023-11-08 08:39:28,103 - __main__ - INFO - Epoch 74 Batch 60: Train Loss = 0.2676
2023-11-08 08:39:44,606 - __main__ - INFO - Epoch 74 Batch 80: Train Loss = 0.2079
2023-11-08 08:40:00,941 - __main__ - INFO - Epoch 74 Batch 100: Train Loss = 0.1839
2023-11-08 08:40:18,075 - __main__ - INFO - Epoch 74 Batch 120: Train Loss = 0.2600
2023-11-08 08:40:29,572 - __main__ - INFO - Epoch 75 Batch 0: Train Loss = 0.2473
2023-11-08 08:40:45,701 - __main__ - INFO - Epoch 75 Batch 20: Train Loss = 0.2581
2023-11-08 08:41:01,699 - __main__ - INFO - Epoch 75 Batch 40: Train Loss = 0.2465
2023-11-08 08:41:17,220 - __main__ - INFO - Epoch 75 Batch 60: Train Loss = 0.2537
2023-11-08 08:41:33,364 - __main__ - INFO - Epoch 75 Batch 80: Train Loss = 0.2169
2023-11-08 08:41:49,019 - __main__ - INFO - Epoch 75 Batch 100: Train Loss = 0.1840
2023-11-08 08:42:04,380 - __main__ - INFO - Epoch 75 Batch 120: Train Loss = 0.2478
2023-11-08 08:42:16,428 - __main__ - INFO - Epoch 76 Batch 0: Train Loss = 0.2377
2023-11-08 08:42:33,277 - __main__ - INFO - Epoch 76 Batch 20: Train Loss = 0.2731
2023-11-08 08:42:49,384 - __main__ - INFO - Epoch 76 Batch 40: Train Loss = 0.2731
2023-11-08 08:43:05,364 - __main__ - INFO - Epoch 76 Batch 60: Train Loss = 0.2419
2023-11-08 08:43:21,663 - __main__ - INFO - Epoch 76 Batch 80: Train Loss = 0.2017
2023-11-08 08:43:38,299 - __main__ - INFO - Epoch 76 Batch 100: Train Loss = 0.1980
2023-11-08 08:43:54,382 - __main__ - INFO - Epoch 76 Batch 120: Train Loss = 0.2390
2023-11-08 08:44:06,529 - __main__ - INFO - Epoch 77 Batch 0: Train Loss = 0.2445
2023-11-08 08:44:22,395 - __main__ - INFO - Epoch 77 Batch 20: Train Loss = 0.2585
2023-11-08 08:44:38,362 - __main__ - INFO - Epoch 77 Batch 40: Train Loss = 0.2751
2023-11-08 08:44:55,666 - __main__ - INFO - Epoch 77 Batch 60: Train Loss = 0.2704
2023-11-08 08:45:12,597 - __main__ - INFO - Epoch 77 Batch 80: Train Loss = 0.2051
2023-11-08 08:45:28,256 - __main__ - INFO - Epoch 77 Batch 100: Train Loss = 0.1972
2023-11-08 08:45:44,051 - __main__ - INFO - Epoch 77 Batch 120: Train Loss = 0.2037
2023-11-08 08:45:55,869 - __main__ - INFO - Epoch 78 Batch 0: Train Loss = 0.2591
2023-11-08 08:46:12,556 - __main__ - INFO - Epoch 78 Batch 20: Train Loss = 0.2600
2023-11-08 08:46:27,690 - __main__ - INFO - Epoch 78 Batch 40: Train Loss = 0.2602
2023-11-08 08:46:43,518 - __main__ - INFO - Epoch 78 Batch 60: Train Loss = 0.2641
2023-11-08 08:47:00,091 - __main__ - INFO - Epoch 78 Batch 80: Train Loss = 0.2445
2023-11-08 08:47:16,834 - __main__ - INFO - Epoch 78 Batch 100: Train Loss = 0.1938
2023-11-08 08:47:32,101 - __main__ - INFO - Epoch 78 Batch 120: Train Loss = 0.2148
2023-11-08 08:47:44,475 - __main__ - INFO - Epoch 79 Batch 0: Train Loss = 0.2643
2023-11-08 08:48:01,344 - __main__ - INFO - Epoch 79 Batch 20: Train Loss = 0.2811
2023-11-08 08:48:17,737 - __main__ - INFO - Epoch 79 Batch 40: Train Loss = 0.2818
2023-11-08 08:48:33,616 - __main__ - INFO - Epoch 79 Batch 60: Train Loss = 0.2581
2023-11-08 08:48:50,382 - __main__ - INFO - Epoch 79 Batch 80: Train Loss = 0.2492
2023-11-08 08:49:06,457 - __main__ - INFO - Epoch 79 Batch 100: Train Loss = 0.1812
2023-11-08 08:49:22,771 - __main__ - INFO - Epoch 79 Batch 120: Train Loss = 0.2467
2023-11-08 08:49:35,268 - __main__ - INFO - Epoch 80 Batch 0: Train Loss = 0.2448
2023-11-08 08:49:51,975 - __main__ - INFO - Epoch 80 Batch 20: Train Loss = 0.2766
2023-11-08 08:50:07,760 - __main__ - INFO - Epoch 80 Batch 40: Train Loss = 0.2674
2023-11-08 08:50:24,231 - __main__ - INFO - Epoch 80 Batch 60: Train Loss = 0.2736
2023-11-08 08:50:40,987 - __main__ - INFO - Epoch 80 Batch 80: Train Loss = 0.2333
2023-11-08 08:50:57,653 - __main__ - INFO - Epoch 80 Batch 100: Train Loss = 0.1917
2023-11-08 08:51:14,308 - __main__ - INFO - Epoch 80 Batch 120: Train Loss = 0.2439
2023-11-08 08:51:25,343 - __main__ - INFO - Epoch 81 Batch 0: Train Loss = 0.2453
2023-11-08 08:51:41,748 - __main__ - INFO - Epoch 81 Batch 20: Train Loss = 0.2664
2023-11-08 08:51:57,326 - __main__ - INFO - Epoch 81 Batch 40: Train Loss = 0.2870
2023-11-08 08:52:12,857 - __main__ - INFO - Epoch 81 Batch 60: Train Loss = 0.2514
2023-11-08 08:52:28,782 - __main__ - INFO - Epoch 81 Batch 80: Train Loss = 0.2165
2023-11-08 08:52:44,758 - __main__ - INFO - Epoch 81 Batch 100: Train Loss = 0.2082
2023-11-08 08:53:00,461 - __main__ - INFO - Epoch 81 Batch 120: Train Loss = 0.2253
2023-11-08 08:53:13,108 - __main__ - INFO - Epoch 82 Batch 0: Train Loss = 0.2400
2023-11-08 08:53:29,561 - __main__ - INFO - Epoch 82 Batch 20: Train Loss = 0.2485
2023-11-08 08:53:45,090 - __main__ - INFO - Epoch 82 Batch 40: Train Loss = 0.2655
2023-11-08 08:54:00,890 - __main__ - INFO - Epoch 82 Batch 60: Train Loss = 0.2667
2023-11-08 08:54:17,352 - __main__ - INFO - Epoch 82 Batch 80: Train Loss = 0.2400
2023-11-08 08:54:34,249 - __main__ - INFO - Epoch 82 Batch 100: Train Loss = 0.1978
2023-11-08 08:54:50,925 - __main__ - INFO - Epoch 82 Batch 120: Train Loss = 0.2355
2023-11-08 08:55:03,083 - __main__ - INFO - Epoch 83 Batch 0: Train Loss = 0.2550
2023-11-08 08:55:19,401 - __main__ - INFO - Epoch 83 Batch 20: Train Loss = 0.2633
2023-11-08 08:55:35,808 - __main__ - INFO - Epoch 83 Batch 40: Train Loss = 0.2771
2023-11-08 08:55:52,042 - __main__ - INFO - Epoch 83 Batch 60: Train Loss = 0.2327
2023-11-08 08:56:08,159 - __main__ - INFO - Epoch 83 Batch 80: Train Loss = 0.2119
2023-11-08 08:56:24,657 - __main__ - INFO - Epoch 83 Batch 100: Train Loss = 0.2090
2023-11-08 08:56:40,714 - __main__ - INFO - Epoch 83 Batch 120: Train Loss = 0.2380
2023-11-08 08:56:54,012 - __main__ - INFO - Epoch 84 Batch 0: Train Loss = 0.2367
2023-11-08 08:57:10,568 - __main__ - INFO - Epoch 84 Batch 20: Train Loss = 0.2417
2023-11-08 08:57:26,495 - __main__ - INFO - Epoch 84 Batch 40: Train Loss = 0.2701
2023-11-08 08:57:42,165 - __main__ - INFO - Epoch 84 Batch 60: Train Loss = 0.2465
2023-11-08 08:57:57,677 - __main__ - INFO - Epoch 84 Batch 80: Train Loss = 0.2213
2023-11-08 08:58:14,349 - __main__ - INFO - Epoch 84 Batch 100: Train Loss = 0.1884
2023-11-08 08:58:30,146 - __main__ - INFO - Epoch 84 Batch 120: Train Loss = 0.2432
2023-11-08 08:58:42,566 - __main__ - INFO - Epoch 85 Batch 0: Train Loss = 0.2700
2023-11-08 08:58:58,775 - __main__ - INFO - Epoch 85 Batch 20: Train Loss = 0.2900
2023-11-08 08:59:14,481 - __main__ - INFO - Epoch 85 Batch 40: Train Loss = 0.2670
2023-11-08 08:59:29,419 - __main__ - INFO - Epoch 85 Batch 60: Train Loss = 0.2702
2023-11-08 08:59:45,190 - __main__ - INFO - Epoch 85 Batch 80: Train Loss = 0.2523
2023-11-08 09:00:02,277 - __main__ - INFO - Epoch 85 Batch 100: Train Loss = 0.1933
2023-11-08 09:00:18,159 - __main__ - INFO - Epoch 85 Batch 120: Train Loss = 0.2454
2023-11-08 09:00:30,152 - __main__ - INFO - Epoch 86 Batch 0: Train Loss = 0.2446
2023-11-08 09:00:47,455 - __main__ - INFO - Epoch 86 Batch 20: Train Loss = 0.2526
2023-11-08 09:01:02,752 - __main__ - INFO - Epoch 86 Batch 40: Train Loss = 0.2802
2023-11-08 09:01:18,726 - __main__ - INFO - Epoch 86 Batch 60: Train Loss = 0.2409
2023-11-08 09:01:35,857 - __main__ - INFO - Epoch 86 Batch 80: Train Loss = 0.1928
2023-11-08 09:01:52,675 - __main__ - INFO - Epoch 86 Batch 100: Train Loss = 0.1850
2023-11-08 09:02:08,088 - __main__ - INFO - Epoch 86 Batch 120: Train Loss = 0.2181
2023-11-08 09:02:21,139 - __main__ - INFO - Epoch 87 Batch 0: Train Loss = 0.2239
2023-11-08 09:02:37,811 - __main__ - INFO - Epoch 87 Batch 20: Train Loss = 0.2604
2023-11-08 09:02:53,600 - __main__ - INFO - Epoch 87 Batch 40: Train Loss = 0.2682
2023-11-08 09:03:10,151 - __main__ - INFO - Epoch 87 Batch 60: Train Loss = 0.2595
2023-11-08 09:03:26,279 - __main__ - INFO - Epoch 87 Batch 80: Train Loss = 0.2269
2023-11-08 09:03:42,759 - __main__ - INFO - Epoch 87 Batch 100: Train Loss = 0.1912
2023-11-08 09:03:59,252 - __main__ - INFO - Epoch 87 Batch 120: Train Loss = 0.2213
2023-11-08 09:04:10,479 - __main__ - INFO - ------------ Save best model - AUROC: 0.8597 ------------
2023-11-08 09:04:11,213 - __main__ - INFO - Epoch 88 Batch 0: Train Loss = 0.2320
2023-11-08 09:04:27,948 - __main__ - INFO - Epoch 88 Batch 20: Train Loss = 0.2678
2023-11-08 09:04:44,529 - __main__ - INFO - Epoch 88 Batch 40: Train Loss = 0.2659
2023-11-08 09:04:59,976 - __main__ - INFO - Epoch 88 Batch 60: Train Loss = 0.2966
2023-11-08 09:05:16,422 - __main__ - INFO - Epoch 88 Batch 80: Train Loss = 0.2330
2023-11-08 09:05:32,717 - __main__ - INFO - Epoch 88 Batch 100: Train Loss = 0.1868
2023-11-08 09:05:47,942 - __main__ - INFO - Epoch 88 Batch 120: Train Loss = 0.2299
2023-11-08 09:06:00,408 - __main__ - INFO - Epoch 89 Batch 0: Train Loss = 0.2417
2023-11-08 09:06:16,836 - __main__ - INFO - Epoch 89 Batch 20: Train Loss = 0.2494
2023-11-08 09:06:32,935 - __main__ - INFO - Epoch 89 Batch 40: Train Loss = 0.2873
2023-11-08 09:06:49,644 - __main__ - INFO - Epoch 89 Batch 60: Train Loss = 0.2511
2023-11-08 09:07:05,582 - __main__ - INFO - Epoch 89 Batch 80: Train Loss = 0.2172
2023-11-08 09:07:20,764 - __main__ - INFO - Epoch 89 Batch 100: Train Loss = 0.1940
2023-11-08 09:07:36,896 - __main__ - INFO - Epoch 89 Batch 120: Train Loss = 0.2651
2023-11-08 09:07:48,431 - __main__ - INFO - Epoch 90 Batch 0: Train Loss = 0.2586
2023-11-08 09:08:06,062 - __main__ - INFO - Epoch 90 Batch 20: Train Loss = 0.2591
2023-11-08 09:08:21,995 - __main__ - INFO - Epoch 90 Batch 40: Train Loss = 0.2800
2023-11-08 09:08:37,565 - __main__ - INFO - Epoch 90 Batch 60: Train Loss = 0.2350
2023-11-08 09:08:53,360 - __main__ - INFO - Epoch 90 Batch 80: Train Loss = 0.2085
2023-11-08 09:09:09,386 - __main__ - INFO - Epoch 90 Batch 100: Train Loss = 0.2080
2023-11-08 09:09:26,275 - __main__ - INFO - Epoch 90 Batch 120: Train Loss = 0.2397
2023-11-08 09:09:37,898 - __main__ - INFO - Epoch 91 Batch 0: Train Loss = 0.2526
2023-11-08 09:09:54,855 - __main__ - INFO - Epoch 91 Batch 20: Train Loss = 0.2782
2023-11-08 09:10:10,801 - __main__ - INFO - Epoch 91 Batch 40: Train Loss = 0.2984
2023-11-08 09:10:28,029 - __main__ - INFO - Epoch 91 Batch 60: Train Loss = 0.2388
2023-11-08 09:10:44,433 - __main__ - INFO - Epoch 91 Batch 80: Train Loss = 0.2289
2023-11-08 09:10:59,969 - __main__ - INFO - Epoch 91 Batch 100: Train Loss = 0.1985
2023-11-08 09:11:15,840 - __main__ - INFO - Epoch 91 Batch 120: Train Loss = 0.2450
2023-11-08 09:11:28,090 - __main__ - INFO - Epoch 92 Batch 0: Train Loss = 0.2336
2023-11-08 09:11:44,020 - __main__ - INFO - Epoch 92 Batch 20: Train Loss = 0.2623
2023-11-08 09:11:59,173 - __main__ - INFO - Epoch 92 Batch 40: Train Loss = 0.2759
2023-11-08 09:12:15,043 - __main__ - INFO - Epoch 92 Batch 60: Train Loss = 0.2604
2023-11-08 09:12:31,211 - __main__ - INFO - Epoch 92 Batch 80: Train Loss = 0.1992
2023-11-08 09:12:48,714 - __main__ - INFO - Epoch 92 Batch 100: Train Loss = 0.1914
2023-11-08 09:13:05,240 - __main__ - INFO - Epoch 92 Batch 120: Train Loss = 0.2137
2023-11-08 09:13:17,208 - __main__ - INFO - Epoch 93 Batch 0: Train Loss = 0.2486
2023-11-08 09:13:33,209 - __main__ - INFO - Epoch 93 Batch 20: Train Loss = 0.2483
2023-11-08 09:13:49,382 - __main__ - INFO - Epoch 93 Batch 40: Train Loss = 0.2576
2023-11-08 09:14:04,599 - __main__ - INFO - Epoch 93 Batch 60: Train Loss = 0.2713
2023-11-08 09:14:21,227 - __main__ - INFO - Epoch 93 Batch 80: Train Loss = 0.2218
2023-11-08 09:14:37,108 - __main__ - INFO - Epoch 93 Batch 100: Train Loss = 0.1883
2023-11-08 09:14:53,371 - __main__ - INFO - Epoch 93 Batch 120: Train Loss = 0.2360
2023-11-08 09:15:05,222 - __main__ - INFO - Epoch 94 Batch 0: Train Loss = 0.2464
2023-11-08 09:15:21,318 - __main__ - INFO - Epoch 94 Batch 20: Train Loss = 0.2585
2023-11-08 09:15:37,214 - __main__ - INFO - Epoch 94 Batch 40: Train Loss = 0.2827
2023-11-08 09:15:52,795 - __main__ - INFO - Epoch 94 Batch 60: Train Loss = 0.2482
2023-11-08 09:16:09,289 - __main__ - INFO - Epoch 94 Batch 80: Train Loss = 0.2134
2023-11-08 09:16:26,343 - __main__ - INFO - Epoch 94 Batch 100: Train Loss = 0.2054
2023-11-08 09:16:42,116 - __main__ - INFO - Epoch 94 Batch 120: Train Loss = 0.2388
2023-11-08 09:16:53,749 - __main__ - INFO - Epoch 95 Batch 0: Train Loss = 0.2389
2023-11-08 09:17:10,403 - __main__ - INFO - Epoch 95 Batch 20: Train Loss = 0.2645
2023-11-08 09:17:26,403 - __main__ - INFO - Epoch 95 Batch 40: Train Loss = 0.2685
2023-11-08 09:17:43,260 - __main__ - INFO - Epoch 95 Batch 60: Train Loss = 0.2629
2023-11-08 09:17:59,399 - __main__ - INFO - Epoch 95 Batch 80: Train Loss = 0.2139
2023-11-08 09:18:15,582 - __main__ - INFO - Epoch 95 Batch 100: Train Loss = 0.1810
2023-11-08 09:18:32,315 - __main__ - INFO - Epoch 95 Batch 120: Train Loss = 0.2259
2023-11-08 09:18:44,368 - __main__ - INFO - Epoch 96 Batch 0: Train Loss = 0.2384
2023-11-08 09:19:01,484 - __main__ - INFO - Epoch 96 Batch 20: Train Loss = 0.2856
2023-11-08 09:19:16,943 - __main__ - INFO - Epoch 96 Batch 40: Train Loss = 0.2813
2023-11-08 09:19:34,042 - __main__ - INFO - Epoch 96 Batch 60: Train Loss = 0.2504
2023-11-08 09:19:50,599 - __main__ - INFO - Epoch 96 Batch 80: Train Loss = 0.2244
2023-11-08 09:20:07,074 - __main__ - INFO - Epoch 96 Batch 100: Train Loss = 0.2058
2023-11-08 09:20:23,598 - __main__ - INFO - Epoch 96 Batch 120: Train Loss = 0.2471
2023-11-08 09:20:35,776 - __main__ - INFO - Epoch 97 Batch 0: Train Loss = 0.2429
2023-11-08 09:20:53,113 - __main__ - INFO - Epoch 97 Batch 20: Train Loss = 0.2569
2023-11-08 09:21:08,611 - __main__ - INFO - Epoch 97 Batch 40: Train Loss = 0.2711
2023-11-08 09:21:24,683 - __main__ - INFO - Epoch 97 Batch 60: Train Loss = 0.2384
2023-11-08 09:21:40,511 - __main__ - INFO - Epoch 97 Batch 80: Train Loss = 0.2178
2023-11-08 09:21:57,342 - __main__ - INFO - Epoch 97 Batch 100: Train Loss = 0.1934
2023-11-08 09:22:13,435 - __main__ - INFO - Epoch 97 Batch 120: Train Loss = 0.2421
2023-11-08 09:22:26,001 - __main__ - INFO - Epoch 98 Batch 0: Train Loss = 0.2632
2023-11-08 09:22:41,467 - __main__ - INFO - Epoch 98 Batch 20: Train Loss = 0.3069
2023-11-08 09:22:57,492 - __main__ - INFO - Epoch 98 Batch 40: Train Loss = 0.2566
2023-11-08 09:23:12,846 - __main__ - INFO - Epoch 98 Batch 60: Train Loss = 0.2703
2023-11-08 09:23:29,551 - __main__ - INFO - Epoch 98 Batch 80: Train Loss = 0.2155
2023-11-08 09:23:46,081 - __main__ - INFO - Epoch 98 Batch 100: Train Loss = 0.1812
2023-11-08 09:24:02,225 - __main__ - INFO - Epoch 98 Batch 120: Train Loss = 0.2413
2023-11-08 09:24:14,248 - __main__ - INFO - Epoch 99 Batch 0: Train Loss = 0.2394
2023-11-08 09:24:30,335 - __main__ - INFO - Epoch 99 Batch 20: Train Loss = 0.2609
2023-11-08 09:24:47,021 - __main__ - INFO - Epoch 99 Batch 40: Train Loss = 0.2396
2023-11-08 09:25:02,816 - __main__ - INFO - Epoch 99 Batch 60: Train Loss = 0.2534
2023-11-08 09:25:20,070 - __main__ - INFO - Epoch 99 Batch 80: Train Loss = 0.2168
2023-11-08 09:25:36,838 - __main__ - INFO - Epoch 99 Batch 100: Train Loss = 0.2045
2023-11-08 09:25:52,892 - __main__ - INFO - Epoch 99 Batch 120: Train Loss = 0.2161
2023-11-08 09:26:04,765 - __main__ - INFO - Epoch 100 Batch 0: Train Loss = 0.2410
2023-11-08 09:26:22,062 - __main__ - INFO - Epoch 100 Batch 20: Train Loss = 0.2557
2023-11-08 09:26:38,442 - __main__ - INFO - Epoch 100 Batch 40: Train Loss = 0.2670
2023-11-08 09:26:54,116 - __main__ - INFO - Epoch 100 Batch 60: Train Loss = 0.2511
2023-11-08 09:27:10,608 - __main__ - INFO - Epoch 100 Batch 80: Train Loss = 0.2324
2023-11-08 09:27:26,917 - __main__ - INFO - Epoch 100 Batch 100: Train Loss = 0.2004
2023-11-08 09:27:43,171 - __main__ - INFO - Epoch 100 Batch 120: Train Loss = 0.2241
2023-11-08 09:27:55,036 - __main__ - INFO - Epoch 101 Batch 0: Train Loss = 0.2507
2023-11-08 09:28:11,602 - __main__ - INFO - Epoch 101 Batch 20: Train Loss = 0.2510
2023-11-08 09:28:27,928 - __main__ - INFO - Epoch 101 Batch 40: Train Loss = 0.2828
2023-11-08 09:28:45,035 - __main__ - INFO - Epoch 101 Batch 60: Train Loss = 0.2283
2023-11-08 09:29:01,197 - __main__ - INFO - Epoch 101 Batch 80: Train Loss = 0.1918
2023-11-08 09:29:16,912 - __main__ - INFO - Epoch 101 Batch 100: Train Loss = 0.1831
2023-11-08 09:29:33,033 - __main__ - INFO - Epoch 101 Batch 120: Train Loss = 0.2255
2023-11-08 09:29:45,063 - __main__ - INFO - Epoch 102 Batch 0: Train Loss = 0.2263
2023-11-08 09:30:02,279 - __main__ - INFO - Epoch 102 Batch 20: Train Loss = 0.2649
2023-11-08 09:30:18,461 - __main__ - INFO - Epoch 102 Batch 40: Train Loss = 0.2674
2023-11-08 09:30:34,395 - __main__ - INFO - Epoch 102 Batch 60: Train Loss = 0.2496
2023-11-08 09:30:51,280 - __main__ - INFO - Epoch 102 Batch 80: Train Loss = 0.2122
2023-11-08 09:31:08,506 - __main__ - INFO - Epoch 102 Batch 100: Train Loss = 0.1869
2023-11-08 09:31:24,939 - __main__ - INFO - Epoch 102 Batch 120: Train Loss = 0.2431
2023-11-08 09:31:37,392 - __main__ - INFO - Epoch 103 Batch 0: Train Loss = 0.2456
2023-11-08 09:31:54,627 - __main__ - INFO - Epoch 103 Batch 20: Train Loss = 0.2647
2023-11-08 09:32:10,628 - __main__ - INFO - Epoch 103 Batch 40: Train Loss = 0.2713
2023-11-08 09:32:27,408 - __main__ - INFO - Epoch 103 Batch 60: Train Loss = 0.2639
2023-11-08 09:32:44,056 - __main__ - INFO - Epoch 103 Batch 80: Train Loss = 0.2467
2023-11-08 09:33:00,452 - __main__ - INFO - Epoch 103 Batch 100: Train Loss = 0.1998
2023-11-08 09:33:17,291 - __main__ - INFO - Epoch 103 Batch 120: Train Loss = 0.2346
2023-11-08 09:33:30,740 - __main__ - INFO - Epoch 104 Batch 0: Train Loss = 0.2433
2023-11-08 09:33:47,803 - __main__ - INFO - Epoch 104 Batch 20: Train Loss = 0.2498
2023-11-08 09:34:04,162 - __main__ - INFO - Epoch 104 Batch 40: Train Loss = 0.2523
2023-11-08 09:34:19,636 - __main__ - INFO - Epoch 104 Batch 60: Train Loss = 0.2333
2023-11-08 09:34:34,975 - __main__ - INFO - Epoch 104 Batch 80: Train Loss = 0.2179
2023-11-08 09:34:51,387 - __main__ - INFO - Epoch 104 Batch 100: Train Loss = 0.1990
2023-11-08 09:35:08,407 - __main__ - INFO - Epoch 104 Batch 120: Train Loss = 0.2394
2023-11-08 09:35:20,589 - __main__ - INFO - Epoch 105 Batch 0: Train Loss = 0.2467
2023-11-08 09:35:37,791 - __main__ - INFO - Epoch 105 Batch 20: Train Loss = 0.2598
2023-11-08 09:35:54,122 - __main__ - INFO - Epoch 105 Batch 40: Train Loss = 0.2623
2023-11-08 09:36:09,873 - __main__ - INFO - Epoch 105 Batch 60: Train Loss = 0.2317
2023-11-08 09:36:26,789 - __main__ - INFO - Epoch 105 Batch 80: Train Loss = 0.2218
2023-11-08 09:36:43,015 - __main__ - INFO - Epoch 105 Batch 100: Train Loss = 0.1786
2023-11-08 09:36:59,171 - __main__ - INFO - Epoch 105 Batch 120: Train Loss = 0.2608
2023-11-08 09:37:11,695 - __main__ - INFO - Epoch 106 Batch 0: Train Loss = 0.2402
2023-11-08 09:37:29,598 - __main__ - INFO - Epoch 106 Batch 20: Train Loss = 0.2715
2023-11-08 09:37:45,669 - __main__ - INFO - Epoch 106 Batch 40: Train Loss = 0.2662
2023-11-08 09:38:02,861 - __main__ - INFO - Epoch 106 Batch 60: Train Loss = 0.2447
2023-11-08 09:38:19,875 - __main__ - INFO - Epoch 106 Batch 80: Train Loss = 0.2359
2023-11-08 09:38:36,309 - __main__ - INFO - Epoch 106 Batch 100: Train Loss = 0.1816
2023-11-08 09:38:53,395 - __main__ - INFO - Epoch 106 Batch 120: Train Loss = 0.2395
2023-11-08 09:39:05,730 - __main__ - INFO - ------------ Save best model - AUROC: 0.8600 ------------
2023-11-08 09:39:06,568 - __main__ - INFO - Epoch 107 Batch 0: Train Loss = 0.2106
2023-11-08 09:39:24,597 - __main__ - INFO - Epoch 107 Batch 20: Train Loss = 0.2627
2023-11-08 09:39:41,197 - __main__ - INFO - Epoch 107 Batch 40: Train Loss = 0.2476
2023-11-08 09:39:58,702 - __main__ - INFO - Epoch 107 Batch 60: Train Loss = 0.2529
2023-11-08 09:40:16,018 - __main__ - INFO - Epoch 107 Batch 80: Train Loss = 0.2312
2023-11-08 09:40:33,652 - __main__ - INFO - Epoch 107 Batch 100: Train Loss = 0.1954
2023-11-08 09:40:51,072 - __main__ - INFO - Epoch 107 Batch 120: Train Loss = 0.2440
2023-11-08 09:41:03,109 - __main__ - INFO - Epoch 108 Batch 0: Train Loss = 0.2355
2023-11-08 09:41:20,656 - __main__ - INFO - Epoch 108 Batch 20: Train Loss = 0.2412
2023-11-08 09:41:38,098 - __main__ - INFO - Epoch 108 Batch 40: Train Loss = 0.2712
2023-11-08 09:41:55,172 - __main__ - INFO - Epoch 108 Batch 60: Train Loss = 0.2543
2023-11-08 09:42:12,857 - __main__ - INFO - Epoch 108 Batch 80: Train Loss = 0.2154
2023-11-08 09:42:30,726 - __main__ - INFO - Epoch 108 Batch 100: Train Loss = 0.2067
2023-11-08 09:42:48,790 - __main__ - INFO - Epoch 108 Batch 120: Train Loss = 0.2338
2023-11-08 09:43:02,622 - __main__ - INFO - Epoch 109 Batch 0: Train Loss = 0.2110
2023-11-08 09:43:20,104 - __main__ - INFO - Epoch 109 Batch 20: Train Loss = 0.2555
2023-11-08 09:43:37,129 - __main__ - INFO - Epoch 109 Batch 40: Train Loss = 0.2588
2023-11-08 09:43:54,480 - __main__ - INFO - Epoch 109 Batch 60: Train Loss = 0.2441
2023-11-08 09:44:11,542 - __main__ - INFO - Epoch 109 Batch 80: Train Loss = 0.2354
2023-11-08 09:44:29,350 - __main__ - INFO - Epoch 109 Batch 100: Train Loss = 0.1957
2023-11-08 09:44:46,246 - __main__ - INFO - Epoch 109 Batch 120: Train Loss = 0.2482
2023-11-08 09:44:58,958 - __main__ - INFO - Epoch 110 Batch 0: Train Loss = 0.2376
2023-11-08 09:45:17,922 - __main__ - INFO - Epoch 110 Batch 20: Train Loss = 0.2534
2023-11-08 09:45:35,141 - __main__ - INFO - Epoch 110 Batch 40: Train Loss = 0.2430
2023-11-08 09:45:52,413 - __main__ - INFO - Epoch 110 Batch 60: Train Loss = 0.2627
2023-11-08 09:46:09,187 - __main__ - INFO - Epoch 110 Batch 80: Train Loss = 0.2099
2023-11-08 09:46:27,321 - __main__ - INFO - Epoch 110 Batch 100: Train Loss = 0.1966
2023-11-08 09:46:44,827 - __main__ - INFO - Epoch 110 Batch 120: Train Loss = 0.2309
2023-11-08 09:46:58,322 - __main__ - INFO - Epoch 111 Batch 0: Train Loss = 0.2220
2023-11-08 09:47:17,267 - __main__ - INFO - Epoch 111 Batch 20: Train Loss = 0.2638
2023-11-08 09:47:34,607 - __main__ - INFO - Epoch 111 Batch 40: Train Loss = 0.2927
2023-11-08 09:47:52,196 - __main__ - INFO - Epoch 111 Batch 60: Train Loss = 0.2667
2023-11-08 09:48:09,558 - __main__ - INFO - Epoch 111 Batch 80: Train Loss = 0.2366
2023-11-08 09:48:26,906 - __main__ - INFO - Epoch 111 Batch 100: Train Loss = 0.1700
2023-11-08 09:48:44,792 - __main__ - INFO - Epoch 111 Batch 120: Train Loss = 0.2293
2023-11-08 09:48:58,174 - __main__ - INFO - Epoch 112 Batch 0: Train Loss = 0.2503
2023-11-08 09:49:15,975 - __main__ - INFO - Epoch 112 Batch 20: Train Loss = 0.2757
2023-11-08 09:49:34,180 - __main__ - INFO - Epoch 112 Batch 40: Train Loss = 0.2586
2023-11-08 09:49:52,406 - __main__ - INFO - Epoch 112 Batch 60: Train Loss = 0.2623
2023-11-08 09:50:11,276 - __main__ - INFO - Epoch 112 Batch 80: Train Loss = 0.1987
2023-11-08 09:50:29,842 - __main__ - INFO - Epoch 112 Batch 100: Train Loss = 0.1984
2023-11-08 09:50:48,390 - __main__ - INFO - Epoch 112 Batch 120: Train Loss = 0.2521
2023-11-08 09:51:01,785 - __main__ - INFO - Epoch 113 Batch 0: Train Loss = 0.2291
2023-11-08 09:51:20,219 - __main__ - INFO - Epoch 113 Batch 20: Train Loss = 0.2728
2023-11-08 09:51:38,483 - __main__ - INFO - Epoch 113 Batch 40: Train Loss = 0.2732
2023-11-08 09:51:56,030 - __main__ - INFO - Epoch 113 Batch 60: Train Loss = 0.2353
2023-11-08 09:52:14,632 - __main__ - INFO - Epoch 113 Batch 80: Train Loss = 0.2899
2023-11-08 09:52:32,870 - __main__ - INFO - Epoch 113 Batch 100: Train Loss = 0.1937
2023-11-08 09:52:50,687 - __main__ - INFO - Epoch 113 Batch 120: Train Loss = 0.2391
2023-11-08 09:53:04,568 - __main__ - INFO - Epoch 114 Batch 0: Train Loss = 0.2434
2023-11-08 09:53:22,916 - __main__ - INFO - Epoch 114 Batch 20: Train Loss = 0.2477
2023-11-08 09:53:40,836 - __main__ - INFO - Epoch 114 Batch 40: Train Loss = 0.2779
2023-11-08 09:53:58,897 - __main__ - INFO - Epoch 114 Batch 60: Train Loss = 0.2728
2023-11-08 09:54:17,633 - __main__ - INFO - Epoch 114 Batch 80: Train Loss = 0.2366
2023-11-08 09:54:35,806 - __main__ - INFO - Epoch 114 Batch 100: Train Loss = 0.2052
2023-11-08 09:54:54,208 - __main__ - INFO - Epoch 114 Batch 120: Train Loss = 0.2303
2023-11-08 09:55:07,749 - __main__ - INFO - Epoch 115 Batch 0: Train Loss = 0.2167
2023-11-08 09:55:25,586 - __main__ - INFO - Epoch 115 Batch 20: Train Loss = 0.2462
2023-11-08 09:55:44,026 - __main__ - INFO - Epoch 115 Batch 40: Train Loss = 0.2792
2023-11-08 09:56:01,692 - __main__ - INFO - Epoch 115 Batch 60: Train Loss = 0.2479
2023-11-08 09:56:19,968 - __main__ - INFO - Epoch 115 Batch 80: Train Loss = 0.2127
2023-11-08 09:56:38,382 - __main__ - INFO - Epoch 115 Batch 100: Train Loss = 0.2233
2023-11-08 09:56:56,667 - __main__ - INFO - Epoch 115 Batch 120: Train Loss = 0.2448
2023-11-08 09:57:10,943 - __main__ - INFO - Epoch 116 Batch 0: Train Loss = 0.2311
2023-11-08 09:57:30,233 - __main__ - INFO - Epoch 116 Batch 20: Train Loss = 0.2720
2023-11-08 09:57:48,183 - __main__ - INFO - Epoch 116 Batch 40: Train Loss = 0.2506
2023-11-08 09:58:06,162 - __main__ - INFO - Epoch 116 Batch 60: Train Loss = 0.2563
2023-11-08 09:58:23,989 - __main__ - INFO - Epoch 116 Batch 80: Train Loss = 0.2422
2023-11-08 09:58:42,048 - __main__ - INFO - Epoch 116 Batch 100: Train Loss = 0.1949
2023-11-08 09:59:00,365 - __main__ - INFO - Epoch 116 Batch 120: Train Loss = 0.2363
2023-11-08 09:59:14,125 - __main__ - INFO - Epoch 117 Batch 0: Train Loss = 0.2547
2023-11-08 09:59:32,397 - __main__ - INFO - Epoch 117 Batch 20: Train Loss = 0.2482
2023-11-08 09:59:50,449 - __main__ - INFO - Epoch 117 Batch 40: Train Loss = 0.2739
2023-11-08 10:00:07,561 - __main__ - INFO - Epoch 117 Batch 60: Train Loss = 0.2605
2023-11-08 10:00:26,177 - __main__ - INFO - Epoch 117 Batch 80: Train Loss = 0.2252
2023-11-08 10:00:44,572 - __main__ - INFO - Epoch 117 Batch 100: Train Loss = 0.1879
2023-11-08 10:01:02,183 - __main__ - INFO - Epoch 117 Batch 120: Train Loss = 0.2245
2023-11-08 10:01:15,625 - __main__ - INFO - Epoch 118 Batch 0: Train Loss = 0.2329
2023-11-08 10:01:34,176 - __main__ - INFO - Epoch 118 Batch 20: Train Loss = 0.2474
2023-11-08 10:01:52,163 - __main__ - INFO - Epoch 118 Batch 40: Train Loss = 0.2647
2023-11-08 10:02:10,543 - __main__ - INFO - Epoch 118 Batch 60: Train Loss = 0.2308
2023-11-08 10:02:27,591 - __main__ - INFO - Epoch 118 Batch 80: Train Loss = 0.2105
2023-11-08 10:02:46,003 - __main__ - INFO - Epoch 118 Batch 100: Train Loss = 0.1796
2023-11-08 10:03:02,777 - __main__ - INFO - Epoch 118 Batch 120: Train Loss = 0.2263
2023-11-08 10:03:15,329 - __main__ - INFO - Epoch 119 Batch 0: Train Loss = 0.2443
2023-11-08 10:03:31,385 - __main__ - INFO - Epoch 119 Batch 20: Train Loss = 0.2545
2023-11-08 10:03:46,572 - __main__ - INFO - Epoch 119 Batch 40: Train Loss = 0.2605
2023-11-08 10:04:01,898 - __main__ - INFO - Epoch 119 Batch 60: Train Loss = 0.2535
2023-11-08 10:04:18,486 - __main__ - INFO - Epoch 119 Batch 80: Train Loss = 0.2604
2023-11-08 10:04:34,279 - __main__ - INFO - Epoch 119 Batch 100: Train Loss = 0.1977
2023-11-08 10:04:49,605 - __main__ - INFO - Epoch 119 Batch 120: Train Loss = 0.2233
2023-11-08 10:05:01,689 - __main__ - INFO - Epoch 120 Batch 0: Train Loss = 0.2478
2023-11-08 10:05:17,817 - __main__ - INFO - Epoch 120 Batch 20: Train Loss = 0.2587
2023-11-08 10:05:33,659 - __main__ - INFO - Epoch 120 Batch 40: Train Loss = 0.2853
2023-11-08 10:05:49,380 - __main__ - INFO - Epoch 120 Batch 60: Train Loss = 0.2547
2023-11-08 10:06:04,762 - __main__ - INFO - Epoch 120 Batch 80: Train Loss = 0.2409
2023-11-08 10:06:20,589 - __main__ - INFO - Epoch 120 Batch 100: Train Loss = 0.2126
2023-11-08 10:06:36,130 - __main__ - INFO - Epoch 120 Batch 120: Train Loss = 0.2304
2023-11-08 10:06:48,630 - __main__ - INFO - Epoch 121 Batch 0: Train Loss = 0.2293
2023-11-08 10:07:04,767 - __main__ - INFO - Epoch 121 Batch 20: Train Loss = 0.2737
2023-11-08 10:07:20,056 - __main__ - INFO - Epoch 121 Batch 40: Train Loss = 0.2583
2023-11-08 10:07:36,491 - __main__ - INFO - Epoch 121 Batch 60: Train Loss = 0.2710
2023-11-08 10:07:52,600 - __main__ - INFO - Epoch 121 Batch 80: Train Loss = 0.2270
2023-11-08 10:08:08,594 - __main__ - INFO - Epoch 121 Batch 100: Train Loss = 0.1864
2023-11-08 10:08:25,144 - __main__ - INFO - Epoch 121 Batch 120: Train Loss = 0.2223
2023-11-08 10:08:36,313 - __main__ - INFO - Epoch 122 Batch 0: Train Loss = 0.2308
2023-11-08 10:08:53,409 - __main__ - INFO - Epoch 122 Batch 20: Train Loss = 0.2393
2023-11-08 10:09:09,409 - __main__ - INFO - Epoch 122 Batch 40: Train Loss = 0.2590
2023-11-08 10:09:25,469 - __main__ - INFO - Epoch 122 Batch 60: Train Loss = 0.2545
2023-11-08 10:09:41,708 - __main__ - INFO - Epoch 122 Batch 80: Train Loss = 0.2274
2023-11-08 10:09:58,553 - __main__ - INFO - Epoch 122 Batch 100: Train Loss = 0.2056
2023-11-08 10:10:12,943 - __main__ - INFO - Epoch 122 Batch 120: Train Loss = 0.2524
2023-11-08 10:10:24,545 - __main__ - INFO - Epoch 123 Batch 0: Train Loss = 0.2043
2023-11-08 10:10:41,079 - __main__ - INFO - Epoch 123 Batch 20: Train Loss = 0.2565
2023-11-08 10:10:57,467 - __main__ - INFO - Epoch 123 Batch 40: Train Loss = 0.2793
2023-11-08 10:11:12,905 - __main__ - INFO - Epoch 123 Batch 60: Train Loss = 0.2612
2023-11-08 10:11:28,822 - __main__ - INFO - Epoch 123 Batch 80: Train Loss = 0.2225
2023-11-08 10:11:43,953 - __main__ - INFO - Epoch 123 Batch 100: Train Loss = 0.2067
2023-11-08 10:11:59,972 - __main__ - INFO - Epoch 123 Batch 120: Train Loss = 0.2491
2023-11-08 10:12:11,824 - __main__ - INFO - Epoch 124 Batch 0: Train Loss = 0.2332
2023-11-08 10:12:28,027 - __main__ - INFO - Epoch 124 Batch 20: Train Loss = 0.2468
2023-11-08 10:12:43,587 - __main__ - INFO - Epoch 124 Batch 40: Train Loss = 0.2799
2023-11-08 10:13:00,064 - __main__ - INFO - Epoch 124 Batch 60: Train Loss = 0.2541
2023-11-08 10:13:16,580 - __main__ - INFO - Epoch 124 Batch 80: Train Loss = 0.2293
2023-11-08 10:13:31,582 - __main__ - INFO - Epoch 124 Batch 100: Train Loss = 0.2118
2023-11-08 10:13:46,783 - __main__ - INFO - Epoch 124 Batch 120: Train Loss = 0.2325
2023-11-08 10:13:56,959 - __main__ - INFO - ------------ Save best model - AUROC: 0.8613 ------------
2023-11-08 10:13:57,769 - __main__ - INFO - Epoch 125 Batch 0: Train Loss = 0.2277
2023-11-08 10:14:11,691 - __main__ - INFO - Epoch 125 Batch 20: Train Loss = 0.2878
2023-11-08 10:14:25,248 - __main__ - INFO - Epoch 125 Batch 40: Train Loss = 0.2447
2023-11-08 10:14:38,865 - __main__ - INFO - Epoch 125 Batch 60: Train Loss = 0.2433
2023-11-08 10:14:52,535 - __main__ - INFO - Epoch 125 Batch 80: Train Loss = 0.2147
2023-11-08 10:15:07,198 - __main__ - INFO - Epoch 125 Batch 100: Train Loss = 0.2098
2023-11-08 10:15:20,874 - __main__ - INFO - Epoch 125 Batch 120: Train Loss = 0.2610
2023-11-08 10:15:31,758 - __main__ - INFO - Epoch 126 Batch 0: Train Loss = 0.2220
2023-11-08 10:15:45,975 - __main__ - INFO - Epoch 126 Batch 20: Train Loss = 0.2680
2023-11-08 10:15:59,257 - __main__ - INFO - Epoch 126 Batch 40: Train Loss = 0.2607
2023-11-08 10:16:12,702 - __main__ - INFO - Epoch 126 Batch 60: Train Loss = 0.2383
2023-11-08 10:16:26,256 - __main__ - INFO - Epoch 126 Batch 80: Train Loss = 0.2475
2023-11-08 10:16:40,157 - __main__ - INFO - Epoch 126 Batch 100: Train Loss = 0.1993
2023-11-08 10:16:54,629 - __main__ - INFO - Epoch 126 Batch 120: Train Loss = 0.2349
2023-11-08 10:17:05,476 - __main__ - INFO - Epoch 127 Batch 0: Train Loss = 0.2537
2023-11-08 10:17:19,204 - __main__ - INFO - Epoch 127 Batch 20: Train Loss = 0.2392
2023-11-08 10:17:32,823 - __main__ - INFO - Epoch 127 Batch 40: Train Loss = 0.2901
2023-11-08 10:17:46,297 - __main__ - INFO - Epoch 127 Batch 60: Train Loss = 0.2717
2023-11-08 10:18:00,273 - __main__ - INFO - Epoch 127 Batch 80: Train Loss = 0.2360
2023-11-08 10:18:14,078 - __main__ - INFO - Epoch 127 Batch 100: Train Loss = 0.1906
2023-11-08 10:18:28,065 - __main__ - INFO - Epoch 127 Batch 120: Train Loss = 0.2227
2023-11-08 10:18:38,638 - __main__ - INFO - Epoch 128 Batch 0: Train Loss = 0.2425
2023-11-08 10:18:53,267 - __main__ - INFO - Epoch 128 Batch 20: Train Loss = 0.2465
2023-11-08 10:19:06,772 - __main__ - INFO - Epoch 128 Batch 40: Train Loss = 0.2618
2023-11-08 10:19:20,359 - __main__ - INFO - Epoch 128 Batch 60: Train Loss = 0.2399
2023-11-08 10:19:34,201 - __main__ - INFO - Epoch 128 Batch 80: Train Loss = 0.1984
2023-11-08 10:19:48,690 - __main__ - INFO - Epoch 128 Batch 100: Train Loss = 0.1890
2023-11-08 10:20:02,440 - __main__ - INFO - Epoch 128 Batch 120: Train Loss = 0.2235
2023-11-08 10:20:13,052 - __main__ - INFO - Epoch 129 Batch 0: Train Loss = 0.2001
2023-11-08 10:20:27,404 - __main__ - INFO - Epoch 129 Batch 20: Train Loss = 0.2717
2023-11-08 10:20:41,589 - __main__ - INFO - Epoch 129 Batch 40: Train Loss = 0.2420
2023-11-08 10:20:55,116 - __main__ - INFO - Epoch 129 Batch 60: Train Loss = 0.2628
2023-11-08 10:21:08,930 - __main__ - INFO - Epoch 129 Batch 80: Train Loss = 0.2378
2023-11-08 10:21:23,251 - __main__ - INFO - Epoch 129 Batch 100: Train Loss = 0.1712
2023-11-08 10:21:37,165 - __main__ - INFO - Epoch 129 Batch 120: Train Loss = 0.2397
2023-11-08 10:21:47,915 - __main__ - INFO - Epoch 130 Batch 0: Train Loss = 0.2374
2023-11-08 10:22:02,018 - __main__ - INFO - Epoch 130 Batch 20: Train Loss = 0.2446
2023-11-08 10:22:15,541 - __main__ - INFO - Epoch 130 Batch 40: Train Loss = 0.2575
2023-11-08 10:22:29,531 - __main__ - INFO - Epoch 130 Batch 60: Train Loss = 0.2488
2023-11-08 10:22:43,433 - __main__ - INFO - Epoch 130 Batch 80: Train Loss = 0.2306
2023-11-08 10:22:57,222 - __main__ - INFO - Epoch 130 Batch 100: Train Loss = 0.1926
2023-11-08 10:23:11,081 - __main__ - INFO - Epoch 130 Batch 120: Train Loss = 0.2519
2023-11-08 10:23:21,629 - __main__ - INFO - Epoch 131 Batch 0: Train Loss = 0.2303
2023-11-08 10:23:36,220 - __main__ - INFO - Epoch 131 Batch 20: Train Loss = 0.2645
2023-11-08 10:23:49,518 - __main__ - INFO - Epoch 131 Batch 40: Train Loss = 0.2577
2023-11-08 10:24:03,181 - __main__ - INFO - Epoch 131 Batch 60: Train Loss = 0.2724
2023-11-08 10:24:17,556 - __main__ - INFO - Epoch 131 Batch 80: Train Loss = 0.2058
2023-11-08 10:24:31,601 - __main__ - INFO - Epoch 131 Batch 100: Train Loss = 0.1889
2023-11-08 10:24:45,224 - __main__ - INFO - Epoch 131 Batch 120: Train Loss = 0.2363
2023-11-08 10:24:55,765 - __main__ - INFO - Epoch 132 Batch 0: Train Loss = 0.2354
2023-11-08 10:25:09,741 - __main__ - INFO - Epoch 132 Batch 20: Train Loss = 0.2756
2023-11-08 10:25:22,799 - __main__ - INFO - Epoch 132 Batch 40: Train Loss = 0.2428
2023-11-08 10:25:36,666 - __main__ - INFO - Epoch 132 Batch 60: Train Loss = 0.2553
2023-11-08 10:25:50,204 - __main__ - INFO - Epoch 132 Batch 80: Train Loss = 0.2308
2023-11-08 10:26:04,656 - __main__ - INFO - Epoch 132 Batch 100: Train Loss = 0.2033
2023-11-08 10:26:19,135 - __main__ - INFO - Epoch 132 Batch 120: Train Loss = 0.2510
2023-11-08 10:26:29,501 - __main__ - INFO - Epoch 133 Batch 0: Train Loss = 0.2242
2023-11-08 10:26:43,738 - __main__ - INFO - Epoch 133 Batch 20: Train Loss = 0.2426
2023-11-08 10:26:57,015 - __main__ - INFO - Epoch 133 Batch 40: Train Loss = 0.2745
2023-11-08 10:27:10,490 - __main__ - INFO - Epoch 133 Batch 60: Train Loss = 0.2465
2023-11-08 10:27:24,280 - __main__ - INFO - Epoch 133 Batch 80: Train Loss = 0.1894
2023-11-08 10:27:38,259 - __main__ - INFO - Epoch 133 Batch 100: Train Loss = 0.2004
2023-11-08 10:27:51,618 - __main__ - INFO - Epoch 133 Batch 120: Train Loss = 0.2493
2023-11-08 10:28:02,533 - __main__ - INFO - Epoch 134 Batch 0: Train Loss = 0.2239
2023-11-08 10:28:16,504 - __main__ - INFO - Epoch 134 Batch 20: Train Loss = 0.2494
2023-11-08 10:28:29,814 - __main__ - INFO - Epoch 134 Batch 40: Train Loss = 0.2464
2023-11-08 10:28:43,585 - __main__ - INFO - Epoch 134 Batch 60: Train Loss = 0.2705
2023-11-08 10:28:57,351 - __main__ - INFO - Epoch 134 Batch 80: Train Loss = 0.2062
2023-11-08 10:29:11,120 - __main__ - INFO - Epoch 134 Batch 100: Train Loss = 0.2200
2023-11-08 10:29:24,924 - __main__ - INFO - Epoch 134 Batch 120: Train Loss = 0.2280
2023-11-08 10:29:35,393 - __main__ - INFO - Epoch 135 Batch 0: Train Loss = 0.2249
2023-11-08 10:29:49,466 - __main__ - INFO - Epoch 135 Batch 20: Train Loss = 0.2451
2023-11-08 10:30:03,608 - __main__ - INFO - Epoch 135 Batch 40: Train Loss = 0.2541
2023-11-08 10:30:16,899 - __main__ - INFO - Epoch 135 Batch 60: Train Loss = 0.2445
2023-11-08 10:30:30,172 - __main__ - INFO - Epoch 135 Batch 80: Train Loss = 0.2311
2023-11-08 10:30:43,940 - __main__ - INFO - Epoch 135 Batch 100: Train Loss = 0.1687
2023-11-08 10:30:57,483 - __main__ - INFO - Epoch 135 Batch 120: Train Loss = 0.2615
2023-11-08 10:31:07,888 - __main__ - INFO - Epoch 136 Batch 0: Train Loss = 0.2263
2023-11-08 10:31:21,696 - __main__ - INFO - Epoch 136 Batch 20: Train Loss = 0.2736
2023-11-08 10:31:34,998 - __main__ - INFO - Epoch 136 Batch 40: Train Loss = 0.2563
2023-11-08 10:31:49,208 - __main__ - INFO - Epoch 136 Batch 60: Train Loss = 0.2394
2023-11-08 10:32:02,969 - __main__ - INFO - Epoch 136 Batch 80: Train Loss = 0.2286
2023-11-08 10:32:17,094 - __main__ - INFO - Epoch 136 Batch 100: Train Loss = 0.1752
2023-11-08 10:32:31,369 - __main__ - INFO - Epoch 136 Batch 120: Train Loss = 0.2483
2023-11-08 10:32:41,920 - __main__ - INFO - Epoch 137 Batch 0: Train Loss = 0.2682
2023-11-08 10:32:55,415 - __main__ - INFO - Epoch 137 Batch 20: Train Loss = 0.2502
2023-11-08 10:33:08,510 - __main__ - INFO - Epoch 137 Batch 40: Train Loss = 0.2495
2023-11-08 10:33:21,825 - __main__ - INFO - Epoch 137 Batch 60: Train Loss = 0.2589
2023-11-08 10:33:36,555 - __main__ - INFO - Epoch 137 Batch 80: Train Loss = 0.2252
2023-11-08 10:33:50,351 - __main__ - INFO - Epoch 137 Batch 100: Train Loss = 0.1920
2023-11-08 10:34:03,592 - __main__ - INFO - Epoch 137 Batch 120: Train Loss = 0.2457
2023-11-08 10:34:14,528 - __main__ - INFO - Epoch 138 Batch 0: Train Loss = 0.2286
2023-11-08 10:34:28,488 - __main__ - INFO - Epoch 138 Batch 20: Train Loss = 0.2432
2023-11-08 10:34:41,933 - __main__ - INFO - Epoch 138 Batch 40: Train Loss = 0.2494
2023-11-08 10:34:55,722 - __main__ - INFO - Epoch 138 Batch 60: Train Loss = 0.2350
2023-11-08 10:35:09,678 - __main__ - INFO - Epoch 138 Batch 80: Train Loss = 0.2364
2023-11-08 10:35:23,841 - __main__ - INFO - Epoch 138 Batch 100: Train Loss = 0.1603
2023-11-08 10:35:38,462 - __main__ - INFO - Epoch 138 Batch 120: Train Loss = 0.2364
2023-11-08 10:35:49,096 - __main__ - INFO - Epoch 139 Batch 0: Train Loss = 0.2238
2023-11-08 10:36:02,846 - __main__ - INFO - Epoch 139 Batch 20: Train Loss = 0.2430
2023-11-08 10:36:16,299 - __main__ - INFO - Epoch 139 Batch 40: Train Loss = 0.2576
2023-11-08 10:36:30,194 - __main__ - INFO - Epoch 139 Batch 60: Train Loss = 0.2430
2023-11-08 10:36:44,248 - __main__ - INFO - Epoch 139 Batch 80: Train Loss = 0.2400
2023-11-08 10:36:58,205 - __main__ - INFO - Epoch 139 Batch 100: Train Loss = 0.1890
2023-11-08 10:37:12,158 - __main__ - INFO - Epoch 139 Batch 120: Train Loss = 0.2217
2023-11-08 10:37:22,782 - __main__ - INFO - Epoch 140 Batch 0: Train Loss = 0.2393
2023-11-08 10:37:37,096 - __main__ - INFO - Epoch 140 Batch 20: Train Loss = 0.2570
2023-11-08 10:37:50,915 - __main__ - INFO - Epoch 140 Batch 40: Train Loss = 0.2613
2023-11-08 10:38:04,456 - __main__ - INFO - Epoch 140 Batch 60: Train Loss = 0.2466
2023-11-08 10:38:17,932 - __main__ - INFO - Epoch 140 Batch 80: Train Loss = 0.2002
2023-11-08 10:38:31,710 - __main__ - INFO - Epoch 140 Batch 100: Train Loss = 0.1649
2023-11-08 10:38:44,749 - __main__ - INFO - Epoch 140 Batch 120: Train Loss = 0.2353
2023-11-08 10:38:54,646 - __main__ - INFO - ------------ Save best model - AUROC: 0.8617 ------------
2023-11-08 10:38:55,279 - __main__ - INFO - Epoch 141 Batch 0: Train Loss = 0.2282
2023-11-08 10:39:09,536 - __main__ - INFO - Epoch 141 Batch 20: Train Loss = 0.2734
2023-11-08 10:39:23,326 - __main__ - INFO - Epoch 141 Batch 40: Train Loss = 0.2661
2023-11-08 10:39:37,096 - __main__ - INFO - Epoch 141 Batch 60: Train Loss = 0.2590
2023-11-08 10:39:50,939 - __main__ - INFO - Epoch 141 Batch 80: Train Loss = 0.2734
2023-11-08 10:40:04,383 - __main__ - INFO - Epoch 141 Batch 100: Train Loss = 0.1876
2023-11-08 10:40:18,184 - __main__ - INFO - Epoch 141 Batch 120: Train Loss = 0.2521
2023-11-08 10:40:28,795 - __main__ - INFO - Epoch 142 Batch 0: Train Loss = 0.2233
2023-11-08 10:40:42,233 - __main__ - INFO - Epoch 142 Batch 20: Train Loss = 0.2612
2023-11-08 10:40:55,523 - __main__ - INFO - Epoch 142 Batch 40: Train Loss = 0.2556
2023-11-08 10:41:09,737 - __main__ - INFO - Epoch 142 Batch 60: Train Loss = 0.2696
2023-11-08 10:41:23,529 - __main__ - INFO - Epoch 142 Batch 80: Train Loss = 0.2313
2023-11-08 10:41:37,293 - __main__ - INFO - Epoch 142 Batch 100: Train Loss = 0.1942
2023-11-08 10:41:50,862 - __main__ - INFO - Epoch 142 Batch 120: Train Loss = 0.2410
2023-11-08 10:42:01,559 - __main__ - INFO - Epoch 143 Batch 0: Train Loss = 0.2515
2023-11-08 10:42:15,768 - __main__ - INFO - Epoch 143 Batch 20: Train Loss = 0.2566
2023-11-08 10:42:28,924 - __main__ - INFO - Epoch 143 Batch 40: Train Loss = 0.2789
2023-11-08 10:42:42,093 - __main__ - INFO - Epoch 143 Batch 60: Train Loss = 0.2404
2023-11-08 10:42:56,242 - __main__ - INFO - Epoch 143 Batch 80: Train Loss = 0.2146
2023-11-08 10:43:10,242 - __main__ - INFO - Epoch 143 Batch 100: Train Loss = 0.2004
2023-11-08 10:43:24,136 - __main__ - INFO - Epoch 143 Batch 120: Train Loss = 0.2391
2023-11-08 10:43:34,601 - __main__ - INFO - Epoch 144 Batch 0: Train Loss = 0.2232
2023-11-08 10:43:48,572 - __main__ - INFO - Epoch 144 Batch 20: Train Loss = 0.2212
2023-11-08 10:44:01,680 - __main__ - INFO - Epoch 144 Batch 40: Train Loss = 0.2703
2023-11-08 10:44:15,204 - __main__ - INFO - Epoch 144 Batch 60: Train Loss = 0.2554
2023-11-08 10:44:29,118 - __main__ - INFO - Epoch 144 Batch 80: Train Loss = 0.2486
2023-11-08 10:44:42,763 - __main__ - INFO - Epoch 144 Batch 100: Train Loss = 0.1798
2023-11-08 10:44:56,932 - __main__ - INFO - Epoch 144 Batch 120: Train Loss = 0.2577
2023-11-08 10:45:07,606 - __main__ - INFO - Epoch 145 Batch 0: Train Loss = 0.2395
2023-11-08 10:45:21,823 - __main__ - INFO - Epoch 145 Batch 20: Train Loss = 0.2576
2023-11-08 10:45:35,155 - __main__ - INFO - Epoch 145 Batch 40: Train Loss = 0.2839
2023-11-08 10:45:48,690 - __main__ - INFO - Epoch 145 Batch 60: Train Loss = 0.2416
2023-11-08 10:46:02,020 - __main__ - INFO - Epoch 145 Batch 80: Train Loss = 0.2451
2023-11-08 10:46:15,514 - __main__ - INFO - Epoch 145 Batch 100: Train Loss = 0.1955
2023-11-08 10:46:29,053 - __main__ - INFO - Epoch 145 Batch 120: Train Loss = 0.2341
2023-11-08 10:46:39,593 - __main__ - INFO - Epoch 146 Batch 0: Train Loss = 0.2274
2023-11-08 10:46:54,144 - __main__ - INFO - Epoch 146 Batch 20: Train Loss = 0.2726
2023-11-08 10:47:07,599 - __main__ - INFO - Epoch 146 Batch 40: Train Loss = 0.2558
2023-11-08 10:47:20,941 - __main__ - INFO - Epoch 146 Batch 60: Train Loss = 0.2561
2023-11-08 10:47:34,648 - __main__ - INFO - Epoch 146 Batch 80: Train Loss = 0.2881
2023-11-08 10:47:48,511 - __main__ - INFO - Epoch 146 Batch 100: Train Loss = 0.1696
2023-11-08 10:48:02,094 - __main__ - INFO - Epoch 146 Batch 120: Train Loss = 0.2555
2023-11-08 10:48:12,886 - __main__ - INFO - Epoch 147 Batch 0: Train Loss = 0.2191
2023-11-08 10:48:26,634 - __main__ - INFO - Epoch 147 Batch 20: Train Loss = 0.2674
2023-11-08 10:48:40,738 - __main__ - INFO - Epoch 147 Batch 40: Train Loss = 0.2447
2023-11-08 10:48:54,337 - __main__ - INFO - Epoch 147 Batch 60: Train Loss = 0.2480
2023-11-08 10:49:08,288 - __main__ - INFO - Epoch 147 Batch 80: Train Loss = 0.2244
2023-11-08 10:49:22,033 - __main__ - INFO - Epoch 147 Batch 100: Train Loss = 0.2330
2023-11-08 10:49:35,586 - __main__ - INFO - Epoch 147 Batch 120: Train Loss = 0.2310
2023-11-08 10:49:46,367 - __main__ - INFO - Epoch 148 Batch 0: Train Loss = 0.2166
2023-11-08 10:50:00,405 - __main__ - INFO - Epoch 148 Batch 20: Train Loss = 0.2632
2023-11-08 10:50:13,704 - __main__ - INFO - Epoch 148 Batch 40: Train Loss = 0.2736
2023-11-08 10:50:27,907 - __main__ - INFO - Epoch 148 Batch 60: Train Loss = 0.2415
2023-11-08 10:50:41,461 - __main__ - INFO - Epoch 148 Batch 80: Train Loss = 0.2036
2023-11-08 10:50:55,236 - __main__ - INFO - Epoch 148 Batch 100: Train Loss = 0.1777
2023-11-08 10:51:08,683 - __main__ - INFO - Epoch 148 Batch 120: Train Loss = 0.2566
2023-11-08 10:51:18,957 - __main__ - INFO - Epoch 149 Batch 0: Train Loss = 0.2239
2023-11-08 10:51:32,659 - __main__ - INFO - Epoch 149 Batch 20: Train Loss = 0.2658
2023-11-08 10:51:46,063 - __main__ - INFO - Epoch 149 Batch 40: Train Loss = 0.2514
2023-11-08 10:51:59,376 - __main__ - INFO - Epoch 149 Batch 60: Train Loss = 0.2851
2023-11-08 10:52:14,036 - __main__ - INFO - Epoch 149 Batch 80: Train Loss = 0.1985
2023-11-08 10:52:27,243 - __main__ - INFO - Epoch 149 Batch 100: Train Loss = 0.1990
2023-11-08 10:52:40,672 - __main__ - INFO - Epoch 149 Batch 120: Train Loss = 0.2451
2023-11-08 10:52:51,176 - __main__ - INFO - Epoch 150 Batch 0: Train Loss = 0.2145
2023-11-08 10:53:05,046 - __main__ - INFO - Epoch 150 Batch 20: Train Loss = 0.2402
2023-11-08 10:53:18,765 - __main__ - INFO - Epoch 150 Batch 40: Train Loss = 0.2777
2023-11-08 10:53:32,228 - __main__ - INFO - Epoch 150 Batch 60: Train Loss = 0.2725
2023-11-08 10:53:45,992 - __main__ - INFO - Epoch 150 Batch 80: Train Loss = 0.2150
2023-11-08 10:54:00,768 - __main__ - INFO - Epoch 150 Batch 100: Train Loss = 0.2180
2023-11-08 10:54:14,430 - __main__ - INFO - Epoch 150 Batch 120: Train Loss = 0.2370
2023-11-08 10:54:25,234 - __main__ - INFO - Epoch 151 Batch 0: Train Loss = 0.2268
2023-11-08 10:54:39,090 - __main__ - INFO - Epoch 151 Batch 20: Train Loss = 0.2446
2023-11-08 10:54:52,095 - __main__ - INFO - Epoch 151 Batch 40: Train Loss = 0.2467
2023-11-08 10:55:05,589 - __main__ - INFO - Epoch 151 Batch 60: Train Loss = 0.2484
2023-11-08 10:55:19,608 - __main__ - INFO - Epoch 151 Batch 80: Train Loss = 0.2175
2023-11-08 10:55:33,631 - __main__ - INFO - Epoch 151 Batch 100: Train Loss = 0.1762
2023-11-08 10:55:46,905 - __main__ - INFO - Epoch 151 Batch 120: Train Loss = 0.2523
2023-11-08 10:55:57,543 - __main__ - INFO - Epoch 152 Batch 0: Train Loss = 0.2327
2023-11-08 10:56:11,319 - __main__ - INFO - Epoch 152 Batch 20: Train Loss = 0.2813
2023-11-08 10:56:24,773 - __main__ - INFO - Epoch 152 Batch 40: Train Loss = 0.2602
2023-11-08 10:56:38,260 - __main__ - INFO - Epoch 152 Batch 60: Train Loss = 0.2557
2023-11-08 10:56:51,978 - __main__ - INFO - Epoch 152 Batch 80: Train Loss = 0.2125
2023-11-08 10:57:05,605 - __main__ - INFO - Epoch 152 Batch 100: Train Loss = 0.1809
2023-11-08 10:57:18,655 - __main__ - INFO - Epoch 152 Batch 120: Train Loss = 0.2363
2023-11-08 10:57:28,940 - __main__ - INFO - Epoch 153 Batch 0: Train Loss = 0.2435
2023-11-08 10:57:42,660 - __main__ - INFO - Epoch 153 Batch 20: Train Loss = 0.2511
2023-11-08 10:57:56,679 - __main__ - INFO - Epoch 153 Batch 40: Train Loss = 0.2713
2023-11-08 10:58:10,431 - __main__ - INFO - Epoch 153 Batch 60: Train Loss = 0.2622
2023-11-08 10:58:24,244 - __main__ - INFO - Epoch 153 Batch 80: Train Loss = 0.2236
2023-11-08 10:58:38,084 - __main__ - INFO - Epoch 153 Batch 100: Train Loss = 0.1818
2023-11-08 10:58:51,318 - __main__ - INFO - Epoch 153 Batch 120: Train Loss = 0.2154
2023-11-08 10:59:01,582 - __main__ - INFO - Epoch 154 Batch 0: Train Loss = 0.2035
2023-11-08 10:59:15,308 - __main__ - INFO - Epoch 154 Batch 20: Train Loss = 0.2573
2023-11-08 10:59:28,399 - __main__ - INFO - Epoch 154 Batch 40: Train Loss = 0.2567
2023-11-08 10:59:42,588 - __main__ - INFO - Epoch 154 Batch 60: Train Loss = 0.2449
2023-11-08 10:59:56,237 - __main__ - INFO - Epoch 154 Batch 80: Train Loss = 0.2089
2023-11-08 11:00:09,647 - __main__ - INFO - Epoch 154 Batch 100: Train Loss = 0.1879
2023-11-08 11:00:23,364 - __main__ - INFO - Epoch 154 Batch 120: Train Loss = 0.2136
2023-11-08 11:00:34,051 - __main__ - INFO - Epoch 155 Batch 0: Train Loss = 0.2513
2023-11-08 11:00:48,263 - __main__ - INFO - Epoch 155 Batch 20: Train Loss = 0.2498
2023-11-08 11:01:01,549 - __main__ - INFO - Epoch 155 Batch 40: Train Loss = 0.2519
2023-11-08 11:01:14,911 - __main__ - INFO - Epoch 155 Batch 60: Train Loss = 0.2456
2023-11-08 11:01:28,981 - __main__ - INFO - Epoch 155 Batch 80: Train Loss = 0.2258
2023-11-08 11:01:43,229 - __main__ - INFO - Epoch 155 Batch 100: Train Loss = 0.2112
2023-11-08 11:01:56,515 - __main__ - INFO - Epoch 155 Batch 120: Train Loss = 0.2390
2023-11-08 11:02:06,768 - __main__ - INFO - Epoch 156 Batch 0: Train Loss = 0.2291
2023-11-08 11:02:20,911 - __main__ - INFO - Epoch 156 Batch 20: Train Loss = 0.2704
2023-11-08 11:02:34,125 - __main__ - INFO - Epoch 156 Batch 40: Train Loss = 0.2518
2023-11-08 11:02:47,790 - __main__ - INFO - Epoch 156 Batch 60: Train Loss = 0.2548
2023-11-08 11:03:01,538 - __main__ - INFO - Epoch 156 Batch 80: Train Loss = 0.2200
2023-11-08 11:03:14,988 - __main__ - INFO - Epoch 156 Batch 100: Train Loss = 0.1946
2023-11-08 11:03:29,044 - __main__ - INFO - Epoch 156 Batch 120: Train Loss = 0.2416
2023-11-08 11:03:39,579 - __main__ - INFO - Epoch 157 Batch 0: Train Loss = 0.2287
2023-11-08 11:03:53,613 - __main__ - INFO - Epoch 157 Batch 20: Train Loss = 0.2476
2023-11-08 11:04:06,858 - __main__ - INFO - Epoch 157 Batch 40: Train Loss = 0.2654
2023-11-08 11:04:20,285 - __main__ - INFO - Epoch 157 Batch 60: Train Loss = 0.2493
2023-11-08 11:04:33,959 - __main__ - INFO - Epoch 157 Batch 80: Train Loss = 0.2083
2023-11-08 11:04:47,715 - __main__ - INFO - Epoch 157 Batch 100: Train Loss = 0.1975
2023-11-08 11:05:01,177 - __main__ - INFO - Epoch 157 Batch 120: Train Loss = 0.2273
2023-11-08 11:05:11,881 - __main__ - INFO - Epoch 158 Batch 0: Train Loss = 0.2375
2023-11-08 11:05:26,611 - __main__ - INFO - Epoch 158 Batch 20: Train Loss = 0.2433
2023-11-08 11:05:39,479 - __main__ - INFO - Epoch 158 Batch 40: Train Loss = 0.2744
2023-11-08 11:05:52,964 - __main__ - INFO - Epoch 158 Batch 60: Train Loss = 0.2575
2023-11-08 11:06:06,819 - __main__ - INFO - Epoch 158 Batch 80: Train Loss = 0.2026
2023-11-08 11:06:20,468 - __main__ - INFO - Epoch 158 Batch 100: Train Loss = 0.1786
2023-11-08 11:06:33,697 - __main__ - INFO - Epoch 158 Batch 120: Train Loss = 0.2246
2023-11-08 11:06:44,076 - __main__ - INFO - Epoch 159 Batch 0: Train Loss = 0.2253
2023-11-08 11:06:58,017 - __main__ - INFO - Epoch 159 Batch 20: Train Loss = 0.2544
2023-11-08 11:07:12,216 - __main__ - INFO - Epoch 159 Batch 40: Train Loss = 0.2484
2023-11-08 11:07:25,074 - __main__ - INFO - Epoch 159 Batch 60: Train Loss = 0.2447
2023-11-08 11:07:38,094 - __main__ - INFO - Epoch 159 Batch 80: Train Loss = 0.2016
2023-11-08 11:07:51,243 - __main__ - INFO - Epoch 159 Batch 100: Train Loss = 0.1784
2023-11-08 11:08:04,052 - __main__ - INFO - Epoch 159 Batch 120: Train Loss = 0.2356
2023-11-08 11:08:14,190 - __main__ - INFO - Epoch 160 Batch 0: Train Loss = 0.2142
2023-11-08 11:08:27,781 - __main__ - INFO - Epoch 160 Batch 20: Train Loss = 0.2421
2023-11-08 11:08:40,393 - __main__ - INFO - Epoch 160 Batch 40: Train Loss = 0.2526
2023-11-08 11:08:53,012 - __main__ - INFO - Epoch 160 Batch 60: Train Loss = 0.2388
2023-11-08 11:09:05,974 - __main__ - INFO - Epoch 160 Batch 80: Train Loss = 0.2327
2023-11-08 11:09:19,374 - __main__ - INFO - Epoch 160 Batch 100: Train Loss = 0.2118
2023-11-08 11:09:31,801 - __main__ - INFO - Epoch 160 Batch 120: Train Loss = 0.2559
2023-11-08 11:09:41,837 - __main__ - INFO - Epoch 161 Batch 0: Train Loss = 0.2129
2023-11-08 11:09:55,198 - __main__ - INFO - Epoch 161 Batch 20: Train Loss = 0.2582
2023-11-08 11:10:07,826 - __main__ - INFO - Epoch 161 Batch 40: Train Loss = 0.2493
2023-11-08 11:10:20,566 - __main__ - INFO - Epoch 161 Batch 60: Train Loss = 0.2449
2023-11-08 11:10:33,755 - __main__ - INFO - Epoch 161 Batch 80: Train Loss = 0.2298
2023-11-08 11:10:46,871 - __main__ - INFO - Epoch 161 Batch 100: Train Loss = 0.2027
2023-11-08 11:10:59,385 - __main__ - INFO - Epoch 161 Batch 120: Train Loss = 0.2096
2023-11-08 11:11:10,118 - __main__ - INFO - Epoch 162 Batch 0: Train Loss = 0.2183
2023-11-08 11:11:24,584 - __main__ - INFO - Epoch 162 Batch 20: Train Loss = 0.3066
2023-11-08 11:11:38,770 - __main__ - INFO - Epoch 162 Batch 40: Train Loss = 0.2697
2023-11-08 11:11:52,787 - __main__ - INFO - Epoch 162 Batch 60: Train Loss = 0.2247
2023-11-08 11:12:06,148 - __main__ - INFO - Epoch 162 Batch 80: Train Loss = 0.1936
2023-11-08 11:12:19,529 - __main__ - INFO - Epoch 162 Batch 100: Train Loss = 0.2022
2023-11-08 11:12:32,537 - __main__ - INFO - Epoch 162 Batch 120: Train Loss = 0.2424
2023-11-08 11:12:42,409 - __main__ - INFO - Epoch 163 Batch 0: Train Loss = 0.2407
2023-11-08 11:12:55,768 - __main__ - INFO - Epoch 163 Batch 20: Train Loss = 0.2592
2023-11-08 11:13:08,477 - __main__ - INFO - Epoch 163 Batch 40: Train Loss = 0.2598
2023-11-08 11:13:21,490 - __main__ - INFO - Epoch 163 Batch 60: Train Loss = 0.2758
2023-11-08 11:13:34,352 - __main__ - INFO - Epoch 163 Batch 80: Train Loss = 0.2106
2023-11-08 11:13:47,144 - __main__ - INFO - Epoch 163 Batch 100: Train Loss = 0.1993
2023-11-08 11:13:59,853 - __main__ - INFO - Epoch 163 Batch 120: Train Loss = 0.2365
2023-11-08 11:14:09,796 - __main__ - INFO - Epoch 164 Batch 0: Train Loss = 0.2276
2023-11-08 11:14:22,745 - __main__ - INFO - Epoch 164 Batch 20: Train Loss = 0.2393
2023-11-08 11:14:35,397 - __main__ - INFO - Epoch 164 Batch 40: Train Loss = 0.2575
2023-11-08 11:14:48,586 - __main__ - INFO - Epoch 164 Batch 60: Train Loss = 0.2412
2023-11-08 11:15:02,124 - __main__ - INFO - Epoch 164 Batch 80: Train Loss = 0.1887
2023-11-08 11:15:15,204 - __main__ - INFO - Epoch 164 Batch 100: Train Loss = 0.1599
2023-11-08 11:15:27,633 - __main__ - INFO - Epoch 164 Batch 120: Train Loss = 0.2682
2023-11-08 11:15:37,921 - __main__ - INFO - Epoch 165 Batch 0: Train Loss = 0.2094
2023-11-08 11:15:52,103 - __main__ - INFO - Epoch 165 Batch 20: Train Loss = 0.2516
2023-11-08 11:16:06,306 - __main__ - INFO - Epoch 165 Batch 40: Train Loss = 0.2583
2023-11-08 11:16:20,461 - __main__ - INFO - Epoch 165 Batch 60: Train Loss = 0.2269
2023-11-08 11:16:34,295 - __main__ - INFO - Epoch 165 Batch 80: Train Loss = 0.2039
2023-11-08 11:16:47,600 - __main__ - INFO - Epoch 165 Batch 100: Train Loss = 0.1851
2023-11-08 11:17:00,166 - __main__ - INFO - Epoch 165 Batch 120: Train Loss = 0.2152
2023-11-08 11:17:10,321 - __main__ - INFO - Epoch 166 Batch 0: Train Loss = 0.2072
2023-11-08 11:17:23,502 - __main__ - INFO - Epoch 166 Batch 20: Train Loss = 0.2593
2023-11-08 11:17:36,711 - __main__ - INFO - Epoch 166 Batch 40: Train Loss = 0.2527
2023-11-08 11:17:49,557 - __main__ - INFO - Epoch 166 Batch 60: Train Loss = 0.2585
2023-11-08 11:18:02,630 - __main__ - INFO - Epoch 166 Batch 80: Train Loss = 0.2122
2023-11-08 11:18:15,367 - __main__ - INFO - Epoch 166 Batch 100: Train Loss = 0.1825
2023-11-08 11:18:28,178 - __main__ - INFO - Epoch 166 Batch 120: Train Loss = 0.2253
2023-11-08 11:18:38,052 - __main__ - INFO - Epoch 167 Batch 0: Train Loss = 0.1854
2023-11-08 11:18:51,290 - __main__ - INFO - Epoch 167 Batch 20: Train Loss = 0.2365
2023-11-08 11:19:04,019 - __main__ - INFO - Epoch 167 Batch 40: Train Loss = 0.2506
2023-11-08 11:19:16,873 - __main__ - INFO - Epoch 167 Batch 60: Train Loss = 0.2507
2023-11-08 11:19:29,968 - __main__ - INFO - Epoch 167 Batch 80: Train Loss = 0.2093
2023-11-08 11:19:43,373 - __main__ - INFO - Epoch 167 Batch 100: Train Loss = 0.1664
2023-11-08 11:19:55,977 - __main__ - INFO - Epoch 167 Batch 120: Train Loss = 0.2427
2023-11-08 11:20:05,406 - __main__ - INFO - ------------ Save best model - AUROC: 0.8618 ------------
2023-11-08 11:20:05,996 - __main__ - INFO - Epoch 168 Batch 0: Train Loss = 0.2127
2023-11-08 11:20:19,893 - __main__ - INFO - Epoch 168 Batch 20: Train Loss = 0.2506
2023-11-08 11:20:33,685 - __main__ - INFO - Epoch 168 Batch 40: Train Loss = 0.2539
2023-11-08 11:20:47,644 - __main__ - INFO - Epoch 168 Batch 60: Train Loss = 0.2338
2023-11-08 11:21:01,240 - __main__ - INFO - Epoch 168 Batch 80: Train Loss = 0.1578
2023-11-08 11:21:14,014 - __main__ - INFO - Epoch 168 Batch 100: Train Loss = 0.1835
2023-11-08 11:21:26,413 - __main__ - INFO - Epoch 168 Batch 120: Train Loss = 0.2232
2023-11-08 11:21:35,447 - __main__ - INFO - ------------ Save best model - AUROC: 0.8820 ------------
2023-11-08 11:21:36,027 - __main__ - INFO - Epoch 169 Batch 0: Train Loss = 0.2178
2023-11-08 11:21:48,977 - __main__ - INFO - Epoch 169 Batch 20: Train Loss = 0.2028
2023-11-08 11:22:01,115 - __main__ - INFO - Epoch 169 Batch 40: Train Loss = 0.2344
2023-11-08 11:22:14,013 - __main__ - INFO - Epoch 169 Batch 60: Train Loss = 0.2243
2023-11-08 11:22:26,747 - __main__ - INFO - Epoch 169 Batch 80: Train Loss = 0.1828
2023-11-08 11:22:39,502 - __main__ - INFO - Epoch 169 Batch 100: Train Loss = 0.1809
2023-11-08 11:22:51,835 - __main__ - INFO - Epoch 169 Batch 120: Train Loss = 0.2041
2023-11-08 11:23:01,730 - __main__ - INFO - Epoch 170 Batch 0: Train Loss = 0.1984
2023-11-08 11:23:14,469 - __main__ - INFO - Epoch 170 Batch 20: Train Loss = 0.2214
2023-11-08 11:23:26,659 - __main__ - INFO - Epoch 170 Batch 40: Train Loss = 0.2594
2023-11-08 11:23:39,510 - __main__ - INFO - Epoch 170 Batch 60: Train Loss = 0.2244
2023-11-08 11:23:52,523 - __main__ - INFO - Epoch 170 Batch 80: Train Loss = 0.2060
2023-11-08 11:24:05,230 - __main__ - INFO - Epoch 170 Batch 100: Train Loss = 0.1405
2023-11-08 11:24:17,366 - __main__ - INFO - Epoch 170 Batch 120: Train Loss = 0.1779
2023-11-08 11:24:27,161 - __main__ - INFO - Epoch 171 Batch 0: Train Loss = 0.1849
2023-11-08 11:24:40,093 - __main__ - INFO - Epoch 171 Batch 20: Train Loss = 0.2243
2023-11-08 11:24:53,212 - __main__ - INFO - Epoch 171 Batch 40: Train Loss = 0.2674
2023-11-08 11:25:06,471 - __main__ - INFO - Epoch 171 Batch 60: Train Loss = 0.2151
2023-11-08 11:25:19,807 - __main__ - INFO - Epoch 171 Batch 80: Train Loss = 0.1960
2023-11-08 11:25:33,235 - __main__ - INFO - Epoch 171 Batch 100: Train Loss = 0.1648
2023-11-08 11:25:45,384 - __main__ - INFO - Epoch 171 Batch 120: Train Loss = 0.2055
2023-11-08 11:25:55,627 - __main__ - INFO - Epoch 172 Batch 0: Train Loss = 0.2050
2023-11-08 11:26:08,756 - __main__ - INFO - Epoch 172 Batch 20: Train Loss = 0.2395
2023-11-08 11:26:21,161 - __main__ - INFO - Epoch 172 Batch 40: Train Loss = 0.2491
2023-11-08 11:26:33,872 - __main__ - INFO - Epoch 172 Batch 60: Train Loss = 0.2682
2023-11-08 11:26:47,796 - __main__ - INFO - Epoch 172 Batch 80: Train Loss = 0.2117
2023-11-08 11:27:02,823 - __main__ - INFO - Epoch 172 Batch 100: Train Loss = 0.1994
2023-11-08 11:27:17,382 - __main__ - INFO - Epoch 172 Batch 120: Train Loss = 0.2319
2023-11-08 11:27:28,556 - __main__ - INFO - Epoch 173 Batch 0: Train Loss = 0.2329
2023-11-08 11:27:44,628 - __main__ - INFO - Epoch 173 Batch 20: Train Loss = 0.2511
2023-11-08 11:27:59,180 - __main__ - INFO - Epoch 173 Batch 40: Train Loss = 0.2352
2023-11-08 11:28:13,865 - __main__ - INFO - Epoch 173 Batch 60: Train Loss = 0.2658
2023-11-08 11:28:27,896 - __main__ - INFO - Epoch 173 Batch 80: Train Loss = 0.1897
2023-11-08 11:28:42,098 - __main__ - INFO - Epoch 173 Batch 100: Train Loss = 0.1692
2023-11-08 11:28:55,444 - __main__ - INFO - Epoch 173 Batch 120: Train Loss = 0.2428
2023-11-08 11:29:06,216 - __main__ - INFO - Epoch 174 Batch 0: Train Loss = 0.2192
2023-11-08 11:29:19,146 - __main__ - INFO - Epoch 174 Batch 20: Train Loss = 0.2321
2023-11-08 11:29:34,560 - __main__ - INFO - Epoch 174 Batch 40: Train Loss = 0.2568
2023-11-08 11:29:50,450 - __main__ - INFO - Epoch 174 Batch 60: Train Loss = 0.2580
2023-11-08 11:30:07,911 - __main__ - INFO - Epoch 174 Batch 80: Train Loss = 0.1890
2023-11-08 11:30:22,365 - __main__ - INFO - Epoch 174 Batch 100: Train Loss = 0.1975
2023-11-08 11:30:34,706 - __main__ - INFO - Epoch 174 Batch 120: Train Loss = 0.2277
2023-11-08 11:30:45,010 - __main__ - INFO - Epoch 175 Batch 0: Train Loss = 0.2040
2023-11-08 11:31:00,062 - __main__ - INFO - Epoch 175 Batch 20: Train Loss = 0.2340
2023-11-08 11:31:14,443 - __main__ - INFO - Epoch 175 Batch 40: Train Loss = 0.2492
2023-11-08 11:31:28,808 - __main__ - INFO - Epoch 175 Batch 60: Train Loss = 0.2590
2023-11-08 11:31:43,265 - __main__ - INFO - Epoch 175 Batch 80: Train Loss = 0.1866
2023-11-08 11:31:57,798 - __main__ - INFO - Epoch 175 Batch 100: Train Loss = 0.1827
2023-11-08 11:32:12,700 - __main__ - INFO - Epoch 175 Batch 120: Train Loss = 0.2157
2023-11-08 11:32:24,033 - __main__ - INFO - Epoch 176 Batch 0: Train Loss = 0.2037
2023-11-08 11:32:39,372 - __main__ - INFO - Epoch 176 Batch 20: Train Loss = 0.2241
2023-11-08 11:32:53,204 - __main__ - INFO - Epoch 176 Batch 40: Train Loss = 0.2442
2023-11-08 11:33:07,512 - __main__ - INFO - Epoch 176 Batch 60: Train Loss = 0.2601
2023-11-08 11:33:20,437 - __main__ - INFO - Epoch 176 Batch 80: Train Loss = 0.2023
2023-11-08 11:33:33,739 - __main__ - INFO - Epoch 176 Batch 100: Train Loss = 0.1823
2023-11-08 11:33:47,754 - __main__ - INFO - Epoch 176 Batch 120: Train Loss = 0.2501
2023-11-08 11:33:58,891 - __main__ - INFO - Epoch 177 Batch 0: Train Loss = 0.2593
2023-11-08 11:34:13,649 - __main__ - INFO - Epoch 177 Batch 20: Train Loss = 0.2431
2023-11-08 11:34:26,379 - __main__ - INFO - Epoch 177 Batch 40: Train Loss = 0.2658
2023-11-08 11:34:40,401 - __main__ - INFO - Epoch 177 Batch 60: Train Loss = 0.2550
2023-11-08 11:34:54,810 - __main__ - INFO - Epoch 177 Batch 80: Train Loss = 0.2048
2023-11-08 11:35:09,582 - __main__ - INFO - Epoch 177 Batch 100: Train Loss = 0.1921
2023-11-08 11:35:22,804 - __main__ - INFO - Epoch 177 Batch 120: Train Loss = 0.2488
2023-11-08 11:35:33,164 - __main__ - INFO - Epoch 178 Batch 0: Train Loss = 0.2229
2023-11-08 11:35:46,414 - __main__ - INFO - Epoch 178 Batch 20: Train Loss = 0.2708
2023-11-08 11:35:59,001 - __main__ - INFO - Epoch 178 Batch 40: Train Loss = 0.2461
2023-11-08 11:36:11,591 - __main__ - INFO - Epoch 178 Batch 60: Train Loss = 0.2338
2023-11-08 11:36:24,576 - __main__ - INFO - Epoch 178 Batch 80: Train Loss = 0.1998
2023-11-08 11:36:37,806 - __main__ - INFO - Epoch 178 Batch 100: Train Loss = 0.1881
2023-11-08 11:36:50,183 - __main__ - INFO - Epoch 178 Batch 120: Train Loss = 0.2473
2023-11-08 11:37:00,126 - __main__ - INFO - Epoch 179 Batch 0: Train Loss = 0.2461
2023-11-08 11:37:13,535 - __main__ - INFO - Epoch 179 Batch 20: Train Loss = 0.2305
2023-11-08 11:37:26,238 - __main__ - INFO - Epoch 179 Batch 40: Train Loss = 0.2588
2023-11-08 11:37:39,174 - __main__ - INFO - Epoch 179 Batch 60: Train Loss = 0.2611
2023-11-08 11:37:52,326 - __main__ - INFO - Epoch 179 Batch 80: Train Loss = 0.2207
2023-11-08 11:38:05,428 - __main__ - INFO - Epoch 179 Batch 100: Train Loss = 0.1748
2023-11-08 11:38:18,309 - __main__ - INFO - Epoch 179 Batch 120: Train Loss = 0.2433
2023-11-08 11:38:28,772 - __main__ - INFO - Epoch 180 Batch 0: Train Loss = 0.2470
2023-11-08 11:38:42,049 - __main__ - INFO - Epoch 180 Batch 20: Train Loss = 0.2387
2023-11-08 11:38:54,840 - __main__ - INFO - Epoch 180 Batch 40: Train Loss = 0.2407
2023-11-08 11:39:08,273 - __main__ - INFO - Epoch 180 Batch 60: Train Loss = 0.2404
2023-11-08 11:39:22,843 - __main__ - INFO - Epoch 180 Batch 80: Train Loss = 0.2108
2023-11-08 11:39:37,341 - __main__ - INFO - Epoch 180 Batch 100: Train Loss = 0.1963
2023-11-08 11:39:51,073 - __main__ - INFO - Epoch 180 Batch 120: Train Loss = 0.2425
2023-11-08 11:40:01,478 - __main__ - INFO - Epoch 181 Batch 0: Train Loss = 0.2214
2023-11-08 11:40:14,668 - __main__ - INFO - Epoch 181 Batch 20: Train Loss = 0.2495
2023-11-08 11:40:27,012 - __main__ - INFO - Epoch 181 Batch 40: Train Loss = 0.2366
2023-11-08 11:40:41,129 - __main__ - INFO - Epoch 181 Batch 60: Train Loss = 0.2331
2023-11-08 11:40:56,521 - __main__ - INFO - Epoch 181 Batch 80: Train Loss = 0.1789
2023-11-08 11:41:11,891 - __main__ - INFO - Epoch 181 Batch 100: Train Loss = 0.1698
2023-11-08 11:41:26,595 - __main__ - INFO - Epoch 181 Batch 120: Train Loss = 0.2330
2023-11-08 11:41:38,783 - __main__ - INFO - Epoch 182 Batch 0: Train Loss = 0.2080
2023-11-08 11:41:54,440 - __main__ - INFO - Epoch 182 Batch 20: Train Loss = 0.2336
2023-11-08 11:42:10,407 - __main__ - INFO - Epoch 182 Batch 40: Train Loss = 0.2568
2023-11-08 11:42:25,069 - __main__ - INFO - Epoch 182 Batch 60: Train Loss = 0.2307
2023-11-08 11:42:40,848 - __main__ - INFO - Epoch 182 Batch 80: Train Loss = 0.1896
2023-11-08 11:42:56,808 - __main__ - INFO - Epoch 182 Batch 100: Train Loss = 0.1841
2023-11-08 11:43:11,873 - __main__ - INFO - Epoch 182 Batch 120: Train Loss = 0.2160
2023-11-08 11:43:23,028 - __main__ - INFO - Epoch 183 Batch 0: Train Loss = 0.1998
2023-11-08 11:43:39,090 - __main__ - INFO - Epoch 183 Batch 20: Train Loss = 0.2435
2023-11-08 11:43:53,410 - __main__ - INFO - Epoch 183 Batch 40: Train Loss = 0.2702
2023-11-08 11:44:09,030 - __main__ - INFO - Epoch 183 Batch 60: Train Loss = 0.2434
2023-11-08 11:44:25,082 - __main__ - INFO - Epoch 183 Batch 80: Train Loss = 0.1906
2023-11-08 11:44:41,019 - __main__ - INFO - Epoch 183 Batch 100: Train Loss = 0.1714
2023-11-08 11:44:57,398 - __main__ - INFO - Epoch 183 Batch 120: Train Loss = 0.2280
2023-11-08 11:45:09,234 - __main__ - INFO - Epoch 184 Batch 0: Train Loss = 0.2475
2023-11-08 11:45:25,939 - __main__ - INFO - Epoch 184 Batch 20: Train Loss = 0.2628
2023-11-08 11:45:43,685 - __main__ - INFO - Epoch 184 Batch 40: Train Loss = 0.2802
2023-11-08 11:46:01,414 - __main__ - INFO - Epoch 184 Batch 60: Train Loss = 0.2435
2023-11-08 11:46:19,798 - __main__ - INFO - Epoch 184 Batch 80: Train Loss = 0.2104
2023-11-08 11:46:37,305 - __main__ - INFO - Epoch 184 Batch 100: Train Loss = 0.1843
2023-11-08 11:46:53,739 - __main__ - INFO - Epoch 184 Batch 120: Train Loss = 0.2136
2023-11-08 11:47:06,647 - __main__ - INFO - Epoch 185 Batch 0: Train Loss = 0.2232
2023-11-08 11:47:23,360 - __main__ - INFO - Epoch 185 Batch 20: Train Loss = 0.2319
2023-11-08 11:47:39,975 - __main__ - INFO - Epoch 185 Batch 40: Train Loss = 0.2380
2023-11-08 11:47:57,151 - __main__ - INFO - Epoch 185 Batch 60: Train Loss = 0.2312
2023-11-08 11:48:14,976 - __main__ - INFO - Epoch 185 Batch 80: Train Loss = 0.2053
2023-11-08 11:48:31,818 - __main__ - INFO - Epoch 185 Batch 100: Train Loss = 0.1701
2023-11-08 11:48:48,260 - __main__ - INFO - Epoch 185 Batch 120: Train Loss = 0.2410
2023-11-08 11:49:00,886 - __main__ - INFO - Epoch 186 Batch 0: Train Loss = 0.1976
2023-11-08 11:49:16,532 - __main__ - INFO - Epoch 186 Batch 20: Train Loss = 0.2536
2023-11-08 11:49:31,616 - __main__ - INFO - Epoch 186 Batch 40: Train Loss = 0.2350
2023-11-08 11:49:47,412 - __main__ - INFO - Epoch 186 Batch 60: Train Loss = 0.2291
2023-11-08 11:50:03,807 - __main__ - INFO - Epoch 186 Batch 80: Train Loss = 0.1843
2023-11-08 11:50:19,978 - __main__ - INFO - Epoch 186 Batch 100: Train Loss = 0.1755
2023-11-08 11:50:36,175 - __main__ - INFO - Epoch 186 Batch 120: Train Loss = 0.2225
2023-11-08 11:50:47,808 - __main__ - INFO - Epoch 187 Batch 0: Train Loss = 0.2043
2023-11-08 11:51:03,946 - __main__ - INFO - Epoch 187 Batch 20: Train Loss = 0.2308
2023-11-08 11:51:19,155 - __main__ - INFO - Epoch 187 Batch 40: Train Loss = 0.2389
2023-11-08 11:51:33,790 - __main__ - INFO - Epoch 187 Batch 60: Train Loss = 0.2615
2023-11-08 11:51:48,935 - __main__ - INFO - Epoch 187 Batch 80: Train Loss = 0.2183
2023-11-08 11:52:03,429 - __main__ - INFO - Epoch 187 Batch 100: Train Loss = 0.1633
2023-11-08 11:52:18,787 - __main__ - INFO - Epoch 187 Batch 120: Train Loss = 0.2056
2023-11-08 11:52:29,580 - __main__ - INFO - Epoch 188 Batch 0: Train Loss = 0.2236
2023-11-08 11:52:46,053 - __main__ - INFO - Epoch 188 Batch 20: Train Loss = 0.2434
2023-11-08 11:53:00,974 - __main__ - INFO - Epoch 188 Batch 40: Train Loss = 0.2496
2023-11-08 11:53:15,662 - __main__ - INFO - Epoch 188 Batch 60: Train Loss = 0.2223
2023-11-08 11:53:30,935 - __main__ - INFO - Epoch 188 Batch 80: Train Loss = 0.1883
2023-11-08 11:53:45,892 - __main__ - INFO - Epoch 188 Batch 100: Train Loss = 0.1521
2023-11-08 11:54:01,081 - __main__ - INFO - Epoch 188 Batch 120: Train Loss = 0.1809
2023-11-08 11:54:13,492 - __main__ - INFO - Epoch 189 Batch 0: Train Loss = 0.2156
2023-11-08 11:54:31,683 - __main__ - INFO - Epoch 189 Batch 20: Train Loss = 0.2566
2023-11-08 11:54:49,374 - __main__ - INFO - Epoch 189 Batch 40: Train Loss = 0.2702
2023-11-08 11:55:06,382 - __main__ - INFO - Epoch 189 Batch 60: Train Loss = 0.2620
2023-11-08 11:55:21,897 - __main__ - INFO - Epoch 189 Batch 80: Train Loss = 0.2097
2023-11-08 11:55:38,508 - __main__ - INFO - Epoch 189 Batch 100: Train Loss = 0.1915
2023-11-08 11:55:54,964 - __main__ - INFO - Epoch 189 Batch 120: Train Loss = 0.2549
2023-11-08 11:56:07,261 - __main__ - INFO - Epoch 190 Batch 0: Train Loss = 0.2408
2023-11-08 11:56:23,123 - __main__ - INFO - Epoch 190 Batch 20: Train Loss = 0.2271
2023-11-08 11:56:38,416 - __main__ - INFO - Epoch 190 Batch 40: Train Loss = 0.3601
2023-11-08 11:56:53,803 - __main__ - INFO - Epoch 190 Batch 60: Train Loss = 0.2802
2023-11-08 11:57:09,593 - __main__ - INFO - Epoch 190 Batch 80: Train Loss = 0.1899
2023-11-08 11:57:25,114 - __main__ - INFO - Epoch 190 Batch 100: Train Loss = 0.1770
2023-11-08 11:57:39,682 - __main__ - INFO - Epoch 190 Batch 120: Train Loss = 0.2323
2023-11-08 11:57:51,210 - __main__ - INFO - Epoch 191 Batch 0: Train Loss = 0.2089
2023-11-08 11:58:07,466 - __main__ - INFO - Epoch 191 Batch 20: Train Loss = 0.2492
2023-11-08 11:58:22,313 - __main__ - INFO - Epoch 191 Batch 40: Train Loss = 0.2695
2023-11-08 11:58:36,429 - __main__ - INFO - Epoch 191 Batch 60: Train Loss = 0.2268
2023-11-08 11:58:52,078 - __main__ - INFO - Epoch 191 Batch 80: Train Loss = 0.1838
2023-11-08 11:59:06,967 - __main__ - INFO - Epoch 191 Batch 100: Train Loss = 0.1749
2023-11-08 11:59:22,416 - __main__ - INFO - Epoch 191 Batch 120: Train Loss = 0.1923
2023-11-08 11:59:33,477 - __main__ - INFO - Epoch 192 Batch 0: Train Loss = 0.2013
2023-11-08 11:59:49,856 - __main__ - INFO - Epoch 192 Batch 20: Train Loss = 0.2357
2023-11-08 12:00:04,896 - __main__ - INFO - Epoch 192 Batch 40: Train Loss = 0.2445
2023-11-08 12:00:20,244 - __main__ - INFO - Epoch 192 Batch 60: Train Loss = 0.2258
2023-11-08 12:00:36,339 - __main__ - INFO - Epoch 192 Batch 80: Train Loss = 0.1736
2023-11-08 12:00:52,807 - __main__ - INFO - Epoch 192 Batch 100: Train Loss = 0.1207
2023-11-08 12:01:08,615 - __main__ - INFO - Epoch 192 Batch 120: Train Loss = 0.2117
2023-11-08 12:01:19,860 - __main__ - INFO - Epoch 193 Batch 0: Train Loss = 0.2146
2023-11-08 12:01:35,773 - __main__ - INFO - Epoch 193 Batch 20: Train Loss = 0.2474
2023-11-08 12:01:50,833 - __main__ - INFO - Epoch 193 Batch 40: Train Loss = 0.2375
2023-11-08 12:02:06,020 - __main__ - INFO - Epoch 193 Batch 60: Train Loss = 0.1975
2023-11-08 12:02:21,711 - __main__ - INFO - Epoch 193 Batch 80: Train Loss = 0.1762
2023-11-08 12:02:37,044 - __main__ - INFO - Epoch 193 Batch 100: Train Loss = 0.1603
2023-11-08 12:02:51,810 - __main__ - INFO - Epoch 193 Batch 120: Train Loss = 0.1619
2023-11-08 12:03:02,978 - __main__ - INFO - Epoch 194 Batch 0: Train Loss = 0.1988
2023-11-08 12:03:18,446 - __main__ - INFO - Epoch 194 Batch 20: Train Loss = 0.2975
2023-11-08 12:03:33,291 - __main__ - INFO - Epoch 194 Batch 40: Train Loss = 0.2451
2023-11-08 12:03:48,639 - __main__ - INFO - Epoch 194 Batch 60: Train Loss = 0.2532
2023-11-08 12:04:04,614 - __main__ - INFO - Epoch 194 Batch 80: Train Loss = 0.2139
2023-11-08 12:04:21,588 - __main__ - INFO - Epoch 194 Batch 100: Train Loss = 0.1897
2023-11-08 12:04:38,836 - __main__ - INFO - Epoch 194 Batch 120: Train Loss = 0.2503
2023-11-08 12:04:52,051 - __main__ - INFO - Epoch 195 Batch 0: Train Loss = 0.1946
2023-11-08 12:05:09,062 - __main__ - INFO - Epoch 195 Batch 20: Train Loss = 0.2491
2023-11-08 12:05:23,911 - __main__ - INFO - Epoch 195 Batch 40: Train Loss = 0.2918
2023-11-08 12:05:38,695 - __main__ - INFO - Epoch 195 Batch 60: Train Loss = 0.2513
2023-11-08 12:05:54,427 - __main__ - INFO - Epoch 195 Batch 80: Train Loss = 0.1997
2023-11-08 12:06:10,363 - __main__ - INFO - Epoch 195 Batch 100: Train Loss = 0.1805
2023-11-08 12:06:26,836 - __main__ - INFO - Epoch 195 Batch 120: Train Loss = 0.1964
2023-11-08 12:06:38,513 - __main__ - INFO - Epoch 196 Batch 0: Train Loss = 0.2091
2023-11-08 12:06:54,149 - __main__ - INFO - Epoch 196 Batch 20: Train Loss = 0.2296
2023-11-08 12:07:08,688 - __main__ - INFO - Epoch 196 Batch 40: Train Loss = 0.3089
2023-11-08 12:07:24,499 - __main__ - INFO - Epoch 196 Batch 60: Train Loss = 0.2176
2023-11-08 12:07:39,664 - __main__ - INFO - Epoch 196 Batch 80: Train Loss = 0.1786
2023-11-08 12:07:56,182 - __main__ - INFO - Epoch 196 Batch 100: Train Loss = 0.1384
2023-11-08 12:08:10,974 - __main__ - INFO - Epoch 196 Batch 120: Train Loss = 0.1832
2023-11-08 12:08:22,080 - __main__ - INFO - Epoch 197 Batch 0: Train Loss = 0.1821
2023-11-08 12:08:37,304 - __main__ - INFO - Epoch 197 Batch 20: Train Loss = 0.2202
2023-11-08 12:08:51,549 - __main__ - INFO - Epoch 197 Batch 40: Train Loss = 0.2111
2023-11-08 12:09:06,743 - __main__ - INFO - Epoch 197 Batch 60: Train Loss = 0.2248
2023-11-08 12:09:22,475 - __main__ - INFO - Epoch 197 Batch 80: Train Loss = 0.1796
2023-11-08 12:09:38,064 - __main__ - INFO - Epoch 197 Batch 100: Train Loss = 0.1485
2023-11-08 12:09:54,037 - __main__ - INFO - Epoch 197 Batch 120: Train Loss = 0.1712
2023-11-08 12:10:06,134 - __main__ - INFO - Epoch 198 Batch 0: Train Loss = 0.1957
2023-11-08 12:10:25,050 - __main__ - INFO - Epoch 198 Batch 20: Train Loss = 0.1977
2023-11-08 12:10:43,414 - __main__ - INFO - Epoch 198 Batch 40: Train Loss = 0.2340
2023-11-08 12:11:01,537 - __main__ - INFO - Epoch 198 Batch 60: Train Loss = 0.1838
2023-11-08 12:11:17,640 - __main__ - INFO - Epoch 198 Batch 80: Train Loss = 0.1669
2023-11-08 12:11:35,351 - __main__ - INFO - Epoch 198 Batch 100: Train Loss = 0.1473
2023-11-08 12:11:52,544 - __main__ - INFO - Epoch 198 Batch 120: Train Loss = 0.1525
2023-11-08 12:12:05,293 - __main__ - INFO - Epoch 199 Batch 0: Train Loss = 0.1909
2023-11-08 12:12:22,881 - __main__ - INFO - Epoch 199 Batch 20: Train Loss = 0.2392
2023-11-08 12:12:40,131 - __main__ - INFO - Epoch 199 Batch 40: Train Loss = 0.2377
2023-11-08 12:12:56,499 - __main__ - INFO - Epoch 199 Batch 60: Train Loss = 0.2664
2023-11-08 12:13:13,842 - __main__ - INFO - Epoch 199 Batch 80: Train Loss = 0.2062
2023-11-08 12:13:30,873 - __main__ - INFO - Epoch 199 Batch 100: Train Loss = 0.1677
2023-11-08 12:13:47,779 - __main__ - INFO - Epoch 199 Batch 120: Train Loss = 0.2209
2023-11-08 12:13:59,461 - __main__ - INFO - auroc 0.8820
2023-11-08 12:13:59,464 - __main__ - INFO - auprc 0.5419
2023-11-08 12:13:59,465 - __main__ - INFO - minpse 0.5034
2023-11-08 12:13:59,622 - __main__ - INFO - last saved model is in epoch 168
2023-11-08 12:14:00,092 - __main__ - INFO - Batch 0: Test Loss = 0.1575
2023-11-08 12:14:07,721 - __main__ - INFO - 
==>Predicting on test
2023-11-08 12:14:07,723 - __main__ - INFO - Test Loss = 0.1882
2023-11-08 12:14:07,768 - __main__ - INFO - Transfer Target Dataset & Model
2023-11-08 12:14:07,824 - __main__ - INFO - [[-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8285543981909917, 2.770245567717861, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8374745525934485, 2.8093811444689463, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5338330207864587, 0.431156978499758, 0.8964660630930379, -1.342421748214616, -0.6946051308187712, -0.2120300841305121, 0.8952797203562032, 0.0387041303378994, -0.8612693268611251, 0.0726334784031089, -0.5051102137388988, -0.2865727201409924, 0.3988826585206329, -1.3794377690564883, -0.8237089728907875, -0.9221119476695248, -1.6036614531597553, -0.2379932071009605, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -1.1501193786706505, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, 1.0826051686869729, -0.609009148503521, 0.9684393533852332, -0.3880554131475662, 0.8439788318452395, 2.837917502516613, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 0.278477698357045, 1.2505677424119097, -0.6305581644917595, -0.9062745442328174, -0.7536814885080627, -0.4374103947888615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, -0.7710128163592748, -0.6738408269117502, -2.438483340619628, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8608899578998963, 2.912112033440545, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3396023676711391, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8763143915541441, 2.979783968239297, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5125273793649132, 0.601421850901989, 0.5034187451388209, -1.455529079732558, -0.5254345623915906, -0.4163803392884947, 0.7885904984786939, 0.2816638966057675, -0.5890172833864098, 0.5902661209826593, -0.3430637446868887, -0.7482466726318323, -0.1864257310498265, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.2249939001501796, -0.0341172340589738, 0.3466058957088718, 0.3482841380580905, -0.2890718868318484, 0.1923438589112976, -0.2031244129114749, -1.100153472115602, -0.7598199909551685, -0.6868858002659636, 0.3602180776283341, -0.700061185363224, -0.501359972189453, -0.2332733777972843, -1.36588558517574, 0.3571411263970316, -0.7447833078367583, 0.8765002281041955, 2.9805992927549454, 0.0776143028335215, -0.2769313825285214, 0.0755136678180621, -0.1474826847594624, -0.3999358135056357, -0.0169688050451549, -0.0937841116273902, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, -0.0122781725754626, -0.3292808750219816, -0.2769313825285214, -0.107625012968948, -0.1474826847594624, -0.5354516373428909, -0.0002229391773086, -0.0862088042532133, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, 0.0766269712424727, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.3951350096437179, -0.5239726809327129, -0.8378690925056301, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.8988006141103361, 3.0784382346326584, 0.0776143028335215, -0.3487779453829513, 0.070457637721598, -0.1474826847594624, -0.3999358135056357, 0.0045338857876385, -0.0729488956980065, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.1275947723114954, -0.4255010572823463, -0.3487779453829513, -0.1777070069371955, -0.1474826847594624, -0.5354516373428909, 0.029608437271828, -0.0566564538358881, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.0232601678654147, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.9106941533136118, 3.1306190036341057, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9108799898636633, 3.1314343281497536, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 1.0542620417323658, -0.6738408269117502, -0.0796833960305785, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9222160194167848, 3.1811691236042576, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.5107528363891513, 1.4769583166408096, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-1.038066534429697, 0.260892106097527, 0.1645848503507034, -1.455529079732558, -1.6391408045371951, 0.196670426185453, 1.2420196914581063, 0.4706326037029981, -0.6616178283130005, -0.5004598044528213, -0.4609157221792596, 0.021209914852902, -1.1769476210921423, 0.0909225332951111, -1.348247483817418, -1.2590833683251033, -0.9252694826312332, 0.781386658108973, -0.0798138719333467, -0.2032969502372001, -0.5728175028694968, 0.103242147293012, -0.2031244129114749, -0.6504603131201653, -0.5868915885967425, -0.6218024077192129, -0.0284729839577739, -0.3925684926321655, -0.4454382040900255, -0.2791761177909213, -1.5149673075505703, 0.3413397023846657, -0.8352815289623093, 0.9329945393197532, 3.228457945511819, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8995798651896526, 0.4879119359671683, 0.1510314945591795, -1.4131138304133295, -0.6523124887119757, 0.196670426185453, 0.148455167213642, 0.4706326037029981, -0.752368509471239, -0.9441449266638648, -0.9507380036319262, 0.1751012323498492, -1.0418764542681906, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -1.3629417216818924, 0.1697587389830128, -0.364093717028159, -0.2645837378255657, -0.2890718868318484, 0.0364158635792983, -0.2031244129114749, -1.8246591171638051, -0.4061028043129334, -0.5350245509902118, 0.0492652283594477, 0.0357249008146661, -0.4929416415078188, -0.5851943844151671, -1.170932563608653, 0.051337096981241, -0.8763418268751098, 0.9447022419729768, 3.279823389997619, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.962914223877992, 3.359725192531085, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.973321070680858, 3.405383365407351, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9735069072309092, 3.4061986899229986, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9980373318376644, 3.5138215259884835, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0340896225475933, 3.671994482024121, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.2639920224449301, -0.6738408269117502, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0396647190491284, 3.696454217493548, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7418059691584676, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, -0.6693833821778409, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.057505027854041, 3.774725370995719, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5792130238763558, -0.9062745442328174, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, 0.0888023142972107, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0760886828591592, 3.856257822560481, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]
2023-11-08 12:14:07,829 - __main__ - INFO - 69
2023-11-08 12:14:07,830 - __main__ - INFO - 325
2023-11-08 12:14:07,987 - __main__ - INFO - {'los_mean': 1055.0307777880782, 'los_std': 799.0879849276147}
2023-11-08 12:14:11,334 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 1.0481
2023-11-08 12:14:29,949 - __main__ - INFO - Fold 1, epoch 0: Loss = 1.0142 Valid loss = 1.0022 MSE = 701.8374
2023-11-08 12:14:29,952 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 701.8374 ------------
2023-11-08 12:14:30,479 - __main__ - INFO - ------------ Save best model - MSE: 701.8374 ------------
2023-11-08 12:14:30,483 - __main__ - INFO - Fold 1, mse = 701.8374, mad = 21.5754
2023-11-08 12:14:31,359 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 1.0490
2023-11-08 12:14:49,713 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 698.1661 ------------
2023-11-08 12:14:50,163 - __main__ - INFO - ------------ Save best model - MSE: 698.1661 ------------
2023-11-08 12:14:50,166 - __main__ - INFO - Fold 1, mse = 698.1661, mad = 21.3172
2023-11-08 12:14:51,047 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 0.9162
2023-11-08 12:15:11,361 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 687.6633 ------------
2023-11-08 12:15:11,937 - __main__ - INFO - ------------ Save best model - MSE: 687.6633 ------------
2023-11-08 12:15:11,938 - __main__ - INFO - Fold 1, mse = 687.6633, mad = 21.3391
2023-11-08 12:15:12,574 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 0.9928
2023-11-08 12:15:30,271 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 678.6415 ------------
2023-11-08 12:15:30,811 - __main__ - INFO - ------------ Save best model - MSE: 678.6415 ------------
2023-11-08 12:15:30,813 - __main__ - INFO - Fold 1, mse = 678.6415, mad = 21.0005
2023-11-08 12:15:31,456 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 0.9170
2023-11-08 12:15:49,395 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 672.8111 ------------
2023-11-08 12:15:49,742 - __main__ - INFO - ------------ Save best model - MSE: 672.8111 ------------
2023-11-08 12:15:49,744 - __main__ - INFO - Fold 1, mse = 672.8111, mad = 20.9520
2023-11-08 12:15:50,474 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 0.7854
2023-11-08 12:16:07,536 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 671.3596 ------------
2023-11-08 12:16:07,925 - __main__ - INFO - ------------ Save best model - MSE: 671.3596 ------------
2023-11-08 12:16:07,927 - __main__ - INFO - Fold 1, mse = 671.3596, mad = 20.9039
2023-11-08 12:16:08,613 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 0.8368
2023-11-08 12:16:26,014 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 668.6471 ------------
2023-11-08 12:16:26,314 - __main__ - INFO - ------------ Save best model - MSE: 668.6471 ------------
2023-11-08 12:16:26,316 - __main__ - INFO - Fold 1, mse = 668.6471, mad = 20.8468
2023-11-08 12:16:26,985 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 0.7088
2023-11-08 12:16:44,497 - __main__ - INFO - Fold 1, mse = 672.3567, mad = 20.8164
2023-11-08 12:16:45,436 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 0.7117
2023-11-08 12:17:03,906 - __main__ - INFO - Fold 1, mse = 672.2359, mad = 20.6747
2023-11-08 12:17:04,554 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 0.7356
2023-11-08 12:17:24,486 - __main__ - INFO - Fold 1, mse = 673.6284, mad = 20.8229
2023-11-08 12:17:25,474 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 0.7499
2023-11-08 12:17:44,377 - __main__ - INFO - Fold 1, epoch 10: Loss = 0.7194 Valid loss = 0.9681 MSE = 686.6686
2023-11-08 12:17:44,379 - __main__ - INFO - Fold 1, mse = 686.6686, mad = 20.9165
2023-11-08 12:17:45,162 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 0.6487
2023-11-08 12:18:03,570 - __main__ - INFO - Fold 1, mse = 686.0279, mad = 21.0377
2023-11-08 12:18:04,315 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 0.6268
2023-11-08 12:18:22,564 - __main__ - INFO - Fold 1, mse = 673.7126, mad = 20.8540
2023-11-08 12:18:23,223 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 0.6113
2023-11-08 12:18:41,320 - __main__ - INFO - Fold 1, mse = 670.8569, mad = 20.7697
2023-11-08 12:18:41,995 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 0.6208
2023-11-08 12:18:59,667 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 658.4264 ------------
2023-11-08 12:18:59,942 - __main__ - INFO - ------------ Save best model - MSE: 658.4264 ------------
2023-11-08 12:18:59,944 - __main__ - INFO - Fold 1, mse = 658.4264, mad = 20.5665
2023-11-08 12:19:00,586 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 0.6379
2023-11-08 12:19:18,807 - __main__ - INFO - Fold 1, mse = 692.7227, mad = 20.8989
2023-11-08 12:19:19,553 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 0.6302
2023-11-08 12:19:37,679 - __main__ - INFO - Fold 1, mse = 686.9677, mad = 20.9006
2023-11-08 12:19:38,454 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 0.6801
2023-11-08 12:19:55,731 - __main__ - INFO - Fold 1, mse = 689.2659, mad = 20.9402
2023-11-08 12:19:56,457 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 0.6770
2023-11-08 12:20:13,917 - __main__ - INFO - Fold 1, mse = 697.9359, mad = 21.0366
2023-11-08 12:20:14,641 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 0.6596
2023-11-08 12:20:33,669 - __main__ - INFO - Fold 1, mse = 716.1785, mad = 21.2056
2023-11-08 12:20:34,321 - __main__ - INFO - Fold 1 Epoch 20 Batch 0: Train Loss = 0.5327
2023-11-08 12:20:52,165 - __main__ - INFO - Fold 1, epoch 20: Loss = 0.5808 Valid loss = 0.9630 MSE = 681.5868
2023-11-08 12:20:52,168 - __main__ - INFO - Fold 1, mse = 681.5868, mad = 20.9101
2023-11-08 12:20:52,830 - __main__ - INFO - Fold 1 Epoch 21 Batch 0: Train Loss = 0.6112
2023-11-08 12:21:10,541 - __main__ - INFO - Fold 1, mse = 685.5095, mad = 20.9265
2023-11-08 12:21:11,308 - __main__ - INFO - Fold 1 Epoch 22 Batch 0: Train Loss = 0.6367
2023-11-08 12:21:29,679 - __main__ - INFO - Fold 1, mse = 683.3948, mad = 21.0222
2023-11-08 12:21:30,314 - __main__ - INFO - Fold 1 Epoch 23 Batch 0: Train Loss = 0.5053
2023-11-08 12:21:48,226 - __main__ - INFO - Fold 1, mse = 717.2983, mad = 21.1150
2023-11-08 12:21:48,970 - __main__ - INFO - Fold 1 Epoch 24 Batch 0: Train Loss = 0.5803
2023-11-08 12:22:07,406 - __main__ - INFO - Fold 1, mse = 700.4560, mad = 20.9911
2023-11-08 12:22:08,281 - __main__ - INFO - Fold 1 Epoch 25 Batch 0: Train Loss = 0.5717
2023-11-08 12:22:26,232 - __main__ - INFO - Fold 1, mse = 687.7115, mad = 20.9968
2023-11-08 12:22:26,989 - __main__ - INFO - Fold 1 Epoch 26 Batch 0: Train Loss = 0.5135
2023-11-08 12:22:44,861 - __main__ - INFO - Fold 1, mse = 706.8900, mad = 21.0050
2023-11-08 12:22:45,606 - __main__ - INFO - Fold 1 Epoch 27 Batch 0: Train Loss = 0.4896
2023-11-08 12:23:04,486 - __main__ - INFO - Fold 1, mse = 708.3778, mad = 20.8885
2023-11-08 12:23:05,269 - __main__ - INFO - Fold 1 Epoch 28 Batch 0: Train Loss = 0.5960
2023-11-08 12:23:24,800 - __main__ - INFO - Fold 1, mse = 682.9665, mad = 20.9812
2023-11-08 12:23:25,666 - __main__ - INFO - Fold 1 Epoch 29 Batch 0: Train Loss = 0.5364
2023-11-08 12:23:44,552 - __main__ - INFO - Fold 1, mse = 700.1267, mad = 20.9716
2023-11-08 12:23:45,613 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 1.0500
2023-11-08 12:24:04,478 - __main__ - INFO - Fold 2, epoch 0: Loss = 0.9658 Valid loss = 1.0405 MSE = 738.2781
2023-11-08 12:24:04,480 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 738.2781 ------------
2023-11-08 12:24:04,702 - __main__ - INFO - Fold 2, mse = 738.2781, mad = 21.6341
2023-11-08 12:24:05,346 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 0.8888
2023-11-08 12:24:23,149 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 717.0804 ------------
2023-11-08 12:24:23,455 - __main__ - INFO - Fold 2, mse = 717.0804, mad = 21.6102
2023-11-08 12:24:24,318 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 0.8715
2023-11-08 12:24:43,340 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 706.3829 ------------
2023-11-08 12:24:43,604 - __main__ - INFO - Fold 2, mse = 706.3829, mad = 21.5765
2023-11-08 12:24:44,339 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 0.8919
2023-11-08 12:25:01,486 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 697.4685 ------------
2023-11-08 12:25:01,644 - __main__ - INFO - Fold 2, mse = 697.4685, mad = 21.5159
2023-11-08 12:25:02,323 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 0.6633
2023-11-08 12:25:21,182 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 689.5439 ------------
2023-11-08 12:25:21,375 - __main__ - INFO - Fold 2, mse = 689.5439, mad = 21.3857
2023-11-08 12:25:22,062 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 0.7571
2023-11-08 12:25:39,895 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 683.4118 ------------
2023-11-08 12:25:40,158 - __main__ - INFO - Fold 2, mse = 683.4118, mad = 21.1831
2023-11-08 12:25:40,937 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 0.6408
2023-11-08 12:25:57,987 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 681.4291 ------------
2023-11-08 12:25:58,274 - __main__ - INFO - Fold 2, mse = 681.4291, mad = 21.0294
2023-11-08 12:25:59,004 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 0.7130
2023-11-08 12:26:17,026 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 672.2106 ------------
2023-11-08 12:26:17,236 - __main__ - INFO - Fold 2, mse = 672.2106, mad = 20.8933
2023-11-08 12:26:17,993 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 0.6534
2023-11-08 12:26:35,720 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 672.1922 ------------
2023-11-08 12:26:35,985 - __main__ - INFO - Fold 2, mse = 672.1922, mad = 20.8499
2023-11-08 12:26:36,764 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 0.7401
2023-11-08 12:26:55,223 - __main__ - INFO - Fold 2, mse = 683.4946, mad = 20.8805
2023-11-08 12:26:55,942 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 0.5531
2023-11-08 12:27:13,362 - __main__ - INFO - Fold 2, epoch 10: Loss = 0.6870 Valid loss = 0.9449 MSE = 670.3766
2023-11-08 12:27:13,363 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 670.3766 ------------
2023-11-08 12:27:13,637 - __main__ - INFO - Fold 2, mse = 670.3766, mad = 20.9025
2023-11-08 12:27:14,385 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 0.5728
2023-11-08 12:27:32,700 - __main__ - INFO - Fold 2, mse = 689.8734, mad = 20.8646
2023-11-08 12:27:33,382 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 0.6061
2023-11-08 12:27:51,279 - __main__ - INFO - Fold 2, mse = 677.9670, mad = 20.8854
2023-11-08 12:27:52,071 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 0.5439
2023-11-08 12:28:09,332 - __main__ - INFO - Fold 2, mse = 695.3179, mad = 20.8470
2023-11-08 12:28:10,107 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 0.5589
2023-11-08 12:28:27,977 - __main__ - INFO - Fold 2, mse = 687.5038, mad = 20.9033
2023-11-08 12:28:28,605 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 0.6255
2023-11-08 12:28:46,314 - __main__ - INFO - Fold 2, mse = 696.5958, mad = 21.0484
2023-11-08 12:28:47,002 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 0.5913
2023-11-08 12:29:05,000 - __main__ - INFO - Fold 2, mse = 691.7453, mad = 21.0248
2023-11-08 12:29:05,844 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 0.5646
2023-11-08 12:29:24,718 - __main__ - INFO - Fold 2, mse = 705.4157, mad = 21.0563
2023-11-08 12:29:25,526 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 0.5862
2023-11-08 12:29:43,986 - __main__ - INFO - Fold 2, mse = 691.8294, mad = 20.9915
2023-11-08 12:29:44,623 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 0.7084
2023-11-08 12:30:02,503 - __main__ - INFO - Fold 2, mse = 690.9398, mad = 21.0225
2023-11-08 12:30:03,287 - __main__ - INFO - Fold 2 Epoch 20 Batch 0: Train Loss = 0.5677
2023-11-08 12:30:21,340 - __main__ - INFO - Fold 2, epoch 20: Loss = 0.5391 Valid loss = 0.9948 MSE = 705.7526
2023-11-08 12:30:21,341 - __main__ - INFO - Fold 2, mse = 705.7526, mad = 21.0944
2023-11-08 12:30:22,053 - __main__ - INFO - Fold 2 Epoch 21 Batch 0: Train Loss = 0.4924
2023-11-08 12:30:39,795 - __main__ - INFO - Fold 2, mse = 710.2647, mad = 21.1631
2023-11-08 12:30:40,686 - __main__ - INFO - Fold 2 Epoch 22 Batch 0: Train Loss = 0.4703
2023-11-08 12:30:59,256 - __main__ - INFO - Fold 2, mse = 696.5375, mad = 21.0121
2023-11-08 12:31:00,048 - __main__ - INFO - Fold 2 Epoch 23 Batch 0: Train Loss = 0.5094
2023-11-08 12:31:17,219 - __main__ - INFO - Fold 2, mse = 712.2026, mad = 21.1847
2023-11-08 12:31:17,848 - __main__ - INFO - Fold 2 Epoch 24 Batch 0: Train Loss = 0.4621
2023-11-08 12:31:36,414 - __main__ - INFO - Fold 2, mse = 713.1339, mad = 21.1656
2023-11-08 12:31:37,294 - __main__ - INFO - Fold 2 Epoch 25 Batch 0: Train Loss = 0.5883
2023-11-08 12:31:55,215 - __main__ - INFO - Fold 2, mse = 724.7579, mad = 21.2231
2023-11-08 12:31:55,876 - __main__ - INFO - Fold 2 Epoch 26 Batch 0: Train Loss = 0.5637
2023-11-08 12:32:13,188 - __main__ - INFO - Fold 2, mse = 694.4883, mad = 21.0127
2023-11-08 12:32:13,845 - __main__ - INFO - Fold 2 Epoch 27 Batch 0: Train Loss = 0.4352
2023-11-08 12:32:32,726 - __main__ - INFO - Fold 2, mse = 689.7307, mad = 20.9005
2023-11-08 12:32:33,497 - __main__ - INFO - Fold 2 Epoch 28 Batch 0: Train Loss = 0.4974
2023-11-08 12:32:50,687 - __main__ - INFO - Fold 2, mse = 697.2694, mad = 21.0180
2023-11-08 12:32:51,352 - __main__ - INFO - Fold 2 Epoch 29 Batch 0: Train Loss = 0.4208
2023-11-08 12:33:09,083 - __main__ - INFO - Fold 2, mse = 728.8851, mad = 21.3135
2023-11-08 12:33:10,127 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 1.0374
2023-11-08 12:33:29,439 - __main__ - INFO - Fold 3, epoch 0: Loss = 0.9849 Valid loss = 1.0173 MSE = 717.8817
2023-11-08 12:33:29,442 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 717.8817 ------------
2023-11-08 12:33:29,801 - __main__ - INFO - Fold 3, mse = 717.8817, mad = 22.0247
2023-11-08 12:33:30,575 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 1.1350
2023-11-08 12:33:49,585 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 711.2144 ------------
2023-11-08 12:33:49,936 - __main__ - INFO - Fold 3, mse = 711.2144, mad = 21.8538
2023-11-08 12:33:50,608 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 0.8203
2023-11-08 12:34:08,780 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 707.5714 ------------
2023-11-08 12:34:08,985 - __main__ - INFO - Fold 3, mse = 707.5714, mad = 21.7369
2023-11-08 12:34:09,798 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 0.8384
2023-11-08 12:34:28,099 - __main__ - INFO - Fold 3, mse = 708.6669, mad = 21.6789
2023-11-08 12:34:28,931 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 0.7928
2023-11-08 12:34:49,384 - __main__ - INFO - Fold 3, mse = 715.1392, mad = 21.5632
2023-11-08 12:34:50,163 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 0.6046
2023-11-08 12:35:09,338 - __main__ - INFO - Fold 3, mse = 721.8113, mad = 21.6091
2023-11-08 12:35:10,204 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 0.5402
2023-11-08 12:35:29,694 - __main__ - INFO - Fold 3, mse = 750.1586, mad = 21.5946
2023-11-08 12:35:30,443 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 0.6671
2023-11-08 12:35:49,873 - __main__ - INFO - Fold 3, mse = 767.7916, mad = 22.0330
2023-11-08 12:35:50,605 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 0.5616
2023-11-08 12:36:10,057 - __main__ - INFO - Fold 3, mse = 740.5921, mad = 21.9208
2023-11-08 12:36:10,915 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 0.6354
2023-11-08 12:36:30,051 - __main__ - INFO - Fold 3, mse = 753.5771, mad = 21.9070
2023-11-08 12:36:30,731 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 0.5683
2023-11-08 12:36:49,004 - __main__ - INFO - Fold 3, epoch 10: Loss = 0.5937 Valid loss = 1.0797 MSE = 765.6365
2023-11-08 12:36:49,006 - __main__ - INFO - Fold 3, mse = 765.6365, mad = 22.1467
2023-11-08 12:36:49,813 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 0.5622
2023-11-08 12:37:08,465 - __main__ - INFO - Fold 3, mse = 786.6998, mad = 22.1467
2023-11-08 12:37:09,220 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 0.5284
2023-11-08 12:37:27,752 - __main__ - INFO - Fold 3, mse = 758.8531, mad = 22.0662
2023-11-08 12:37:28,501 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 0.5653
2023-11-08 12:37:47,974 - __main__ - INFO - Fold 3, mse = 791.4464, mad = 22.3689
2023-11-08 12:37:48,856 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 0.4874
2023-11-08 12:38:07,829 - __main__ - INFO - Fold 3, mse = 804.8926, mad = 22.4242
2023-11-08 12:38:08,547 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 0.5939
2023-11-08 12:38:26,138 - __main__ - INFO - Fold 3, mse = 815.5129, mad = 22.4757
2023-11-08 12:38:26,856 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 0.6439
2023-11-08 12:38:45,289 - __main__ - INFO - Fold 3, mse = 853.6135, mad = 22.9238
2023-11-08 12:38:46,045 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 0.6240
2023-11-08 12:39:04,364 - __main__ - INFO - Fold 3, mse = 787.2084, mad = 22.3113
2023-11-08 12:39:05,179 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 0.5041
2023-11-08 12:39:24,134 - __main__ - INFO - Fold 3, mse = 777.6131, mad = 22.1326
2023-11-08 12:39:25,026 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 0.4413
2023-11-08 12:39:44,477 - __main__ - INFO - Fold 3, mse = 803.0714, mad = 22.5437
2023-11-08 12:39:45,335 - __main__ - INFO - Fold 3 Epoch 20 Batch 0: Train Loss = 0.5440
2023-11-08 12:40:04,391 - __main__ - INFO - Fold 3, epoch 20: Loss = 0.5131 Valid loss = 1.1055 MSE = 784.8924
2023-11-08 12:40:04,394 - __main__ - INFO - Fold 3, mse = 784.8924, mad = 22.0768
2023-11-08 12:40:05,256 - __main__ - INFO - Fold 3 Epoch 21 Batch 0: Train Loss = 0.5413
2023-11-08 12:40:24,682 - __main__ - INFO - Fold 3, mse = 759.7185, mad = 21.8884
2023-11-08 12:40:25,477 - __main__ - INFO - Fold 3 Epoch 22 Batch 0: Train Loss = 0.5266
2023-11-08 12:40:45,031 - __main__ - INFO - Fold 3, mse = 788.9754, mad = 22.1344
2023-11-08 12:40:45,948 - __main__ - INFO - Fold 3 Epoch 23 Batch 0: Train Loss = 0.4331
2023-11-08 12:41:05,840 - __main__ - INFO - Fold 3, mse = 805.8976, mad = 22.3099
2023-11-08 12:41:06,667 - __main__ - INFO - Fold 3 Epoch 24 Batch 0: Train Loss = 0.5397
2023-11-08 12:41:25,230 - __main__ - INFO - Fold 3, mse = 773.4110, mad = 21.8473
2023-11-08 12:41:26,043 - __main__ - INFO - Fold 3 Epoch 25 Batch 0: Train Loss = 0.4849
2023-11-08 12:41:44,182 - __main__ - INFO - Fold 3, mse = 761.6441, mad = 21.9270
2023-11-08 12:41:44,915 - __main__ - INFO - Fold 3 Epoch 26 Batch 0: Train Loss = 0.4611
2023-11-08 12:42:04,647 - __main__ - INFO - Fold 3, mse = 758.0222, mad = 21.8521
2023-11-08 12:42:05,594 - __main__ - INFO - Fold 3 Epoch 27 Batch 0: Train Loss = 0.4329
2023-11-08 12:42:24,376 - __main__ - INFO - Fold 3, mse = 771.8850, mad = 22.0824
2023-11-08 12:42:25,101 - __main__ - INFO - Fold 3 Epoch 28 Batch 0: Train Loss = 0.4092
2023-11-08 12:42:44,268 - __main__ - INFO - Fold 3, mse = 775.6526, mad = 22.1407
2023-11-08 12:42:45,078 - __main__ - INFO - Fold 3 Epoch 29 Batch 0: Train Loss = 0.4853
2023-11-08 12:43:03,992 - __main__ - INFO - Fold 3, mse = 784.2835, mad = 21.9502
2023-11-08 12:43:05,393 - __main__ - INFO - Fold 4 Epoch 0 Batch 0: Train Loss = 1.2813
2023-11-08 12:43:23,513 - __main__ - INFO - Fold 4, epoch 0: Loss = 1.2617 Valid loss = 0.9566 MSE = 678.5098
2023-11-08 12:43:23,514 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 678.5098 ------------
2023-11-08 12:43:23,916 - __main__ - INFO - Fold 4, mse = 678.5098, mad = 21.7338
2023-11-08 12:43:24,796 - __main__ - INFO - Fold 4 Epoch 1 Batch 0: Train Loss = 1.1034
2023-11-08 12:43:43,664 - __main__ - INFO - Fold 4, mse = 681.8365, mad = 21.9198
2023-11-08 12:43:44,426 - __main__ - INFO - Fold 4 Epoch 2 Batch 0: Train Loss = 1.3255
2023-11-08 12:44:03,763 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 675.4145 ------------
2023-11-08 12:44:04,060 - __main__ - INFO - Fold 4, mse = 675.4145, mad = 21.7836
2023-11-08 12:44:04,888 - __main__ - INFO - Fold 4 Epoch 3 Batch 0: Train Loss = 1.1670
2023-11-08 12:44:23,307 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 663.9177 ------------
2023-11-08 12:44:23,561 - __main__ - INFO - Fold 4, mse = 663.9177, mad = 21.5104
2023-11-08 12:44:24,336 - __main__ - INFO - Fold 4 Epoch 4 Batch 0: Train Loss = 1.1634
2023-11-08 12:44:42,168 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 652.3882 ------------
2023-11-08 12:44:42,682 - __main__ - INFO - ------------ Save best model - MSE: 652.3882 ------------
2023-11-08 12:44:42,684 - __main__ - INFO - Fold 4, mse = 652.3882, mad = 21.2150
2023-11-08 12:44:43,467 - __main__ - INFO - Fold 4 Epoch 5 Batch 0: Train Loss = 0.9799
2023-11-08 12:45:02,346 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 650.6055 ------------
2023-11-08 12:45:02,664 - __main__ - INFO - ------------ Save best model - MSE: 650.6055 ------------
2023-11-08 12:45:02,667 - __main__ - INFO - Fold 4, mse = 650.6055, mad = 21.2951
2023-11-08 12:45:03,598 - __main__ - INFO - Fold 4 Epoch 6 Batch 0: Train Loss = 0.9575
2023-11-08 12:45:21,941 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 639.7120 ------------
2023-11-08 12:45:22,306 - __main__ - INFO - ------------ Save best model - MSE: 639.7120 ------------
2023-11-08 12:45:22,308 - __main__ - INFO - Fold 4, mse = 639.7120, mad = 20.9463
2023-11-08 12:45:23,095 - __main__ - INFO - Fold 4 Epoch 7 Batch 0: Train Loss = 0.9151
2023-11-08 12:45:41,762 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 634.0337 ------------
2023-11-08 12:45:42,113 - __main__ - INFO - ------------ Save best model - MSE: 634.0337 ------------
2023-11-08 12:45:42,115 - __main__ - INFO - Fold 4, mse = 634.0337, mad = 20.8526
2023-11-08 12:45:43,091 - __main__ - INFO - Fold 4 Epoch 8 Batch 0: Train Loss = 0.9501
2023-11-08 12:46:02,710 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 628.8340 ------------
2023-11-08 12:46:03,110 - __main__ - INFO - ------------ Save best model - MSE: 628.8340 ------------
2023-11-08 12:46:03,112 - __main__ - INFO - Fold 4, mse = 628.8340, mad = 20.6554
2023-11-08 12:46:03,910 - __main__ - INFO - Fold 4 Epoch 9 Batch 0: Train Loss = 0.7774
2023-11-08 12:46:23,148 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 628.3262 ------------
2023-11-08 12:46:23,589 - __main__ - INFO - ------------ Save best model - MSE: 628.3262 ------------
2023-11-08 12:46:23,591 - __main__ - INFO - Fold 4, mse = 628.3262, mad = 20.4275
2023-11-08 12:46:24,659 - __main__ - INFO - Fold 4 Epoch 10 Batch 0: Train Loss = 0.7788
2023-11-08 12:46:43,464 - __main__ - INFO - Fold 4, epoch 10: Loss = 0.8278 Valid loss = 0.8881 MSE = 629.8894
2023-11-08 12:46:43,466 - __main__ - INFO - Fold 4, mse = 629.8894, mad = 20.4029
2023-11-08 12:46:44,296 - __main__ - INFO - Fold 4 Epoch 11 Batch 0: Train Loss = 0.8826
2023-11-08 12:47:02,610 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 624.4495 ------------
2023-11-08 12:47:03,135 - __main__ - INFO - ------------ Save best model - MSE: 624.4495 ------------
2023-11-08 12:47:03,137 - __main__ - INFO - Fold 4, mse = 624.4495, mad = 20.3896
2023-11-08 12:47:03,956 - __main__ - INFO - Fold 4 Epoch 12 Batch 0: Train Loss = 0.6795
2023-11-08 12:47:23,273 - __main__ - INFO - Fold 4, mse = 625.3310, mad = 20.3745
2023-11-08 12:47:23,966 - __main__ - INFO - Fold 4 Epoch 13 Batch 0: Train Loss = 0.7677
2023-11-08 12:47:42,130 - __main__ - INFO - Fold 4, mse = 627.0619, mad = 20.2401
2023-11-08 12:47:42,855 - __main__ - INFO - Fold 4 Epoch 14 Batch 0: Train Loss = 0.6752
2023-11-08 12:48:01,984 - __main__ - INFO - Fold 4, mse = 634.9459, mad = 20.2440
2023-11-08 12:48:02,700 - __main__ - INFO - Fold 4 Epoch 15 Batch 0: Train Loss = 0.5783
2023-11-08 12:48:21,025 - __main__ - INFO - Fold 4, mse = 632.6660, mad = 20.1907
2023-11-08 12:48:21,902 - __main__ - INFO - Fold 4 Epoch 16 Batch 0: Train Loss = 0.6624
2023-11-08 12:48:40,683 - __main__ - INFO - Fold 4, mse = 634.5982, mad = 20.1303
2023-11-08 12:48:41,457 - __main__ - INFO - Fold 4 Epoch 17 Batch 0: Train Loss = 0.6901
2023-11-08 12:48:59,883 - __main__ - INFO - Fold 4, mse = 639.2343, mad = 20.1192
2023-11-08 12:49:00,635 - __main__ - INFO - Fold 4 Epoch 18 Batch 0: Train Loss = 0.6891
2023-11-08 12:49:18,307 - __main__ - INFO - Fold 4, mse = 631.6007, mad = 20.4551
2023-11-08 12:49:19,112 - __main__ - INFO - Fold 4 Epoch 19 Batch 0: Train Loss = 0.5700
2023-11-08 12:49:37,820 - __main__ - INFO - Fold 4, mse = 629.8875, mad = 20.0540
2023-11-08 12:49:38,675 - __main__ - INFO - Fold 4 Epoch 20 Batch 0: Train Loss = 0.6751
2023-11-08 12:49:57,309 - __main__ - INFO - Fold 4, epoch 20: Loss = 0.6182 Valid loss = 0.8977 MSE = 636.4653
2023-11-08 12:49:57,311 - __main__ - INFO - Fold 4, mse = 636.4653, mad = 20.2091
2023-11-08 12:49:58,071 - __main__ - INFO - Fold 4 Epoch 21 Batch 0: Train Loss = 0.6624
2023-11-08 12:50:16,718 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 624.4453 ------------
2023-11-08 12:50:17,287 - __main__ - INFO - ------------ Save best model - MSE: 624.4453 ------------
2023-11-08 12:50:17,290 - __main__ - INFO - Fold 4, mse = 624.4453, mad = 19.9974
2023-11-08 12:50:18,225 - __main__ - INFO - Fold 4 Epoch 22 Batch 0: Train Loss = 0.6142
2023-11-08 12:50:37,059 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 623.6871 ------------
2023-11-08 12:50:37,524 - __main__ - INFO - ------------ Save best model - MSE: 623.6871 ------------
2023-11-08 12:50:37,526 - __main__ - INFO - Fold 4, mse = 623.6871, mad = 20.0385
2023-11-08 12:50:38,389 - __main__ - INFO - Fold 4 Epoch 23 Batch 0: Train Loss = 0.5925
2023-11-08 12:50:56,849 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 601.1936 ------------
2023-11-08 12:50:57,319 - __main__ - INFO - ------------ Save best model - MSE: 601.1936 ------------
2023-11-08 12:50:57,321 - __main__ - INFO - Fold 4, mse = 601.1936, mad = 19.6778
2023-11-08 12:50:58,069 - __main__ - INFO - Fold 4 Epoch 24 Batch 0: Train Loss = 0.6424
2023-11-08 12:51:17,270 - __main__ - INFO - Fold 4, mse = 608.3053, mad = 19.8347
2023-11-08 12:51:18,254 - __main__ - INFO - Fold 4 Epoch 25 Batch 0: Train Loss = 0.6359
2023-11-08 12:51:37,040 - __main__ - INFO - Fold 4, mse = 610.4766, mad = 19.7757
2023-11-08 12:51:37,874 - __main__ - INFO - Fold 4 Epoch 26 Batch 0: Train Loss = 0.5763
2023-11-08 12:51:57,808 - __main__ - INFO - Fold 4, mse = 625.8619, mad = 19.9408
2023-11-08 12:51:58,731 - __main__ - INFO - Fold 4 Epoch 27 Batch 0: Train Loss = 0.6494
2023-11-08 12:52:18,000 - __main__ - INFO - Fold 4, mse = 616.1946, mad = 19.9259
2023-11-08 12:52:18,734 - __main__ - INFO - Fold 4 Epoch 28 Batch 0: Train Loss = 0.6447
2023-11-08 12:52:37,012 - __main__ - INFO - Fold 4, mse = 610.5027, mad = 19.8565
2023-11-08 12:52:37,784 - __main__ - INFO - Fold 4 Epoch 29 Batch 0: Train Loss = 0.5670
2023-11-08 12:52:55,481 - __main__ - INFO - Fold 4, mse = 614.7725, mad = 19.8201
2023-11-08 12:52:56,462 - __main__ - INFO - Fold 5 Epoch 0 Batch 0: Train Loss = 1.0865
2023-11-08 12:53:13,713 - __main__ - INFO - Fold 5, epoch 0: Loss = 1.0831 Valid loss = 1.0052 MSE = 703.7173
2023-11-08 12:53:13,714 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 703.7173 ------------
2023-11-08 12:53:14,029 - __main__ - INFO - Fold 5, mse = 703.7173, mad = 21.5841
2023-11-08 12:53:14,812 - __main__ - INFO - Fold 5 Epoch 1 Batch 0: Train Loss = 1.0204
2023-11-08 12:53:32,249 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 702.4213 ------------
2023-11-08 12:53:32,555 - __main__ - INFO - Fold 5, mse = 702.4213, mad = 21.4313
2023-11-08 12:53:33,255 - __main__ - INFO - Fold 5 Epoch 2 Batch 0: Train Loss = 0.9203
2023-11-08 12:53:50,370 - __main__ - INFO - Fold 5, mse = 708.8049, mad = 21.2969
2023-11-08 12:53:51,074 - __main__ - INFO - Fold 5 Epoch 3 Batch 0: Train Loss = 0.9560
2023-11-08 12:54:08,619 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 700.6392 ------------
2023-11-08 12:54:08,895 - __main__ - INFO - Fold 5, mse = 700.6392, mad = 21.3038
2023-11-08 12:54:09,566 - __main__ - INFO - Fold 5 Epoch 4 Batch 0: Train Loss = 0.7681
2023-11-08 12:54:27,054 - __main__ - INFO - Fold 5, mse = 703.0403, mad = 21.1962
2023-11-08 12:54:27,730 - __main__ - INFO - Fold 5 Epoch 5 Batch 0: Train Loss = 0.9001
2023-11-08 12:54:45,408 - __main__ - INFO - Fold 5, mse = 706.1433, mad = 21.1103
2023-11-08 12:54:46,099 - __main__ - INFO - Fold 5 Epoch 6 Batch 0: Train Loss = 0.7678
2023-11-08 12:55:03,232 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 700.1101 ------------
2023-11-08 12:55:03,412 - __main__ - INFO - Fold 5, mse = 700.1101, mad = 21.1189
2023-11-08 12:55:04,029 - __main__ - INFO - Fold 5 Epoch 7 Batch 0: Train Loss = 0.6533
2023-11-08 12:55:21,334 - __main__ - INFO - Fold 5, mse = 715.8459, mad = 21.2745
2023-11-08 12:55:21,980 - __main__ - INFO - Fold 5 Epoch 8 Batch 0: Train Loss = 0.8462
2023-11-08 12:55:39,453 - __main__ - INFO - Fold 5, mse = 710.1907, mad = 21.1884
2023-11-08 12:55:40,104 - __main__ - INFO - Fold 5 Epoch 9 Batch 0: Train Loss = 0.7482
2023-11-08 12:55:57,426 - __main__ - INFO - Fold 5, mse = 700.8958, mad = 21.0393
2023-11-08 12:55:58,037 - __main__ - INFO - Fold 5 Epoch 10 Batch 0: Train Loss = 0.6576
2023-11-08 12:56:15,636 - __main__ - INFO - Fold 5, epoch 10: Loss = 0.6621 Valid loss = 1.0203 MSE = 721.4928
2023-11-08 12:56:15,642 - __main__ - INFO - Fold 5, mse = 721.4928, mad = 21.1270
2023-11-08 12:56:16,408 - __main__ - INFO - Fold 5 Epoch 11 Batch 0: Train Loss = 0.6166
2023-11-08 12:56:33,874 - __main__ - INFO - Fold 5, mse = 732.6135, mad = 21.2214
2023-11-08 12:56:34,406 - __main__ - INFO - Fold 5 Epoch 12 Batch 0: Train Loss = 0.6363
2023-11-08 12:56:51,965 - __main__ - INFO - Fold 5, mse = 737.3930, mad = 21.4945
2023-11-08 12:56:52,526 - __main__ - INFO - Fold 5 Epoch 13 Batch 0: Train Loss = 0.5853
2023-11-08 12:57:11,769 - __main__ - INFO - Fold 5, mse = 756.6128, mad = 21.5674
2023-11-08 12:57:12,507 - __main__ - INFO - Fold 5 Epoch 14 Batch 0: Train Loss = 0.6590
2023-11-08 12:57:30,861 - __main__ - INFO - Fold 5, mse = 745.5861, mad = 21.5117
2023-11-08 12:57:31,679 - __main__ - INFO - Fold 5 Epoch 15 Batch 0: Train Loss = 0.5068
2023-11-08 12:57:50,480 - __main__ - INFO - Fold 5, mse = 744.0418, mad = 21.5135
2023-11-08 12:57:51,238 - __main__ - INFO - Fold 5 Epoch 16 Batch 0: Train Loss = 0.6167
2023-11-08 12:58:09,347 - __main__ - INFO - Fold 5, mse = 731.5574, mad = 21.3681
2023-11-08 12:58:10,165 - __main__ - INFO - Fold 5 Epoch 17 Batch 0: Train Loss = 0.5997
2023-11-08 12:58:27,619 - __main__ - INFO - Fold 5, mse = 750.1408, mad = 21.5783
2023-11-08 12:58:28,459 - __main__ - INFO - Fold 5 Epoch 18 Batch 0: Train Loss = 0.5668
2023-11-08 12:58:45,625 - __main__ - INFO - Fold 5, mse = 747.2490, mad = 21.4800
2023-11-08 12:58:46,396 - __main__ - INFO - Fold 5 Epoch 19 Batch 0: Train Loss = 0.6195
2023-11-08 12:59:03,792 - __main__ - INFO - Fold 5, mse = 748.3655, mad = 21.5507
2023-11-08 12:59:04,576 - __main__ - INFO - Fold 5 Epoch 20 Batch 0: Train Loss = 0.5016
2023-11-08 12:59:22,669 - __main__ - INFO - Fold 5, epoch 20: Loss = 0.5849 Valid loss = 1.0911 MSE = 770.5144
2023-11-08 12:59:22,671 - __main__ - INFO - Fold 5, mse = 770.5144, mad = 21.7932
2023-11-08 12:59:23,249 - __main__ - INFO - Fold 5 Epoch 21 Batch 0: Train Loss = 0.5604
2023-11-08 12:59:41,067 - __main__ - INFO - Fold 5, mse = 777.7993, mad = 21.6911
2023-11-08 12:59:41,817 - __main__ - INFO - Fold 5 Epoch 22 Batch 0: Train Loss = 0.5719
2023-11-08 12:59:59,518 - __main__ - INFO - Fold 5, mse = 791.0294, mad = 21.9275
2023-11-08 13:00:00,207 - __main__ - INFO - Fold 5 Epoch 23 Batch 0: Train Loss = 0.5023
2023-11-08 13:00:18,279 - __main__ - INFO - Fold 5, mse = 730.1051, mad = 21.3655
2023-11-08 13:00:19,029 - __main__ - INFO - Fold 5 Epoch 24 Batch 0: Train Loss = 0.5531
2023-11-08 13:00:37,434 - __main__ - INFO - Fold 5, mse = 773.8955, mad = 21.6716
2023-11-08 13:00:38,123 - __main__ - INFO - Fold 5 Epoch 25 Batch 0: Train Loss = 0.5369
2023-11-08 13:00:55,355 - __main__ - INFO - Fold 5, mse = 744.8016, mad = 21.4541
2023-11-08 13:00:56,060 - __main__ - INFO - Fold 5 Epoch 26 Batch 0: Train Loss = 0.5858
2023-11-08 13:01:13,262 - __main__ - INFO - Fold 5, mse = 744.9878, mad = 21.3738
2023-11-08 13:01:13,883 - __main__ - INFO - Fold 5 Epoch 27 Batch 0: Train Loss = 0.5898
2023-11-08 13:01:31,611 - __main__ - INFO - Fold 5, mse = 757.8248, mad = 21.5721
2023-11-08 13:01:32,271 - __main__ - INFO - Fold 5 Epoch 28 Batch 0: Train Loss = 0.5531
2023-11-08 13:01:49,601 - __main__ - INFO - Fold 5, mse = 803.6397, mad = 22.0569
2023-11-08 13:01:50,356 - __main__ - INFO - Fold 5 Epoch 29 Batch 0: Train Loss = 0.6042
2023-11-08 13:02:06,994 - __main__ - INFO - Fold 5, mse = 727.7618, mad = 21.3066
2023-11-08 13:02:06,997 - __main__ - INFO - mse 667.5356(37.8319)
2023-11-08 13:02:06,998 - __main__ - INFO - mad 20.8005(0.6788)
2023-11-08 13:02:07,007 - __main__ - INFO - mse 667.5356(37.8319)
2023-11-08 13:02:07,008 - __main__ - INFO - mad 20.8005(0.6788)
