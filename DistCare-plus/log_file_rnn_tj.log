2023-11-08 00:38:40,914 - __main__ - INFO - 这是希望输出的info内容
2023-11-08 00:38:40,915 - __main__ - WARNING - 这是希望输出的warning内容
2023-11-08 00:39:00,595 - __main__ - INFO - [[-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 1.2605692444279462, 0.585371321489137, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.9546043520029015, 0.2987563180527675, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 1.0769903089729194, 0.4420638197709522, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.566534136852991, 1.803485086093707, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, 0.46506052412282983, -0.05951243624269434, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, 0.281481588667803, -0.2744736888199714, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.8934113735178925, 0.9436400757845987, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592]]
2023-11-08 00:39:00,600 - __main__ - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
2023-11-08 00:39:00,603 - __main__ - INFO - 110609
2023-11-08 00:39:00,605 - __main__ - INFO - [[-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8962118618028821, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8617355345389544, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8272592072750267, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7927828800110989, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7583065527471711, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7238302254832434, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6893538982193156, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6548775709553878, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.62040124369146, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5859249164275323, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5514485891636045, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5169722618996767, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.48249593463574897, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.4480196073718212, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.41354328010789343, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3790669528439657, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3445906255800379, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.31011429831611015, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.27563797105218235, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.2411616437882546, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.20668531652432684, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.17220898926039907, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.1377326619964713, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.10325633473254354, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.06878000746861578, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.034303680204688006, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.0001726470592397616, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.034648974323167527, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.06912530158709529, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.10360162885102306, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.13807795611495083, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.1725542833788786, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.20703061064280637, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.24150693790673414, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.2759832651706619, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.31045959243458965, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.34493591969851745, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.3794122469624452, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.413888574226373, '0']]
2023-11-08 00:39:20,367 - __main__ - INFO - [[-0.32211134163582006, -0.06645805008034258, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, -0.14487324959363315, -0.0899704549996704, -0.14828727498338712, -0.24435830491350327, -0.34587251014597875, 0.014295447077977357, -0.05532679134287014, -0.1935716453287135, -0.3766591890459441, -0.01724690479013476, 0.3051487380880145, -0.33684497792590695, -0.3160730875843661, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.32211134163582006, -0.06645805008034258, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, -0.14487324959363315, -0.0899704549996704, -0.14828727498338712, -0.24435830491350327, -0.34587251014597875, 0.014295447077977357, -0.05532679134287014, -0.1935716453287135, -0.3766591890459441, -0.01724690479013476, 0.3051487380880145, -0.33684497792590695, -0.3160730875843661, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.32211134163582006, -0.06645805008034258, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, -0.14487324959363315, -0.0899704549996704, -0.14828727498338712, -0.24435830491350327, -0.34587251014597875, 0.014295447077977357, -0.05532679134287014, -0.1935716453287135, -0.3766591890459441, -0.01724690479013476, 0.3051487380880145, -0.33684497792590695, -0.3160730875843661, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 1.2605692444279462, 0.585371321489137, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.9546043520029015, 0.2987563180527675, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 1.0769903089729194, 0.4420638197709522, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.566534136852991, 1.803485086093707, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, 0.46506052412282983, -0.05951243624269434, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, 0.281481588667803, -0.2744736888199714, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.6223326938143058, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.8934113735178925, 0.9436400757845987, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.5053978624091442, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.5053978624091442, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.5053978624091442, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.5053978624091442, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.7268083069317782, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2603960082428797, -0.1992256677835492, -0.3293770132508767, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, -0.36678162470457243, -0.31050647673387505, -0.3766591890459441, -0.01724690479013476, 0.26404955735950336, -0.33684497792590695, -0.2606896206457801, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592], [-0.3333992081010445, -0.16805039728825624, 0.029264930048844468, -0.2371260510881844, -0.2815944741293343, 0.2893438123740592, 0.2742135876132575, -0.17415723759302862, -0.24435830491350327, -0.49591584998532545, 0.014295447077977357, 0.10040062533798204, -0.7002892480844135, -0.3766591890459441, -0.01724690479013476, 0.4695454610020563, -0.33684497792590695, -0.34930316774751763, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.004930129148398766, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592]]
2023-11-08 00:39:20,374 - __main__ - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
2023-11-08 00:39:20,375 - __main__ - INFO - 34
2023-11-08 00:39:22,087 - __main__ - INFO - 32269
2023-11-08 00:39:22,089 - __main__ - INFO - 4034
2023-11-08 00:39:22,090 - __main__ - INFO - 4033
2023-11-08 00:39:30,245 - __main__ - INFO - Training Teacher
2023-11-08 00:39:31,651 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 0.6234
2023-11-08 00:39:45,802 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 0.3011
2023-11-08 00:39:59,372 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 0.2484
2023-11-08 00:40:13,377 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 0.2500
2023-11-08 00:40:27,925 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 0.2759
2023-11-08 00:40:43,076 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 0.2981
2023-11-08 00:40:58,902 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 0.2812
2023-11-08 00:41:11,556 - __main__ - INFO - Epoch 0: Loss = 0.2920 Valid loss = 0.2379 roc = 0.7863
2023-11-08 00:41:12,509 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 0.1618
2023-11-08 00:41:28,075 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 0.2566
2023-11-08 00:41:43,757 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 0.2770
2023-11-08 00:41:58,440 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 0.2508
2023-11-08 00:42:13,674 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 0.2605
2023-11-08 00:42:28,010 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 0.2520
2023-11-08 00:42:44,411 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 0.2108
2023-11-08 00:42:55,901 - __main__ - INFO - Epoch 1: Loss = 0.2488 Valid loss = 0.2292 roc = 0.8102
2023-11-08 00:42:56,947 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 0.1830
2023-11-08 00:43:12,361 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 0.2319
2023-11-08 00:43:27,780 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 0.3637
2023-11-08 00:43:42,695 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 0.1952
2023-11-08 00:43:58,208 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 0.2652
2023-11-08 00:44:14,081 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 0.2824
2023-11-08 00:44:28,905 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 0.2039
2023-11-08 00:44:40,236 - __main__ - INFO - Epoch 2: Loss = 0.2457 Valid loss = 0.2281 roc = 0.8081
2023-11-08 00:44:41,089 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 0.2638
2023-11-08 00:44:56,093 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 0.3028
2023-11-08 00:45:11,098 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 0.2515
2023-11-08 00:45:26,170 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 0.2761
2023-11-08 00:45:41,991 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 0.2669
2023-11-08 00:45:58,437 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 0.2910
2023-11-08 00:46:14,311 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 0.1739
2023-11-08 00:46:26,497 - __main__ - INFO - Epoch 3: Loss = 0.2502 Valid loss = 0.2280 roc = 0.7989
2023-11-08 00:46:27,362 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 0.2202
2023-11-08 00:46:43,862 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 0.2368
2023-11-08 00:46:58,634 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 0.2542
2023-11-08 00:47:13,752 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 0.2868
2023-11-08 00:47:29,373 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 0.2753
2023-11-08 00:47:44,054 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 0.2488
2023-11-08 00:47:57,571 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 0.2818
2023-11-08 00:48:09,858 - __main__ - INFO - Epoch 4: Loss = 0.2471 Valid loss = 0.2260 roc = 0.8121
2023-11-08 00:48:10,872 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 0.2726
2023-11-08 00:48:27,002 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 0.2626
2023-11-08 00:48:41,275 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 0.2783
2023-11-08 00:48:56,933 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 0.2419
2023-11-08 00:49:12,473 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 0.2306
2023-11-08 00:49:27,833 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 0.2154
2023-11-08 00:49:42,545 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 0.1929
2023-11-08 00:49:55,209 - __main__ - INFO - Epoch 5: Loss = 0.2424 Valid loss = 0.2218 roc = 0.8314
2023-11-08 00:49:56,052 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 0.2900
2023-11-08 00:50:11,153 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 0.2488
2023-11-08 00:50:26,188 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 0.2655
2023-11-08 00:50:40,793 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 0.2421
2023-11-08 00:50:57,018 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 0.3697
2023-11-08 00:51:12,668 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 0.2502
2023-11-08 00:51:29,001 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 0.2183
2023-11-08 00:51:40,888 - __main__ - INFO - Epoch 6: Loss = 0.2411 Valid loss = 0.2197 roc = 0.8401
2023-11-08 00:51:41,764 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 0.2591
2023-11-08 00:51:57,984 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 0.2219
2023-11-08 00:52:12,043 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 0.2507
2023-11-08 00:52:26,942 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 0.2155
2023-11-08 00:52:42,131 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 0.2239
2023-11-08 00:52:57,757 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 0.2600
2023-11-08 00:53:12,504 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 0.2767
2023-11-08 00:53:23,564 - __main__ - INFO - Epoch 7: Loss = 0.2396 Valid loss = 0.2214 roc = 0.8392
2023-11-08 00:53:24,206 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 0.2431
2023-11-08 00:53:39,590 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 0.1738
2023-11-08 00:53:55,067 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 0.2200
2023-11-08 00:54:09,640 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 0.1721
2023-11-08 00:54:23,791 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 0.2293
2023-11-08 00:54:39,647 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 0.2322
2023-11-08 00:54:54,293 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 0.2691
2023-11-08 00:55:05,490 - __main__ - INFO - Epoch 8: Loss = 0.2392 Valid loss = 0.2189 roc = 0.8359
2023-11-08 00:55:06,156 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 0.2682
2023-11-08 00:55:21,919 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 0.2678
2023-11-08 00:55:36,571 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 0.2370
2023-11-08 00:55:52,714 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 0.2483
2023-11-08 00:56:09,584 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 0.2495
2023-11-08 00:56:23,989 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 0.2213
2023-11-08 00:56:38,988 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 0.1632
2023-11-08 00:56:51,193 - __main__ - INFO - Epoch 9: Loss = 0.2422 Valid loss = 0.2211 roc = 0.8357
2023-11-08 00:56:52,313 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 0.1914
2023-11-08 00:57:07,834 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 0.3244
2023-11-08 00:57:23,708 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 0.1665
2023-11-08 00:57:39,189 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 0.2940
2023-11-08 00:57:52,987 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 0.2736
2023-11-08 00:58:08,182 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 0.1880
2023-11-08 00:58:23,747 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 0.2509
2023-11-08 00:58:35,546 - __main__ - INFO - Epoch 10: Loss = 0.2374 Valid loss = 0.2173 roc = 0.8346
2023-11-08 00:58:36,302 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 0.2010
2023-11-08 00:58:49,459 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 0.2698
2023-11-08 00:59:04,968 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 0.2951
2023-11-08 00:59:20,178 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 0.1879
2023-11-08 00:59:34,883 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 0.2521
2023-11-08 00:59:51,032 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 0.2413
2023-11-08 01:00:06,844 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 0.2241
2023-11-08 01:00:18,825 - __main__ - INFO - Epoch 11: Loss = 0.2389 Valid loss = 0.2122 roc = 0.8387
2023-11-08 01:00:19,739 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 0.3031
2023-11-08 01:00:34,962 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 0.1877
2023-11-08 01:00:49,639 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 0.2277
2023-11-08 01:01:05,515 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 0.2924
2023-11-08 01:01:20,491 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 0.2086
2023-11-08 01:01:36,246 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 0.2601
2023-11-08 01:01:50,965 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 0.2228
2023-11-08 01:02:03,086 - __main__ - INFO - Epoch 12: Loss = 0.2435 Valid loss = 0.2156 roc = 0.8339
2023-11-08 01:02:03,675 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 0.2145
2023-11-08 01:02:18,555 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 0.2481
2023-11-08 01:02:33,907 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 0.2536
2023-11-08 01:02:49,818 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 0.2429
2023-11-08 01:03:04,913 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 0.2437
2023-11-08 01:03:20,494 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 0.2680
2023-11-08 01:03:35,530 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 0.2616
2023-11-08 01:03:47,209 - __main__ - INFO - Epoch 13: Loss = 0.2414 Valid loss = 0.2155 roc = 0.8406
2023-11-08 01:03:48,052 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 0.2180
2023-11-08 01:04:02,496 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 0.3112
2023-11-08 01:04:16,882 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 0.2045
2023-11-08 01:04:31,657 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 0.2601
2023-11-08 01:04:47,888 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 0.2683
2023-11-08 01:05:03,752 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 0.2355
2023-11-08 01:05:18,953 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 0.1851
2023-11-08 01:05:30,287 - __main__ - INFO - Epoch 14: Loss = 0.2380 Valid loss = 0.2154 roc = 0.8411
2023-11-08 01:05:31,111 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 0.2991
2023-11-08 01:05:46,784 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 0.2928
2023-11-08 01:06:01,598 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 0.2727
2023-11-08 01:06:16,125 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 0.2363
2023-11-08 01:06:31,497 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 0.1860
2023-11-08 01:06:46,121 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 0.2760
2023-11-08 01:07:01,898 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 0.1983
2023-11-08 01:07:13,230 - __main__ - INFO - Epoch 15: Loss = 0.2367 Valid loss = 0.2165 roc = 0.8405
2023-11-08 01:07:14,165 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 0.2865
2023-11-08 01:07:29,920 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 0.2434
2023-11-08 01:07:44,626 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 0.2186
2023-11-08 01:07:59,071 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 0.1932
2023-11-08 01:08:14,759 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 0.2847
2023-11-08 01:08:29,026 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 0.1758
2023-11-08 01:08:44,162 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 0.2417
2023-11-08 01:08:55,849 - __main__ - INFO - Epoch 16: Loss = 0.2383 Valid loss = 0.2111 roc = 0.8488
2023-11-08 01:08:56,631 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 0.2259
2023-11-08 01:09:13,007 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 0.2397
2023-11-08 01:09:28,780 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 0.1944
2023-11-08 01:09:43,803 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 0.2292
2023-11-08 01:09:58,057 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 0.1726
2023-11-08 01:10:14,050 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 0.2183
2023-11-08 01:10:29,352 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 0.2340
2023-11-08 01:10:41,578 - __main__ - INFO - Epoch 17: Loss = 0.2366 Valid loss = 0.2083 roc = 0.8522
2023-11-08 01:10:42,248 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 0.3102
2023-11-08 01:10:55,956 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 0.3408
2023-11-08 01:11:11,759 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 0.1611
2023-11-08 01:11:26,855 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 0.2568
2023-11-08 01:11:41,091 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 0.1905
2023-11-08 01:11:57,821 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 0.2057
2023-11-08 01:12:14,650 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 0.2232
2023-11-08 01:12:28,930 - __main__ - INFO - Epoch 18: Loss = 0.2289 Valid loss = 0.2092 roc = 0.8447
2023-11-08 01:12:29,635 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 0.2401
2023-11-08 01:12:45,175 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 0.2254
2023-11-08 01:13:00,658 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 0.2384
2023-11-08 01:13:16,628 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 0.2326
2023-11-08 01:13:33,855 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 0.2243
2023-11-08 01:13:50,250 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 0.2288
2023-11-08 01:14:08,533 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 0.2637
2023-11-08 01:14:21,085 - __main__ - INFO - Epoch 19: Loss = 0.2284 Valid loss = 0.2038 roc = 0.8443
2023-11-08 01:14:21,824 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 0.2590
2023-11-08 01:14:38,251 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 0.2153
2023-11-08 01:14:53,585 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 0.1915
2023-11-08 01:15:11,527 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 0.2852
2023-11-08 01:15:29,197 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 0.2467
2023-11-08 01:15:46,790 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 0.2180
2023-11-08 01:16:03,303 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 0.1896
2023-11-08 01:16:13,906 - __main__ - INFO - Epoch 20: Loss = 0.2318 Valid loss = 0.2090 roc = 0.8371
2023-11-08 01:16:14,571 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 0.2266
2023-11-08 01:16:30,378 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 0.2743
2023-11-08 01:16:46,145 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 0.1745
2023-11-08 01:17:01,152 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 0.2097
2023-11-08 01:17:15,983 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 0.2401
2023-11-08 01:17:31,750 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 0.2041
2023-11-08 01:17:48,011 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 0.3182
2023-11-08 01:18:00,046 - __main__ - INFO - Epoch 21: Loss = 0.2281 Valid loss = 0.2044 roc = 0.8472
2023-11-08 01:18:00,754 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 0.3023
2023-11-08 01:18:17,030 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 0.2337
2023-11-08 01:18:33,121 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 0.1658
2023-11-08 01:18:48,644 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 0.2133
2023-11-08 01:19:04,359 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 0.2165
2023-11-08 01:19:20,987 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 0.1831
2023-11-08 01:19:36,829 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 0.2275
2023-11-08 01:19:49,938 - __main__ - INFO - Epoch 22: Loss = 0.2257 Valid loss = 0.2066 roc = 0.8402
2023-11-08 01:19:50,704 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 0.2288
2023-11-08 01:20:08,075 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 0.2410
2023-11-08 01:20:25,106 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 0.2140
2023-11-08 01:20:41,419 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 0.2762
2023-11-08 01:20:58,447 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 0.2835
2023-11-08 01:21:14,544 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 0.2500
2023-11-08 01:21:29,488 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 0.2335
2023-11-08 01:21:41,851 - __main__ - INFO - Epoch 23: Loss = 0.2277 Valid loss = 0.2045 roc = 0.8451
2023-11-08 01:21:42,612 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 0.2284
2023-11-08 01:21:58,562 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 0.1912
2023-11-08 01:22:13,770 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 0.2159
2023-11-08 01:22:29,545 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 0.2967
2023-11-08 01:22:44,325 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 0.2656
2023-11-08 01:22:58,672 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 0.2405
2023-11-08 01:23:13,247 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 0.2573
2023-11-08 01:23:25,084 - __main__ - INFO - Epoch 24: Loss = 0.2236 Valid loss = 0.2003 roc = 0.8567
2023-11-08 01:23:26,104 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 0.2072
2023-11-08 01:23:41,271 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 0.2609
2023-11-08 01:23:57,068 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 0.2144
2023-11-08 01:24:11,949 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 0.2000
2023-11-08 01:24:27,623 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 0.2594
2023-11-08 01:24:43,043 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 0.2303
2023-11-08 01:24:59,175 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 0.1565
2023-11-08 01:25:11,835 - __main__ - INFO - Epoch 25: Loss = 0.2246 Valid loss = 0.2018 roc = 0.8509
2023-11-08 01:25:12,569 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 0.1807
2023-11-08 01:25:28,829 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 0.1592
2023-11-08 01:25:44,483 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 0.2061
2023-11-08 01:25:59,633 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 0.1628
2023-11-08 01:26:14,408 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 0.2749
2023-11-08 01:26:29,170 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 0.1924
2023-11-08 01:26:45,424 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 0.2718
2023-11-08 01:26:57,570 - __main__ - INFO - Epoch 26: Loss = 0.2291 Valid loss = 0.2004 roc = 0.8528
2023-11-08 01:26:58,294 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 0.2166
2023-11-08 01:27:13,527 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 0.2000
2023-11-08 01:27:29,506 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 0.1798
2023-11-08 01:27:45,289 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 0.1578
2023-11-08 01:27:59,918 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 0.2216
2023-11-08 01:28:14,708 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 0.2263
2023-11-08 01:28:30,037 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 0.2296
2023-11-08 01:28:41,549 - __main__ - INFO - Epoch 27: Loss = 0.2203 Valid loss = 0.2010 roc = 0.8573
2023-11-08 01:28:42,536 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 0.2354
2023-11-08 01:28:58,282 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 0.1226
2023-11-08 01:29:12,762 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 0.2432
2023-11-08 01:29:28,141 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 0.1987
2023-11-08 01:29:43,955 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 0.3035
2023-11-08 01:29:58,810 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 0.2307
2023-11-08 01:30:16,775 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 0.2493
2023-11-08 01:30:28,379 - __main__ - INFO - Epoch 28: Loss = 0.2221 Valid loss = 0.1990 roc = 0.8560
2023-11-08 01:30:29,094 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 0.2128
2023-11-08 01:30:45,207 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 0.2840
2023-11-08 01:31:01,220 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 0.2593
2023-11-08 01:31:17,585 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 0.2388
2023-11-08 01:31:34,356 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 0.2241
2023-11-08 01:31:50,691 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 0.1453
2023-11-08 01:32:08,519 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 0.1865
2023-11-08 01:32:21,570 - __main__ - INFO - Epoch 29: Loss = 0.2229 Valid loss = 0.1999 roc = 0.8546
2023-11-08 01:32:22,360 - __main__ - INFO - Epoch 30 Batch 0: Train Loss = 0.2037
2023-11-08 01:32:40,775 - __main__ - INFO - Epoch 30 Batch 20: Train Loss = 0.2732
2023-11-08 01:32:57,341 - __main__ - INFO - Epoch 30 Batch 40: Train Loss = 0.2067
2023-11-08 01:33:15,341 - __main__ - INFO - Epoch 30 Batch 60: Train Loss = 0.1743
2023-11-08 01:33:33,806 - __main__ - INFO - Epoch 30 Batch 80: Train Loss = 0.2343
2023-11-08 01:33:51,006 - __main__ - INFO - Epoch 30 Batch 100: Train Loss = 0.2349
2023-11-08 01:34:08,355 - __main__ - INFO - Epoch 30 Batch 120: Train Loss = 0.2463
2023-11-08 01:34:22,182 - __main__ - INFO - Epoch 30: Loss = 0.2177 Valid loss = 0.2010 roc = 0.8597
2023-11-08 01:34:23,039 - __main__ - INFO - Epoch 31 Batch 0: Train Loss = 0.2632
2023-11-08 01:34:40,117 - __main__ - INFO - Epoch 31 Batch 20: Train Loss = 0.2365
2023-11-08 01:34:56,878 - __main__ - INFO - Epoch 31 Batch 40: Train Loss = 0.2885
2023-11-08 01:35:14,121 - __main__ - INFO - Epoch 31 Batch 60: Train Loss = 0.1907
2023-11-08 01:35:30,925 - __main__ - INFO - Epoch 31 Batch 80: Train Loss = 0.1597
2023-11-08 01:35:47,261 - __main__ - INFO - Epoch 31 Batch 100: Train Loss = 0.2495
2023-11-08 01:36:04,405 - __main__ - INFO - Epoch 31 Batch 120: Train Loss = 0.1539
2023-11-08 01:36:17,869 - __main__ - INFO - Epoch 31: Loss = 0.2204 Valid loss = 0.2000 roc = 0.8540
2023-11-08 01:36:18,562 - __main__ - INFO - Epoch 32 Batch 0: Train Loss = 0.1671
2023-11-08 01:36:37,434 - __main__ - INFO - Epoch 32 Batch 20: Train Loss = 0.2104
2023-11-08 01:36:54,594 - __main__ - INFO - Epoch 32 Batch 40: Train Loss = 0.2058
2023-11-08 01:37:10,635 - __main__ - INFO - Epoch 32 Batch 60: Train Loss = 0.2316
2023-11-08 01:37:28,488 - __main__ - INFO - Epoch 32 Batch 80: Train Loss = 0.2354
2023-11-08 01:37:46,744 - __main__ - INFO - Epoch 32 Batch 100: Train Loss = 0.2088
2023-11-08 01:38:04,088 - __main__ - INFO - Epoch 32 Batch 120: Train Loss = 0.2135
2023-11-08 01:38:16,926 - __main__ - INFO - Epoch 32: Loss = 0.2196 Valid loss = 0.1988 roc = 0.8580
2023-11-08 01:38:17,807 - __main__ - INFO - Epoch 33 Batch 0: Train Loss = 0.1501
2023-11-08 01:38:36,110 - __main__ - INFO - Epoch 33 Batch 20: Train Loss = 0.2448
2023-11-08 01:38:52,144 - __main__ - INFO - Epoch 33 Batch 40: Train Loss = 0.1931
2023-11-08 01:39:09,887 - __main__ - INFO - Epoch 33 Batch 60: Train Loss = 0.2531
2023-11-08 01:39:29,670 - __main__ - INFO - Epoch 33 Batch 80: Train Loss = 0.2380
2023-11-08 01:39:46,441 - __main__ - INFO - Epoch 33 Batch 100: Train Loss = 0.1698
2023-11-08 01:40:03,555 - __main__ - INFO - Epoch 33 Batch 120: Train Loss = 0.2268
2023-11-08 01:40:17,188 - __main__ - INFO - Epoch 33: Loss = 0.2219 Valid loss = 0.2027 roc = 0.8561
2023-11-08 01:40:18,120 - __main__ - INFO - Epoch 34 Batch 0: Train Loss = 0.1732
2023-11-08 01:40:37,470 - __main__ - INFO - Epoch 34 Batch 20: Train Loss = 0.3188
2023-11-08 01:40:54,181 - __main__ - INFO - Epoch 34 Batch 40: Train Loss = 0.1682
2023-11-08 01:41:11,717 - __main__ - INFO - Epoch 34 Batch 60: Train Loss = 0.1905
2023-11-08 01:41:29,599 - __main__ - INFO - Epoch 34 Batch 80: Train Loss = 0.2354
2023-11-08 01:41:47,134 - __main__ - INFO - Epoch 34 Batch 100: Train Loss = 0.2513
2023-11-08 01:42:05,143 - __main__ - INFO - Epoch 34 Batch 120: Train Loss = 0.1923
2023-11-08 01:42:18,787 - __main__ - INFO - Epoch 34: Loss = 0.2217 Valid loss = 0.2002 roc = 0.8530
2023-11-08 01:42:19,529 - __main__ - INFO - Epoch 35 Batch 0: Train Loss = 0.2917
2023-11-08 01:42:36,635 - __main__ - INFO - Epoch 35 Batch 20: Train Loss = 0.2515
2023-11-08 01:42:53,237 - __main__ - INFO - Epoch 35 Batch 40: Train Loss = 0.2819
2023-11-08 01:43:10,801 - __main__ - INFO - Epoch 35 Batch 60: Train Loss = 0.2050
2023-11-08 01:43:28,151 - __main__ - INFO - Epoch 35 Batch 80: Train Loss = 0.1950
2023-11-08 01:43:46,274 - __main__ - INFO - Epoch 35 Batch 100: Train Loss = 0.2131
2023-11-08 01:44:03,004 - __main__ - INFO - Epoch 35 Batch 120: Train Loss = 0.2195
2023-11-08 01:44:16,777 - __main__ - INFO - Epoch 35: Loss = 0.2234 Valid loss = 0.2004 roc = 0.8564
2023-11-08 01:44:17,641 - __main__ - INFO - Epoch 36 Batch 0: Train Loss = 0.1338
2023-11-08 01:44:34,096 - __main__ - INFO - Epoch 36 Batch 20: Train Loss = 0.3087
2023-11-08 01:44:51,355 - __main__ - INFO - Epoch 36 Batch 40: Train Loss = 0.2050
2023-11-08 01:45:08,628 - __main__ - INFO - Epoch 36 Batch 60: Train Loss = 0.2562
2023-11-08 01:45:26,081 - __main__ - INFO - Epoch 36 Batch 80: Train Loss = 0.2527
2023-11-08 01:45:42,899 - __main__ - INFO - Epoch 36 Batch 100: Train Loss = 0.2047
2023-11-08 01:46:00,369 - __main__ - INFO - Epoch 36 Batch 120: Train Loss = 0.2745
2023-11-08 01:46:14,848 - __main__ - INFO - Epoch 36: Loss = 0.2197 Valid loss = 0.1987 roc = 0.8603
2023-11-08 01:46:15,814 - __main__ - INFO - Epoch 37 Batch 0: Train Loss = 0.1952
2023-11-08 01:46:34,442 - __main__ - INFO - Epoch 37 Batch 20: Train Loss = 0.2052
2023-11-08 01:46:52,331 - __main__ - INFO - Epoch 37 Batch 40: Train Loss = 0.1621
2023-11-08 01:47:10,363 - __main__ - INFO - Epoch 37 Batch 60: Train Loss = 0.2277
2023-11-08 01:47:27,669 - __main__ - INFO - Epoch 37 Batch 80: Train Loss = 0.2882
2023-11-08 01:47:45,346 - __main__ - INFO - Epoch 37 Batch 100: Train Loss = 0.1713
2023-11-08 01:48:01,602 - __main__ - INFO - Epoch 37 Batch 120: Train Loss = 0.2256
2023-11-08 01:48:15,150 - __main__ - INFO - Epoch 37: Loss = 0.2211 Valid loss = 0.1980 roc = 0.8573
2023-11-08 01:48:16,123 - __main__ - INFO - Epoch 38 Batch 0: Train Loss = 0.2368
2023-11-08 01:48:33,235 - __main__ - INFO - Epoch 38 Batch 20: Train Loss = 0.2737
2023-11-08 01:48:50,105 - __main__ - INFO - Epoch 38 Batch 40: Train Loss = 0.1882
2023-11-08 01:49:07,112 - __main__ - INFO - Epoch 38 Batch 60: Train Loss = 0.2315
2023-11-08 01:49:25,384 - __main__ - INFO - Epoch 38 Batch 80: Train Loss = 0.2383
2023-11-08 01:49:42,200 - __main__ - INFO - Epoch 38 Batch 100: Train Loss = 0.2023
2023-11-08 01:49:59,827 - __main__ - INFO - Epoch 38 Batch 120: Train Loss = 0.2497
2023-11-08 01:50:13,518 - __main__ - INFO - Epoch 38: Loss = 0.2302 Valid loss = 0.2045 roc = 0.8389
2023-11-08 01:50:14,248 - __main__ - INFO - Epoch 39 Batch 0: Train Loss = 0.2586
2023-11-08 01:50:32,402 - __main__ - INFO - Epoch 39 Batch 20: Train Loss = 0.2078
2023-11-08 01:50:49,563 - __main__ - INFO - Epoch 39 Batch 40: Train Loss = 0.2263
2023-11-08 01:51:05,376 - __main__ - INFO - Epoch 39 Batch 60: Train Loss = 0.1850
2023-11-08 01:51:23,118 - __main__ - INFO - Epoch 39 Batch 80: Train Loss = 0.2668
2023-11-08 01:51:39,802 - __main__ - INFO - Epoch 39 Batch 100: Train Loss = 0.2480
2023-11-08 01:51:56,890 - __main__ - INFO - Epoch 39 Batch 120: Train Loss = 0.2545
2023-11-08 01:52:10,718 - __main__ - INFO - Epoch 39: Loss = 0.2300 Valid loss = 0.2075 roc = 0.8430
2023-11-08 01:52:11,850 - __main__ - INFO - Epoch 40 Batch 0: Train Loss = 0.2462
2023-11-08 01:52:27,909 - __main__ - INFO - Epoch 40 Batch 20: Train Loss = 0.2241
2023-11-08 01:52:44,691 - __main__ - INFO - Epoch 40 Batch 40: Train Loss = 0.1582
2023-11-08 01:53:01,347 - __main__ - INFO - Epoch 40 Batch 60: Train Loss = 0.2582
2023-11-08 01:53:19,943 - __main__ - INFO - Epoch 40 Batch 80: Train Loss = 0.2010
2023-11-08 01:53:37,174 - __main__ - INFO - Epoch 40 Batch 100: Train Loss = 0.2169
2023-11-08 01:53:54,796 - __main__ - INFO - Epoch 40 Batch 120: Train Loss = 0.2105
2023-11-08 01:54:08,464 - __main__ - INFO - Epoch 40: Loss = 0.2285 Valid loss = 0.2043 roc = 0.8526
2023-11-08 01:54:09,307 - __main__ - INFO - Epoch 41 Batch 0: Train Loss = 0.2672
2023-11-08 01:54:25,982 - __main__ - INFO - Epoch 41 Batch 20: Train Loss = 0.2619
2023-11-08 01:54:43,591 - __main__ - INFO - Epoch 41 Batch 40: Train Loss = 0.1359
2023-11-08 01:55:01,855 - __main__ - INFO - Epoch 41 Batch 60: Train Loss = 0.2552
2023-11-08 01:55:18,221 - __main__ - INFO - Epoch 41 Batch 80: Train Loss = 0.2797
2023-11-08 01:55:33,552 - __main__ - INFO - Epoch 41 Batch 100: Train Loss = 0.1747
2023-11-08 01:55:52,364 - __main__ - INFO - Epoch 41 Batch 120: Train Loss = 0.2110
2023-11-08 01:56:06,270 - __main__ - INFO - Epoch 41: Loss = 0.2211 Valid loss = 0.1987 roc = 0.8577
2023-11-08 01:56:07,189 - __main__ - INFO - Epoch 42 Batch 0: Train Loss = 0.2620
2023-11-08 01:56:25,851 - __main__ - INFO - Epoch 42 Batch 20: Train Loss = 0.2667
2023-11-08 01:56:44,229 - __main__ - INFO - Epoch 42 Batch 40: Train Loss = 0.1861
2023-11-08 01:57:02,667 - __main__ - INFO - Epoch 42 Batch 60: Train Loss = 0.1911
2023-11-08 01:57:19,837 - __main__ - INFO - Epoch 42 Batch 80: Train Loss = 0.2230
2023-11-08 01:57:37,631 - __main__ - INFO - Epoch 42 Batch 100: Train Loss = 0.2240
2023-11-08 01:57:55,310 - __main__ - INFO - Epoch 42 Batch 120: Train Loss = 0.2719
2023-11-08 01:58:09,390 - __main__ - INFO - Epoch 42: Loss = 0.2206 Valid loss = 0.2001 roc = 0.8619
2023-11-08 01:58:10,268 - __main__ - INFO - Epoch 43 Batch 0: Train Loss = 0.2235
2023-11-08 01:58:26,991 - __main__ - INFO - Epoch 43 Batch 20: Train Loss = 0.2012
2023-11-08 01:58:45,179 - __main__ - INFO - Epoch 43 Batch 40: Train Loss = 0.1817
2023-11-08 01:59:03,614 - __main__ - INFO - Epoch 43 Batch 60: Train Loss = 0.1911
2023-11-08 01:59:21,317 - __main__ - INFO - Epoch 43 Batch 80: Train Loss = 0.1587
2023-11-08 01:59:39,591 - __main__ - INFO - Epoch 43 Batch 100: Train Loss = 0.2044
2023-11-08 01:59:56,503 - __main__ - INFO - Epoch 43 Batch 120: Train Loss = 0.2314
2023-11-08 02:00:09,049 - __main__ - INFO - Epoch 43: Loss = 0.2247 Valid loss = 0.2024 roc = 0.8552
2023-11-08 02:00:10,022 - __main__ - INFO - Epoch 44 Batch 0: Train Loss = 0.2603
2023-11-08 02:00:26,875 - __main__ - INFO - Epoch 44 Batch 20: Train Loss = 0.1927
2023-11-08 02:00:44,301 - __main__ - INFO - Epoch 44 Batch 40: Train Loss = 0.1745
2023-11-08 02:01:02,039 - __main__ - INFO - Epoch 44 Batch 60: Train Loss = 0.2511
2023-11-08 02:01:19,347 - __main__ - INFO - Epoch 44 Batch 80: Train Loss = 0.2002
2023-11-08 02:01:36,578 - __main__ - INFO - Epoch 44 Batch 100: Train Loss = 0.1878
2023-11-08 02:01:54,318 - __main__ - INFO - Epoch 44 Batch 120: Train Loss = 0.2663
2023-11-08 02:02:08,450 - __main__ - INFO - Epoch 44: Loss = 0.2231 Valid loss = 0.2007 roc = 0.8640
2023-11-08 02:02:09,315 - __main__ - INFO - Epoch 45 Batch 0: Train Loss = 0.2090
2023-11-08 02:02:27,788 - __main__ - INFO - Epoch 45 Batch 20: Train Loss = 0.2137
2023-11-08 02:02:45,520 - __main__ - INFO - Epoch 45 Batch 40: Train Loss = 0.1841
2023-11-08 02:03:02,425 - __main__ - INFO - Epoch 45 Batch 60: Train Loss = 0.2062
2023-11-08 02:03:19,642 - __main__ - INFO - Epoch 45 Batch 80: Train Loss = 0.2130
2023-11-08 02:03:36,948 - __main__ - INFO - Epoch 45 Batch 100: Train Loss = 0.2162
2023-11-08 02:03:54,165 - __main__ - INFO - Epoch 45 Batch 120: Train Loss = 0.2069
2023-11-08 02:04:07,507 - __main__ - INFO - Epoch 45: Loss = 0.2204 Valid loss = 0.2006 roc = 0.8589
2023-11-08 02:04:08,686 - __main__ - INFO - Epoch 46 Batch 0: Train Loss = 0.2240
2023-11-08 02:04:24,863 - __main__ - INFO - Epoch 46 Batch 20: Train Loss = 0.1579
2023-11-08 02:04:42,794 - __main__ - INFO - Epoch 46 Batch 40: Train Loss = 0.2457
2023-11-08 02:04:59,632 - __main__ - INFO - Epoch 46 Batch 60: Train Loss = 0.1996
2023-11-08 02:05:17,215 - __main__ - INFO - Epoch 46 Batch 80: Train Loss = 0.1495
2023-11-08 02:05:33,391 - __main__ - INFO - Epoch 46 Batch 100: Train Loss = 0.2222
2023-11-08 02:05:51,607 - __main__ - INFO - Epoch 46 Batch 120: Train Loss = 0.1962
2023-11-08 02:06:05,476 - __main__ - INFO - Epoch 46: Loss = 0.2195 Valid loss = 0.1984 roc = 0.8614
2023-11-08 02:06:06,601 - __main__ - INFO - Epoch 47 Batch 0: Train Loss = 0.2348
2023-11-08 02:06:22,319 - __main__ - INFO - Epoch 47 Batch 20: Train Loss = 0.1891
2023-11-08 02:06:38,998 - __main__ - INFO - Epoch 47 Batch 40: Train Loss = 0.2131
2023-11-08 02:06:55,741 - __main__ - INFO - Epoch 47 Batch 60: Train Loss = 0.1736
2023-11-08 02:07:13,706 - __main__ - INFO - Epoch 47 Batch 80: Train Loss = 0.2305
2023-11-08 02:07:31,941 - __main__ - INFO - Epoch 47 Batch 100: Train Loss = 0.1715
2023-11-08 02:07:49,221 - __main__ - INFO - Epoch 47 Batch 120: Train Loss = 0.1935
2023-11-08 02:08:01,913 - __main__ - INFO - Epoch 47: Loss = 0.2206 Valid loss = 0.2007 roc = 0.8583
2023-11-08 02:08:02,799 - __main__ - INFO - Epoch 48 Batch 0: Train Loss = 0.2032
2023-11-08 02:08:20,173 - __main__ - INFO - Epoch 48 Batch 20: Train Loss = 0.1881
2023-11-08 02:08:39,434 - __main__ - INFO - Epoch 48 Batch 40: Train Loss = 0.2202
2023-11-08 02:08:55,801 - __main__ - INFO - Epoch 48 Batch 60: Train Loss = 0.1958
2023-11-08 02:09:12,642 - __main__ - INFO - Epoch 48 Batch 80: Train Loss = 0.2217
2023-11-08 02:09:30,182 - __main__ - INFO - Epoch 48 Batch 100: Train Loss = 0.2193
2023-11-08 02:09:46,757 - __main__ - INFO - Epoch 48 Batch 120: Train Loss = 0.2069
2023-11-08 02:09:59,873 - __main__ - INFO - Epoch 48: Loss = 0.2244 Valid loss = 0.2005 roc = 0.8569
2023-11-08 02:10:00,711 - __main__ - INFO - Epoch 49 Batch 0: Train Loss = 0.1681
2023-11-08 02:10:18,868 - __main__ - INFO - Epoch 49 Batch 20: Train Loss = 0.2610
2023-11-08 02:10:36,203 - __main__ - INFO - Epoch 49 Batch 40: Train Loss = 0.2323
2023-11-08 02:10:53,056 - __main__ - INFO - Epoch 49 Batch 60: Train Loss = 0.1689
2023-11-08 02:11:11,055 - __main__ - INFO - Epoch 49 Batch 80: Train Loss = 0.2137
2023-11-08 02:11:27,661 - __main__ - INFO - Epoch 49 Batch 100: Train Loss = 0.1427
2023-11-08 02:11:44,752 - __main__ - INFO - Epoch 49 Batch 120: Train Loss = 0.2016
2023-11-08 02:11:58,409 - __main__ - INFO - Epoch 49: Loss = 0.2229 Valid loss = 0.1978 roc = 0.8581
2023-11-08 02:11:59,365 - __main__ - INFO - Epoch 50 Batch 0: Train Loss = 0.2483
2023-11-08 02:12:18,325 - __main__ - INFO - Epoch 50 Batch 20: Train Loss = 0.1650
2023-11-08 02:12:35,987 - __main__ - INFO - Epoch 50 Batch 40: Train Loss = 0.2006
2023-11-08 02:12:52,882 - __main__ - INFO - Epoch 50 Batch 60: Train Loss = 0.2595
2023-11-08 02:13:10,167 - __main__ - INFO - Epoch 50 Batch 80: Train Loss = 0.1739
2023-11-08 02:13:27,589 - __main__ - INFO - Epoch 50 Batch 100: Train Loss = 0.2095
2023-11-08 02:13:44,184 - __main__ - INFO - Epoch 50 Batch 120: Train Loss = 0.2086
2023-11-08 02:13:57,555 - __main__ - INFO - Epoch 50: Loss = 0.2201 Valid loss = 0.1997 roc = 0.8682
2023-11-08 02:13:58,285 - __main__ - INFO - Epoch 51 Batch 0: Train Loss = 0.2101
2023-11-08 02:14:15,475 - __main__ - INFO - Epoch 51 Batch 20: Train Loss = 0.2305
2023-11-08 02:14:33,392 - __main__ - INFO - Epoch 51 Batch 40: Train Loss = 0.2259
2023-11-08 02:14:51,022 - __main__ - INFO - Epoch 51 Batch 60: Train Loss = 0.2322
2023-11-08 02:15:08,147 - __main__ - INFO - Epoch 51 Batch 80: Train Loss = 0.2279
2023-11-08 02:15:24,883 - __main__ - INFO - Epoch 51 Batch 100: Train Loss = 0.2677
2023-11-08 02:15:42,641 - __main__ - INFO - Epoch 51 Batch 120: Train Loss = 0.1531
2023-11-08 02:15:55,840 - __main__ - INFO - Epoch 51: Loss = 0.2190 Valid loss = 0.1980 roc = 0.8654
2023-11-08 02:15:56,638 - __main__ - INFO - Epoch 52 Batch 0: Train Loss = 0.2008
2023-11-08 02:16:13,827 - __main__ - INFO - Epoch 52 Batch 20: Train Loss = 0.2390
2023-11-08 02:16:34,025 - __main__ - INFO - Epoch 52 Batch 40: Train Loss = 0.1863
2023-11-08 02:16:50,918 - __main__ - INFO - Epoch 52 Batch 60: Train Loss = 0.2473
2023-11-08 02:17:07,134 - __main__ - INFO - Epoch 52 Batch 80: Train Loss = 0.1907
2023-11-08 02:17:23,617 - __main__ - INFO - Epoch 52 Batch 100: Train Loss = 0.1772
2023-11-08 02:17:39,719 - __main__ - INFO - Epoch 52 Batch 120: Train Loss = 0.1914
2023-11-08 02:17:53,429 - __main__ - INFO - Epoch 52: Loss = 0.2184 Valid loss = 0.1974 roc = 0.8629
2023-11-08 02:17:54,279 - __main__ - INFO - Epoch 53 Batch 0: Train Loss = 0.2161
2023-11-08 02:18:11,317 - __main__ - INFO - Epoch 53 Batch 20: Train Loss = 0.2346
2023-11-08 02:18:29,578 - __main__ - INFO - Epoch 53 Batch 40: Train Loss = 0.1973
2023-11-08 02:18:46,200 - __main__ - INFO - Epoch 53 Batch 60: Train Loss = 0.2112
2023-11-08 02:19:04,011 - __main__ - INFO - Epoch 53 Batch 80: Train Loss = 0.2175
2023-11-08 02:19:20,061 - __main__ - INFO - Epoch 53 Batch 100: Train Loss = 0.2076
2023-11-08 02:19:37,166 - __main__ - INFO - Epoch 53 Batch 120: Train Loss = 0.2180
2023-11-08 02:19:50,257 - __main__ - INFO - Epoch 53: Loss = 0.2193 Valid loss = 0.1947 roc = 0.8632
2023-11-08 02:19:51,115 - __main__ - INFO - Epoch 54 Batch 0: Train Loss = 0.1934
2023-11-08 02:20:07,573 - __main__ - INFO - Epoch 54 Batch 20: Train Loss = 0.2392
2023-11-08 02:20:23,869 - __main__ - INFO - Epoch 54 Batch 40: Train Loss = 0.2473
2023-11-08 02:20:40,939 - __main__ - INFO - Epoch 54 Batch 60: Train Loss = 0.1902
2023-11-08 02:20:59,120 - __main__ - INFO - Epoch 54 Batch 80: Train Loss = 0.1801
2023-11-08 02:21:17,024 - __main__ - INFO - Epoch 54 Batch 100: Train Loss = 0.2114
2023-11-08 02:21:34,471 - __main__ - INFO - Epoch 54 Batch 120: Train Loss = 0.1229
2023-11-08 02:21:48,621 - __main__ - INFO - Epoch 54: Loss = 0.2210 Valid loss = 0.1990 roc = 0.8621
2023-11-08 02:21:49,438 - __main__ - INFO - Epoch 55 Batch 0: Train Loss = 0.1959
2023-11-08 02:22:06,758 - __main__ - INFO - Epoch 55 Batch 20: Train Loss = 0.1979
2023-11-08 02:22:24,050 - __main__ - INFO - Epoch 55 Batch 40: Train Loss = 0.2073
2023-11-08 02:22:40,784 - __main__ - INFO - Epoch 55 Batch 60: Train Loss = 0.2401
2023-11-08 02:22:59,196 - __main__ - INFO - Epoch 55 Batch 80: Train Loss = 0.1616
2023-11-08 02:23:17,930 - __main__ - INFO - Epoch 55 Batch 100: Train Loss = 0.1759
2023-11-08 02:23:35,545 - __main__ - INFO - Epoch 55 Batch 120: Train Loss = 0.2025
2023-11-08 02:23:49,931 - __main__ - INFO - Epoch 55: Loss = 0.2186 Valid loss = 0.1963 roc = 0.8687
2023-11-08 02:23:50,810 - __main__ - INFO - Epoch 56 Batch 0: Train Loss = 0.1984
2023-11-08 02:24:08,379 - __main__ - INFO - Epoch 56 Batch 20: Train Loss = 0.2412
2023-11-08 02:24:26,391 - __main__ - INFO - Epoch 56 Batch 40: Train Loss = 0.2385
2023-11-08 02:24:44,440 - __main__ - INFO - Epoch 56 Batch 60: Train Loss = 0.1823
2023-11-08 02:25:00,935 - __main__ - INFO - Epoch 56 Batch 80: Train Loss = 0.1197
2023-11-08 02:25:17,768 - __main__ - INFO - Epoch 56 Batch 100: Train Loss = 0.1981
2023-11-08 02:25:34,414 - __main__ - INFO - Epoch 56 Batch 120: Train Loss = 0.2045
2023-11-08 02:25:47,775 - __main__ - INFO - Epoch 56: Loss = 0.2181 Valid loss = 0.1982 roc = 0.8670
2023-11-08 02:25:48,646 - __main__ - INFO - Epoch 57 Batch 0: Train Loss = 0.1886
2023-11-08 02:26:05,120 - __main__ - INFO - Epoch 57 Batch 20: Train Loss = 0.2287
2023-11-08 02:26:23,803 - __main__ - INFO - Epoch 57 Batch 40: Train Loss = 0.2338
2023-11-08 02:26:40,951 - __main__ - INFO - Epoch 57 Batch 60: Train Loss = 0.1984
2023-11-08 02:26:58,567 - __main__ - INFO - Epoch 57 Batch 80: Train Loss = 0.2375
2023-11-08 02:27:16,255 - __main__ - INFO - Epoch 57 Batch 100: Train Loss = 0.2500
2023-11-08 02:27:33,715 - __main__ - INFO - Epoch 57 Batch 120: Train Loss = 0.2826
2023-11-08 02:27:46,597 - __main__ - INFO - Epoch 57: Loss = 0.2197 Valid loss = 0.1985 roc = 0.8651
2023-11-08 02:27:47,495 - __main__ - INFO - Epoch 58 Batch 0: Train Loss = 0.2778
2023-11-08 02:28:05,057 - __main__ - INFO - Epoch 58 Batch 20: Train Loss = 0.1855
2023-11-08 02:28:21,845 - __main__ - INFO - Epoch 58 Batch 40: Train Loss = 0.2170
2023-11-08 02:28:40,089 - __main__ - INFO - Epoch 58 Batch 60: Train Loss = 0.2557
2023-11-08 02:28:57,010 - __main__ - INFO - Epoch 58 Batch 80: Train Loss = 0.2203
2023-11-08 02:29:14,067 - __main__ - INFO - Epoch 58 Batch 100: Train Loss = 0.1154
2023-11-08 02:29:31,826 - __main__ - INFO - Epoch 58 Batch 120: Train Loss = 0.2613
2023-11-08 02:29:44,807 - __main__ - INFO - Epoch 58: Loss = 0.2181 Valid loss = 0.1939 roc = 0.8681
2023-11-08 02:29:45,388 - __main__ - INFO - Epoch 59 Batch 0: Train Loss = 0.1901
2023-11-08 02:30:03,013 - __main__ - INFO - Epoch 59 Batch 20: Train Loss = 0.1762
2023-11-08 02:30:20,711 - __main__ - INFO - Epoch 59 Batch 40: Train Loss = 0.2136
2023-11-08 02:30:38,492 - __main__ - INFO - Epoch 59 Batch 60: Train Loss = 0.1992
2023-11-08 02:30:55,972 - __main__ - INFO - Epoch 59 Batch 80: Train Loss = 0.1746
2023-11-08 02:31:13,966 - __main__ - INFO - Epoch 59 Batch 100: Train Loss = 0.1986
2023-11-08 02:31:30,697 - __main__ - INFO - Epoch 59 Batch 120: Train Loss = 0.1623
2023-11-08 02:31:43,942 - __main__ - INFO - Epoch 59: Loss = 0.2179 Valid loss = 0.1978 roc = 0.8645
2023-11-08 02:31:44,918 - __main__ - INFO - Epoch 60 Batch 0: Train Loss = 0.1562
2023-11-08 02:32:02,701 - __main__ - INFO - Epoch 60 Batch 20: Train Loss = 0.2051
2023-11-08 02:32:21,054 - __main__ - INFO - Epoch 60 Batch 40: Train Loss = 0.1913
2023-11-08 02:32:39,067 - __main__ - INFO - Epoch 60 Batch 60: Train Loss = 0.2351
2023-11-08 02:32:57,918 - __main__ - INFO - Epoch 60 Batch 80: Train Loss = 0.1582
2023-11-08 02:33:16,815 - __main__ - INFO - Epoch 60 Batch 100: Train Loss = 0.2371
2023-11-08 02:33:33,539 - __main__ - INFO - Epoch 60 Batch 120: Train Loss = 0.2419
2023-11-08 02:33:48,218 - __main__ - INFO - Epoch 60: Loss = 0.2158 Valid loss = 0.1955 roc = 0.8658
2023-11-08 02:33:49,406 - __main__ - INFO - Epoch 61 Batch 0: Train Loss = 0.2227
2023-11-08 02:34:05,278 - __main__ - INFO - Epoch 61 Batch 20: Train Loss = 0.1713
2023-11-08 02:34:22,697 - __main__ - INFO - Epoch 61 Batch 40: Train Loss = 0.2065
2023-11-08 02:34:40,363 - __main__ - INFO - Epoch 61 Batch 60: Train Loss = 0.2000
2023-11-08 02:34:58,443 - __main__ - INFO - Epoch 61 Batch 80: Train Loss = 0.2287
2023-11-08 02:35:16,934 - __main__ - INFO - Epoch 61 Batch 100: Train Loss = 0.2186
2023-11-08 02:35:32,725 - __main__ - INFO - Epoch 61 Batch 120: Train Loss = 0.2363
2023-11-08 02:35:45,931 - __main__ - INFO - Epoch 61: Loss = 0.2143 Valid loss = 0.1946 roc = 0.8689
2023-11-08 02:35:46,848 - __main__ - INFO - Epoch 62 Batch 0: Train Loss = 0.1561
2023-11-08 02:36:03,657 - __main__ - INFO - Epoch 62 Batch 20: Train Loss = 0.2176
2023-11-08 02:36:20,967 - __main__ - INFO - Epoch 62 Batch 40: Train Loss = 0.2830
2023-11-08 02:36:38,711 - __main__ - INFO - Epoch 62 Batch 60: Train Loss = 0.2463
2023-11-08 02:36:56,374 - __main__ - INFO - Epoch 62 Batch 80: Train Loss = 0.1852
2023-11-08 02:37:14,401 - __main__ - INFO - Epoch 62 Batch 100: Train Loss = 0.2184
2023-11-08 02:37:32,451 - __main__ - INFO - Epoch 62 Batch 120: Train Loss = 0.1659
2023-11-08 02:37:45,923 - __main__ - INFO - Epoch 62: Loss = 0.2165 Valid loss = 0.1967 roc = 0.8507
2023-11-08 02:37:47,035 - __main__ - INFO - Epoch 63 Batch 0: Train Loss = 0.1829
2023-11-08 02:38:03,299 - __main__ - INFO - Epoch 63 Batch 20: Train Loss = 0.2202
2023-11-08 02:38:20,799 - __main__ - INFO - Epoch 63 Batch 40: Train Loss = 0.1984
2023-11-08 02:38:38,509 - __main__ - INFO - Epoch 63 Batch 60: Train Loss = 0.2001
2023-11-08 02:38:54,705 - __main__ - INFO - Epoch 63 Batch 80: Train Loss = 0.2565
2023-11-08 02:39:12,462 - __main__ - INFO - Epoch 63 Batch 100: Train Loss = 0.2462
2023-11-08 02:39:28,386 - __main__ - INFO - Epoch 63 Batch 120: Train Loss = 0.1385
2023-11-08 02:39:41,154 - __main__ - INFO - Epoch 63: Loss = 0.2185 Valid loss = 0.1975 roc = 0.8689
2023-11-08 02:39:41,711 - __main__ - INFO - Epoch 64 Batch 0: Train Loss = 0.2566
2023-11-08 02:39:59,012 - __main__ - INFO - Epoch 64 Batch 20: Train Loss = 0.2255
2023-11-08 02:40:16,840 - __main__ - INFO - Epoch 64 Batch 40: Train Loss = 0.1832
2023-11-08 02:40:34,246 - __main__ - INFO - Epoch 64 Batch 60: Train Loss = 0.2224
2023-11-08 02:40:51,800 - __main__ - INFO - Epoch 64 Batch 80: Train Loss = 0.2202
2023-11-08 02:41:09,363 - __main__ - INFO - Epoch 64 Batch 100: Train Loss = 0.2031
2023-11-08 02:41:27,193 - __main__ - INFO - Epoch 64 Batch 120: Train Loss = 0.2200
2023-11-08 02:41:39,890 - __main__ - INFO - Epoch 64: Loss = 0.2147 Valid loss = 0.1930 roc = 0.8685
2023-11-08 02:41:41,011 - __main__ - INFO - Epoch 65 Batch 0: Train Loss = 0.2879
2023-11-08 02:41:58,505 - __main__ - INFO - Epoch 65 Batch 20: Train Loss = 0.1954
2023-11-08 02:42:14,675 - __main__ - INFO - Epoch 65 Batch 40: Train Loss = 0.1893
2023-11-08 02:42:31,343 - __main__ - INFO - Epoch 65 Batch 60: Train Loss = 0.1945
2023-11-08 02:42:48,672 - __main__ - INFO - Epoch 65 Batch 80: Train Loss = 0.2740
2023-11-08 02:43:07,212 - __main__ - INFO - Epoch 65 Batch 100: Train Loss = 0.2568
2023-11-08 02:43:24,620 - __main__ - INFO - Epoch 65 Batch 120: Train Loss = 0.2227
2023-11-08 02:43:37,001 - __main__ - INFO - Epoch 65: Loss = 0.2286 Valid loss = 0.2080 roc = 0.8605
2023-11-08 02:43:38,038 - __main__ - INFO - Epoch 66 Batch 0: Train Loss = 0.2894
2023-11-08 02:43:56,033 - __main__ - INFO - Epoch 66 Batch 20: Train Loss = 0.2574
2023-11-08 02:44:13,550 - __main__ - INFO - Epoch 66 Batch 40: Train Loss = 0.2286
2023-11-08 02:44:30,436 - __main__ - INFO - Epoch 66 Batch 60: Train Loss = 0.1966
2023-11-08 02:44:47,664 - __main__ - INFO - Epoch 66 Batch 80: Train Loss = 0.2423
2023-11-08 02:45:05,759 - __main__ - INFO - Epoch 66 Batch 100: Train Loss = 0.2135
2023-11-08 02:45:22,593 - __main__ - INFO - Epoch 66 Batch 120: Train Loss = 0.1856
2023-11-08 02:45:36,714 - __main__ - INFO - Epoch 66: Loss = 0.2241 Valid loss = 0.2027 roc = 0.8519
2023-11-08 02:45:37,710 - __main__ - INFO - Epoch 67 Batch 0: Train Loss = 0.1978
2023-11-08 02:45:53,866 - __main__ - INFO - Epoch 67 Batch 20: Train Loss = 0.2380
2023-11-08 02:46:11,621 - __main__ - INFO - Epoch 67 Batch 40: Train Loss = 0.2347
2023-11-08 02:46:29,836 - __main__ - INFO - Epoch 67 Batch 60: Train Loss = 0.2212
2023-11-08 02:46:47,771 - __main__ - INFO - Epoch 67 Batch 80: Train Loss = 0.1727
2023-11-08 02:47:06,850 - __main__ - INFO - Epoch 67 Batch 100: Train Loss = 0.2136
2023-11-08 02:47:21,880 - __main__ - INFO - Epoch 67 Batch 120: Train Loss = 0.2696
2023-11-08 02:47:35,958 - __main__ - INFO - Epoch 67: Loss = 0.2217 Valid loss = 0.1998 roc = 0.8575
2023-11-08 02:47:36,736 - __main__ - INFO - Epoch 68 Batch 0: Train Loss = 0.1941
2023-11-08 02:47:53,620 - __main__ - INFO - Epoch 68 Batch 20: Train Loss = 0.2575
2023-11-08 02:48:12,028 - __main__ - INFO - Epoch 68 Batch 40: Train Loss = 0.2136
2023-11-08 02:48:28,872 - __main__ - INFO - Epoch 68 Batch 60: Train Loss = 0.1840
2023-11-08 02:48:46,452 - __main__ - INFO - Epoch 68 Batch 80: Train Loss = 0.2288
2023-11-08 02:49:04,384 - __main__ - INFO - Epoch 68 Batch 100: Train Loss = 0.2692
2023-11-08 02:49:21,450 - __main__ - INFO - Epoch 68 Batch 120: Train Loss = 0.2634
2023-11-08 02:49:35,254 - __main__ - INFO - Epoch 68: Loss = 0.2173 Valid loss = 0.1967 roc = 0.8589
2023-11-08 02:49:36,246 - __main__ - INFO - Epoch 69 Batch 0: Train Loss = 0.2391
2023-11-08 02:49:52,141 - __main__ - INFO - Epoch 69 Batch 20: Train Loss = 0.2297
2023-11-08 02:50:10,309 - __main__ - INFO - Epoch 69 Batch 40: Train Loss = 0.2971
2023-11-08 02:50:26,466 - __main__ - INFO - Epoch 69 Batch 60: Train Loss = 0.2047
2023-11-08 02:50:44,980 - __main__ - INFO - Epoch 69 Batch 80: Train Loss = 0.3332
2023-11-08 02:51:03,888 - __main__ - INFO - Epoch 69 Batch 100: Train Loss = 0.2725
2023-11-08 02:51:20,486 - __main__ - INFO - Epoch 69 Batch 120: Train Loss = 0.2197
2023-11-08 02:51:34,044 - __main__ - INFO - Epoch 69: Loss = 0.2201 Valid loss = 0.2004 roc = 0.8522
2023-11-08 02:51:34,912 - __main__ - INFO - Epoch 70 Batch 0: Train Loss = 0.2158
2023-11-08 02:51:51,709 - __main__ - INFO - Epoch 70 Batch 20: Train Loss = 0.2228
2023-11-08 02:52:09,207 - __main__ - INFO - Epoch 70 Batch 40: Train Loss = 0.1956
2023-11-08 02:52:26,027 - __main__ - INFO - Epoch 70 Batch 60: Train Loss = 0.2338
2023-11-08 02:52:41,959 - __main__ - INFO - Epoch 70 Batch 80: Train Loss = 0.2473
2023-11-08 02:52:59,295 - __main__ - INFO - Epoch 70 Batch 100: Train Loss = 0.1956
2023-11-08 02:53:15,994 - __main__ - INFO - Epoch 70 Batch 120: Train Loss = 0.1933
2023-11-08 02:53:29,513 - __main__ - INFO - Epoch 70: Loss = 0.2215 Valid loss = 0.2045 roc = 0.8585
2023-11-08 02:53:30,357 - __main__ - INFO - Epoch 71 Batch 0: Train Loss = 0.2152
2023-11-08 02:53:48,583 - __main__ - INFO - Epoch 71 Batch 20: Train Loss = 0.1955
2023-11-08 02:54:06,435 - __main__ - INFO - Epoch 71 Batch 40: Train Loss = 0.2268
2023-11-08 02:54:23,105 - __main__ - INFO - Epoch 71 Batch 60: Train Loss = 0.2632
2023-11-08 02:54:40,396 - __main__ - INFO - Epoch 71 Batch 80: Train Loss = 0.2358
2023-11-08 02:54:59,439 - __main__ - INFO - Epoch 71 Batch 100: Train Loss = 0.2532
2023-11-08 02:55:16,007 - __main__ - INFO - Epoch 71 Batch 120: Train Loss = 0.2786
2023-11-08 02:55:29,568 - __main__ - INFO - Epoch 71: Loss = 0.2247 Valid loss = 0.2046 roc = 0.8574
2023-11-08 02:55:30,749 - __main__ - INFO - Epoch 72 Batch 0: Train Loss = 0.2239
2023-11-08 02:55:48,276 - __main__ - INFO - Epoch 72 Batch 20: Train Loss = 0.2533
2023-11-08 02:56:05,998 - __main__ - INFO - Epoch 72 Batch 40: Train Loss = 0.2545
2023-11-08 02:56:22,280 - __main__ - INFO - Epoch 72 Batch 60: Train Loss = 0.1448
2023-11-08 02:56:38,749 - __main__ - INFO - Epoch 72 Batch 80: Train Loss = 0.1468
2023-11-08 02:56:58,442 - __main__ - INFO - Epoch 72 Batch 100: Train Loss = 0.1991
2023-11-08 02:57:15,204 - __main__ - INFO - Epoch 72 Batch 120: Train Loss = 0.2255
2023-11-08 02:57:28,653 - __main__ - INFO - Epoch 72: Loss = 0.2209 Valid loss = 0.2003 roc = 0.8526
2023-11-08 02:57:29,478 - __main__ - INFO - Epoch 73 Batch 0: Train Loss = 0.1780
2023-11-08 02:57:45,467 - __main__ - INFO - Epoch 73 Batch 20: Train Loss = 0.2092
2023-11-08 02:58:02,433 - __main__ - INFO - Epoch 73 Batch 40: Train Loss = 0.2647
2023-11-08 02:58:22,054 - __main__ - INFO - Epoch 73 Batch 60: Train Loss = 0.2298
2023-11-08 02:58:38,593 - __main__ - INFO - Epoch 73 Batch 80: Train Loss = 0.2656
2023-11-08 02:58:55,651 - __main__ - INFO - Epoch 73 Batch 100: Train Loss = 0.1401
2023-11-08 02:59:12,254 - __main__ - INFO - Epoch 73 Batch 120: Train Loss = 0.2433
2023-11-08 02:59:25,642 - __main__ - INFO - Epoch 73: Loss = 0.2232 Valid loss = 0.2027 roc = 0.8454
2023-11-08 02:59:26,595 - __main__ - INFO - Epoch 74 Batch 0: Train Loss = 0.2088
2023-11-08 02:59:43,813 - __main__ - INFO - Epoch 74 Batch 20: Train Loss = 0.2556
2023-11-08 03:00:00,136 - __main__ - INFO - Epoch 74 Batch 40: Train Loss = 0.2398
2023-11-08 03:00:17,682 - __main__ - INFO - Epoch 74 Batch 60: Train Loss = 0.2641
2023-11-08 03:00:34,548 - __main__ - INFO - Epoch 74 Batch 80: Train Loss = 0.2031
2023-11-08 03:00:52,836 - __main__ - INFO - Epoch 74 Batch 100: Train Loss = 0.1710
2023-11-08 03:01:11,157 - __main__ - INFO - Epoch 74 Batch 120: Train Loss = 0.3143
2023-11-08 03:01:23,749 - __main__ - INFO - Epoch 74: Loss = 0.2194 Valid loss = 0.2014 roc = 0.8470
2023-11-08 03:01:24,543 - __main__ - INFO - Epoch 75 Batch 0: Train Loss = 0.2652
2023-11-08 03:01:43,258 - __main__ - INFO - Epoch 75 Batch 20: Train Loss = 0.2341
2023-11-08 03:02:01,419 - __main__ - INFO - Epoch 75 Batch 40: Train Loss = 0.2035
2023-11-08 03:02:18,475 - __main__ - INFO - Epoch 75 Batch 60: Train Loss = 0.1848
2023-11-08 03:02:36,909 - __main__ - INFO - Epoch 75 Batch 80: Train Loss = 0.1866
2023-11-08 03:02:54,134 - __main__ - INFO - Epoch 75 Batch 100: Train Loss = 0.1915
2023-11-08 03:03:11,741 - __main__ - INFO - Epoch 75 Batch 120: Train Loss = 0.1931
2023-11-08 03:03:24,672 - __main__ - INFO - Epoch 75: Loss = 0.2209 Valid loss = 0.2006 roc = 0.8517
2023-11-08 03:03:25,410 - __main__ - INFO - Epoch 76 Batch 0: Train Loss = 0.2238
2023-11-08 03:03:43,172 - __main__ - INFO - Epoch 76 Batch 20: Train Loss = 0.2021
2023-11-08 03:04:00,472 - __main__ - INFO - Epoch 76 Batch 40: Train Loss = 0.2255
2023-11-08 03:04:17,159 - __main__ - INFO - Epoch 76 Batch 60: Train Loss = 0.2086
2023-11-08 03:04:34,401 - __main__ - INFO - Epoch 76 Batch 80: Train Loss = 0.1991
2023-11-08 03:04:51,996 - __main__ - INFO - Epoch 76 Batch 100: Train Loss = 0.1614
2023-11-08 03:05:08,459 - __main__ - INFO - Epoch 76 Batch 120: Train Loss = 0.1900
2023-11-08 03:05:20,337 - __main__ - INFO - Epoch 76: Loss = 0.2227 Valid loss = 0.2004 roc = 0.8471
2023-11-08 03:05:20,845 - __main__ - INFO - Epoch 77 Batch 0: Train Loss = 0.1701
2023-11-08 03:05:37,197 - __main__ - INFO - Epoch 77 Batch 20: Train Loss = 0.2448
2023-11-08 03:05:55,216 - __main__ - INFO - Epoch 77 Batch 40: Train Loss = 0.2953
2023-11-08 03:06:12,999 - __main__ - INFO - Epoch 77 Batch 60: Train Loss = 0.2461
2023-11-08 03:06:30,543 - __main__ - INFO - Epoch 77 Batch 80: Train Loss = 0.2440
2023-11-08 03:06:47,889 - __main__ - INFO - Epoch 77 Batch 100: Train Loss = 0.1698
2023-11-08 03:07:04,660 - __main__ - INFO - Epoch 77 Batch 120: Train Loss = 0.1419
2023-11-08 03:07:17,971 - __main__ - INFO - Epoch 77: Loss = 0.2221 Valid loss = 0.2040 roc = 0.8491
2023-11-08 03:07:18,864 - __main__ - INFO - Epoch 78 Batch 0: Train Loss = 0.1643
2023-11-08 03:07:36,291 - __main__ - INFO - Epoch 78 Batch 20: Train Loss = 0.2217
2023-11-08 03:07:54,430 - __main__ - INFO - Epoch 78 Batch 40: Train Loss = 0.3080
2023-11-08 03:08:11,354 - __main__ - INFO - Epoch 78 Batch 60: Train Loss = 0.1832
2023-11-08 03:08:28,799 - __main__ - INFO - Epoch 78 Batch 80: Train Loss = 0.2189
2023-11-08 03:08:45,297 - __main__ - INFO - Epoch 78 Batch 100: Train Loss = 0.1837
2023-11-08 03:09:03,477 - __main__ - INFO - Epoch 78 Batch 120: Train Loss = 0.2649
2023-11-08 03:09:16,496 - __main__ - INFO - Epoch 78: Loss = 0.2205 Valid loss = 0.2011 roc = 0.8466
2023-11-08 03:09:17,483 - __main__ - INFO - Epoch 79 Batch 0: Train Loss = 0.1932
2023-11-08 03:09:34,221 - __main__ - INFO - Epoch 79 Batch 20: Train Loss = 0.1452
2023-11-08 03:09:52,707 - __main__ - INFO - Epoch 79 Batch 40: Train Loss = 0.1760
2023-11-08 03:10:07,906 - __main__ - INFO - Epoch 79 Batch 60: Train Loss = 0.1254
2023-11-08 03:10:24,389 - __main__ - INFO - Epoch 79 Batch 80: Train Loss = 0.2828
2023-11-08 03:10:42,374 - __main__ - INFO - Epoch 79 Batch 100: Train Loss = 0.2119
2023-11-08 03:11:01,852 - __main__ - INFO - Epoch 79 Batch 120: Train Loss = 0.1688
2023-11-08 03:11:14,764 - __main__ - INFO - Epoch 79: Loss = 0.2198 Valid loss = 0.2010 roc = 0.8556
2023-11-08 03:11:15,714 - __main__ - INFO - Epoch 80 Batch 0: Train Loss = 0.1871
2023-11-08 03:11:32,041 - __main__ - INFO - Epoch 80 Batch 20: Train Loss = 0.2618
2023-11-08 03:11:48,950 - __main__ - INFO - Epoch 80 Batch 40: Train Loss = 0.1958
2023-11-08 03:12:05,188 - __main__ - INFO - Epoch 80 Batch 60: Train Loss = 0.2641
2023-11-08 03:12:23,261 - __main__ - INFO - Epoch 80 Batch 80: Train Loss = 0.2702
2023-11-08 03:12:40,385 - __main__ - INFO - Epoch 80 Batch 100: Train Loss = 0.1974
2023-11-08 03:12:57,838 - __main__ - INFO - Epoch 80 Batch 120: Train Loss = 0.1925
2023-11-08 03:13:10,780 - __main__ - INFO - Epoch 80: Loss = 0.2195 Valid loss = 0.2010 roc = 0.8521
2023-11-08 03:13:11,578 - __main__ - INFO - Epoch 81 Batch 0: Train Loss = 0.1801
2023-11-08 03:13:28,843 - __main__ - INFO - Epoch 81 Batch 20: Train Loss = 0.2397
2023-11-08 03:13:47,174 - __main__ - INFO - Epoch 81 Batch 40: Train Loss = 0.2682
2023-11-08 03:14:04,873 - __main__ - INFO - Epoch 81 Batch 60: Train Loss = 0.2500
2023-11-08 03:14:22,337 - __main__ - INFO - Epoch 81 Batch 80: Train Loss = 0.2412
2023-11-08 03:14:38,312 - __main__ - INFO - Epoch 81 Batch 100: Train Loss = 0.1847
2023-11-08 03:14:55,948 - __main__ - INFO - Epoch 81 Batch 120: Train Loss = 0.2688
2023-11-08 03:15:09,670 - __main__ - INFO - Epoch 81: Loss = 0.2208 Valid loss = 0.2000 roc = 0.8525
2023-11-08 03:15:10,483 - __main__ - INFO - Epoch 82 Batch 0: Train Loss = 0.1675
2023-11-08 03:15:25,910 - __main__ - INFO - Epoch 82 Batch 20: Train Loss = 0.1816
2023-11-08 03:15:43,258 - __main__ - INFO - Epoch 82 Batch 40: Train Loss = 0.1682
2023-11-08 03:15:59,198 - __main__ - INFO - Epoch 82 Batch 60: Train Loss = 0.2389
2023-11-08 03:16:16,990 - __main__ - INFO - Epoch 82 Batch 80: Train Loss = 0.1854
2023-11-08 03:16:34,365 - __main__ - INFO - Epoch 82 Batch 100: Train Loss = 0.2183
2023-11-08 03:16:52,840 - __main__ - INFO - Epoch 82 Batch 120: Train Loss = 0.2392
2023-11-08 03:17:07,045 - __main__ - INFO - Epoch 82: Loss = 0.2176 Valid loss = 0.2016 roc = 0.8525
2023-11-08 03:17:08,062 - __main__ - INFO - Epoch 83 Batch 0: Train Loss = 0.2289
2023-11-08 03:17:24,740 - __main__ - INFO - Epoch 83 Batch 20: Train Loss = 0.2145
2023-11-08 03:17:40,500 - __main__ - INFO - Epoch 83 Batch 40: Train Loss = 0.2284
2023-11-08 03:17:56,914 - __main__ - INFO - Epoch 83 Batch 60: Train Loss = 0.2101
2023-11-08 03:18:14,131 - __main__ - INFO - Epoch 83 Batch 80: Train Loss = 0.2402
2023-11-08 03:18:31,728 - __main__ - INFO - Epoch 83 Batch 100: Train Loss = 0.2065
2023-11-08 03:18:50,098 - __main__ - INFO - Epoch 83 Batch 120: Train Loss = 0.2209
2023-11-08 03:19:03,534 - __main__ - INFO - Epoch 83: Loss = 0.2216 Valid loss = 0.2025 roc = 0.8627
2023-11-08 03:19:04,555 - __main__ - INFO - Epoch 84 Batch 0: Train Loss = 0.1439
2023-11-08 03:19:21,672 - __main__ - INFO - Epoch 84 Batch 20: Train Loss = 0.2429
2023-11-08 03:19:39,056 - __main__ - INFO - Epoch 84 Batch 40: Train Loss = 0.2159
2023-11-08 03:19:57,364 - __main__ - INFO - Epoch 84 Batch 60: Train Loss = 0.1484
2023-11-08 03:20:15,866 - __main__ - INFO - Epoch 84 Batch 80: Train Loss = 0.1572
2023-11-08 03:20:32,882 - __main__ - INFO - Epoch 84 Batch 100: Train Loss = 0.2424
2023-11-08 03:20:51,017 - __main__ - INFO - Epoch 84 Batch 120: Train Loss = 0.2760
2023-11-08 03:21:04,830 - __main__ - INFO - Epoch 84: Loss = 0.2222 Valid loss = 0.2021 roc = 0.8475
2023-11-08 03:21:05,606 - __main__ - INFO - Epoch 85 Batch 0: Train Loss = 0.1871
2023-11-08 03:21:22,712 - __main__ - INFO - Epoch 85 Batch 20: Train Loss = 0.2166
2023-11-08 03:21:40,283 - __main__ - INFO - Epoch 85 Batch 40: Train Loss = 0.2297
2023-11-08 03:21:57,952 - __main__ - INFO - Epoch 85 Batch 60: Train Loss = 0.2341
2023-11-08 03:22:15,998 - __main__ - INFO - Epoch 85 Batch 80: Train Loss = 0.1973
2023-11-08 03:22:34,542 - __main__ - INFO - Epoch 85 Batch 100: Train Loss = 0.2418
2023-11-08 03:22:53,066 - __main__ - INFO - Epoch 85 Batch 120: Train Loss = 0.2711
2023-11-08 03:23:06,410 - __main__ - INFO - Epoch 85: Loss = 0.2229 Valid loss = 0.2031 roc = 0.8550
2023-11-08 03:23:07,290 - __main__ - INFO - Epoch 86 Batch 0: Train Loss = 0.2137
2023-11-08 03:23:24,270 - __main__ - INFO - Epoch 86 Batch 20: Train Loss = 0.2335
2023-11-08 03:23:42,219 - __main__ - INFO - Epoch 86 Batch 40: Train Loss = 0.2557
2023-11-08 03:23:59,384 - __main__ - INFO - Epoch 86 Batch 60: Train Loss = 0.2891
2023-11-08 03:24:16,856 - __main__ - INFO - Epoch 86 Batch 80: Train Loss = 0.1783
2023-11-08 03:24:35,109 - __main__ - INFO - Epoch 86 Batch 100: Train Loss = 0.2091
2023-11-08 03:24:54,314 - __main__ - INFO - Epoch 86 Batch 120: Train Loss = 0.2306
2023-11-08 03:25:07,640 - __main__ - INFO - Epoch 86: Loss = 0.2254 Valid loss = 0.2033 roc = 0.8575
2023-11-08 03:25:08,342 - __main__ - INFO - Epoch 87 Batch 0: Train Loss = 0.1762
2023-11-08 03:25:23,722 - __main__ - INFO - Epoch 87 Batch 20: Train Loss = 0.1876
2023-11-08 03:25:42,203 - __main__ - INFO - Epoch 87 Batch 40: Train Loss = 0.1723
2023-11-08 03:25:59,844 - __main__ - INFO - Epoch 87 Batch 60: Train Loss = 0.1901
2023-11-08 03:26:17,079 - __main__ - INFO - Epoch 87 Batch 80: Train Loss = 0.2462
2023-11-08 03:26:36,320 - __main__ - INFO - Epoch 87 Batch 100: Train Loss = 0.2817
2023-11-08 03:26:53,762 - __main__ - INFO - Epoch 87 Batch 120: Train Loss = 0.2311
2023-11-08 03:27:07,686 - __main__ - INFO - Epoch 87: Loss = 0.2194 Valid loss = 0.1991 roc = 0.8585
2023-11-08 03:27:08,584 - __main__ - INFO - Epoch 88 Batch 0: Train Loss = 0.1842
2023-11-08 03:27:25,358 - __main__ - INFO - Epoch 88 Batch 20: Train Loss = 0.2052
2023-11-08 03:27:42,599 - __main__ - INFO - Epoch 88 Batch 40: Train Loss = 0.2255
2023-11-08 03:28:01,201 - __main__ - INFO - Epoch 88 Batch 60: Train Loss = 0.2195
2023-11-08 03:28:19,832 - __main__ - INFO - Epoch 88 Batch 80: Train Loss = 0.2145
2023-11-08 03:28:36,915 - __main__ - INFO - Epoch 88 Batch 100: Train Loss = 0.1460
2023-11-08 03:28:54,302 - __main__ - INFO - Epoch 88 Batch 120: Train Loss = 0.2212
2023-11-08 03:29:08,561 - __main__ - INFO - Epoch 88: Loss = 0.2215 Valid loss = 0.2011 roc = 0.8538
2023-11-08 03:29:09,560 - __main__ - INFO - Epoch 89 Batch 0: Train Loss = 0.1867
2023-11-08 03:29:26,786 - __main__ - INFO - Epoch 89 Batch 20: Train Loss = 0.2383
2023-11-08 03:29:43,442 - __main__ - INFO - Epoch 89 Batch 40: Train Loss = 0.2179
2023-11-08 03:30:00,707 - __main__ - INFO - Epoch 89 Batch 60: Train Loss = 0.1859
2023-11-08 03:30:17,298 - __main__ - INFO - Epoch 89 Batch 80: Train Loss = 0.2590
2023-11-08 03:30:34,041 - __main__ - INFO - Epoch 89 Batch 100: Train Loss = 0.2321
2023-11-08 03:30:51,358 - __main__ - INFO - Epoch 89 Batch 120: Train Loss = 0.2629
2023-11-08 03:31:05,087 - __main__ - INFO - Epoch 89: Loss = 0.2228 Valid loss = 0.1991 roc = 0.8583
2023-11-08 03:31:06,015 - __main__ - INFO - Epoch 90 Batch 0: Train Loss = 0.1664
2023-11-08 03:31:24,031 - __main__ - INFO - Epoch 90 Batch 20: Train Loss = 0.1773
2023-11-08 03:31:40,760 - __main__ - INFO - Epoch 90 Batch 40: Train Loss = 0.2336
2023-11-08 03:31:58,414 - __main__ - INFO - Epoch 90 Batch 60: Train Loss = 0.2128
2023-11-08 03:32:14,775 - __main__ - INFO - Epoch 90 Batch 80: Train Loss = 0.2015
2023-11-08 03:32:32,184 - __main__ - INFO - Epoch 90 Batch 100: Train Loss = 0.1759
2023-11-08 03:32:48,906 - __main__ - INFO - Epoch 90 Batch 120: Train Loss = 0.2421
2023-11-08 03:33:02,816 - __main__ - INFO - Epoch 90: Loss = 0.2224 Valid loss = 0.2014 roc = 0.8557
2023-11-08 03:33:03,825 - __main__ - INFO - Epoch 91 Batch 0: Train Loss = 0.2342
2023-11-08 03:33:21,296 - __main__ - INFO - Epoch 91 Batch 20: Train Loss = 0.1792
2023-11-08 03:33:39,489 - __main__ - INFO - Epoch 91 Batch 40: Train Loss = 0.2618
2023-11-08 03:33:57,375 - __main__ - INFO - Epoch 91 Batch 60: Train Loss = 0.1855
2023-11-08 03:34:15,507 - __main__ - INFO - Epoch 91 Batch 80: Train Loss = 0.2153
2023-11-08 03:34:31,678 - __main__ - INFO - Epoch 91 Batch 100: Train Loss = 0.2290
2023-11-08 03:34:49,207 - __main__ - INFO - Epoch 91 Batch 120: Train Loss = 0.1714
2023-11-08 03:35:03,325 - __main__ - INFO - Epoch 91: Loss = 0.2185 Valid loss = 0.1989 roc = 0.8632
2023-11-08 03:35:04,383 - __main__ - INFO - Epoch 92 Batch 0: Train Loss = 0.2318
2023-11-08 03:35:21,158 - __main__ - INFO - Epoch 92 Batch 20: Train Loss = 0.1872
2023-11-08 03:35:38,458 - __main__ - INFO - Epoch 92 Batch 40: Train Loss = 0.2179
2023-11-08 03:35:55,473 - __main__ - INFO - Epoch 92 Batch 60: Train Loss = 0.1850
2023-11-08 03:36:12,531 - __main__ - INFO - Epoch 92 Batch 80: Train Loss = 0.2235
2023-11-08 03:36:29,710 - __main__ - INFO - Epoch 92 Batch 100: Train Loss = 0.2656
2023-11-08 03:36:47,188 - __main__ - INFO - Epoch 92 Batch 120: Train Loss = 0.2380
2023-11-08 03:37:00,966 - __main__ - INFO - Epoch 92: Loss = 0.2198 Valid loss = 0.2005 roc = 0.8558
2023-11-08 03:37:02,019 - __main__ - INFO - Epoch 93 Batch 0: Train Loss = 0.2061
2023-11-08 03:37:18,805 - __main__ - INFO - Epoch 93 Batch 20: Train Loss = 0.2280
2023-11-08 03:37:36,148 - __main__ - INFO - Epoch 93 Batch 40: Train Loss = 0.2264
2023-11-08 03:37:52,584 - __main__ - INFO - Epoch 93 Batch 60: Train Loss = 0.1191
2023-11-08 03:38:09,731 - __main__ - INFO - Epoch 93 Batch 80: Train Loss = 0.1822
2023-11-08 03:38:26,291 - __main__ - INFO - Epoch 93 Batch 100: Train Loss = 0.2291
2023-11-08 03:38:43,855 - __main__ - INFO - Epoch 93 Batch 120: Train Loss = 0.1990
2023-11-08 03:38:57,179 - __main__ - INFO - Epoch 93: Loss = 0.2179 Valid loss = 0.2008 roc = 0.8540
2023-11-08 03:38:57,887 - __main__ - INFO - Epoch 94 Batch 0: Train Loss = 0.1499
2023-11-08 03:39:15,303 - __main__ - INFO - Epoch 94 Batch 20: Train Loss = 0.1951
2023-11-08 03:39:32,891 - __main__ - INFO - Epoch 94 Batch 40: Train Loss = 0.2201
2023-11-08 03:39:50,159 - __main__ - INFO - Epoch 94 Batch 60: Train Loss = 0.2012
2023-11-08 03:40:07,414 - __main__ - INFO - Epoch 94 Batch 80: Train Loss = 0.2565
2023-11-08 03:40:24,916 - __main__ - INFO - Epoch 94 Batch 100: Train Loss = 0.2139
2023-11-08 03:40:42,872 - __main__ - INFO - Epoch 94 Batch 120: Train Loss = 0.1906
2023-11-08 03:40:56,945 - __main__ - INFO - Epoch 94: Loss = 0.2179 Valid loss = 0.1992 roc = 0.8602
2023-11-08 03:40:57,964 - __main__ - INFO - Epoch 95 Batch 0: Train Loss = 0.2382
2023-11-08 03:41:14,983 - __main__ - INFO - Epoch 95 Batch 20: Train Loss = 0.2076
2023-11-08 03:41:31,444 - __main__ - INFO - Epoch 95 Batch 40: Train Loss = 0.2634
2023-11-08 03:41:48,665 - __main__ - INFO - Epoch 95 Batch 60: Train Loss = 0.2379
2023-11-08 03:42:05,334 - __main__ - INFO - Epoch 95 Batch 80: Train Loss = 0.3023
2023-11-08 03:42:23,423 - __main__ - INFO - Epoch 95 Batch 100: Train Loss = 0.1683
2023-11-08 03:42:40,488 - __main__ - INFO - Epoch 95 Batch 120: Train Loss = 0.2349
2023-11-08 03:42:55,010 - __main__ - INFO - Epoch 95: Loss = 0.2142 Valid loss = 0.1942 roc = 0.8643
2023-11-08 03:42:55,732 - __main__ - INFO - Epoch 96 Batch 0: Train Loss = 0.2482
2023-11-08 03:43:12,466 - __main__ - INFO - Epoch 96 Batch 20: Train Loss = 0.2458
2023-11-08 03:43:29,090 - __main__ - INFO - Epoch 96 Batch 40: Train Loss = 0.2640
2023-11-08 03:43:45,522 - __main__ - INFO - Epoch 96 Batch 60: Train Loss = 0.2207
2023-11-08 03:44:02,692 - __main__ - INFO - Epoch 96 Batch 80: Train Loss = 0.2848
2023-11-08 03:44:20,152 - __main__ - INFO - Epoch 96 Batch 100: Train Loss = 0.2152
2023-11-08 03:44:37,877 - __main__ - INFO - Epoch 96 Batch 120: Train Loss = 0.2475
2023-11-08 03:44:51,222 - __main__ - INFO - Epoch 96: Loss = 0.2132 Valid loss = 0.1942 roc = 0.8679
2023-11-08 03:44:52,056 - __main__ - INFO - Epoch 97 Batch 0: Train Loss = 0.2360
2023-11-08 03:45:08,976 - __main__ - INFO - Epoch 97 Batch 20: Train Loss = 0.1665
2023-11-08 03:45:26,555 - __main__ - INFO - Epoch 97 Batch 40: Train Loss = 0.1978
2023-11-08 03:45:43,596 - __main__ - INFO - Epoch 97 Batch 60: Train Loss = 0.1978
2023-11-08 03:46:01,160 - __main__ - INFO - Epoch 97 Batch 80: Train Loss = 0.2158
2023-11-08 03:46:17,397 - __main__ - INFO - Epoch 97 Batch 100: Train Loss = 0.1933
2023-11-08 03:46:35,234 - __main__ - INFO - Epoch 97 Batch 120: Train Loss = 0.1847
2023-11-08 03:46:49,161 - __main__ - INFO - Epoch 97: Loss = 0.2184 Valid loss = 0.1971 roc = 0.8694
2023-11-08 03:46:50,371 - __main__ - INFO - Epoch 98 Batch 0: Train Loss = 0.2043
2023-11-08 03:47:06,716 - __main__ - INFO - Epoch 98 Batch 20: Train Loss = 0.2280
2023-11-08 03:47:23,543 - __main__ - INFO - Epoch 98 Batch 40: Train Loss = 0.2169
2023-11-08 03:47:39,601 - __main__ - INFO - Epoch 98 Batch 60: Train Loss = 0.1996
2023-11-08 03:47:58,187 - __main__ - INFO - Epoch 98 Batch 80: Train Loss = 0.1547
2023-11-08 03:48:15,883 - __main__ - INFO - Epoch 98 Batch 100: Train Loss = 0.2078
2023-11-08 03:48:33,297 - __main__ - INFO - Epoch 98 Batch 120: Train Loss = 0.2380
2023-11-08 03:48:45,687 - __main__ - INFO - Epoch 98: Loss = 0.2158 Valid loss = 0.1960 roc = 0.8655
2023-11-08 03:48:46,681 - __main__ - INFO - Epoch 99 Batch 0: Train Loss = 0.2136
2023-11-08 03:49:04,400 - __main__ - INFO - Epoch 99 Batch 20: Train Loss = 0.2237
2023-11-08 03:49:22,299 - __main__ - INFO - Epoch 99 Batch 40: Train Loss = 0.2199
2023-11-08 03:49:40,191 - __main__ - INFO - Epoch 99 Batch 60: Train Loss = 0.1950
2023-11-08 03:49:57,958 - __main__ - INFO - Epoch 99 Batch 80: Train Loss = 0.1847
2023-11-08 03:50:15,924 - __main__ - INFO - Epoch 99 Batch 100: Train Loss = 0.2354
2023-11-08 03:50:33,697 - __main__ - INFO - Epoch 99 Batch 120: Train Loss = 0.2655
2023-11-08 03:50:47,732 - __main__ - INFO - Epoch 99: Loss = 0.2162 Valid loss = 0.1965 roc = 0.8656
2023-11-08 03:50:48,793 - __main__ - INFO - Epoch 100 Batch 0: Train Loss = 0.2460
2023-11-08 03:51:06,830 - __main__ - INFO - Epoch 100 Batch 20: Train Loss = 0.1244
2023-11-08 03:51:22,731 - __main__ - INFO - Epoch 100 Batch 40: Train Loss = 0.2552
2023-11-08 03:51:40,974 - __main__ - INFO - Epoch 100 Batch 60: Train Loss = 0.2271
2023-11-08 03:51:57,194 - __main__ - INFO - Epoch 100 Batch 80: Train Loss = 0.2367
2023-11-08 03:52:14,065 - __main__ - INFO - Epoch 100 Batch 100: Train Loss = 0.2416
2023-11-08 03:52:31,837 - __main__ - INFO - Epoch 100 Batch 120: Train Loss = 0.2333
2023-11-08 03:52:46,466 - __main__ - INFO - Epoch 100: Loss = 0.2149 Valid loss = 0.1959 roc = 0.8639
2023-11-08 03:52:47,391 - __main__ - INFO - Epoch 101 Batch 0: Train Loss = 0.2357
2023-11-08 03:53:04,741 - __main__ - INFO - Epoch 101 Batch 20: Train Loss = 0.1898
2023-11-08 03:53:23,288 - __main__ - INFO - Epoch 101 Batch 40: Train Loss = 0.2795
2023-11-08 03:53:40,310 - __main__ - INFO - Epoch 101 Batch 60: Train Loss = 0.2288
2023-11-08 03:53:57,944 - __main__ - INFO - Epoch 101 Batch 80: Train Loss = 0.2026
2023-11-08 03:54:14,167 - __main__ - INFO - Epoch 101 Batch 100: Train Loss = 0.1840
2023-11-08 03:54:31,355 - __main__ - INFO - Epoch 101 Batch 120: Train Loss = 0.2006
2023-11-08 03:54:46,270 - __main__ - INFO - Epoch 101: Loss = 0.2174 Valid loss = 0.1958 roc = 0.8689
2023-11-08 03:54:47,503 - __main__ - INFO - Epoch 102 Batch 0: Train Loss = 0.2243
2023-11-08 03:55:04,534 - __main__ - INFO - Epoch 102 Batch 20: Train Loss = 0.2083
2023-11-08 03:55:23,387 - __main__ - INFO - Epoch 102 Batch 40: Train Loss = 0.1576
2023-11-08 03:55:40,719 - __main__ - INFO - Epoch 102 Batch 60: Train Loss = 0.1888
2023-11-08 03:55:58,635 - __main__ - INFO - Epoch 102 Batch 80: Train Loss = 0.1852
2023-11-08 03:56:15,640 - __main__ - INFO - Epoch 102 Batch 100: Train Loss = 0.2829
2023-11-08 03:56:32,444 - __main__ - INFO - Epoch 102 Batch 120: Train Loss = 0.1692
2023-11-08 03:56:45,916 - __main__ - INFO - Epoch 102: Loss = 0.2156 Valid loss = 0.1994 roc = 0.8550
2023-11-08 03:56:46,989 - __main__ - INFO - Epoch 103 Batch 0: Train Loss = 0.2509
2023-11-08 03:57:04,598 - __main__ - INFO - Epoch 103 Batch 20: Train Loss = 0.3012
2023-11-08 03:57:21,665 - __main__ - INFO - Epoch 103 Batch 40: Train Loss = 0.2296
2023-11-08 03:57:38,069 - __main__ - INFO - Epoch 103 Batch 60: Train Loss = 0.1630
2023-11-08 03:57:57,367 - __main__ - INFO - Epoch 103 Batch 80: Train Loss = 0.1397
2023-11-08 03:58:14,554 - __main__ - INFO - Epoch 103 Batch 100: Train Loss = 0.2370
2023-11-08 03:58:32,031 - __main__ - INFO - Epoch 103 Batch 120: Train Loss = 0.1248
2023-11-08 03:58:46,326 - __main__ - INFO - Epoch 103: Loss = 0.2147 Valid loss = 0.1983 roc = 0.8670
2023-11-08 03:58:46,994 - __main__ - INFO - Epoch 104 Batch 0: Train Loss = 0.2220
2023-11-08 03:59:03,300 - __main__ - INFO - Epoch 104 Batch 20: Train Loss = 0.2565
2023-11-08 03:59:20,521 - __main__ - INFO - Epoch 104 Batch 40: Train Loss = 0.2168
2023-11-08 03:59:37,539 - __main__ - INFO - Epoch 104 Batch 60: Train Loss = 0.1955
2023-11-08 03:59:55,828 - __main__ - INFO - Epoch 104 Batch 80: Train Loss = 0.1741
2023-11-08 04:00:12,765 - __main__ - INFO - Epoch 104 Batch 100: Train Loss = 0.2228
2023-11-08 04:00:31,358 - __main__ - INFO - Epoch 104 Batch 120: Train Loss = 0.2943
2023-11-08 04:00:45,230 - __main__ - INFO - Epoch 104: Loss = 0.2210 Valid loss = 0.1995 roc = 0.8552
2023-11-08 04:00:46,000 - __main__ - INFO - Epoch 105 Batch 0: Train Loss = 0.2397
2023-11-08 04:01:03,018 - __main__ - INFO - Epoch 105 Batch 20: Train Loss = 0.2047
2023-11-08 04:01:21,381 - __main__ - INFO - Epoch 105 Batch 40: Train Loss = 0.2042
2023-11-08 04:01:38,202 - __main__ - INFO - Epoch 105 Batch 60: Train Loss = 0.2462
2023-11-08 04:01:54,768 - __main__ - INFO - Epoch 105 Batch 80: Train Loss = 0.2141
2023-11-08 04:02:12,276 - __main__ - INFO - Epoch 105 Batch 100: Train Loss = 0.1964
2023-11-08 04:02:28,600 - __main__ - INFO - Epoch 105 Batch 120: Train Loss = 0.2565
2023-11-08 04:02:42,574 - __main__ - INFO - Epoch 105: Loss = 0.2160 Valid loss = 0.1977 roc = 0.8615
2023-11-08 04:02:43,488 - __main__ - INFO - Epoch 106 Batch 0: Train Loss = 0.2261
2023-11-08 04:03:01,855 - __main__ - INFO - Epoch 106 Batch 20: Train Loss = 0.2458
2023-11-08 04:03:19,445 - __main__ - INFO - Epoch 106 Batch 40: Train Loss = 0.1932
2023-11-08 04:03:35,858 - __main__ - INFO - Epoch 106 Batch 60: Train Loss = 0.2384
2023-11-08 04:03:52,811 - __main__ - INFO - Epoch 106 Batch 80: Train Loss = 0.2139
2023-11-08 04:04:10,603 - __main__ - INFO - Epoch 106 Batch 100: Train Loss = 0.1951
2023-11-08 04:04:28,639 - __main__ - INFO - Epoch 106 Batch 120: Train Loss = 0.1667
2023-11-08 04:04:42,957 - __main__ - INFO - Epoch 106: Loss = 0.2152 Valid loss = 0.1948 roc = 0.8609
2023-11-08 04:04:44,101 - __main__ - INFO - Epoch 107 Batch 0: Train Loss = 0.2085
2023-11-08 04:05:03,299 - __main__ - INFO - Epoch 107 Batch 20: Train Loss = 0.2279
2023-11-08 04:05:20,189 - __main__ - INFO - Epoch 107 Batch 40: Train Loss = 0.2067
2023-11-08 04:05:37,982 - __main__ - INFO - Epoch 107 Batch 60: Train Loss = 0.2437
2023-11-08 04:05:55,726 - __main__ - INFO - Epoch 107 Batch 80: Train Loss = 0.1802
2023-11-08 04:06:13,371 - __main__ - INFO - Epoch 107 Batch 100: Train Loss = 0.2194
2023-11-08 04:06:28,540 - __main__ - INFO - Epoch 107 Batch 120: Train Loss = 0.1462
2023-11-08 04:06:43,934 - __main__ - INFO - Epoch 107: Loss = 0.2121 Valid loss = 0.1969 roc = 0.8643
2023-11-08 04:06:44,951 - __main__ - INFO - Epoch 108 Batch 0: Train Loss = 0.2786
2023-11-08 04:07:02,327 - __main__ - INFO - Epoch 108 Batch 20: Train Loss = 0.2367
2023-11-08 04:07:19,390 - __main__ - INFO - Epoch 108 Batch 40: Train Loss = 0.2548
2023-11-08 04:07:36,886 - __main__ - INFO - Epoch 108 Batch 60: Train Loss = 0.2085
2023-11-08 04:07:54,448 - __main__ - INFO - Epoch 108 Batch 80: Train Loss = 0.2537
2023-11-08 04:08:12,610 - __main__ - INFO - Epoch 108 Batch 100: Train Loss = 0.1974
2023-11-08 04:08:30,709 - __main__ - INFO - Epoch 108 Batch 120: Train Loss = 0.2095
2023-11-08 04:08:45,654 - __main__ - INFO - Epoch 108: Loss = 0.2140 Valid loss = 0.1940 roc = 0.8664
2023-11-08 04:08:46,525 - __main__ - INFO - Epoch 109 Batch 0: Train Loss = 0.1973
2023-11-08 04:09:04,746 - __main__ - INFO - Epoch 109 Batch 20: Train Loss = 0.2102
2023-11-08 04:09:22,194 - __main__ - INFO - Epoch 109 Batch 40: Train Loss = 0.2760
2023-11-08 04:09:40,017 - __main__ - INFO - Epoch 109 Batch 60: Train Loss = 0.2479
2023-11-08 04:09:58,097 - __main__ - INFO - Epoch 109 Batch 80: Train Loss = 0.2415
2023-11-08 04:10:15,338 - __main__ - INFO - Epoch 109 Batch 100: Train Loss = 0.1623
2023-11-08 04:10:32,860 - __main__ - INFO - Epoch 109 Batch 120: Train Loss = 0.2357
2023-11-08 04:10:46,753 - __main__ - INFO - Epoch 109: Loss = 0.2119 Valid loss = 0.1964 roc = 0.8627
2023-11-08 04:10:47,655 - __main__ - INFO - Epoch 110 Batch 0: Train Loss = 0.2277
2023-11-08 04:11:04,815 - __main__ - INFO - Epoch 110 Batch 20: Train Loss = 0.1937
2023-11-08 04:11:22,801 - __main__ - INFO - Epoch 110 Batch 40: Train Loss = 0.2477
2023-11-08 04:11:39,405 - __main__ - INFO - Epoch 110 Batch 60: Train Loss = 0.1314
2023-11-08 04:11:55,719 - __main__ - INFO - Epoch 110 Batch 80: Train Loss = 0.2406
2023-11-08 04:12:13,359 - __main__ - INFO - Epoch 110 Batch 100: Train Loss = 0.1975
2023-11-08 04:12:31,875 - __main__ - INFO - Epoch 110 Batch 120: Train Loss = 0.2427
2023-11-08 04:12:46,545 - __main__ - INFO - Epoch 110: Loss = 0.2119 Valid loss = 0.1987 roc = 0.8658
2023-11-08 04:12:47,462 - __main__ - INFO - Epoch 111 Batch 0: Train Loss = 0.2291
2023-11-08 04:13:04,897 - __main__ - INFO - Epoch 111 Batch 20: Train Loss = 0.2250
2023-11-08 04:13:21,450 - __main__ - INFO - Epoch 111 Batch 40: Train Loss = 0.2206
2023-11-08 04:13:37,846 - __main__ - INFO - Epoch 111 Batch 60: Train Loss = 0.2033
2023-11-08 04:13:54,828 - __main__ - INFO - Epoch 111 Batch 80: Train Loss = 0.2670
2023-11-08 04:14:12,810 - __main__ - INFO - Epoch 111 Batch 100: Train Loss = 0.1759
2023-11-08 04:14:30,823 - __main__ - INFO - Epoch 111 Batch 120: Train Loss = 0.1955
2023-11-08 04:14:44,891 - __main__ - INFO - Epoch 111: Loss = 0.2127 Valid loss = 0.1951 roc = 0.8633
2023-11-08 04:14:46,031 - __main__ - INFO - Epoch 112 Batch 0: Train Loss = 0.1949
2023-11-08 04:15:02,697 - __main__ - INFO - Epoch 112 Batch 20: Train Loss = 0.2312
2023-11-08 04:15:20,303 - __main__ - INFO - Epoch 112 Batch 40: Train Loss = 0.2267
2023-11-08 04:15:37,278 - __main__ - INFO - Epoch 112 Batch 60: Train Loss = 0.2154
2023-11-08 04:15:55,934 - __main__ - INFO - Epoch 112 Batch 80: Train Loss = 0.1647
2023-11-08 04:16:13,754 - __main__ - INFO - Epoch 112 Batch 100: Train Loss = 0.2402
2023-11-08 04:16:29,993 - __main__ - INFO - Epoch 112 Batch 120: Train Loss = 0.1716
2023-11-08 04:16:44,308 - __main__ - INFO - Epoch 112: Loss = 0.2159 Valid loss = 0.1951 roc = 0.8649
2023-11-08 04:16:45,058 - __main__ - INFO - Epoch 113 Batch 0: Train Loss = 0.2121
2023-11-08 04:17:02,515 - __main__ - INFO - Epoch 113 Batch 20: Train Loss = 0.2450
2023-11-08 04:17:18,956 - __main__ - INFO - Epoch 113 Batch 40: Train Loss = 0.1813
2023-11-08 04:17:36,486 - __main__ - INFO - Epoch 113 Batch 60: Train Loss = 0.1647
2023-11-08 04:17:54,771 - __main__ - INFO - Epoch 113 Batch 80: Train Loss = 0.2405
2023-11-08 04:18:11,211 - __main__ - INFO - Epoch 113 Batch 100: Train Loss = 0.1759
2023-11-08 04:18:29,675 - __main__ - INFO - Epoch 113 Batch 120: Train Loss = 0.1677
2023-11-08 04:18:43,058 - __main__ - INFO - Epoch 113: Loss = 0.2144 Valid loss = 0.1981 roc = 0.8658
2023-11-08 04:18:44,045 - __main__ - INFO - Epoch 114 Batch 0: Train Loss = 0.1618
2023-11-08 04:19:02,130 - __main__ - INFO - Epoch 114 Batch 20: Train Loss = 0.2358
2023-11-08 04:19:19,252 - __main__ - INFO - Epoch 114 Batch 40: Train Loss = 0.2155
2023-11-08 04:19:36,922 - __main__ - INFO - Epoch 114 Batch 60: Train Loss = 0.2573
2023-11-08 04:19:53,499 - __main__ - INFO - Epoch 114 Batch 80: Train Loss = 0.2224
2023-11-08 04:20:11,221 - __main__ - INFO - Epoch 114 Batch 100: Train Loss = 0.2421
2023-11-08 04:20:28,583 - __main__ - INFO - Epoch 114 Batch 120: Train Loss = 0.3668
2023-11-08 04:20:43,072 - __main__ - INFO - Epoch 114: Loss = 0.2209 Valid loss = 0.1955 roc = 0.8674
2023-11-08 04:20:44,081 - __main__ - INFO - Epoch 115 Batch 0: Train Loss = 0.1869
2023-11-08 04:21:00,448 - __main__ - INFO - Epoch 115 Batch 20: Train Loss = 0.1957
2023-11-08 04:21:18,495 - __main__ - INFO - Epoch 115 Batch 40: Train Loss = 0.2360
2023-11-08 04:21:36,134 - __main__ - INFO - Epoch 115 Batch 60: Train Loss = 0.2064
2023-11-08 04:21:52,644 - __main__ - INFO - Epoch 115 Batch 80: Train Loss = 0.2412
2023-11-08 04:22:09,834 - __main__ - INFO - Epoch 115 Batch 100: Train Loss = 0.2521
2023-11-08 04:22:26,749 - __main__ - INFO - Epoch 115 Batch 120: Train Loss = 0.2459
2023-11-08 04:22:41,305 - __main__ - INFO - Epoch 115: Loss = 0.2203 Valid loss = 0.1940 roc = 0.8661
2023-11-08 04:22:42,289 - __main__ - INFO - Epoch 116 Batch 0: Train Loss = 0.1789
2023-11-08 04:22:59,105 - __main__ - INFO - Epoch 116 Batch 20: Train Loss = 0.1773
2023-11-08 04:23:15,736 - __main__ - INFO - Epoch 116 Batch 40: Train Loss = 0.2132
2023-11-08 04:23:33,553 - __main__ - INFO - Epoch 116 Batch 60: Train Loss = 0.2455
2023-11-08 04:23:52,093 - __main__ - INFO - Epoch 116 Batch 80: Train Loss = 0.2147
2023-11-08 04:24:09,263 - __main__ - INFO - Epoch 116 Batch 100: Train Loss = 0.2727
2023-11-08 04:24:26,057 - __main__ - INFO - Epoch 116 Batch 120: Train Loss = 0.2420
2023-11-08 04:24:40,292 - __main__ - INFO - Epoch 116: Loss = 0.2161 Valid loss = 0.1953 roc = 0.8683
2023-11-08 04:24:41,038 - __main__ - INFO - Epoch 117 Batch 0: Train Loss = 0.1798
2023-11-08 04:24:59,107 - __main__ - INFO - Epoch 117 Batch 20: Train Loss = 0.1992
2023-11-08 04:25:16,586 - __main__ - INFO - Epoch 117 Batch 40: Train Loss = 0.3169
2023-11-08 04:25:33,545 - __main__ - INFO - Epoch 117 Batch 60: Train Loss = 0.1904
2023-11-08 04:25:51,859 - __main__ - INFO - Epoch 117 Batch 80: Train Loss = 0.2240
2023-11-08 04:26:10,360 - __main__ - INFO - Epoch 117 Batch 100: Train Loss = 0.2622
2023-11-08 04:26:27,412 - __main__ - INFO - Epoch 117 Batch 120: Train Loss = 0.1778
2023-11-08 04:26:40,995 - __main__ - INFO - Epoch 117: Loss = 0.2134 Valid loss = 0.1955 roc = 0.8704
2023-11-08 04:26:41,921 - __main__ - INFO - Epoch 118 Batch 0: Train Loss = 0.1447
2023-11-08 04:26:58,465 - __main__ - INFO - Epoch 118 Batch 20: Train Loss = 0.2169
2023-11-08 04:27:16,822 - __main__ - INFO - Epoch 118 Batch 40: Train Loss = 0.1721
2023-11-08 04:27:33,742 - __main__ - INFO - Epoch 118 Batch 60: Train Loss = 0.3001
2023-11-08 04:27:51,778 - __main__ - INFO - Epoch 118 Batch 80: Train Loss = 0.2379
2023-11-08 04:28:09,514 - __main__ - INFO - Epoch 118 Batch 100: Train Loss = 0.2065
2023-11-08 04:28:27,795 - __main__ - INFO - Epoch 118 Batch 120: Train Loss = 0.1393
2023-11-08 04:28:41,375 - __main__ - INFO - Epoch 118: Loss = 0.2148 Valid loss = 0.1996 roc = 0.8558
2023-11-08 04:28:42,155 - __main__ - INFO - Epoch 119 Batch 0: Train Loss = 0.1936
2023-11-08 04:28:59,748 - __main__ - INFO - Epoch 119 Batch 20: Train Loss = 0.2357
2023-11-08 04:29:16,856 - __main__ - INFO - Epoch 119 Batch 40: Train Loss = 0.2540
2023-11-08 04:29:34,883 - __main__ - INFO - Epoch 119 Batch 60: Train Loss = 0.2253
2023-11-08 04:29:51,997 - __main__ - INFO - Epoch 119 Batch 80: Train Loss = 0.2117
2023-11-08 04:30:09,359 - __main__ - INFO - Epoch 119 Batch 100: Train Loss = 0.1723
2023-11-08 04:30:25,784 - __main__ - INFO - Epoch 119 Batch 120: Train Loss = 0.2212
2023-11-08 04:30:38,822 - __main__ - INFO - Epoch 119: Loss = 0.2178 Valid loss = 0.1991 roc = 0.8577
2023-11-08 04:30:39,733 - __main__ - INFO - Epoch 120 Batch 0: Train Loss = 0.1773
2023-11-08 04:30:56,059 - __main__ - INFO - Epoch 120 Batch 20: Train Loss = 0.2682
2023-11-08 04:31:13,878 - __main__ - INFO - Epoch 120 Batch 40: Train Loss = 0.1828
2023-11-08 04:31:30,641 - __main__ - INFO - Epoch 120 Batch 60: Train Loss = 0.2010
2023-11-08 04:31:48,203 - __main__ - INFO - Epoch 120 Batch 80: Train Loss = 0.1902
2023-11-08 04:32:05,769 - __main__ - INFO - Epoch 120 Batch 100: Train Loss = 0.2433
2023-11-08 04:32:23,809 - __main__ - INFO - Epoch 120 Batch 120: Train Loss = 0.1873
2023-11-08 04:32:37,504 - __main__ - INFO - Epoch 120: Loss = 0.2167 Valid loss = 0.1966 roc = 0.8649
2023-11-08 04:32:38,568 - __main__ - INFO - Epoch 121 Batch 0: Train Loss = 0.2102
2023-11-08 04:32:55,884 - __main__ - INFO - Epoch 121 Batch 20: Train Loss = 0.2184
2023-11-08 04:33:13,089 - __main__ - INFO - Epoch 121 Batch 40: Train Loss = 0.2798
2023-11-08 04:33:30,667 - __main__ - INFO - Epoch 121 Batch 60: Train Loss = 0.1807
2023-11-08 04:33:48,121 - __main__ - INFO - Epoch 121 Batch 80: Train Loss = 0.2412
2023-11-08 04:34:05,830 - __main__ - INFO - Epoch 121 Batch 100: Train Loss = 0.2286
2023-11-08 04:34:23,618 - __main__ - INFO - Epoch 121 Batch 120: Train Loss = 0.1809
2023-11-08 04:34:37,376 - __main__ - INFO - Epoch 121: Loss = 0.2168 Valid loss = 0.1962 roc = 0.8618
2023-11-08 04:34:38,275 - __main__ - INFO - Epoch 122 Batch 0: Train Loss = 0.2069
2023-11-08 04:34:55,209 - __main__ - INFO - Epoch 122 Batch 20: Train Loss = 0.1940
2023-11-08 04:35:12,703 - __main__ - INFO - Epoch 122 Batch 40: Train Loss = 0.2440
2023-11-08 04:35:30,091 - __main__ - INFO - Epoch 122 Batch 60: Train Loss = 0.2602
2023-11-08 04:35:47,292 - __main__ - INFO - Epoch 122 Batch 80: Train Loss = 0.2634
2023-11-08 04:36:04,753 - __main__ - INFO - Epoch 122 Batch 100: Train Loss = 0.2405
2023-11-08 04:36:22,172 - __main__ - INFO - Epoch 122 Batch 120: Train Loss = 0.1960
2023-11-08 04:36:35,859 - __main__ - INFO - Epoch 122: Loss = 0.2110 Valid loss = 0.1924 roc = 0.8763
2023-11-08 04:36:36,954 - __main__ - INFO - Epoch 123 Batch 0: Train Loss = 0.1796
2023-11-08 04:36:54,231 - __main__ - INFO - Epoch 123 Batch 20: Train Loss = 0.1966
2023-11-08 04:37:11,833 - __main__ - INFO - Epoch 123 Batch 40: Train Loss = 0.2760
2023-11-08 04:37:28,592 - __main__ - INFO - Epoch 123 Batch 60: Train Loss = 0.1586
2023-11-08 04:37:45,722 - __main__ - INFO - Epoch 123 Batch 80: Train Loss = 0.2649
2023-11-08 04:38:01,878 - __main__ - INFO - Epoch 123 Batch 100: Train Loss = 0.2022
2023-11-08 04:38:19,818 - __main__ - INFO - Epoch 123 Batch 120: Train Loss = 0.1667
2023-11-08 04:38:33,803 - __main__ - INFO - Epoch 123: Loss = 0.2177 Valid loss = 0.1961 roc = 0.8592
2023-11-08 04:38:34,809 - __main__ - INFO - Epoch 124 Batch 0: Train Loss = 0.3034
2023-11-08 04:38:51,049 - __main__ - INFO - Epoch 124 Batch 20: Train Loss = 0.2211
2023-11-08 04:39:08,998 - __main__ - INFO - Epoch 124 Batch 40: Train Loss = 0.3175
2023-11-08 04:39:25,921 - __main__ - INFO - Epoch 124 Batch 60: Train Loss = 0.1670
2023-11-08 04:39:42,267 - __main__ - INFO - Epoch 124 Batch 80: Train Loss = 0.2209
2023-11-08 04:39:59,795 - __main__ - INFO - Epoch 124 Batch 100: Train Loss = 0.1375
2023-11-08 04:40:17,447 - __main__ - INFO - Epoch 124 Batch 120: Train Loss = 0.1947
2023-11-08 04:40:30,706 - __main__ - INFO - Epoch 124: Loss = 0.2134 Valid loss = 0.1944 roc = 0.8667
2023-11-08 04:40:31,668 - __main__ - INFO - Epoch 125 Batch 0: Train Loss = 0.1726
2023-11-08 04:40:50,271 - __main__ - INFO - Epoch 125 Batch 20: Train Loss = 0.2185
2023-11-08 04:41:09,292 - __main__ - INFO - Epoch 125 Batch 40: Train Loss = 0.1987
2023-11-08 04:41:26,228 - __main__ - INFO - Epoch 125 Batch 60: Train Loss = 0.2822
2023-11-08 04:41:44,087 - __main__ - INFO - Epoch 125 Batch 80: Train Loss = 0.2090
2023-11-08 04:42:00,234 - __main__ - INFO - Epoch 125 Batch 100: Train Loss = 0.2766
2023-11-08 04:42:17,481 - __main__ - INFO - Epoch 125 Batch 120: Train Loss = 0.2678
2023-11-08 04:42:31,210 - __main__ - INFO - Epoch 125: Loss = 0.2149 Valid loss = 0.1982 roc = 0.8654
2023-11-08 04:42:32,160 - __main__ - INFO - Epoch 126 Batch 0: Train Loss = 0.1548
2023-11-08 04:42:49,442 - __main__ - INFO - Epoch 126 Batch 20: Train Loss = 0.2205
2023-11-08 04:43:07,995 - __main__ - INFO - Epoch 126 Batch 40: Train Loss = 0.2100
2023-11-08 04:43:26,000 - __main__ - INFO - Epoch 126 Batch 60: Train Loss = 0.1695
2023-11-08 04:43:44,117 - __main__ - INFO - Epoch 126 Batch 80: Train Loss = 0.1575
2023-11-08 04:44:01,402 - __main__ - INFO - Epoch 126 Batch 100: Train Loss = 0.2035
2023-11-08 04:44:18,365 - __main__ - INFO - Epoch 126 Batch 120: Train Loss = 0.2368
2023-11-08 04:44:32,595 - __main__ - INFO - Epoch 126: Loss = 0.2215 Valid loss = 0.1992 roc = 0.8616
2023-11-08 04:44:33,458 - __main__ - INFO - Epoch 127 Batch 0: Train Loss = 0.1883
2023-11-08 04:44:50,895 - __main__ - INFO - Epoch 127 Batch 20: Train Loss = 0.2230
2023-11-08 04:45:08,257 - __main__ - INFO - Epoch 127 Batch 40: Train Loss = 0.1953
2023-11-08 04:45:25,885 - __main__ - INFO - Epoch 127 Batch 60: Train Loss = 0.1706
2023-11-08 04:45:42,664 - __main__ - INFO - Epoch 127 Batch 80: Train Loss = 0.1560
2023-11-08 04:45:59,257 - __main__ - INFO - Epoch 127 Batch 100: Train Loss = 0.1890
2023-11-08 04:46:17,405 - __main__ - INFO - Epoch 127 Batch 120: Train Loss = 0.2778
2023-11-08 04:46:30,678 - __main__ - INFO - Epoch 127: Loss = 0.2195 Valid loss = 0.1993 roc = 0.8619
2023-11-08 04:46:31,952 - __main__ - INFO - Epoch 128 Batch 0: Train Loss = 0.2894
2023-11-08 04:46:48,727 - __main__ - INFO - Epoch 128 Batch 20: Train Loss = 0.1950
2023-11-08 04:47:05,499 - __main__ - INFO - Epoch 128 Batch 40: Train Loss = 0.2252
2023-11-08 04:47:24,091 - __main__ - INFO - Epoch 128 Batch 60: Train Loss = 0.2186
2023-11-08 04:47:40,972 - __main__ - INFO - Epoch 128 Batch 80: Train Loss = 0.2591
2023-11-08 04:47:59,001 - __main__ - INFO - Epoch 128 Batch 100: Train Loss = 0.2290
2023-11-08 04:48:15,101 - __main__ - INFO - Epoch 128 Batch 120: Train Loss = 0.2011
2023-11-08 04:48:28,605 - __main__ - INFO - Epoch 128: Loss = 0.2171 Valid loss = 0.1964 roc = 0.8669
2023-11-08 04:48:29,494 - __main__ - INFO - Epoch 129 Batch 0: Train Loss = 0.2485
2023-11-08 04:48:47,835 - __main__ - INFO - Epoch 129 Batch 20: Train Loss = 0.2437
2023-11-08 04:49:04,782 - __main__ - INFO - Epoch 129 Batch 40: Train Loss = 0.2492
2023-11-08 04:49:22,535 - __main__ - INFO - Epoch 129 Batch 60: Train Loss = 0.2191
2023-11-08 04:49:39,686 - __main__ - INFO - Epoch 129 Batch 80: Train Loss = 0.1770
2023-11-08 04:49:57,416 - __main__ - INFO - Epoch 129 Batch 100: Train Loss = 0.1794
2023-11-08 04:50:14,548 - __main__ - INFO - Epoch 129 Batch 120: Train Loss = 0.2212
2023-11-08 04:50:27,337 - __main__ - INFO - Epoch 129: Loss = 0.2168 Valid loss = 0.2031 roc = 0.8526
2023-11-08 04:50:28,259 - __main__ - INFO - Epoch 130 Batch 0: Train Loss = 0.1666
2023-11-08 04:50:46,572 - __main__ - INFO - Epoch 130 Batch 20: Train Loss = 0.2180
2023-11-08 04:51:03,658 - __main__ - INFO - Epoch 130 Batch 40: Train Loss = 0.1894
2023-11-08 04:51:20,828 - __main__ - INFO - Epoch 130 Batch 60: Train Loss = 0.2316
2023-11-08 04:51:37,883 - __main__ - INFO - Epoch 130 Batch 80: Train Loss = 0.1965
2023-11-08 04:51:55,512 - __main__ - INFO - Epoch 130 Batch 100: Train Loss = 0.2754
2023-11-08 04:52:11,425 - __main__ - INFO - Epoch 130 Batch 120: Train Loss = 0.2462
2023-11-08 04:52:24,530 - __main__ - INFO - Epoch 130: Loss = 0.2193 Valid loss = 0.1985 roc = 0.8648
2023-11-08 04:52:25,298 - __main__ - INFO - Epoch 131 Batch 0: Train Loss = 0.2833
2023-11-08 04:52:42,338 - __main__ - INFO - Epoch 131 Batch 20: Train Loss = 0.1870
2023-11-08 04:52:59,542 - __main__ - INFO - Epoch 131 Batch 40: Train Loss = 0.2104
2023-11-08 04:53:17,812 - __main__ - INFO - Epoch 131 Batch 60: Train Loss = 0.2580
2023-11-08 04:53:34,197 - __main__ - INFO - Epoch 131 Batch 80: Train Loss = 0.1428
2023-11-08 04:53:51,343 - __main__ - INFO - Epoch 131 Batch 100: Train Loss = 0.2206
2023-11-08 04:54:09,279 - __main__ - INFO - Epoch 131 Batch 120: Train Loss = 0.1619
2023-11-08 04:54:22,775 - __main__ - INFO - Epoch 131: Loss = 0.2200 Valid loss = 0.1985 roc = 0.8603
2023-11-08 04:54:23,599 - __main__ - INFO - Epoch 132 Batch 0: Train Loss = 0.2314
2023-11-08 04:54:40,909 - __main__ - INFO - Epoch 132 Batch 20: Train Loss = 0.2345
2023-11-08 04:54:58,638 - __main__ - INFO - Epoch 132 Batch 40: Train Loss = 0.2120
2023-11-08 04:55:15,576 - __main__ - INFO - Epoch 132 Batch 60: Train Loss = 0.2186
2023-11-08 04:55:33,397 - __main__ - INFO - Epoch 132 Batch 80: Train Loss = 0.1802
2023-11-08 04:55:51,061 - __main__ - INFO - Epoch 132 Batch 100: Train Loss = 0.1943
2023-11-08 04:56:09,275 - __main__ - INFO - Epoch 132 Batch 120: Train Loss = 0.1558
2023-11-08 04:56:23,586 - __main__ - INFO - Epoch 132: Loss = 0.2161 Valid loss = 0.1947 roc = 0.8657
2023-11-08 04:56:24,398 - __main__ - INFO - Epoch 133 Batch 0: Train Loss = 0.1852
2023-11-08 04:56:41,436 - __main__ - INFO - Epoch 133 Batch 20: Train Loss = 0.2807
2023-11-08 04:56:58,077 - __main__ - INFO - Epoch 133 Batch 40: Train Loss = 0.2337
2023-11-08 04:57:15,540 - __main__ - INFO - Epoch 133 Batch 60: Train Loss = 0.2659
2023-11-08 04:57:34,182 - __main__ - INFO - Epoch 133 Batch 80: Train Loss = 0.2368
2023-11-08 04:57:52,541 - __main__ - INFO - Epoch 133 Batch 100: Train Loss = 0.1867
2023-11-08 04:58:09,771 - __main__ - INFO - Epoch 133 Batch 120: Train Loss = 0.2468
2023-11-08 04:58:23,149 - __main__ - INFO - Epoch 133: Loss = 0.2140 Valid loss = 0.1950 roc = 0.8704
2023-11-08 04:58:23,866 - __main__ - INFO - Epoch 134 Batch 0: Train Loss = 0.2014
2023-11-08 04:58:42,066 - __main__ - INFO - Epoch 134 Batch 20: Train Loss = 0.2030
2023-11-08 04:59:00,395 - __main__ - INFO - Epoch 134 Batch 40: Train Loss = 0.1987
2023-11-08 04:59:18,808 - __main__ - INFO - Epoch 134 Batch 60: Train Loss = 0.2309
2023-11-08 04:59:36,646 - __main__ - INFO - Epoch 134 Batch 80: Train Loss = 0.2430
2023-11-08 04:59:53,838 - __main__ - INFO - Epoch 134 Batch 100: Train Loss = 0.2506
2023-11-08 05:00:11,216 - __main__ - INFO - Epoch 134 Batch 120: Train Loss = 0.2241
2023-11-08 05:00:24,312 - __main__ - INFO - Epoch 134: Loss = 0.2160 Valid loss = 0.1957 roc = 0.8681
2023-11-08 05:00:25,246 - __main__ - INFO - Epoch 135 Batch 0: Train Loss = 0.2366
2023-11-08 05:00:43,949 - __main__ - INFO - Epoch 135 Batch 20: Train Loss = 0.1703
2023-11-08 05:01:01,953 - __main__ - INFO - Epoch 135 Batch 40: Train Loss = 0.1613
2023-11-08 05:01:18,375 - __main__ - INFO - Epoch 135 Batch 60: Train Loss = 0.1607
2023-11-08 05:01:33,913 - __main__ - INFO - Epoch 135 Batch 80: Train Loss = 0.2147
2023-11-08 05:01:51,751 - __main__ - INFO - Epoch 135 Batch 100: Train Loss = 0.2181
2023-11-08 05:02:09,382 - __main__ - INFO - Epoch 135 Batch 120: Train Loss = 0.1923
2023-11-08 05:02:23,250 - __main__ - INFO - Epoch 135: Loss = 0.2141 Valid loss = 0.1958 roc = 0.8647
2023-11-08 05:02:24,116 - __main__ - INFO - Epoch 136 Batch 0: Train Loss = 0.2522
2023-11-08 05:02:40,664 - __main__ - INFO - Epoch 136 Batch 20: Train Loss = 0.1303
2023-11-08 05:02:57,959 - __main__ - INFO - Epoch 136 Batch 40: Train Loss = 0.1928
2023-11-08 05:03:14,099 - __main__ - INFO - Epoch 136 Batch 60: Train Loss = 0.2771
2023-11-08 05:03:32,480 - __main__ - INFO - Epoch 136 Batch 80: Train Loss = 0.2302
2023-11-08 05:03:49,680 - __main__ - INFO - Epoch 136 Batch 100: Train Loss = 0.2061
2023-11-08 05:04:07,580 - __main__ - INFO - Epoch 136 Batch 120: Train Loss = 0.1973
2023-11-08 05:04:21,228 - __main__ - INFO - Epoch 136: Loss = 0.2139 Valid loss = 0.1936 roc = 0.8695
2023-11-08 05:04:21,950 - __main__ - INFO - Epoch 137 Batch 0: Train Loss = 0.1897
2023-11-08 05:04:38,520 - __main__ - INFO - Epoch 137 Batch 20: Train Loss = 0.2746
2023-11-08 05:04:56,312 - __main__ - INFO - Epoch 137 Batch 40: Train Loss = 0.2671
2023-11-08 05:05:14,298 - __main__ - INFO - Epoch 137 Batch 60: Train Loss = 0.1791
2023-11-08 05:05:31,719 - __main__ - INFO - Epoch 137 Batch 80: Train Loss = 0.1692
2023-11-08 05:05:50,566 - __main__ - INFO - Epoch 137 Batch 100: Train Loss = 0.2105
2023-11-08 05:06:07,242 - __main__ - INFO - Epoch 137 Batch 120: Train Loss = 0.2278
2023-11-08 05:06:20,988 - __main__ - INFO - Epoch 137: Loss = 0.2138 Valid loss = 0.1916 roc = 0.8782
2023-11-08 05:06:22,226 - __main__ - INFO - Epoch 138 Batch 0: Train Loss = 0.2765
2023-11-08 05:06:39,045 - __main__ - INFO - Epoch 138 Batch 20: Train Loss = 0.2241
2023-11-08 05:06:57,626 - __main__ - INFO - Epoch 138 Batch 40: Train Loss = 0.1796
2023-11-08 05:07:13,278 - __main__ - INFO - Epoch 138 Batch 60: Train Loss = 0.2236
2023-11-08 05:07:30,488 - __main__ - INFO - Epoch 138 Batch 80: Train Loss = 0.2147
2023-11-08 05:07:47,093 - __main__ - INFO - Epoch 138 Batch 100: Train Loss = 0.2065
2023-11-08 05:08:05,556 - __main__ - INFO - Epoch 138 Batch 120: Train Loss = 0.2094
2023-11-08 05:08:18,136 - __main__ - INFO - Epoch 138: Loss = 0.2134 Valid loss = 0.1953 roc = 0.8718
2023-11-08 05:08:19,459 - __main__ - INFO - Epoch 139 Batch 0: Train Loss = 0.2090
2023-11-08 05:08:37,108 - __main__ - INFO - Epoch 139 Batch 20: Train Loss = 0.2765
2023-11-08 05:08:55,527 - __main__ - INFO - Epoch 139 Batch 40: Train Loss = 0.2341
2023-11-08 05:09:12,362 - __main__ - INFO - Epoch 139 Batch 60: Train Loss = 0.1888
2023-11-08 05:09:29,184 - __main__ - INFO - Epoch 139 Batch 80: Train Loss = 0.1885
2023-11-08 05:09:46,824 - __main__ - INFO - Epoch 139 Batch 100: Train Loss = 0.2003
2023-11-08 05:10:03,671 - __main__ - INFO - Epoch 139 Batch 120: Train Loss = 0.2049
2023-11-08 05:10:17,142 - __main__ - INFO - Epoch 139: Loss = 0.2104 Valid loss = 0.1959 roc = 0.8716
2023-11-08 05:10:18,402 - __main__ - INFO - Epoch 140 Batch 0: Train Loss = 0.1630
2023-11-08 05:10:35,283 - __main__ - INFO - Epoch 140 Batch 20: Train Loss = 0.1229
2023-11-08 05:10:51,127 - __main__ - INFO - Epoch 140 Batch 40: Train Loss = 0.2516
2023-11-08 05:11:09,397 - __main__ - INFO - Epoch 140 Batch 60: Train Loss = 0.1944
2023-11-08 05:11:26,241 - __main__ - INFO - Epoch 140 Batch 80: Train Loss = 0.1819
2023-11-08 05:11:44,684 - __main__ - INFO - Epoch 140 Batch 100: Train Loss = 0.2412
2023-11-08 05:12:00,759 - __main__ - INFO - Epoch 140 Batch 120: Train Loss = 0.1772
2023-11-08 05:12:14,316 - __main__ - INFO - Epoch 140: Loss = 0.2161 Valid loss = 0.1977 roc = 0.8763
2023-11-08 05:12:15,276 - __main__ - INFO - Epoch 141 Batch 0: Train Loss = 0.2026
2023-11-08 05:12:32,493 - __main__ - INFO - Epoch 141 Batch 20: Train Loss = 0.2223
2023-11-08 05:12:49,654 - __main__ - INFO - Epoch 141 Batch 40: Train Loss = 0.1914
2023-11-08 05:13:06,623 - __main__ - INFO - Epoch 141 Batch 60: Train Loss = 0.1821
2023-11-08 05:13:24,328 - __main__ - INFO - Epoch 141 Batch 80: Train Loss = 0.2238
2023-11-08 05:13:42,116 - __main__ - INFO - Epoch 141 Batch 100: Train Loss = 0.1857
2023-11-08 05:14:00,281 - __main__ - INFO - Epoch 141 Batch 120: Train Loss = 0.2018
2023-11-08 05:14:13,341 - __main__ - INFO - Epoch 141: Loss = 0.2130 Valid loss = 0.1985 roc = 0.8697
2023-11-08 05:14:14,176 - __main__ - INFO - Epoch 142 Batch 0: Train Loss = 0.2228
2023-11-08 05:14:31,586 - __main__ - INFO - Epoch 142 Batch 20: Train Loss = 0.2035
2023-11-08 05:14:50,174 - __main__ - INFO - Epoch 142 Batch 40: Train Loss = 0.1555
2023-11-08 05:15:06,709 - __main__ - INFO - Epoch 142 Batch 60: Train Loss = 0.1921
2023-11-08 05:15:22,663 - __main__ - INFO - Epoch 142 Batch 80: Train Loss = 0.1874
2023-11-08 05:15:39,377 - __main__ - INFO - Epoch 142 Batch 100: Train Loss = 0.2968
2023-11-08 05:15:57,959 - __main__ - INFO - Epoch 142 Batch 120: Train Loss = 0.1647
2023-11-08 05:16:12,363 - __main__ - INFO - Epoch 142: Loss = 0.2110 Valid loss = 0.1939 roc = 0.8729
2023-11-08 05:16:13,147 - __main__ - INFO - Epoch 143 Batch 0: Train Loss = 0.2845
2023-11-08 05:16:31,278 - __main__ - INFO - Epoch 143 Batch 20: Train Loss = 0.2083
2023-11-08 05:16:50,240 - __main__ - INFO - Epoch 143 Batch 40: Train Loss = 0.1811
2023-11-08 05:17:07,317 - __main__ - INFO - Epoch 143 Batch 60: Train Loss = 0.2112
2023-11-08 05:17:24,004 - __main__ - INFO - Epoch 143 Batch 80: Train Loss = 0.2053
2023-11-08 05:17:42,132 - __main__ - INFO - Epoch 143 Batch 100: Train Loss = 0.1728
2023-11-08 05:17:58,708 - __main__ - INFO - Epoch 143 Batch 120: Train Loss = 0.2241
2023-11-08 05:18:11,506 - __main__ - INFO - Epoch 143: Loss = 0.2158 Valid loss = 0.1906 roc = 0.8734
2023-11-08 05:18:12,496 - __main__ - INFO - Epoch 144 Batch 0: Train Loss = 0.2829
2023-11-08 05:18:29,868 - __main__ - INFO - Epoch 144 Batch 20: Train Loss = 0.2441
2023-11-08 05:18:46,442 - __main__ - INFO - Epoch 144 Batch 40: Train Loss = 0.1871
2023-11-08 05:19:04,073 - __main__ - INFO - Epoch 144 Batch 60: Train Loss = 0.1750
2023-11-08 05:19:22,458 - __main__ - INFO - Epoch 144 Batch 80: Train Loss = 0.1959
2023-11-08 05:19:39,639 - __main__ - INFO - Epoch 144 Batch 100: Train Loss = 0.2666
2023-11-08 05:19:57,227 - __main__ - INFO - Epoch 144 Batch 120: Train Loss = 0.2709
2023-11-08 05:20:10,304 - __main__ - INFO - Epoch 144: Loss = 0.2131 Valid loss = 0.1917 roc = 0.8755
2023-11-08 05:20:11,235 - __main__ - INFO - Epoch 145 Batch 0: Train Loss = 0.2566
2023-11-08 05:20:28,204 - __main__ - INFO - Epoch 145 Batch 20: Train Loss = 0.1807
2023-11-08 05:20:44,926 - __main__ - INFO - Epoch 145 Batch 40: Train Loss = 0.2138
2023-11-08 05:21:00,964 - __main__ - INFO - Epoch 145 Batch 60: Train Loss = 0.2493
2023-11-08 05:21:18,579 - __main__ - INFO - Epoch 145 Batch 80: Train Loss = 0.1811
2023-11-08 05:21:35,532 - __main__ - INFO - Epoch 145 Batch 100: Train Loss = 0.2380
2023-11-08 05:21:52,085 - __main__ - INFO - Epoch 145 Batch 120: Train Loss = 0.2413
2023-11-08 05:22:04,659 - __main__ - INFO - Epoch 145: Loss = 0.2094 Valid loss = 0.1935 roc = 0.8775
2023-11-08 05:22:05,317 - __main__ - INFO - Epoch 146 Batch 0: Train Loss = 0.1585
2023-11-08 05:22:21,620 - __main__ - INFO - Epoch 146 Batch 20: Train Loss = 0.2322
2023-11-08 05:22:40,231 - __main__ - INFO - Epoch 146 Batch 40: Train Loss = 0.2474
2023-11-08 05:22:57,959 - __main__ - INFO - Epoch 146 Batch 60: Train Loss = 0.1588
2023-11-08 05:23:15,130 - __main__ - INFO - Epoch 146 Batch 80: Train Loss = 0.2976
2023-11-08 05:23:33,464 - __main__ - INFO - Epoch 146 Batch 100: Train Loss = 0.1508
2023-11-08 05:23:50,677 - __main__ - INFO - Epoch 146 Batch 120: Train Loss = 0.1765
2023-11-08 05:24:04,010 - __main__ - INFO - Epoch 146: Loss = 0.2151 Valid loss = 0.1936 roc = 0.8726
2023-11-08 05:24:04,930 - __main__ - INFO - Epoch 147 Batch 0: Train Loss = 0.2214
2023-11-08 05:24:22,039 - __main__ - INFO - Epoch 147 Batch 20: Train Loss = 0.2388
2023-11-08 05:24:40,030 - __main__ - INFO - Epoch 147 Batch 40: Train Loss = 0.2108
2023-11-08 05:24:56,752 - __main__ - INFO - Epoch 147 Batch 60: Train Loss = 0.1970
2023-11-08 05:25:14,510 - __main__ - INFO - Epoch 147 Batch 80: Train Loss = 0.1650
2023-11-08 05:25:31,229 - __main__ - INFO - Epoch 147 Batch 100: Train Loss = 0.2344
2023-11-08 05:25:47,699 - __main__ - INFO - Epoch 147 Batch 120: Train Loss = 0.2299
2023-11-08 05:25:59,657 - __main__ - INFO - Epoch 147: Loss = 0.2140 Valid loss = 0.1924 roc = 0.8733
2023-11-08 05:26:00,857 - __main__ - INFO - Epoch 148 Batch 0: Train Loss = 0.1902
2023-11-08 05:26:17,283 - __main__ - INFO - Epoch 148 Batch 20: Train Loss = 0.1927
2023-11-08 05:26:34,077 - __main__ - INFO - Epoch 148 Batch 40: Train Loss = 0.1777
2023-11-08 05:26:51,794 - __main__ - INFO - Epoch 148 Batch 60: Train Loss = 0.2449
2023-11-08 05:27:08,119 - __main__ - INFO - Epoch 148 Batch 80: Train Loss = 0.1672
2023-11-08 05:27:26,943 - __main__ - INFO - Epoch 148 Batch 100: Train Loss = 0.2261
2023-11-08 05:27:45,537 - __main__ - INFO - Epoch 148 Batch 120: Train Loss = 0.2108
2023-11-08 05:27:58,442 - __main__ - INFO - Epoch 148: Loss = 0.2121 Valid loss = 0.1913 roc = 0.8763
2023-11-08 05:27:59,507 - __main__ - INFO - Epoch 149 Batch 0: Train Loss = 0.2435
2023-11-08 05:28:16,135 - __main__ - INFO - Epoch 149 Batch 20: Train Loss = 0.1845
2023-11-08 05:28:34,252 - __main__ - INFO - Epoch 149 Batch 40: Train Loss = 0.1691
2023-11-08 05:28:50,211 - __main__ - INFO - Epoch 149 Batch 60: Train Loss = 0.3113
2023-11-08 05:29:06,924 - __main__ - INFO - Epoch 149 Batch 80: Train Loss = 0.1697
2023-11-08 05:29:25,072 - __main__ - INFO - Epoch 149 Batch 100: Train Loss = 0.2243
2023-11-08 05:29:42,796 - __main__ - INFO - Epoch 149 Batch 120: Train Loss = 0.2231
2023-11-08 05:29:56,440 - __main__ - INFO - Epoch 149: Loss = 0.2133 Valid loss = 0.1943 roc = 0.8758
2023-11-08 05:29:56,488 - __main__ - INFO - auroc 0.8782
2023-11-08 05:29:56,494 - __main__ - INFO - auprc 0.5018
2023-11-08 05:29:56,495 - __main__ - INFO - minpse 0.4744
2023-11-08 05:29:56,752 - __main__ - INFO - last saved model is in epoch 137
2023-11-08 05:29:57,537 - __main__ - INFO - Batch 0: Test Loss = 0.2392
2023-11-08 05:30:06,300 - __main__ - INFO - 
==>Predicting on test
2023-11-08 05:30:06,302 - __main__ - INFO - Test Loss = 0.1998
2023-11-08 05:30:06,471 - __main__ - INFO - load target data
2023-11-08 05:30:06,519 - __main__ - INFO - [[-0.43249782  0.7734225   0.11121635 -1.15107593 -0.57751738 -0.89497949
   0.64182481 -1.1001333  -0.09626591  0.07837977  0.27346416 -0.29216681
  -0.20501606 -0.4488011  -0.6600819  -0.53421373 -0.16268639  1.01340074]
 [-0.43249782  0.7734225   0.11121635 -1.15107593 -0.57751738 -0.89497949
   0.64182481 -1.1001333  -0.09626591  0.07837977  0.27346416 -0.29216681
  -0.20501606 -0.4488011  -0.6600819  -0.53421373 -0.16268639  1.01340074]
 [-0.43249782  1.06266555 -0.26070216 -0.8723303  -0.69876022  0.42060383
   0.77351631 -0.44220536 -0.09626591 -0.57033811  0.27346416  0.40542139
  -0.64784203 -0.65264891  1.05825687  1.3623417  -0.16268639  0.06953911]
 [-0.43249782  0.33955793 -0.89515138 -0.76780069  0.54061107 -0.13459647
   0.32576523  0.68002608 -0.73854097 -0.0884334   0.27346416 -0.50144327
   2.83902535 -0.1708268   0.71458912 -0.36929586 -0.4389748   0.28190798]
 [-0.43249782  0.26724717 -0.98266161 -0.62842788 -0.40238883 -0.43633576
   0.03604394 -0.01741705 -0.73854097 -0.21817698  0.27346416  0.19614493
  -0.4868144  -0.52292757  2.40168536  1.3623417  -0.4389748   0.02234603]
 [-0.43249782  0.41186869 -0.56698799 -0.27999585 -0.84694592 -0.46047491
   0.32576523  0.10508004 -0.82053353 -0.29231616  0.27346416  1.13788901
  -0.39081717 -0.70824377  1.46440967  1.3623417  -0.26629454  0.02234603]
 [-0.43249782  0.41186869 -0.56698799 -0.27999585 -0.84694592 -0.46047491
   0.32576523  0.10508004 -0.82053353 -0.29231616  0.27346416  1.13788901
  -0.39081717 -0.70824377  1.46440967  1.3623417  -0.26629454  0.02234603]]
2023-11-08 05:30:06,523 - __main__ - INFO - 18
2023-11-08 05:30:06,524 - __main__ - INFO - 361
2023-11-08 05:30:07,339 - __main__ - INFO - Batch 0: Test Loss = 0.1839
2023-11-08 05:30:18,554 - __main__ - INFO - Batch 20: Test Loss = 0.2156
2023-11-08 05:30:28,891 - __main__ - INFO - Batch 40: Test Loss = 0.2353
2023-11-08 05:30:39,141 - __main__ - INFO - Batch 60: Test Loss = 0.2236
2023-11-08 05:30:49,962 - __main__ - INFO - Batch 80: Test Loss = 0.1622
2023-11-08 05:31:00,204 - __main__ - INFO - Batch 100: Test Loss = 0.1482
2023-11-08 05:31:11,181 - __main__ - INFO - Batch 120: Test Loss = 0.1922
2023-11-08 05:31:13,920 - __main__ - INFO - Training Student
2023-11-08 05:31:14,783 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 1.0550
2023-11-08 05:31:33,287 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 0.6057
2023-11-08 05:31:49,269 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 0.3384
2023-11-08 05:32:06,877 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 0.3997
2023-11-08 05:32:25,323 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 0.2730
2023-11-08 05:32:43,097 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 0.2725
2023-11-08 05:33:00,118 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 0.2813
2023-11-08 05:33:13,454 - __main__ - INFO - ------------ Save best model - AUROC: 0.6920 ------------
2023-11-08 05:33:14,494 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 0.3332
2023-11-08 05:33:32,815 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 0.3545
2023-11-08 05:33:49,210 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 0.3366
2023-11-08 05:34:06,636 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 0.3752
2023-11-08 05:34:25,023 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 0.2709
2023-11-08 05:34:42,041 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 0.2540
2023-11-08 05:35:00,006 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 0.2751
2023-11-08 05:35:13,771 - __main__ - INFO - ------------ Save best model - AUROC: 0.6966 ------------
2023-11-08 05:35:14,857 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 0.3001
2023-11-08 05:35:33,785 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 0.3340
2023-11-08 05:35:51,122 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 0.3114
2023-11-08 05:36:10,237 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 0.3466
2023-11-08 05:36:28,287 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 0.2484
2023-11-08 05:36:46,336 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 0.2611
2023-11-08 05:37:02,304 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 0.2827
2023-11-08 05:37:14,300 - __main__ - INFO - ------------ Save best model - AUROC: 0.7506 ------------
2023-11-08 05:37:14,867 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 0.2990
2023-11-08 05:37:31,559 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 0.3352
2023-11-08 05:37:46,978 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 0.3174
2023-11-08 05:38:02,901 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 0.3279
2023-11-08 05:38:19,612 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 0.2450
2023-11-08 05:38:35,284 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 0.2265
2023-11-08 05:38:52,268 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 0.2723
2023-11-08 05:39:05,328 - __main__ - INFO - ------------ Save best model - AUROC: 0.7850 ------------
2023-11-08 05:39:06,125 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 0.3170
2023-11-08 05:39:23,248 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 0.2995
2023-11-08 05:39:39,524 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 0.3120
2023-11-08 05:39:54,655 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 0.3384
2023-11-08 05:40:10,451 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 0.2513
2023-11-08 05:40:26,050 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 0.2451
2023-11-08 05:40:41,781 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 0.2797
2023-11-08 05:40:54,040 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 0.3070
2023-11-08 05:41:10,563 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 0.3156
2023-11-08 05:41:26,283 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 0.3126
2023-11-08 05:41:42,263 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 0.3469
2023-11-08 05:41:58,323 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 0.2540
2023-11-08 05:42:14,141 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 0.2386
2023-11-08 05:42:29,293 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 0.2782
2023-11-08 05:42:41,355 - __main__ - INFO - ------------ Save best model - AUROC: 0.7879 ------------
2023-11-08 05:42:42,013 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 0.3192
2023-11-08 05:42:57,631 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 0.2970
2023-11-08 05:43:12,500 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 0.2880
2023-11-08 05:43:27,177 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 0.3158
2023-11-08 05:43:43,474 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 0.2418
2023-11-08 05:43:59,842 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 0.2244
2023-11-08 05:44:15,675 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 0.2707
2023-11-08 05:44:29,144 - __main__ - INFO - ------------ Save best model - AUROC: 0.7983 ------------
2023-11-08 05:44:30,025 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 0.3311
2023-11-08 05:44:46,724 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 0.2875
2023-11-08 05:45:02,819 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 0.2918
2023-11-08 05:45:18,698 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 0.3309
2023-11-08 05:45:34,175 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 0.2138
2023-11-08 05:45:49,027 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 0.2143
2023-11-08 05:46:03,992 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 0.2548
2023-11-08 05:46:16,716 - __main__ - INFO - ------------ Save best model - AUROC: 0.8154 ------------
2023-11-08 05:46:17,461 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 0.3167
2023-11-08 05:46:34,083 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 0.2602
2023-11-08 05:46:50,707 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 0.2908
2023-11-08 05:47:06,942 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 0.3106
2023-11-08 05:47:22,752 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 0.2203
2023-11-08 05:47:38,504 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 0.2040
2023-11-08 05:47:53,976 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 0.2666
2023-11-08 05:48:06,932 - __main__ - INFO - ------------ Save best model - AUROC: 0.8177 ------------
2023-11-08 05:48:07,829 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 0.2758
2023-11-08 05:48:23,525 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 0.2722
2023-11-08 05:48:38,983 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 0.2799
2023-11-08 05:48:55,348 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 0.2959
2023-11-08 05:49:12,332 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 0.2147
2023-11-08 05:49:27,335 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 0.2267
2023-11-08 05:49:42,662 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 0.2338
2023-11-08 05:49:55,262 - __main__ - INFO - ------------ Save best model - AUROC: 0.8211 ------------
2023-11-08 05:49:56,120 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 0.4035
2023-11-08 05:50:13,080 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 0.2624
2023-11-08 05:50:28,593 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 0.2776
2023-11-08 05:50:44,486 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 0.2993
2023-11-08 05:51:01,098 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 0.2321
2023-11-08 05:51:16,788 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 0.1967
2023-11-08 05:51:32,275 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 0.2366
2023-11-08 05:51:44,649 - __main__ - INFO - ------------ Save best model - AUROC: 0.8236 ------------
2023-11-08 05:51:45,435 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 0.2822
2023-11-08 05:52:02,599 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 0.2689
2023-11-08 05:52:17,882 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 0.2967
2023-11-08 05:52:33,300 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 0.2693
2023-11-08 05:52:49,032 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 0.2322
2023-11-08 05:53:04,282 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 0.1987
2023-11-08 05:53:20,189 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 0.2402
2023-11-08 05:53:33,019 - __main__ - INFO - ------------ Save best model - AUROC: 0.8250 ------------
2023-11-08 05:53:33,741 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 0.2810
2023-11-08 05:53:50,270 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 0.2595
2023-11-08 05:54:06,328 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 0.2753
2023-11-08 05:54:22,152 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 0.2568
2023-11-08 05:54:37,800 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 0.2337
2023-11-08 05:54:54,892 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 0.2216
2023-11-08 05:55:10,647 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 0.2320
2023-11-08 05:55:22,841 - __main__ - INFO - ------------ Save best model - AUROC: 0.8301 ------------
2023-11-08 05:55:23,685 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 0.2646
2023-11-08 05:55:40,023 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 0.2861
2023-11-08 05:55:55,693 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 0.2863
2023-11-08 05:56:10,972 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 0.2792
2023-11-08 05:56:26,993 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 0.2355
2023-11-08 05:56:43,688 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 0.2118
2023-11-08 05:56:59,463 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 0.2240
2023-11-08 05:57:12,394 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 0.3096
2023-11-08 05:57:28,909 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 0.2556
2023-11-08 05:57:44,862 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 0.2737
2023-11-08 05:58:00,122 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 0.2809
2023-11-08 05:58:15,700 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 0.2286
2023-11-08 05:58:32,573 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 0.2015
2023-11-08 05:58:47,886 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 0.2247
2023-11-08 05:59:00,568 - __main__ - INFO - ------------ Save best model - AUROC: 0.8332 ------------
2023-11-08 05:59:01,436 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 0.2791
2023-11-08 05:59:18,050 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 0.2683
2023-11-08 05:59:33,821 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 0.2724
2023-11-08 05:59:50,059 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 0.2894
2023-11-08 06:00:07,101 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 0.2464
2023-11-08 06:00:23,679 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 0.1971
2023-11-08 06:00:39,997 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 0.2647
2023-11-08 06:00:51,975 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 0.3366
2023-11-08 06:01:07,469 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 0.2569
2023-11-08 06:01:24,051 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 0.2773
2023-11-08 06:01:40,121 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 0.2592
2023-11-08 06:01:56,250 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 0.2385
2023-11-08 06:02:12,201 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 0.2107
2023-11-08 06:02:28,200 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 0.2210
2023-11-08 06:02:40,588 - __main__ - INFO - ------------ Save best model - AUROC: 0.8350 ------------
2023-11-08 06:02:41,275 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 0.2661
2023-11-08 06:02:57,944 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 0.2568
2023-11-08 06:03:13,313 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 0.2930
2023-11-08 06:03:29,132 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 0.2709
2023-11-08 06:03:45,377 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 0.2345
2023-11-08 06:04:01,906 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 0.1785
2023-11-08 06:04:16,329 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 0.2378
2023-11-08 06:04:29,289 - __main__ - INFO - ------------ Save best model - AUROC: 0.8360 ------------
2023-11-08 06:04:29,991 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 0.2757
2023-11-08 06:04:46,511 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 0.2666
2023-11-08 06:05:02,125 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 0.2705
2023-11-08 06:05:17,759 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 0.2834
2023-11-08 06:05:34,162 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 0.2242
2023-11-08 06:05:50,367 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 0.2009
2023-11-08 06:06:06,226 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 0.2233
2023-11-08 06:06:18,849 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 0.2567
2023-11-08 06:06:35,681 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 0.2695
2023-11-08 06:06:51,446 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 0.2788
2023-11-08 06:07:06,627 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 0.2969
2023-11-08 06:07:22,986 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 0.2139
2023-11-08 06:07:39,954 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 0.1990
2023-11-08 06:07:55,415 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 0.2501
2023-11-08 06:08:07,250 - __main__ - INFO - ------------ Save best model - AUROC: 0.8382 ------------
2023-11-08 06:08:08,131 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 0.2578
2023-11-08 06:08:23,976 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 0.2747
2023-11-08 06:08:39,040 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 0.2812
2023-11-08 06:08:54,540 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 0.2891
2023-11-08 06:09:11,095 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 0.2367
2023-11-08 06:09:27,474 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 0.2004
2023-11-08 06:09:43,632 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 0.2362
2023-11-08 06:09:56,290 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 0.2411
2023-11-08 06:10:13,731 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 0.2702
2023-11-08 06:10:28,693 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 0.2475
2023-11-08 06:10:44,384 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 0.2681
2023-11-08 06:11:00,827 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 0.2207
2023-11-08 06:11:17,732 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 0.2186
2023-11-08 06:11:32,455 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 0.2350
2023-11-08 06:11:45,514 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 0.2417
2023-11-08 06:12:02,009 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 0.2592
2023-11-08 06:12:17,332 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 0.2579
2023-11-08 06:12:32,626 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 0.2804
2023-11-08 06:12:48,478 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 0.2418
2023-11-08 06:13:04,625 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 0.2086
2023-11-08 06:13:21,694 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 0.2237
2023-11-08 06:13:33,173 - __main__ - INFO - ------------ Save best model - AUROC: 0.8393 ------------
2023-11-08 06:13:33,762 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 0.2565
2023-11-08 06:13:50,573 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 0.2643
2023-11-08 06:14:06,212 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 0.2863
2023-11-08 06:14:22,661 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 0.2810
2023-11-08 06:14:39,491 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 0.2140
2023-11-08 06:14:56,460 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 0.2005
2023-11-08 06:15:12,240 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 0.2046
2023-11-08 06:15:23,687 - __main__ - INFO - ------------ Save best model - AUROC: 0.8411 ------------
2023-11-08 06:15:24,491 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 0.2486
2023-11-08 06:15:41,736 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 0.2427
2023-11-08 06:15:57,806 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 0.2813
2023-11-08 06:16:12,244 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 0.2713
2023-11-08 06:16:28,356 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 0.2152
2023-11-08 06:16:44,645 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 0.1829
2023-11-08 06:17:01,555 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 0.2370
2023-11-08 06:17:14,389 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 0.2543
2023-11-08 06:17:30,819 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 0.2567
2023-11-08 06:17:45,387 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 0.2936
2023-11-08 06:18:02,098 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 0.2704
2023-11-08 06:18:18,441 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 0.2331
2023-11-08 06:18:34,693 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 0.1891
2023-11-08 06:18:50,146 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 0.2310
2023-11-08 06:19:02,905 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 0.2475
2023-11-08 06:19:19,674 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 0.2529
2023-11-08 06:19:35,123 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 0.2767
2023-11-08 06:19:51,344 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 0.2635
2023-11-08 06:20:08,446 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 0.2213
2023-11-08 06:20:25,238 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 0.1957
2023-11-08 06:20:40,903 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 0.2023
2023-11-08 06:20:53,582 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 0.2544
2023-11-08 06:21:09,020 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 0.2667
2023-11-08 06:21:24,239 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 0.2930
2023-11-08 06:21:40,149 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 0.2779
2023-11-08 06:21:56,417 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 0.2248
2023-11-08 06:22:12,515 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 0.2005
2023-11-08 06:22:28,159 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 0.2415
2023-11-08 06:22:41,962 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 0.2361
2023-11-08 06:22:58,212 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 0.2742
2023-11-08 06:23:14,664 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 0.2898
2023-11-08 06:23:30,459 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 0.2811
2023-11-08 06:23:46,647 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 0.2170
2023-11-08 06:24:01,693 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 0.1936
2023-11-08 06:24:17,529 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 0.2409
2023-11-08 06:24:28,313 - __main__ - INFO - ------------ Save best model - AUROC: 0.8413 ------------
2023-11-08 06:24:28,955 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 0.2275
2023-11-08 06:24:43,067 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 0.2513
2023-11-08 06:24:57,367 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 0.2807
2023-11-08 06:25:12,062 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 0.2692
2023-11-08 06:25:25,609 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 0.2383
2023-11-08 06:25:40,112 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 0.1930
2023-11-08 06:25:53,927 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 0.2457
2023-11-08 06:26:05,418 - __main__ - INFO - ------------ Save best model - AUROC: 0.8431 ------------
2023-11-08 06:26:06,003 - __main__ - INFO - Epoch 30 Batch 0: Train Loss = 0.2529
2023-11-08 06:26:20,573 - __main__ - INFO - Epoch 30 Batch 20: Train Loss = 0.2470
2023-11-08 06:26:34,781 - __main__ - INFO - Epoch 30 Batch 40: Train Loss = 0.2793
2023-11-08 06:26:49,643 - __main__ - INFO - Epoch 30 Batch 60: Train Loss = 0.2745
2023-11-08 06:27:04,772 - __main__ - INFO - Epoch 30 Batch 80: Train Loss = 0.2060
2023-11-08 06:27:18,400 - __main__ - INFO - Epoch 30 Batch 100: Train Loss = 0.1975
2023-11-08 06:27:32,428 - __main__ - INFO - Epoch 30 Batch 120: Train Loss = 0.2221
2023-11-08 06:27:43,396 - __main__ - INFO - ------------ Save best model - AUROC: 0.8444 ------------
2023-11-08 06:27:43,924 - __main__ - INFO - Epoch 31 Batch 0: Train Loss = 0.2464
2023-11-08 06:27:59,578 - __main__ - INFO - Epoch 31 Batch 20: Train Loss = 0.2632
2023-11-08 06:28:14,311 - __main__ - INFO - Epoch 31 Batch 40: Train Loss = 0.2805
2023-11-08 06:28:29,351 - __main__ - INFO - Epoch 31 Batch 60: Train Loss = 0.2831
2023-11-08 06:28:44,632 - __main__ - INFO - Epoch 31 Batch 80: Train Loss = 0.2092
2023-11-08 06:28:59,397 - __main__ - INFO - Epoch 31 Batch 100: Train Loss = 0.1883
2023-11-08 06:29:13,668 - __main__ - INFO - Epoch 31 Batch 120: Train Loss = 0.2048
2023-11-08 06:29:23,492 - __main__ - INFO - ------------ Save best model - AUROC: 0.8453 ------------
2023-11-08 06:29:24,208 - __main__ - INFO - Epoch 32 Batch 0: Train Loss = 0.2606
2023-11-08 06:29:38,452 - __main__ - INFO - Epoch 32 Batch 20: Train Loss = 0.2607
2023-11-08 06:29:52,269 - __main__ - INFO - Epoch 32 Batch 40: Train Loss = 0.2495
2023-11-08 06:30:05,939 - __main__ - INFO - Epoch 32 Batch 60: Train Loss = 0.2611
2023-11-08 06:30:20,947 - __main__ - INFO - Epoch 32 Batch 80: Train Loss = 0.2095
2023-11-08 06:30:35,407 - __main__ - INFO - Epoch 32 Batch 100: Train Loss = 0.1952
2023-11-08 06:30:49,750 - __main__ - INFO - Epoch 32 Batch 120: Train Loss = 0.2290
2023-11-08 06:31:01,325 - __main__ - INFO - Epoch 33 Batch 0: Train Loss = 0.2349
2023-11-08 06:31:16,694 - __main__ - INFO - Epoch 33 Batch 20: Train Loss = 0.2619
2023-11-08 06:31:30,475 - __main__ - INFO - Epoch 33 Batch 40: Train Loss = 0.2990
2023-11-08 06:31:44,059 - __main__ - INFO - Epoch 33 Batch 60: Train Loss = 0.2614
2023-11-08 06:31:59,541 - __main__ - INFO - Epoch 33 Batch 80: Train Loss = 0.2102
2023-11-08 06:32:13,678 - __main__ - INFO - Epoch 33 Batch 100: Train Loss = 0.2009
2023-11-08 06:32:27,296 - __main__ - INFO - Epoch 33 Batch 120: Train Loss = 0.2582
2023-11-08 06:32:38,949 - __main__ - INFO - Epoch 34 Batch 0: Train Loss = 0.2417
2023-11-08 06:32:53,248 - __main__ - INFO - Epoch 34 Batch 20: Train Loss = 0.2434
2023-11-08 06:33:07,849 - __main__ - INFO - Epoch 34 Batch 40: Train Loss = 0.2844
2023-11-08 06:33:22,574 - __main__ - INFO - Epoch 34 Batch 60: Train Loss = 0.2604
2023-11-08 06:33:38,913 - __main__ - INFO - Epoch 34 Batch 80: Train Loss = 0.2380
2023-11-08 06:33:52,510 - __main__ - INFO - Epoch 34 Batch 100: Train Loss = 0.2053
2023-11-08 06:34:06,952 - __main__ - INFO - Epoch 34 Batch 120: Train Loss = 0.2047
2023-11-08 06:34:19,105 - __main__ - INFO - Epoch 35 Batch 0: Train Loss = 0.2719
2023-11-08 06:34:34,370 - __main__ - INFO - Epoch 35 Batch 20: Train Loss = 0.2518
2023-11-08 06:34:48,852 - __main__ - INFO - Epoch 35 Batch 40: Train Loss = 0.2940
2023-11-08 06:35:02,659 - __main__ - INFO - Epoch 35 Batch 60: Train Loss = 0.2582
2023-11-08 06:35:17,196 - __main__ - INFO - Epoch 35 Batch 80: Train Loss = 0.2373
2023-11-08 06:35:31,955 - __main__ - INFO - Epoch 35 Batch 100: Train Loss = 0.1856
2023-11-08 06:35:46,203 - __main__ - INFO - Epoch 35 Batch 120: Train Loss = 0.1969
2023-11-08 06:35:59,437 - __main__ - INFO - Epoch 36 Batch 0: Train Loss = 0.2428
2023-11-08 06:36:14,555 - __main__ - INFO - Epoch 36 Batch 20: Train Loss = 0.2685
2023-11-08 06:36:29,783 - __main__ - INFO - Epoch 36 Batch 40: Train Loss = 0.2919
2023-11-08 06:36:43,483 - __main__ - INFO - Epoch 36 Batch 60: Train Loss = 0.2547
2023-11-08 06:36:57,781 - __main__ - INFO - Epoch 36 Batch 80: Train Loss = 0.2104
2023-11-08 06:37:12,251 - __main__ - INFO - Epoch 36 Batch 100: Train Loss = 0.1903
2023-11-08 06:37:25,954 - __main__ - INFO - Epoch 36 Batch 120: Train Loss = 0.2170
2023-11-08 06:37:36,978 - __main__ - INFO - Epoch 37 Batch 0: Train Loss = 0.2137
2023-11-08 06:37:51,181 - __main__ - INFO - Epoch 37 Batch 20: Train Loss = 0.2465
2023-11-08 06:38:05,086 - __main__ - INFO - Epoch 37 Batch 40: Train Loss = 0.2630
2023-11-08 06:38:20,316 - __main__ - INFO - Epoch 37 Batch 60: Train Loss = 0.2651
2023-11-08 06:38:34,635 - __main__ - INFO - Epoch 37 Batch 80: Train Loss = 0.2101
2023-11-08 06:38:49,902 - __main__ - INFO - Epoch 37 Batch 100: Train Loss = 0.1973
2023-11-08 06:39:03,255 - __main__ - INFO - Epoch 37 Batch 120: Train Loss = 0.2007
2023-11-08 06:39:14,969 - __main__ - INFO - Epoch 38 Batch 0: Train Loss = 0.2476
2023-11-08 06:39:29,108 - __main__ - INFO - Epoch 38 Batch 20: Train Loss = 0.2677
2023-11-08 06:39:42,464 - __main__ - INFO - Epoch 38 Batch 40: Train Loss = 0.2988
2023-11-08 06:39:55,472 - __main__ - INFO - Epoch 38 Batch 60: Train Loss = 0.2926
2023-11-08 06:40:10,436 - __main__ - INFO - Epoch 38 Batch 80: Train Loss = 0.2192
2023-11-08 06:40:25,092 - __main__ - INFO - Epoch 38 Batch 100: Train Loss = 0.1951
2023-11-08 06:40:39,073 - __main__ - INFO - Epoch 38 Batch 120: Train Loss = 0.2116
2023-11-08 06:40:51,146 - __main__ - INFO - Epoch 39 Batch 0: Train Loss = 0.2598
2023-11-08 06:41:05,226 - __main__ - INFO - Epoch 39 Batch 20: Train Loss = 0.2296
2023-11-08 06:41:19,224 - __main__ - INFO - Epoch 39 Batch 40: Train Loss = 0.2884
2023-11-08 06:41:33,747 - __main__ - INFO - Epoch 39 Batch 60: Train Loss = 0.2661
2023-11-08 06:41:48,491 - __main__ - INFO - Epoch 39 Batch 80: Train Loss = 0.2312
2023-11-08 06:42:02,456 - __main__ - INFO - Epoch 39 Batch 100: Train Loss = 0.1736
2023-11-08 06:42:16,131 - __main__ - INFO - Epoch 39 Batch 120: Train Loss = 0.2199
2023-11-08 06:42:27,904 - __main__ - INFO - Epoch 40 Batch 0: Train Loss = 0.2501
2023-11-08 06:42:42,752 - __main__ - INFO - Epoch 40 Batch 20: Train Loss = 0.2331
2023-11-08 06:42:56,376 - __main__ - INFO - Epoch 40 Batch 40: Train Loss = 0.2774
2023-11-08 06:43:10,672 - __main__ - INFO - Epoch 40 Batch 60: Train Loss = 0.2879
2023-11-08 06:43:25,754 - __main__ - INFO - Epoch 40 Batch 80: Train Loss = 0.2156
2023-11-08 06:43:41,228 - __main__ - INFO - Epoch 40 Batch 100: Train Loss = 0.1965
2023-11-08 06:43:55,452 - __main__ - INFO - Epoch 40 Batch 120: Train Loss = 0.2275
2023-11-08 06:44:05,986 - __main__ - INFO - ------------ Save best model - AUROC: 0.8458 ------------
2023-11-08 06:44:06,896 - __main__ - INFO - Epoch 41 Batch 0: Train Loss = 0.2438
2023-11-08 06:44:21,034 - __main__ - INFO - Epoch 41 Batch 20: Train Loss = 0.2713
2023-11-08 06:44:35,104 - __main__ - INFO - Epoch 41 Batch 40: Train Loss = 0.2857
2023-11-08 06:44:48,869 - __main__ - INFO - Epoch 41 Batch 60: Train Loss = 0.2552
2023-11-08 06:45:03,299 - __main__ - INFO - Epoch 41 Batch 80: Train Loss = 0.2296
2023-11-08 06:45:17,627 - __main__ - INFO - Epoch 41 Batch 100: Train Loss = 0.1965
2023-11-08 06:45:31,961 - __main__ - INFO - Epoch 41 Batch 120: Train Loss = 0.1902
2023-11-08 06:45:42,338 - __main__ - INFO - ------------ Save best model - AUROC: 0.8463 ------------
2023-11-08 06:45:43,029 - __main__ - INFO - Epoch 42 Batch 0: Train Loss = 0.2249
2023-11-08 06:45:57,194 - __main__ - INFO - Epoch 42 Batch 20: Train Loss = 0.2741
2023-11-08 06:46:10,209 - __main__ - INFO - Epoch 42 Batch 40: Train Loss = 0.2886
2023-11-08 06:46:24,025 - __main__ - INFO - Epoch 42 Batch 60: Train Loss = 0.2718
2023-11-08 06:46:38,315 - __main__ - INFO - Epoch 42 Batch 80: Train Loss = 0.2288
2023-11-08 06:46:51,836 - __main__ - INFO - Epoch 42 Batch 100: Train Loss = 0.1877
2023-11-08 06:47:05,875 - __main__ - INFO - Epoch 42 Batch 120: Train Loss = 0.2294
2023-11-08 06:47:17,730 - __main__ - INFO - Epoch 43 Batch 0: Train Loss = 0.2474
2023-11-08 06:47:33,405 - __main__ - INFO - Epoch 43 Batch 20: Train Loss = 0.2679
2023-11-08 06:47:47,015 - __main__ - INFO - Epoch 43 Batch 40: Train Loss = 0.2785
2023-11-08 06:48:00,533 - __main__ - INFO - Epoch 43 Batch 60: Train Loss = 0.2720
2023-11-08 06:48:15,614 - __main__ - INFO - Epoch 43 Batch 80: Train Loss = 0.2350
2023-11-08 06:48:30,295 - __main__ - INFO - Epoch 43 Batch 100: Train Loss = 0.1735
2023-11-08 06:48:43,979 - __main__ - INFO - Epoch 43 Batch 120: Train Loss = 0.2194
2023-11-08 06:48:53,747 - __main__ - INFO - ------------ Save best model - AUROC: 0.8478 ------------
2023-11-08 06:48:54,392 - __main__ - INFO - Epoch 44 Batch 0: Train Loss = 0.2532
2023-11-08 06:49:09,632 - __main__ - INFO - Epoch 44 Batch 20: Train Loss = 0.2487
2023-11-08 06:49:22,853 - __main__ - INFO - Epoch 44 Batch 40: Train Loss = 0.2752
2023-11-08 06:49:35,821 - __main__ - INFO - Epoch 44 Batch 60: Train Loss = 0.2553
2023-11-08 06:49:51,437 - __main__ - INFO - Epoch 44 Batch 80: Train Loss = 0.2291
2023-11-08 06:50:05,489 - __main__ - INFO - Epoch 44 Batch 100: Train Loss = 0.1902
2023-11-08 06:50:19,713 - __main__ - INFO - Epoch 44 Batch 120: Train Loss = 0.2263
2023-11-08 06:50:30,768 - __main__ - INFO - Epoch 45 Batch 0: Train Loss = 0.2530
2023-11-08 06:50:45,416 - __main__ - INFO - Epoch 45 Batch 20: Train Loss = 0.2578
2023-11-08 06:50:59,528 - __main__ - INFO - Epoch 45 Batch 40: Train Loss = 0.2958
2023-11-08 06:51:12,783 - __main__ - INFO - Epoch 45 Batch 60: Train Loss = 0.2584
2023-11-08 06:51:26,650 - __main__ - INFO - Epoch 45 Batch 80: Train Loss = 0.1968
2023-11-08 06:51:41,023 - __main__ - INFO - Epoch 45 Batch 100: Train Loss = 0.2103
2023-11-08 06:51:55,165 - __main__ - INFO - Epoch 45 Batch 120: Train Loss = 0.2422
2023-11-08 06:52:07,294 - __main__ - INFO - Epoch 46 Batch 0: Train Loss = 0.2747
2023-11-08 06:52:21,633 - __main__ - INFO - Epoch 46 Batch 20: Train Loss = 0.2550
2023-11-08 06:52:34,731 - __main__ - INFO - Epoch 46 Batch 40: Train Loss = 0.2608
2023-11-08 06:52:48,199 - __main__ - INFO - Epoch 46 Batch 60: Train Loss = 0.2552
2023-11-08 06:53:02,192 - __main__ - INFO - Epoch 46 Batch 80: Train Loss = 0.2096
2023-11-08 06:53:16,231 - __main__ - INFO - Epoch 46 Batch 100: Train Loss = 0.1926
2023-11-08 06:53:31,250 - __main__ - INFO - Epoch 46 Batch 120: Train Loss = 0.2295
2023-11-08 06:53:42,847 - __main__ - INFO - Epoch 47 Batch 0: Train Loss = 0.2612
2023-11-08 06:53:56,932 - __main__ - INFO - Epoch 47 Batch 20: Train Loss = 0.2600
2023-11-08 06:54:10,056 - __main__ - INFO - Epoch 47 Batch 40: Train Loss = 0.2753
2023-11-08 06:54:23,897 - __main__ - INFO - Epoch 47 Batch 60: Train Loss = 0.2708
2023-11-08 06:54:38,013 - __main__ - INFO - Epoch 47 Batch 80: Train Loss = 0.2088
2023-11-08 06:54:51,137 - __main__ - INFO - Epoch 47 Batch 100: Train Loss = 0.1881
2023-11-08 06:55:04,529 - __main__ - INFO - Epoch 47 Batch 120: Train Loss = 0.2320
2023-11-08 06:55:15,345 - __main__ - INFO - Epoch 48 Batch 0: Train Loss = 0.2632
2023-11-08 06:55:29,275 - __main__ - INFO - Epoch 48 Batch 20: Train Loss = 0.2319
2023-11-08 06:55:43,003 - __main__ - INFO - Epoch 48 Batch 40: Train Loss = 0.2843
2023-11-08 06:55:56,006 - __main__ - INFO - Epoch 48 Batch 60: Train Loss = 0.2548
2023-11-08 06:56:09,751 - __main__ - INFO - Epoch 48 Batch 80: Train Loss = 0.2150
2023-11-08 06:56:23,379 - __main__ - INFO - Epoch 48 Batch 100: Train Loss = 0.1800
2023-11-08 06:56:37,663 - __main__ - INFO - Epoch 48 Batch 120: Train Loss = 0.2311
2023-11-08 06:56:49,097 - __main__ - INFO - Epoch 49 Batch 0: Train Loss = 0.2319
2023-11-08 06:57:03,360 - __main__ - INFO - Epoch 49 Batch 20: Train Loss = 0.2672
2023-11-08 06:57:17,267 - __main__ - INFO - Epoch 49 Batch 40: Train Loss = 0.2894
2023-11-08 06:57:31,255 - __main__ - INFO - Epoch 49 Batch 60: Train Loss = 0.2624
2023-11-08 06:57:45,342 - __main__ - INFO - Epoch 49 Batch 80: Train Loss = 0.2049
2023-11-08 06:57:59,864 - __main__ - INFO - Epoch 49 Batch 100: Train Loss = 0.1790
2023-11-08 06:58:13,587 - __main__ - INFO - Epoch 49 Batch 120: Train Loss = 0.2194
2023-11-08 06:58:25,729 - __main__ - INFO - Epoch 50 Batch 0: Train Loss = 0.2520
2023-11-08 06:58:40,885 - __main__ - INFO - Epoch 50 Batch 20: Train Loss = 0.2775
2023-11-08 06:58:55,073 - __main__ - INFO - Epoch 50 Batch 40: Train Loss = 0.2976
2023-11-08 06:59:08,763 - __main__ - INFO - Epoch 50 Batch 60: Train Loss = 0.2521
2023-11-08 06:59:23,549 - __main__ - INFO - Epoch 50 Batch 80: Train Loss = 0.2198
2023-11-08 06:59:36,997 - __main__ - INFO - Epoch 50 Batch 100: Train Loss = 0.2035
2023-11-08 06:59:50,838 - __main__ - INFO - Epoch 50 Batch 120: Train Loss = 0.2209
2023-11-08 07:00:02,357 - __main__ - INFO - Epoch 51 Batch 0: Train Loss = 0.2386
2023-11-08 07:00:17,616 - __main__ - INFO - Epoch 51 Batch 20: Train Loss = 0.2580
2023-11-08 07:00:31,716 - __main__ - INFO - Epoch 51 Batch 40: Train Loss = 0.2694
2023-11-08 07:00:45,860 - __main__ - INFO - Epoch 51 Batch 60: Train Loss = 0.2444
2023-11-08 07:01:00,277 - __main__ - INFO - Epoch 51 Batch 80: Train Loss = 0.2217
2023-11-08 07:01:14,664 - __main__ - INFO - Epoch 51 Batch 100: Train Loss = 0.1937
2023-11-08 07:01:29,428 - __main__ - INFO - Epoch 51 Batch 120: Train Loss = 0.2195
2023-11-08 07:01:40,506 - __main__ - INFO - Epoch 52 Batch 0: Train Loss = 0.2406
2023-11-08 07:01:56,360 - __main__ - INFO - Epoch 52 Batch 20: Train Loss = 0.2339
2023-11-08 07:02:10,398 - __main__ - INFO - Epoch 52 Batch 40: Train Loss = 0.2629
2023-11-08 07:02:23,781 - __main__ - INFO - Epoch 52 Batch 60: Train Loss = 0.2682
2023-11-08 07:02:37,665 - __main__ - INFO - Epoch 52 Batch 80: Train Loss = 0.2041
2023-11-08 07:02:51,732 - __main__ - INFO - Epoch 52 Batch 100: Train Loss = 0.1677
2023-11-08 07:03:05,047 - __main__ - INFO - Epoch 52 Batch 120: Train Loss = 0.2234
2023-11-08 07:03:16,104 - __main__ - INFO - ------------ Save best model - AUROC: 0.8510 ------------
2023-11-08 07:03:16,675 - __main__ - INFO - Epoch 53 Batch 0: Train Loss = 0.2737
2023-11-08 07:03:31,541 - __main__ - INFO - Epoch 53 Batch 20: Train Loss = 0.2501
2023-11-08 07:03:44,747 - __main__ - INFO - Epoch 53 Batch 40: Train Loss = 0.2740
2023-11-08 07:03:58,935 - __main__ - INFO - Epoch 53 Batch 60: Train Loss = 0.2735
2023-11-08 07:04:13,728 - __main__ - INFO - Epoch 53 Batch 80: Train Loss = 0.2162
2023-11-08 07:04:27,312 - __main__ - INFO - Epoch 53 Batch 100: Train Loss = 0.2091
2023-11-08 07:04:40,686 - __main__ - INFO - Epoch 53 Batch 120: Train Loss = 0.2237
2023-11-08 07:04:51,315 - __main__ - INFO - Epoch 54 Batch 0: Train Loss = 0.2284
2023-11-08 07:05:06,321 - __main__ - INFO - Epoch 54 Batch 20: Train Loss = 0.2653
2023-11-08 07:05:20,367 - __main__ - INFO - Epoch 54 Batch 40: Train Loss = 0.2575
2023-11-08 07:05:34,173 - __main__ - INFO - Epoch 54 Batch 60: Train Loss = 0.3063
2023-11-08 07:05:49,170 - __main__ - INFO - Epoch 54 Batch 80: Train Loss = 0.2123
2023-11-08 07:06:04,470 - __main__ - INFO - Epoch 54 Batch 100: Train Loss = 0.1970
2023-11-08 07:06:18,056 - __main__ - INFO - Epoch 54 Batch 120: Train Loss = 0.2218
2023-11-08 07:06:29,724 - __main__ - INFO - Epoch 55 Batch 0: Train Loss = 0.2609
2023-11-08 07:06:44,349 - __main__ - INFO - Epoch 55 Batch 20: Train Loss = 0.2651
2023-11-08 07:06:57,727 - __main__ - INFO - Epoch 55 Batch 40: Train Loss = 0.2984
2023-11-08 07:07:11,880 - __main__ - INFO - Epoch 55 Batch 60: Train Loss = 0.2659
2023-11-08 07:07:25,997 - __main__ - INFO - Epoch 55 Batch 80: Train Loss = 0.2198
2023-11-08 07:07:40,235 - __main__ - INFO - Epoch 55 Batch 100: Train Loss = 0.1905
2023-11-08 07:07:53,917 - __main__ - INFO - Epoch 55 Batch 120: Train Loss = 0.1990
2023-11-08 07:08:05,040 - __main__ - INFO - Epoch 56 Batch 0: Train Loss = 0.2348
2023-11-08 07:08:19,869 - __main__ - INFO - Epoch 56 Batch 20: Train Loss = 0.2590
2023-11-08 07:08:34,302 - __main__ - INFO - Epoch 56 Batch 40: Train Loss = 0.2667
2023-11-08 07:08:48,546 - __main__ - INFO - Epoch 56 Batch 60: Train Loss = 0.2717
2023-11-08 07:09:02,280 - __main__ - INFO - Epoch 56 Batch 80: Train Loss = 0.2266
2023-11-08 07:09:17,135 - __main__ - INFO - Epoch 56 Batch 100: Train Loss = 0.1738
2023-11-08 07:09:31,167 - __main__ - INFO - Epoch 56 Batch 120: Train Loss = 0.2232
2023-11-08 07:09:42,812 - __main__ - INFO - Epoch 57 Batch 0: Train Loss = 0.2393
2023-11-08 07:09:56,770 - __main__ - INFO - Epoch 57 Batch 20: Train Loss = 0.2483
2023-11-08 07:10:10,657 - __main__ - INFO - Epoch 57 Batch 40: Train Loss = 0.2736
2023-11-08 07:10:24,106 - __main__ - INFO - Epoch 57 Batch 60: Train Loss = 0.2826
2023-11-08 07:10:38,072 - __main__ - INFO - Epoch 57 Batch 80: Train Loss = 0.1922
2023-11-08 07:10:52,128 - __main__ - INFO - Epoch 57 Batch 100: Train Loss = 0.1826
2023-11-08 07:11:06,416 - __main__ - INFO - Epoch 57 Batch 120: Train Loss = 0.2413
2023-11-08 07:11:17,405 - __main__ - INFO - Epoch 58 Batch 0: Train Loss = 0.2439
2023-11-08 07:11:32,070 - __main__ - INFO - Epoch 58 Batch 20: Train Loss = 0.2656
2023-11-08 07:11:46,097 - __main__ - INFO - Epoch 58 Batch 40: Train Loss = 0.2637
2023-11-08 07:11:58,817 - __main__ - INFO - Epoch 58 Batch 60: Train Loss = 0.2441
2023-11-08 07:12:12,936 - __main__ - INFO - Epoch 58 Batch 80: Train Loss = 0.2158
2023-11-08 07:12:26,681 - __main__ - INFO - Epoch 58 Batch 100: Train Loss = 0.2023
2023-11-08 07:12:39,823 - __main__ - INFO - Epoch 58 Batch 120: Train Loss = 0.2232
2023-11-08 07:12:51,862 - __main__ - INFO - Epoch 59 Batch 0: Train Loss = 0.2490
2023-11-08 07:13:05,868 - __main__ - INFO - Epoch 59 Batch 20: Train Loss = 0.2336
2023-11-08 07:13:20,031 - __main__ - INFO - Epoch 59 Batch 40: Train Loss = 0.2707
2023-11-08 07:13:34,602 - __main__ - INFO - Epoch 59 Batch 60: Train Loss = 0.2808
2023-11-08 07:13:49,594 - __main__ - INFO - Epoch 59 Batch 80: Train Loss = 0.2096
2023-11-08 07:14:03,098 - __main__ - INFO - Epoch 59 Batch 100: Train Loss = 0.1817
2023-11-08 07:14:17,019 - __main__ - INFO - Epoch 59 Batch 120: Train Loss = 0.2528
2023-11-08 07:14:28,446 - __main__ - INFO - Epoch 60 Batch 0: Train Loss = 0.2747
2023-11-08 07:14:42,173 - __main__ - INFO - Epoch 60 Batch 20: Train Loss = 0.2685
2023-11-08 07:14:56,247 - __main__ - INFO - Epoch 60 Batch 40: Train Loss = 0.2721
2023-11-08 07:15:12,443 - __main__ - INFO - Epoch 60 Batch 60: Train Loss = 0.2815
2023-11-08 07:15:26,354 - __main__ - INFO - Epoch 60 Batch 80: Train Loss = 0.2303
2023-11-08 07:15:40,597 - __main__ - INFO - Epoch 60 Batch 100: Train Loss = 0.1650
2023-11-08 07:15:54,158 - __main__ - INFO - Epoch 60 Batch 120: Train Loss = 0.2420
2023-11-08 07:16:05,735 - __main__ - INFO - Epoch 61 Batch 0: Train Loss = 0.2094
2023-11-08 07:16:20,212 - __main__ - INFO - Epoch 61 Batch 20: Train Loss = 0.2463
2023-11-08 07:16:34,338 - __main__ - INFO - Epoch 61 Batch 40: Train Loss = 0.2890
2023-11-08 07:16:49,636 - __main__ - INFO - Epoch 61 Batch 60: Train Loss = 0.3138
2023-11-08 07:17:04,501 - __main__ - INFO - Epoch 61 Batch 80: Train Loss = 0.2107
2023-11-08 07:17:19,371 - __main__ - INFO - Epoch 61 Batch 100: Train Loss = 0.1625
2023-11-08 07:17:33,706 - __main__ - INFO - Epoch 61 Batch 120: Train Loss = 0.2478
2023-11-08 07:17:45,105 - __main__ - INFO - Epoch 62 Batch 0: Train Loss = 0.2719
2023-11-08 07:17:59,081 - __main__ - INFO - Epoch 62 Batch 20: Train Loss = 0.2715
2023-11-08 07:18:12,375 - __main__ - INFO - Epoch 62 Batch 40: Train Loss = 0.2747
2023-11-08 07:18:27,844 - __main__ - INFO - Epoch 62 Batch 60: Train Loss = 0.2916
2023-11-08 07:18:41,670 - __main__ - INFO - Epoch 62 Batch 80: Train Loss = 0.2230
2023-11-08 07:18:56,481 - __main__ - INFO - Epoch 62 Batch 100: Train Loss = 0.1734
2023-11-08 07:19:10,223 - __main__ - INFO - Epoch 62 Batch 120: Train Loss = 0.2423
2023-11-08 07:19:21,671 - __main__ - INFO - Epoch 63 Batch 0: Train Loss = 0.2648
2023-11-08 07:19:36,420 - __main__ - INFO - Epoch 63 Batch 20: Train Loss = 0.2520
2023-11-08 07:19:49,570 - __main__ - INFO - Epoch 63 Batch 40: Train Loss = 0.2672
2023-11-08 07:20:04,011 - __main__ - INFO - Epoch 63 Batch 60: Train Loss = 0.2490
2023-11-08 07:20:18,485 - __main__ - INFO - Epoch 63 Batch 80: Train Loss = 0.2219
2023-11-08 07:20:32,544 - __main__ - INFO - Epoch 63 Batch 100: Train Loss = 0.1770
2023-11-08 07:20:46,928 - __main__ - INFO - Epoch 63 Batch 120: Train Loss = 0.2564
2023-11-08 07:20:57,445 - __main__ - INFO - Epoch 64 Batch 0: Train Loss = 0.2597
2023-11-08 07:21:10,885 - __main__ - INFO - Epoch 64 Batch 20: Train Loss = 0.2382
2023-11-08 07:21:24,717 - __main__ - INFO - Epoch 64 Batch 40: Train Loss = 0.2617
2023-11-08 07:21:38,304 - __main__ - INFO - Epoch 64 Batch 60: Train Loss = 0.2535
2023-11-08 07:21:52,272 - __main__ - INFO - Epoch 64 Batch 80: Train Loss = 0.2572
2023-11-08 07:22:06,970 - __main__ - INFO - Epoch 64 Batch 100: Train Loss = 0.1733
2023-11-08 07:22:21,087 - __main__ - INFO - Epoch 64 Batch 120: Train Loss = 0.2142
2023-11-08 07:22:32,265 - __main__ - INFO - ------------ Save best model - AUROC: 0.8524 ------------
2023-11-08 07:22:32,855 - __main__ - INFO - Epoch 65 Batch 0: Train Loss = 0.2257
2023-11-08 07:22:47,247 - __main__ - INFO - Epoch 65 Batch 20: Train Loss = 0.2752
2023-11-08 07:23:00,636 - __main__ - INFO - Epoch 65 Batch 40: Train Loss = 0.2774
2023-11-08 07:23:14,848 - __main__ - INFO - Epoch 65 Batch 60: Train Loss = 0.2530
2023-11-08 07:23:29,475 - __main__ - INFO - Epoch 65 Batch 80: Train Loss = 0.2123
2023-11-08 07:23:43,695 - __main__ - INFO - Epoch 65 Batch 100: Train Loss = 0.1874
2023-11-08 07:23:57,137 - __main__ - INFO - Epoch 65 Batch 120: Train Loss = 0.2270
2023-11-08 07:24:09,200 - __main__ - INFO - Epoch 66 Batch 0: Train Loss = 0.2457
2023-11-08 07:24:24,119 - __main__ - INFO - Epoch 66 Batch 20: Train Loss = 0.2711
2023-11-08 07:24:37,684 - __main__ - INFO - Epoch 66 Batch 40: Train Loss = 0.2790
2023-11-08 07:24:51,076 - __main__ - INFO - Epoch 66 Batch 60: Train Loss = 0.2537
2023-11-08 07:25:04,576 - __main__ - INFO - Epoch 66 Batch 80: Train Loss = 0.2169
2023-11-08 07:25:18,031 - __main__ - INFO - Epoch 66 Batch 100: Train Loss = 0.1518
2023-11-08 07:25:31,569 - __main__ - INFO - Epoch 66 Batch 120: Train Loss = 0.2292
2023-11-08 07:25:41,835 - __main__ - INFO - Epoch 67 Batch 0: Train Loss = 0.2909
2023-11-08 07:25:56,597 - __main__ - INFO - Epoch 67 Batch 20: Train Loss = 0.2860
2023-11-08 07:26:10,553 - __main__ - INFO - Epoch 67 Batch 40: Train Loss = 0.2591
2023-11-08 07:26:24,031 - __main__ - INFO - Epoch 67 Batch 60: Train Loss = 0.3203
2023-11-08 07:26:39,200 - __main__ - INFO - Epoch 67 Batch 80: Train Loss = 0.2002
2023-11-08 07:26:52,972 - __main__ - INFO - Epoch 67 Batch 100: Train Loss = 0.1745
2023-11-08 07:27:06,941 - __main__ - INFO - Epoch 67 Batch 120: Train Loss = 0.2426
2023-11-08 07:27:17,838 - __main__ - INFO - Epoch 68 Batch 0: Train Loss = 0.2370
2023-11-08 07:27:33,219 - __main__ - INFO - Epoch 68 Batch 20: Train Loss = 0.2487
2023-11-08 07:27:45,988 - __main__ - INFO - Epoch 68 Batch 40: Train Loss = 0.2670
2023-11-08 07:27:59,433 - __main__ - INFO - Epoch 68 Batch 60: Train Loss = 0.3086
2023-11-08 07:28:13,858 - __main__ - INFO - Epoch 68 Batch 80: Train Loss = 0.2413
2023-11-08 07:28:28,521 - __main__ - INFO - Epoch 68 Batch 100: Train Loss = 0.1730
2023-11-08 07:28:42,388 - __main__ - INFO - Epoch 68 Batch 120: Train Loss = 0.2690
2023-11-08 07:28:54,181 - __main__ - INFO - Epoch 69 Batch 0: Train Loss = 0.2474
2023-11-08 07:29:08,489 - __main__ - INFO - Epoch 69 Batch 20: Train Loss = 0.2928
2023-11-08 07:29:21,974 - __main__ - INFO - Epoch 69 Batch 40: Train Loss = 0.2641
2023-11-08 07:29:36,142 - __main__ - INFO - Epoch 69 Batch 60: Train Loss = 0.2675
2023-11-08 07:29:50,044 - __main__ - INFO - Epoch 69 Batch 80: Train Loss = 0.2043
2023-11-08 07:30:05,255 - __main__ - INFO - Epoch 69 Batch 100: Train Loss = 0.2016
2023-11-08 07:30:18,673 - __main__ - INFO - Epoch 69 Batch 120: Train Loss = 0.1991
2023-11-08 07:30:30,199 - __main__ - INFO - Epoch 70 Batch 0: Train Loss = 0.2533
2023-11-08 07:30:44,684 - __main__ - INFO - Epoch 70 Batch 20: Train Loss = 0.2517
2023-11-08 07:30:58,044 - __main__ - INFO - Epoch 70 Batch 40: Train Loss = 0.2720
2023-11-08 07:31:12,717 - __main__ - INFO - Epoch 70 Batch 60: Train Loss = 0.3372
2023-11-08 07:31:26,885 - __main__ - INFO - Epoch 70 Batch 80: Train Loss = 0.1992
2023-11-08 07:31:43,071 - __main__ - INFO - Epoch 70 Batch 100: Train Loss = 0.1859
2023-11-08 07:31:57,069 - __main__ - INFO - Epoch 70 Batch 120: Train Loss = 0.2196
2023-11-08 07:32:08,563 - __main__ - INFO - Epoch 71 Batch 0: Train Loss = 0.2107
2023-11-08 07:32:22,922 - __main__ - INFO - Epoch 71 Batch 20: Train Loss = 0.2464
2023-11-08 07:32:36,764 - __main__ - INFO - Epoch 71 Batch 40: Train Loss = 0.2470
2023-11-08 07:32:50,046 - __main__ - INFO - Epoch 71 Batch 60: Train Loss = 0.2579
2023-11-08 07:33:04,141 - __main__ - INFO - Epoch 71 Batch 80: Train Loss = 0.2299
2023-11-08 07:33:18,499 - __main__ - INFO - Epoch 71 Batch 100: Train Loss = 0.2075
2023-11-08 07:33:33,548 - __main__ - INFO - Epoch 71 Batch 120: Train Loss = 0.2362
2023-11-08 07:33:43,943 - __main__ - INFO - ------------ Save best model - AUROC: 0.8556 ------------
2023-11-08 07:33:44,495 - __main__ - INFO - Epoch 72 Batch 0: Train Loss = 0.2741
2023-11-08 07:33:58,946 - __main__ - INFO - Epoch 72 Batch 20: Train Loss = 0.2344
2023-11-08 07:34:12,694 - __main__ - INFO - Epoch 72 Batch 40: Train Loss = 0.2890
2023-11-08 07:34:26,605 - __main__ - INFO - Epoch 72 Batch 60: Train Loss = 0.2927
2023-11-08 07:34:41,406 - __main__ - INFO - Epoch 72 Batch 80: Train Loss = 0.1962
2023-11-08 07:34:55,778 - __main__ - INFO - Epoch 72 Batch 100: Train Loss = 0.1938
2023-11-08 07:35:09,148 - __main__ - INFO - Epoch 72 Batch 120: Train Loss = 0.2082
2023-11-08 07:35:20,397 - __main__ - INFO - Epoch 73 Batch 0: Train Loss = 0.2301
2023-11-08 07:35:34,080 - __main__ - INFO - Epoch 73 Batch 20: Train Loss = 0.2374
2023-11-08 07:35:47,795 - __main__ - INFO - Epoch 73 Batch 40: Train Loss = 0.2812
2023-11-08 07:36:01,127 - __main__ - INFO - Epoch 73 Batch 60: Train Loss = 0.2995
2023-11-08 07:36:15,002 - __main__ - INFO - Epoch 73 Batch 80: Train Loss = 0.6264
2023-11-08 07:36:29,263 - __main__ - INFO - Epoch 73 Batch 100: Train Loss = 0.1845
2023-11-08 07:36:44,227 - __main__ - INFO - Epoch 73 Batch 120: Train Loss = 0.2411
2023-11-08 07:36:55,483 - __main__ - INFO - Epoch 74 Batch 0: Train Loss = 0.2719
2023-11-08 07:37:09,137 - __main__ - INFO - Epoch 74 Batch 20: Train Loss = 0.2775
2023-11-08 07:37:22,601 - __main__ - INFO - Epoch 74 Batch 40: Train Loss = 0.2678
2023-11-08 07:37:36,275 - __main__ - INFO - Epoch 74 Batch 60: Train Loss = 0.2705
2023-11-08 07:37:50,443 - __main__ - INFO - Epoch 74 Batch 80: Train Loss = 0.2203
2023-11-08 07:38:05,330 - __main__ - INFO - Epoch 74 Batch 100: Train Loss = 0.1675
2023-11-08 07:38:19,004 - __main__ - INFO - Epoch 74 Batch 120: Train Loss = 0.2006
2023-11-08 07:38:31,239 - __main__ - INFO - Epoch 75 Batch 0: Train Loss = 0.2319
2023-11-08 07:38:45,910 - __main__ - INFO - Epoch 75 Batch 20: Train Loss = 0.2454
2023-11-08 07:38:59,898 - __main__ - INFO - Epoch 75 Batch 40: Train Loss = 0.2783
2023-11-08 07:39:12,891 - __main__ - INFO - Epoch 75 Batch 60: Train Loss = 0.2627
2023-11-08 07:39:26,691 - __main__ - INFO - Epoch 75 Batch 80: Train Loss = 0.2264
2023-11-08 07:39:40,203 - __main__ - INFO - Epoch 75 Batch 100: Train Loss = 0.1824
2023-11-08 07:39:53,320 - __main__ - INFO - Epoch 75 Batch 120: Train Loss = 0.2852
2023-11-08 07:40:05,121 - __main__ - INFO - Epoch 76 Batch 0: Train Loss = 0.2548
2023-11-08 07:40:19,856 - __main__ - INFO - Epoch 76 Batch 20: Train Loss = 0.2427
2023-11-08 07:40:33,945 - __main__ - INFO - Epoch 76 Batch 40: Train Loss = 0.2712
2023-11-08 07:40:47,374 - __main__ - INFO - Epoch 76 Batch 60: Train Loss = 0.2944
2023-11-08 07:41:01,784 - __main__ - INFO - Epoch 76 Batch 80: Train Loss = 0.2091
2023-11-08 07:41:16,089 - __main__ - INFO - Epoch 76 Batch 100: Train Loss = 0.1697
2023-11-08 07:41:28,780 - __main__ - INFO - Epoch 76 Batch 120: Train Loss = 0.2183
2023-11-08 07:41:40,010 - __main__ - INFO - Epoch 77 Batch 0: Train Loss = 0.2208
2023-11-08 07:41:54,344 - __main__ - INFO - Epoch 77 Batch 20: Train Loss = 0.2327
2023-11-08 07:42:08,248 - __main__ - INFO - Epoch 77 Batch 40: Train Loss = 0.2638
2023-11-08 07:42:22,835 - __main__ - INFO - Epoch 77 Batch 60: Train Loss = 0.4150
2023-11-08 07:42:37,739 - __main__ - INFO - Epoch 77 Batch 80: Train Loss = 0.2146
2023-11-08 07:42:51,541 - __main__ - INFO - Epoch 77 Batch 100: Train Loss = 0.1687
2023-11-08 07:43:04,705 - __main__ - INFO - Epoch 77 Batch 120: Train Loss = 0.2294
2023-11-08 07:43:14,661 - __main__ - INFO - Epoch 78 Batch 0: Train Loss = 0.2392
2023-11-08 07:43:28,653 - __main__ - INFO - Epoch 78 Batch 20: Train Loss = 0.2731
2023-11-08 07:43:41,758 - __main__ - INFO - Epoch 78 Batch 40: Train Loss = 0.2767
2023-11-08 07:43:55,154 - __main__ - INFO - Epoch 78 Batch 60: Train Loss = 0.2598
2023-11-08 07:44:09,780 - __main__ - INFO - Epoch 78 Batch 80: Train Loss = 0.2053
2023-11-08 07:44:23,070 - __main__ - INFO - Epoch 78 Batch 100: Train Loss = 0.1756
2023-11-08 07:44:36,898 - __main__ - INFO - Epoch 78 Batch 120: Train Loss = 0.2103
2023-11-08 07:44:48,071 - __main__ - INFO - Epoch 79 Batch 0: Train Loss = 0.2619
2023-11-08 07:45:03,464 - __main__ - INFO - Epoch 79 Batch 20: Train Loss = 0.2400
2023-11-08 07:45:17,120 - __main__ - INFO - Epoch 79 Batch 40: Train Loss = 0.2758
2023-11-08 07:45:31,001 - __main__ - INFO - Epoch 79 Batch 60: Train Loss = 0.2783
2023-11-08 07:45:44,933 - __main__ - INFO - Epoch 79 Batch 80: Train Loss = 0.2230
2023-11-08 07:45:59,114 - __main__ - INFO - Epoch 79 Batch 100: Train Loss = 0.2055
2023-11-08 07:46:13,090 - __main__ - INFO - Epoch 79 Batch 120: Train Loss = 0.2621
2023-11-08 07:46:25,042 - __main__ - INFO - Epoch 80 Batch 0: Train Loss = 0.2250
2023-11-08 07:46:38,985 - __main__ - INFO - Epoch 80 Batch 20: Train Loss = 0.2534
2023-11-08 07:46:52,975 - __main__ - INFO - Epoch 80 Batch 40: Train Loss = 0.2753
2023-11-08 07:47:07,319 - __main__ - INFO - Epoch 80 Batch 60: Train Loss = 0.2906
2023-11-08 07:47:22,119 - __main__ - INFO - Epoch 80 Batch 80: Train Loss = 0.2128
2023-11-08 07:47:35,547 - __main__ - INFO - Epoch 80 Batch 100: Train Loss = 0.1794
2023-11-08 07:47:49,736 - __main__ - INFO - Epoch 80 Batch 120: Train Loss = 0.2353
2023-11-08 07:48:00,668 - __main__ - INFO - Epoch 81 Batch 0: Train Loss = 0.2565
2023-11-08 07:48:15,815 - __main__ - INFO - Epoch 81 Batch 20: Train Loss = 0.2738
2023-11-08 07:48:30,617 - __main__ - INFO - Epoch 81 Batch 40: Train Loss = 0.2516
2023-11-08 07:48:44,381 - __main__ - INFO - Epoch 81 Batch 60: Train Loss = 0.3113
2023-11-08 07:48:58,195 - __main__ - INFO - Epoch 81 Batch 80: Train Loss = 0.2072
2023-11-08 07:49:11,483 - __main__ - INFO - Epoch 81 Batch 100: Train Loss = 0.1803
2023-11-08 07:49:25,273 - __main__ - INFO - Epoch 81 Batch 120: Train Loss = 0.2365
2023-11-08 07:49:36,205 - __main__ - INFO - Epoch 82 Batch 0: Train Loss = 0.2521
2023-11-08 07:49:50,193 - __main__ - INFO - Epoch 82 Batch 20: Train Loss = 0.2544
2023-11-08 07:50:04,408 - __main__ - INFO - Epoch 82 Batch 40: Train Loss = 0.2554
2023-11-08 07:50:18,776 - __main__ - INFO - Epoch 82 Batch 60: Train Loss = 0.2693
2023-11-08 07:50:32,323 - __main__ - INFO - Epoch 82 Batch 80: Train Loss = 0.1995
2023-11-08 07:50:46,618 - __main__ - INFO - Epoch 82 Batch 100: Train Loss = 0.1595
2023-11-08 07:50:59,989 - __main__ - INFO - Epoch 82 Batch 120: Train Loss = 0.2308
2023-11-08 07:51:11,469 - __main__ - INFO - Epoch 83 Batch 0: Train Loss = 0.2656
2023-11-08 07:51:26,821 - __main__ - INFO - Epoch 83 Batch 20: Train Loss = 0.2570
2023-11-08 07:51:40,049 - __main__ - INFO - Epoch 83 Batch 40: Train Loss = 0.2601
2023-11-08 07:51:54,353 - __main__ - INFO - Epoch 83 Batch 60: Train Loss = 0.2722
2023-11-08 07:52:09,200 - __main__ - INFO - Epoch 83 Batch 80: Train Loss = 0.1983
2023-11-08 07:52:23,441 - __main__ - INFO - Epoch 83 Batch 100: Train Loss = 0.3103
2023-11-08 07:52:37,179 - __main__ - INFO - Epoch 83 Batch 120: Train Loss = 0.2214
2023-11-08 07:52:48,407 - __main__ - INFO - Epoch 84 Batch 0: Train Loss = 0.2569
2023-11-08 07:53:02,450 - __main__ - INFO - Epoch 84 Batch 20: Train Loss = 0.2401
2023-11-08 07:53:16,744 - __main__ - INFO - Epoch 84 Batch 40: Train Loss = 0.2553
2023-11-08 07:53:30,434 - __main__ - INFO - Epoch 84 Batch 60: Train Loss = 0.2734
2023-11-08 07:53:43,878 - __main__ - INFO - Epoch 84 Batch 80: Train Loss = 0.2196
2023-11-08 07:53:57,292 - __main__ - INFO - Epoch 84 Batch 100: Train Loss = 0.1879
2023-11-08 07:54:12,260 - __main__ - INFO - Epoch 84 Batch 120: Train Loss = 0.2554
2023-11-08 07:54:23,845 - __main__ - INFO - Epoch 85 Batch 0: Train Loss = 0.2690
2023-11-08 07:54:37,733 - __main__ - INFO - Epoch 85 Batch 20: Train Loss = 0.2625
2023-11-08 07:54:51,500 - __main__ - INFO - Epoch 85 Batch 40: Train Loss = 0.2585
2023-11-08 07:55:05,462 - __main__ - INFO - Epoch 85 Batch 60: Train Loss = 0.2835
2023-11-08 07:55:20,079 - __main__ - INFO - Epoch 85 Batch 80: Train Loss = 0.2368
2023-11-08 07:55:34,599 - __main__ - INFO - Epoch 85 Batch 100: Train Loss = 0.2052
2023-11-08 07:55:48,306 - __main__ - INFO - Epoch 85 Batch 120: Train Loss = 0.2067
2023-11-08 07:55:59,510 - __main__ - INFO - Epoch 86 Batch 0: Train Loss = 0.2406
2023-11-08 07:56:13,140 - __main__ - INFO - Epoch 86 Batch 20: Train Loss = 0.3163
2023-11-08 07:56:27,965 - __main__ - INFO - Epoch 86 Batch 40: Train Loss = 0.2757
2023-11-08 07:56:42,082 - __main__ - INFO - Epoch 86 Batch 60: Train Loss = 0.3058
2023-11-08 07:56:56,072 - __main__ - INFO - Epoch 86 Batch 80: Train Loss = 0.2139
2023-11-08 07:57:10,346 - __main__ - INFO - Epoch 86 Batch 100: Train Loss = 0.1817
2023-11-08 07:57:24,102 - __main__ - INFO - Epoch 86 Batch 120: Train Loss = 0.2179
2023-11-08 07:57:35,081 - __main__ - INFO - Epoch 87 Batch 0: Train Loss = 0.3039
2023-11-08 07:57:49,152 - __main__ - INFO - Epoch 87 Batch 20: Train Loss = 0.2638
2023-11-08 07:58:02,639 - __main__ - INFO - Epoch 87 Batch 40: Train Loss = 0.2731
2023-11-08 07:58:17,166 - __main__ - INFO - Epoch 87 Batch 60: Train Loss = 0.2426
2023-11-08 07:58:32,632 - __main__ - INFO - Epoch 87 Batch 80: Train Loss = 0.2270
2023-11-08 07:58:47,741 - __main__ - INFO - Epoch 87 Batch 100: Train Loss = 0.1808
2023-11-08 07:59:01,903 - __main__ - INFO - Epoch 87 Batch 120: Train Loss = 0.2389
2023-11-08 07:59:12,433 - __main__ - INFO - Epoch 88 Batch 0: Train Loss = 0.2902
2023-11-08 07:59:27,140 - __main__ - INFO - Epoch 88 Batch 20: Train Loss = 0.2505
2023-11-08 07:59:41,793 - __main__ - INFO - Epoch 88 Batch 40: Train Loss = 0.2686
2023-11-08 07:59:55,311 - __main__ - INFO - Epoch 88 Batch 60: Train Loss = 0.2627
2023-11-08 08:00:08,768 - __main__ - INFO - Epoch 88 Batch 80: Train Loss = 0.2397
2023-11-08 08:00:22,553 - __main__ - INFO - Epoch 88 Batch 100: Train Loss = 0.2027
2023-11-08 08:00:36,475 - __main__ - INFO - Epoch 88 Batch 120: Train Loss = 0.2134
2023-11-08 08:00:48,081 - __main__ - INFO - Epoch 89 Batch 0: Train Loss = 0.2585
2023-11-08 08:01:02,408 - __main__ - INFO - Epoch 89 Batch 20: Train Loss = 0.2565
2023-11-08 08:01:15,312 - __main__ - INFO - Epoch 89 Batch 40: Train Loss = 0.2887
2023-11-08 08:01:29,282 - __main__ - INFO - Epoch 89 Batch 60: Train Loss = 0.3038
2023-11-08 08:01:44,351 - __main__ - INFO - Epoch 89 Batch 80: Train Loss = 0.2019
2023-11-08 08:01:59,087 - __main__ - INFO - Epoch 89 Batch 100: Train Loss = 0.2324
2023-11-08 08:02:12,736 - __main__ - INFO - Epoch 89 Batch 120: Train Loss = 0.2338
2023-11-08 08:02:24,401 - __main__ - INFO - Epoch 90 Batch 0: Train Loss = 0.2442
2023-11-08 08:02:38,615 - __main__ - INFO - Epoch 90 Batch 20: Train Loss = 0.2547
2023-11-08 08:02:51,649 - __main__ - INFO - Epoch 90 Batch 40: Train Loss = 0.2867
2023-11-08 08:03:05,594 - __main__ - INFO - Epoch 90 Batch 60: Train Loss = 0.4052
2023-11-08 08:03:20,290 - __main__ - INFO - Epoch 90 Batch 80: Train Loss = 0.2033
2023-11-08 08:03:34,377 - __main__ - INFO - Epoch 90 Batch 100: Train Loss = 0.1927
2023-11-08 08:03:47,940 - __main__ - INFO - Epoch 90 Batch 120: Train Loss = 0.2173
2023-11-08 08:03:59,154 - __main__ - INFO - Epoch 91 Batch 0: Train Loss = 0.2399
2023-11-08 08:04:13,857 - __main__ - INFO - Epoch 91 Batch 20: Train Loss = 0.2943
2023-11-08 08:04:27,919 - __main__ - INFO - Epoch 91 Batch 40: Train Loss = 0.2895
2023-11-08 08:04:42,499 - __main__ - INFO - Epoch 91 Batch 60: Train Loss = 0.2685
2023-11-08 08:04:57,535 - __main__ - INFO - Epoch 91 Batch 80: Train Loss = 0.2075
2023-11-08 08:05:11,159 - __main__ - INFO - Epoch 91 Batch 100: Train Loss = 0.1823
2023-11-08 08:05:24,635 - __main__ - INFO - Epoch 91 Batch 120: Train Loss = 0.2379
2023-11-08 08:05:36,028 - __main__ - INFO - Epoch 92 Batch 0: Train Loss = 0.3073
2023-11-08 08:05:50,755 - __main__ - INFO - Epoch 92 Batch 20: Train Loss = 0.2594
2023-11-08 08:06:04,286 - __main__ - INFO - Epoch 92 Batch 40: Train Loss = 0.2969
2023-11-08 08:06:18,058 - __main__ - INFO - Epoch 92 Batch 60: Train Loss = 0.2688
2023-11-08 08:06:32,394 - __main__ - INFO - Epoch 92 Batch 80: Train Loss = 0.2195
2023-11-08 08:06:45,699 - __main__ - INFO - Epoch 92 Batch 100: Train Loss = 0.1900
2023-11-08 08:06:59,921 - __main__ - INFO - Epoch 92 Batch 120: Train Loss = 0.2336
2023-11-08 08:07:10,673 - __main__ - INFO - Epoch 93 Batch 0: Train Loss = 0.3285
2023-11-08 08:07:24,472 - __main__ - INFO - Epoch 93 Batch 20: Train Loss = 0.2421
2023-11-08 08:07:37,757 - __main__ - INFO - Epoch 93 Batch 40: Train Loss = 0.2758
2023-11-08 08:07:52,717 - __main__ - INFO - Epoch 93 Batch 60: Train Loss = 0.3562
2023-11-08 08:08:07,703 - __main__ - INFO - Epoch 93 Batch 80: Train Loss = 0.2035
2023-11-08 08:08:22,241 - __main__ - INFO - Epoch 93 Batch 100: Train Loss = 0.1865
2023-11-08 08:08:35,927 - __main__ - INFO - Epoch 93 Batch 120: Train Loss = 0.2420
2023-11-08 08:08:46,816 - __main__ - INFO - Epoch 94 Batch 0: Train Loss = 0.2732
2023-11-08 08:09:01,504 - __main__ - INFO - Epoch 94 Batch 20: Train Loss = 0.2526
2023-11-08 08:09:14,939 - __main__ - INFO - Epoch 94 Batch 40: Train Loss = 0.2985
2023-11-08 08:09:29,499 - __main__ - INFO - Epoch 94 Batch 60: Train Loss = 0.2738
2023-11-08 08:09:43,375 - __main__ - INFO - Epoch 94 Batch 80: Train Loss = 0.2411
2023-11-08 08:09:58,489 - __main__ - INFO - Epoch 94 Batch 100: Train Loss = 0.1749
2023-11-08 08:10:12,596 - __main__ - INFO - Epoch 94 Batch 120: Train Loss = 0.2507
2023-11-08 08:10:24,156 - __main__ - INFO - Epoch 95 Batch 0: Train Loss = 0.2431
2023-11-08 08:10:38,301 - __main__ - INFO - Epoch 95 Batch 20: Train Loss = 0.2551
2023-11-08 08:10:52,017 - __main__ - INFO - Epoch 95 Batch 40: Train Loss = 0.2732
2023-11-08 08:11:06,236 - __main__ - INFO - Epoch 95 Batch 60: Train Loss = 0.2490
2023-11-08 08:11:21,111 - __main__ - INFO - Epoch 95 Batch 80: Train Loss = 0.2175
2023-11-08 08:11:35,260 - __main__ - INFO - Epoch 95 Batch 100: Train Loss = 0.2470
2023-11-08 08:11:49,527 - __main__ - INFO - Epoch 95 Batch 120: Train Loss = 0.2281
2023-11-08 08:12:01,075 - __main__ - INFO - Epoch 96 Batch 0: Train Loss = 0.2405
2023-11-08 08:12:14,717 - __main__ - INFO - Epoch 96 Batch 20: Train Loss = 0.2736
2023-11-08 08:12:27,839 - __main__ - INFO - Epoch 96 Batch 40: Train Loss = 0.2562
2023-11-08 08:12:41,545 - __main__ - INFO - Epoch 96 Batch 60: Train Loss = 0.2598
2023-11-08 08:12:55,384 - __main__ - INFO - Epoch 96 Batch 80: Train Loss = 0.2105
2023-11-08 08:13:10,019 - __main__ - INFO - Epoch 96 Batch 100: Train Loss = 0.2000
2023-11-08 08:13:24,343 - __main__ - INFO - Epoch 96 Batch 120: Train Loss = 0.2671
2023-11-08 08:13:36,442 - __main__ - INFO - Epoch 97 Batch 0: Train Loss = 0.2484
2023-11-08 08:13:50,291 - __main__ - INFO - Epoch 97 Batch 20: Train Loss = 0.2761
2023-11-08 08:14:04,020 - __main__ - INFO - Epoch 97 Batch 40: Train Loss = 0.2569
2023-11-08 08:14:17,740 - __main__ - INFO - Epoch 97 Batch 60: Train Loss = 0.2680
2023-11-08 08:14:31,303 - __main__ - INFO - Epoch 97 Batch 80: Train Loss = 0.2177
2023-11-08 08:14:44,939 - __main__ - INFO - Epoch 97 Batch 100: Train Loss = 0.1879
2023-11-08 08:15:00,026 - __main__ - INFO - Epoch 97 Batch 120: Train Loss = 0.2077
2023-11-08 08:15:11,344 - __main__ - INFO - Epoch 98 Batch 0: Train Loss = 0.2500
2023-11-08 08:15:25,644 - __main__ - INFO - Epoch 98 Batch 20: Train Loss = 0.2582
2023-11-08 08:15:40,115 - __main__ - INFO - Epoch 98 Batch 40: Train Loss = 0.2744
2023-11-08 08:15:54,164 - __main__ - INFO - Epoch 98 Batch 60: Train Loss = 0.2912
2023-11-08 08:16:08,007 - __main__ - INFO - Epoch 98 Batch 80: Train Loss = 0.2136
2023-11-08 08:16:21,638 - __main__ - INFO - Epoch 98 Batch 100: Train Loss = 0.1665
2023-11-08 08:16:35,915 - __main__ - INFO - Epoch 98 Batch 120: Train Loss = 0.2775
2023-11-08 08:16:47,408 - __main__ - INFO - Epoch 99 Batch 0: Train Loss = 0.2468
2023-11-08 08:17:01,618 - __main__ - INFO - Epoch 99 Batch 20: Train Loss = 0.2590
2023-11-08 08:17:15,367 - __main__ - INFO - Epoch 99 Batch 40: Train Loss = 0.2696
2023-11-08 08:17:29,202 - __main__ - INFO - Epoch 99 Batch 60: Train Loss = 0.2764
2023-11-08 08:17:42,937 - __main__ - INFO - Epoch 99 Batch 80: Train Loss = 0.2129
2023-11-08 08:17:56,895 - __main__ - INFO - Epoch 99 Batch 100: Train Loss = 0.1889
2023-11-08 08:18:10,347 - __main__ - INFO - Epoch 99 Batch 120: Train Loss = 0.2200
2023-11-08 08:18:22,131 - __main__ - INFO - Epoch 100 Batch 0: Train Loss = 0.2239
2023-11-08 08:18:36,531 - __main__ - INFO - Epoch 100 Batch 20: Train Loss = 0.2836
2023-11-08 08:18:49,842 - __main__ - INFO - Epoch 100 Batch 40: Train Loss = 0.2989
2023-11-08 08:19:03,840 - __main__ - INFO - Epoch 100 Batch 60: Train Loss = 0.2774
2023-11-08 08:19:18,000 - __main__ - INFO - Epoch 100 Batch 80: Train Loss = 0.1973
2023-11-08 08:19:33,222 - __main__ - INFO - Epoch 100 Batch 100: Train Loss = 0.1858
2023-11-08 08:19:46,592 - __main__ - INFO - Epoch 100 Batch 120: Train Loss = 0.2329
2023-11-08 08:19:58,573 - __main__ - INFO - Epoch 101 Batch 0: Train Loss = 0.2445
2023-11-08 08:20:12,665 - __main__ - INFO - Epoch 101 Batch 20: Train Loss = 0.2872
2023-11-08 08:20:27,339 - __main__ - INFO - Epoch 101 Batch 40: Train Loss = 0.2530
2023-11-08 08:20:41,692 - __main__ - INFO - Epoch 101 Batch 60: Train Loss = 0.2686
2023-11-08 08:20:55,593 - __main__ - INFO - Epoch 101 Batch 80: Train Loss = 0.1934
2023-11-08 08:21:09,566 - __main__ - INFO - Epoch 101 Batch 100: Train Loss = 0.1784
2023-11-08 08:21:23,202 - __main__ - INFO - Epoch 101 Batch 120: Train Loss = 0.2058
2023-11-08 08:21:34,927 - __main__ - INFO - Epoch 102 Batch 0: Train Loss = 0.2308
2023-11-08 08:21:50,218 - __main__ - INFO - Epoch 102 Batch 20: Train Loss = 0.2912
2023-11-08 08:22:04,078 - __main__ - INFO - Epoch 102 Batch 40: Train Loss = 0.2624
2023-11-08 08:22:18,386 - __main__ - INFO - Epoch 102 Batch 60: Train Loss = 0.3063
2023-11-08 08:22:32,413 - __main__ - INFO - Epoch 102 Batch 80: Train Loss = 0.2034
2023-11-08 08:22:46,275 - __main__ - INFO - Epoch 102 Batch 100: Train Loss = 0.1824
2023-11-08 08:22:59,824 - __main__ - INFO - Epoch 102 Batch 120: Train Loss = 0.2456
2023-11-08 08:23:10,162 - __main__ - INFO - Epoch 103 Batch 0: Train Loss = 0.2235
2023-11-08 08:23:25,391 - __main__ - INFO - Epoch 103 Batch 20: Train Loss = 0.2657
2023-11-08 08:23:38,597 - __main__ - INFO - Epoch 103 Batch 40: Train Loss = 0.2872
2023-11-08 08:23:52,535 - __main__ - INFO - Epoch 103 Batch 60: Train Loss = 0.2506
2023-11-08 08:24:08,267 - __main__ - INFO - Epoch 103 Batch 80: Train Loss = 0.2218
2023-11-08 08:24:22,258 - __main__ - INFO - Epoch 103 Batch 100: Train Loss = 0.1659
2023-11-08 08:24:35,764 - __main__ - INFO - Epoch 103 Batch 120: Train Loss = 0.2352
2023-11-08 08:24:47,388 - __main__ - INFO - Epoch 104 Batch 0: Train Loss = 0.2397
2023-11-08 08:25:02,081 - __main__ - INFO - Epoch 104 Batch 20: Train Loss = 0.3540
2023-11-08 08:25:16,033 - __main__ - INFO - Epoch 104 Batch 40: Train Loss = 0.2806
2023-11-08 08:25:29,164 - __main__ - INFO - Epoch 104 Batch 60: Train Loss = 0.2418
2023-11-08 08:25:43,249 - __main__ - INFO - Epoch 104 Batch 80: Train Loss = 0.2173
2023-11-08 08:25:57,355 - __main__ - INFO - Epoch 104 Batch 100: Train Loss = 0.2011
2023-11-08 08:26:11,155 - __main__ - INFO - Epoch 104 Batch 120: Train Loss = 0.2594
2023-11-08 08:26:22,281 - __main__ - INFO - Epoch 105 Batch 0: Train Loss = 0.3183
2023-11-08 08:26:37,712 - __main__ - INFO - Epoch 105 Batch 20: Train Loss = 0.2642
2023-11-08 08:26:51,741 - __main__ - INFO - Epoch 105 Batch 40: Train Loss = 0.2710
2023-11-08 08:27:05,126 - __main__ - INFO - Epoch 105 Batch 60: Train Loss = 0.3131
2023-11-08 08:27:19,117 - __main__ - INFO - Epoch 105 Batch 80: Train Loss = 0.1893
2023-11-08 08:27:33,716 - __main__ - INFO - Epoch 105 Batch 100: Train Loss = 0.1683
2023-11-08 08:27:47,423 - __main__ - INFO - Epoch 105 Batch 120: Train Loss = 0.2327
2023-11-08 08:27:59,598 - __main__ - INFO - Epoch 106 Batch 0: Train Loss = 0.2454
2023-11-08 08:28:14,831 - __main__ - INFO - Epoch 106 Batch 20: Train Loss = 0.2585
2023-11-08 08:28:28,298 - __main__ - INFO - Epoch 106 Batch 40: Train Loss = 0.2659
2023-11-08 08:28:43,191 - __main__ - INFO - Epoch 106 Batch 60: Train Loss = 0.2658
2023-11-08 08:28:57,779 - __main__ - INFO - Epoch 106 Batch 80: Train Loss = 0.2098
2023-11-08 08:29:11,731 - __main__ - INFO - Epoch 106 Batch 100: Train Loss = 0.2060
2023-11-08 08:29:26,054 - __main__ - INFO - Epoch 106 Batch 120: Train Loss = 0.2072
2023-11-08 08:29:37,340 - __main__ - INFO - Epoch 107 Batch 0: Train Loss = 0.2301
2023-11-08 08:29:52,053 - __main__ - INFO - Epoch 107 Batch 20: Train Loss = 0.2450
2023-11-08 08:30:05,703 - __main__ - INFO - Epoch 107 Batch 40: Train Loss = 0.2623
2023-11-08 08:30:19,277 - __main__ - INFO - Epoch 107 Batch 60: Train Loss = 0.2603
2023-11-08 08:30:34,231 - __main__ - INFO - Epoch 107 Batch 80: Train Loss = 0.2013
2023-11-08 08:30:48,694 - __main__ - INFO - Epoch 107 Batch 100: Train Loss = 0.1738
2023-11-08 08:31:03,096 - __main__ - INFO - Epoch 107 Batch 120: Train Loss = 0.2652
2023-11-08 08:31:14,053 - __main__ - INFO - Epoch 108 Batch 0: Train Loss = 0.2866
2023-11-08 08:31:28,658 - __main__ - INFO - Epoch 108 Batch 20: Train Loss = 0.2326
2023-11-08 08:31:42,781 - __main__ - INFO - Epoch 108 Batch 40: Train Loss = 0.2779
2023-11-08 08:31:56,305 - __main__ - INFO - Epoch 108 Batch 60: Train Loss = 0.2751
2023-11-08 08:32:10,844 - __main__ - INFO - Epoch 108 Batch 80: Train Loss = 0.2091
2023-11-08 08:32:24,728 - __main__ - INFO - Epoch 108 Batch 100: Train Loss = 0.1886
2023-11-08 08:32:38,313 - __main__ - INFO - Epoch 108 Batch 120: Train Loss = 0.2186
2023-11-08 08:32:49,770 - __main__ - INFO - Epoch 109 Batch 0: Train Loss = 0.4806
2023-11-08 08:33:04,055 - __main__ - INFO - Epoch 109 Batch 20: Train Loss = 0.2622
2023-11-08 08:33:19,486 - __main__ - INFO - Epoch 109 Batch 40: Train Loss = 0.2802
2023-11-08 08:33:32,961 - __main__ - INFO - Epoch 109 Batch 60: Train Loss = 0.2834
2023-11-08 08:33:47,778 - __main__ - INFO - Epoch 109 Batch 80: Train Loss = 0.2121
2023-11-08 08:34:01,909 - __main__ - INFO - Epoch 109 Batch 100: Train Loss = 0.1978
2023-11-08 08:34:16,164 - __main__ - INFO - Epoch 109 Batch 120: Train Loss = 0.2988
2023-11-08 08:34:27,331 - __main__ - INFO - Epoch 110 Batch 0: Train Loss = 0.2523
2023-11-08 08:34:41,920 - __main__ - INFO - Epoch 110 Batch 20: Train Loss = 0.3098
2023-11-08 08:34:56,712 - __main__ - INFO - Epoch 110 Batch 40: Train Loss = 0.2652
2023-11-08 08:35:10,756 - __main__ - INFO - Epoch 110 Batch 60: Train Loss = 0.2774
2023-11-08 08:35:24,588 - __main__ - INFO - Epoch 110 Batch 80: Train Loss = 0.2118
2023-11-08 08:35:38,056 - __main__ - INFO - Epoch 110 Batch 100: Train Loss = 0.1930
2023-11-08 08:35:51,571 - __main__ - INFO - Epoch 110 Batch 120: Train Loss = 0.2348
2023-11-08 08:36:03,073 - __main__ - INFO - Epoch 111 Batch 0: Train Loss = 0.2313
2023-11-08 08:36:17,430 - __main__ - INFO - Epoch 111 Batch 20: Train Loss = 0.2645
2023-11-08 08:36:31,859 - __main__ - INFO - Epoch 111 Batch 40: Train Loss = 0.3068
2023-11-08 08:36:45,746 - __main__ - INFO - Epoch 111 Batch 60: Train Loss = 0.2682
2023-11-08 08:36:59,730 - __main__ - INFO - Epoch 111 Batch 80: Train Loss = 0.2310
2023-11-08 08:37:13,982 - __main__ - INFO - Epoch 111 Batch 100: Train Loss = 0.1727
2023-11-08 08:37:27,601 - __main__ - INFO - Epoch 111 Batch 120: Train Loss = 0.2409
2023-11-08 08:37:39,029 - __main__ - INFO - Epoch 112 Batch 0: Train Loss = 0.2656
2023-11-08 08:37:54,168 - __main__ - INFO - Epoch 112 Batch 20: Train Loss = 0.2873
2023-11-08 08:38:08,388 - __main__ - INFO - Epoch 112 Batch 40: Train Loss = 0.2859
2023-11-08 08:38:22,225 - __main__ - INFO - Epoch 112 Batch 60: Train Loss = 0.2516
2023-11-08 08:38:36,880 - __main__ - INFO - Epoch 112 Batch 80: Train Loss = 0.2052
2023-11-08 08:38:50,422 - __main__ - INFO - Epoch 112 Batch 100: Train Loss = 0.1803
2023-11-08 08:39:04,176 - __main__ - INFO - Epoch 112 Batch 120: Train Loss = 0.2344
2023-11-08 08:39:15,840 - __main__ - INFO - Epoch 113 Batch 0: Train Loss = 0.2383
2023-11-08 08:39:29,499 - __main__ - INFO - Epoch 113 Batch 20: Train Loss = 0.2730
2023-11-08 08:39:42,206 - __main__ - INFO - Epoch 113 Batch 40: Train Loss = 0.2568
2023-11-08 08:39:56,411 - __main__ - INFO - Epoch 113 Batch 60: Train Loss = 0.3215
2023-11-08 08:40:11,103 - __main__ - INFO - Epoch 113 Batch 80: Train Loss = 0.2469
2023-11-08 08:40:25,521 - __main__ - INFO - Epoch 113 Batch 100: Train Loss = 0.1963
2023-11-08 08:40:39,918 - __main__ - INFO - Epoch 113 Batch 120: Train Loss = 0.2574
2023-11-08 08:40:50,987 - __main__ - INFO - Epoch 114 Batch 0: Train Loss = 0.2418
2023-11-08 08:41:05,029 - __main__ - INFO - Epoch 114 Batch 20: Train Loss = 0.2769
2023-11-08 08:41:18,760 - __main__ - INFO - Epoch 114 Batch 40: Train Loss = 0.2465
2023-11-08 08:41:32,479 - __main__ - INFO - Epoch 114 Batch 60: Train Loss = 0.2769
2023-11-08 08:41:47,383 - __main__ - INFO - Epoch 114 Batch 80: Train Loss = 0.2368
2023-11-08 08:42:01,989 - __main__ - INFO - Epoch 114 Batch 100: Train Loss = 0.2069
2023-11-08 08:42:16,936 - __main__ - INFO - Epoch 114 Batch 120: Train Loss = 0.2337
2023-11-08 08:42:28,635 - __main__ - INFO - Epoch 115 Batch 0: Train Loss = 0.2615
2023-11-08 08:42:42,688 - __main__ - INFO - Epoch 115 Batch 20: Train Loss = 0.2752
2023-11-08 08:42:56,047 - __main__ - INFO - Epoch 115 Batch 40: Train Loss = 0.2645
2023-11-08 08:43:09,882 - __main__ - INFO - Epoch 115 Batch 60: Train Loss = 0.2821
2023-11-08 08:43:24,097 - __main__ - INFO - Epoch 115 Batch 80: Train Loss = 0.2291
2023-11-08 08:43:37,081 - __main__ - INFO - Epoch 115 Batch 100: Train Loss = 0.1913
2023-11-08 08:43:50,623 - __main__ - INFO - Epoch 115 Batch 120: Train Loss = 0.2455
2023-11-08 08:44:01,167 - __main__ - INFO - Epoch 116 Batch 0: Train Loss = 0.2739
2023-11-08 08:44:15,292 - __main__ - INFO - Epoch 116 Batch 20: Train Loss = 0.2937
2023-11-08 08:44:28,764 - __main__ - INFO - Epoch 116 Batch 40: Train Loss = 0.2657
2023-11-08 08:44:43,466 - __main__ - INFO - Epoch 116 Batch 60: Train Loss = 0.3254
2023-11-08 08:44:58,613 - __main__ - INFO - Epoch 116 Batch 80: Train Loss = 0.2018
2023-11-08 08:45:12,400 - __main__ - INFO - Epoch 116 Batch 100: Train Loss = 0.1818
2023-11-08 08:45:26,248 - __main__ - INFO - Epoch 116 Batch 120: Train Loss = 0.2237
2023-11-08 08:45:37,396 - __main__ - INFO - Epoch 117 Batch 0: Train Loss = 0.2322
2023-11-08 08:45:52,324 - __main__ - INFO - Epoch 117 Batch 20: Train Loss = 0.2621
2023-11-08 08:46:06,756 - __main__ - INFO - Epoch 117 Batch 40: Train Loss = 0.2490
2023-11-08 08:46:20,623 - __main__ - INFO - Epoch 117 Batch 60: Train Loss = 0.2779
2023-11-08 08:46:35,432 - __main__ - INFO - Epoch 117 Batch 80: Train Loss = 0.2107
2023-11-08 08:46:49,106 - __main__ - INFO - Epoch 117 Batch 100: Train Loss = 0.1742
2023-11-08 08:47:03,556 - __main__ - INFO - Epoch 117 Batch 120: Train Loss = 0.2339
2023-11-08 08:47:14,667 - __main__ - INFO - Epoch 118 Batch 0: Train Loss = 0.2418
2023-11-08 08:47:28,751 - __main__ - INFO - Epoch 118 Batch 20: Train Loss = 0.2475
2023-11-08 08:47:41,918 - __main__ - INFO - Epoch 118 Batch 40: Train Loss = 0.2452
2023-11-08 08:47:55,516 - __main__ - INFO - Epoch 118 Batch 60: Train Loss = 0.2716
2023-11-08 08:48:10,147 - __main__ - INFO - Epoch 118 Batch 80: Train Loss = 0.2278
2023-11-08 08:48:24,537 - __main__ - INFO - Epoch 118 Batch 100: Train Loss = 0.1856
2023-11-08 08:48:37,900 - __main__ - INFO - Epoch 118 Batch 120: Train Loss = 0.2344
2023-11-08 08:48:48,923 - __main__ - INFO - Epoch 119 Batch 0: Train Loss = 0.2219
2023-11-08 08:49:03,102 - __main__ - INFO - Epoch 119 Batch 20: Train Loss = 0.2686
2023-11-08 08:49:17,159 - __main__ - INFO - Epoch 119 Batch 40: Train Loss = 0.2509
2023-11-08 08:49:31,361 - __main__ - INFO - Epoch 119 Batch 60: Train Loss = 0.2836
2023-11-08 08:49:45,740 - __main__ - INFO - Epoch 119 Batch 80: Train Loss = 0.2049
2023-11-08 08:49:59,373 - __main__ - INFO - Epoch 119 Batch 100: Train Loss = 0.1747
2023-11-08 08:50:13,158 - __main__ - INFO - Epoch 119 Batch 120: Train Loss = 0.2181
2023-11-08 08:50:24,696 - __main__ - INFO - Epoch 120 Batch 0: Train Loss = 0.2437
2023-11-08 08:50:39,164 - __main__ - INFO - Epoch 120 Batch 20: Train Loss = 0.2812
2023-11-08 08:50:53,423 - __main__ - INFO - Epoch 120 Batch 40: Train Loss = 0.2599
2023-11-08 08:51:06,785 - __main__ - INFO - Epoch 120 Batch 60: Train Loss = 0.2880
2023-11-08 08:51:21,282 - __main__ - INFO - Epoch 120 Batch 80: Train Loss = 0.2115
2023-11-08 08:51:35,324 - __main__ - INFO - Epoch 120 Batch 100: Train Loss = 0.1694
2023-11-08 08:51:50,012 - __main__ - INFO - Epoch 120 Batch 120: Train Loss = 0.2384
2023-11-08 08:52:01,446 - __main__ - INFO - Epoch 121 Batch 0: Train Loss = 0.3647
2023-11-08 08:52:16,949 - __main__ - INFO - Epoch 121 Batch 20: Train Loss = 0.2733
2023-11-08 08:52:30,000 - __main__ - INFO - Epoch 121 Batch 40: Train Loss = 0.2878
2023-11-08 08:52:44,138 - __main__ - INFO - Epoch 121 Batch 60: Train Loss = 0.2566
2023-11-08 08:52:57,751 - __main__ - INFO - Epoch 121 Batch 80: Train Loss = 0.1939
2023-11-08 08:53:12,397 - __main__ - INFO - Epoch 121 Batch 100: Train Loss = 0.2448
2023-11-08 08:53:25,336 - __main__ - INFO - Epoch 121 Batch 120: Train Loss = 0.2389
2023-11-08 08:53:36,194 - __main__ - INFO - Epoch 122 Batch 0: Train Loss = 0.2371
2023-11-08 08:53:50,381 - __main__ - INFO - Epoch 122 Batch 20: Train Loss = 0.3465
2023-11-08 08:54:05,538 - __main__ - INFO - Epoch 122 Batch 40: Train Loss = 0.2656
2023-11-08 08:54:18,994 - __main__ - INFO - Epoch 122 Batch 60: Train Loss = 0.2837
2023-11-08 08:54:33,016 - __main__ - INFO - Epoch 122 Batch 80: Train Loss = 0.2093
2023-11-08 08:54:47,476 - __main__ - INFO - Epoch 122 Batch 100: Train Loss = 0.1701
2023-11-08 08:55:01,090 - __main__ - INFO - Epoch 122 Batch 120: Train Loss = 0.2447
2023-11-08 08:55:12,540 - __main__ - INFO - Epoch 123 Batch 0: Train Loss = 0.2317
2023-11-08 08:55:25,760 - __main__ - INFO - Epoch 123 Batch 20: Train Loss = 0.2568
2023-11-08 08:55:38,758 - __main__ - INFO - Epoch 123 Batch 40: Train Loss = 0.2577
2023-11-08 08:55:52,141 - __main__ - INFO - Epoch 123 Batch 60: Train Loss = 0.2727
2023-11-08 08:56:06,392 - __main__ - INFO - Epoch 123 Batch 80: Train Loss = 0.2028
2023-11-08 08:56:20,534 - __main__ - INFO - Epoch 123 Batch 100: Train Loss = 0.1715
2023-11-08 08:56:34,329 - __main__ - INFO - Epoch 123 Batch 120: Train Loss = 0.2223
2023-11-08 08:56:44,987 - __main__ - INFO - Epoch 124 Batch 0: Train Loss = 0.2571
2023-11-08 08:56:59,671 - __main__ - INFO - Epoch 124 Batch 20: Train Loss = 0.2599
2023-11-08 08:57:13,485 - __main__ - INFO - Epoch 124 Batch 40: Train Loss = 0.2679
2023-11-08 08:57:27,112 - __main__ - INFO - Epoch 124 Batch 60: Train Loss = 0.2693
2023-11-08 08:57:40,635 - __main__ - INFO - Epoch 124 Batch 80: Train Loss = 0.1933
2023-11-08 08:57:54,418 - __main__ - INFO - Epoch 124 Batch 100: Train Loss = 0.1873
2023-11-08 08:58:08,125 - __main__ - INFO - Epoch 124 Batch 120: Train Loss = 0.2078
2023-11-08 08:58:18,709 - __main__ - INFO - Epoch 125 Batch 0: Train Loss = 0.2187
2023-11-08 08:58:33,213 - __main__ - INFO - Epoch 125 Batch 20: Train Loss = 0.3063
2023-11-08 08:58:47,987 - __main__ - INFO - Epoch 125 Batch 40: Train Loss = 0.2555
2023-11-08 08:59:01,800 - __main__ - INFO - Epoch 125 Batch 60: Train Loss = 0.2882
2023-11-08 08:59:15,749 - __main__ - INFO - Epoch 125 Batch 80: Train Loss = 0.2010
2023-11-08 08:59:29,667 - __main__ - INFO - Epoch 125 Batch 100: Train Loss = 0.1972
2023-11-08 08:59:44,655 - __main__ - INFO - Epoch 125 Batch 120: Train Loss = 0.2644
2023-11-08 08:59:56,301 - __main__ - INFO - Epoch 126 Batch 0: Train Loss = 0.2690
2023-11-08 09:00:10,458 - __main__ - INFO - Epoch 126 Batch 20: Train Loss = 0.3148
2023-11-08 09:00:24,051 - __main__ - INFO - Epoch 126 Batch 40: Train Loss = 0.2610
2023-11-08 09:00:38,556 - __main__ - INFO - Epoch 126 Batch 60: Train Loss = 0.2541
2023-11-08 09:00:52,493 - __main__ - INFO - Epoch 126 Batch 80: Train Loss = 0.2205
2023-11-08 09:01:07,131 - __main__ - INFO - Epoch 126 Batch 100: Train Loss = 0.1762
2023-11-08 09:01:20,760 - __main__ - INFO - Epoch 126 Batch 120: Train Loss = 0.2347
2023-11-08 09:01:32,117 - __main__ - INFO - Epoch 127 Batch 0: Train Loss = 0.2922
2023-11-08 09:01:47,241 - __main__ - INFO - Epoch 127 Batch 20: Train Loss = 0.2511
2023-11-08 09:02:00,614 - __main__ - INFO - Epoch 127 Batch 40: Train Loss = 0.3054
2023-11-08 09:02:14,195 - __main__ - INFO - Epoch 127 Batch 60: Train Loss = 0.4084
2023-11-08 09:02:28,538 - __main__ - INFO - Epoch 127 Batch 80: Train Loss = 0.2647
2023-11-08 09:02:42,291 - __main__ - INFO - Epoch 127 Batch 100: Train Loss = 0.1576
2023-11-08 09:02:56,022 - __main__ - INFO - Epoch 127 Batch 120: Train Loss = 0.2332
2023-11-08 09:03:07,468 - __main__ - INFO - Epoch 128 Batch 0: Train Loss = 0.3274
2023-11-08 09:03:22,578 - __main__ - INFO - Epoch 128 Batch 20: Train Loss = 0.2804
2023-11-08 09:03:36,490 - __main__ - INFO - Epoch 128 Batch 40: Train Loss = 0.2677
2023-11-08 09:03:50,099 - __main__ - INFO - Epoch 128 Batch 60: Train Loss = 0.2602
2023-11-08 09:04:03,697 - __main__ - INFO - Epoch 128 Batch 80: Train Loss = 0.2311
2023-11-08 09:04:17,786 - __main__ - INFO - Epoch 128 Batch 100: Train Loss = 0.1896
2023-11-08 09:04:31,647 - __main__ - INFO - Epoch 128 Batch 120: Train Loss = 0.2254
2023-11-08 09:04:43,899 - __main__ - INFO - Epoch 129 Batch 0: Train Loss = 0.2782
2023-11-08 09:04:58,037 - __main__ - INFO - Epoch 129 Batch 20: Train Loss = 0.3802
2023-11-08 09:05:11,617 - __main__ - INFO - Epoch 129 Batch 40: Train Loss = 0.2735
2023-11-08 09:05:25,302 - __main__ - INFO - Epoch 129 Batch 60: Train Loss = 0.2667
2023-11-08 09:05:39,600 - __main__ - INFO - Epoch 129 Batch 80: Train Loss = 0.2161
2023-11-08 09:05:54,474 - __main__ - INFO - Epoch 129 Batch 100: Train Loss = 0.1820
2023-11-08 09:06:08,683 - __main__ - INFO - Epoch 129 Batch 120: Train Loss = 0.2168
2023-11-08 09:06:19,680 - __main__ - INFO - Epoch 130 Batch 0: Train Loss = 0.2563
2023-11-08 09:06:34,737 - __main__ - INFO - Epoch 130 Batch 20: Train Loss = 0.2791
2023-11-08 09:06:48,003 - __main__ - INFO - Epoch 130 Batch 40: Train Loss = 0.2596
2023-11-08 09:07:01,848 - __main__ - INFO - Epoch 130 Batch 60: Train Loss = 0.2556
2023-11-08 09:07:16,584 - __main__ - INFO - Epoch 130 Batch 80: Train Loss = 0.2374
2023-11-08 09:07:30,308 - __main__ - INFO - Epoch 130 Batch 100: Train Loss = 0.1734
2023-11-08 09:07:44,069 - __main__ - INFO - Epoch 130 Batch 120: Train Loss = 0.2120
2023-11-08 09:07:55,935 - __main__ - INFO - Epoch 131 Batch 0: Train Loss = 0.2577
2023-11-08 09:08:10,856 - __main__ - INFO - Epoch 131 Batch 20: Train Loss = 0.2733
2023-11-08 09:08:24,316 - __main__ - INFO - Epoch 131 Batch 40: Train Loss = 0.2860
2023-11-08 09:08:37,818 - __main__ - INFO - Epoch 131 Batch 60: Train Loss = 0.2873
2023-11-08 09:08:51,957 - __main__ - INFO - Epoch 131 Batch 80: Train Loss = 0.2331
2023-11-08 09:09:05,877 - __main__ - INFO - Epoch 131 Batch 100: Train Loss = 0.2134
2023-11-08 09:09:19,002 - __main__ - INFO - Epoch 131 Batch 120: Train Loss = 0.2200
2023-11-08 09:09:29,903 - __main__ - INFO - Epoch 132 Batch 0: Train Loss = 0.4005
2023-11-08 09:09:45,898 - __main__ - INFO - Epoch 132 Batch 20: Train Loss = 0.2464
2023-11-08 09:09:59,569 - __main__ - INFO - Epoch 132 Batch 40: Train Loss = 0.2505
2023-11-08 09:10:13,337 - __main__ - INFO - Epoch 132 Batch 60: Train Loss = 0.2955
2023-11-08 09:10:27,617 - __main__ - INFO - Epoch 132 Batch 80: Train Loss = 0.2382
2023-11-08 09:10:41,772 - __main__ - INFO - Epoch 132 Batch 100: Train Loss = 0.1838
2023-11-08 09:10:55,222 - __main__ - INFO - Epoch 132 Batch 120: Train Loss = 0.2242
2023-11-08 09:11:06,763 - __main__ - INFO - Epoch 133 Batch 0: Train Loss = 0.3026
2023-11-08 09:11:21,227 - __main__ - INFO - Epoch 133 Batch 20: Train Loss = 0.2894
2023-11-08 09:11:35,113 - __main__ - INFO - Epoch 133 Batch 40: Train Loss = 0.2723
2023-11-08 09:11:49,563 - __main__ - INFO - Epoch 133 Batch 60: Train Loss = 0.3004
2023-11-08 09:12:03,327 - __main__ - INFO - Epoch 133 Batch 80: Train Loss = 0.2133
2023-11-08 09:12:17,016 - __main__ - INFO - Epoch 133 Batch 100: Train Loss = 0.1971
2023-11-08 09:12:30,784 - __main__ - INFO - Epoch 133 Batch 120: Train Loss = 0.2267
2023-11-08 09:12:42,791 - __main__ - INFO - Epoch 134 Batch 0: Train Loss = 0.2244
2023-11-08 09:12:57,163 - __main__ - INFO - Epoch 134 Batch 20: Train Loss = 0.2747
2023-11-08 09:13:11,469 - __main__ - INFO - Epoch 134 Batch 40: Train Loss = 0.2849
2023-11-08 09:13:25,291 - __main__ - INFO - Epoch 134 Batch 60: Train Loss = 0.2635
2023-11-08 09:13:38,801 - __main__ - INFO - Epoch 134 Batch 80: Train Loss = 0.2151
2023-11-08 09:13:53,047 - __main__ - INFO - Epoch 134 Batch 100: Train Loss = 0.1795
2023-11-08 09:14:07,277 - __main__ - INFO - Epoch 134 Batch 120: Train Loss = 0.2139
2023-11-08 09:14:18,416 - __main__ - INFO - Epoch 135 Batch 0: Train Loss = 0.2729
2023-11-08 09:14:33,323 - __main__ - INFO - Epoch 135 Batch 20: Train Loss = 0.2321
2023-11-08 09:14:48,034 - __main__ - INFO - Epoch 135 Batch 40: Train Loss = 0.2434
2023-11-08 09:15:02,381 - __main__ - INFO - Epoch 135 Batch 60: Train Loss = 0.2673
2023-11-08 09:15:16,590 - __main__ - INFO - Epoch 135 Batch 80: Train Loss = 0.2063
2023-11-08 09:15:29,799 - __main__ - INFO - Epoch 135 Batch 100: Train Loss = 0.1763
2023-11-08 09:15:44,389 - __main__ - INFO - Epoch 135 Batch 120: Train Loss = 0.2354
2023-11-08 09:15:55,592 - __main__ - INFO - Epoch 136 Batch 0: Train Loss = 0.2323
2023-11-08 09:16:09,710 - __main__ - INFO - Epoch 136 Batch 20: Train Loss = 0.2817
2023-11-08 09:16:23,799 - __main__ - INFO - Epoch 136 Batch 40: Train Loss = 0.2662
2023-11-08 09:16:37,560 - __main__ - INFO - Epoch 136 Batch 60: Train Loss = 0.2695
2023-11-08 09:16:52,091 - __main__ - INFO - Epoch 136 Batch 80: Train Loss = 0.2009
2023-11-08 09:17:05,843 - __main__ - INFO - Epoch 136 Batch 100: Train Loss = 0.1849
2023-11-08 09:17:20,398 - __main__ - INFO - Epoch 136 Batch 120: Train Loss = 0.2225
2023-11-08 09:17:32,388 - __main__ - INFO - Epoch 137 Batch 0: Train Loss = 0.2669
2023-11-08 09:17:46,484 - __main__ - INFO - Epoch 137 Batch 20: Train Loss = 0.2548
2023-11-08 09:18:00,933 - __main__ - INFO - Epoch 137 Batch 40: Train Loss = 0.2669
2023-11-08 09:18:14,863 - __main__ - INFO - Epoch 137 Batch 60: Train Loss = 0.2637
2023-11-08 09:18:28,944 - __main__ - INFO - Epoch 137 Batch 80: Train Loss = 0.1936
2023-11-08 09:18:43,731 - __main__ - INFO - Epoch 137 Batch 100: Train Loss = 0.1889
2023-11-08 09:18:56,858 - __main__ - INFO - Epoch 137 Batch 120: Train Loss = 0.2237
2023-11-08 09:19:08,351 - __main__ - INFO - Epoch 138 Batch 0: Train Loss = 0.3230
2023-11-08 09:19:22,520 - __main__ - INFO - Epoch 138 Batch 20: Train Loss = 0.2531
2023-11-08 09:19:36,614 - __main__ - INFO - Epoch 138 Batch 40: Train Loss = 0.2564
2023-11-08 09:19:50,842 - __main__ - INFO - Epoch 138 Batch 60: Train Loss = 0.2733
2023-11-08 09:20:04,964 - __main__ - INFO - Epoch 138 Batch 80: Train Loss = 0.2036
2023-11-08 09:20:18,855 - __main__ - INFO - Epoch 138 Batch 100: Train Loss = 0.1836
2023-11-08 09:20:33,324 - __main__ - INFO - Epoch 138 Batch 120: Train Loss = 0.2386
2023-11-08 09:20:44,610 - __main__ - INFO - Epoch 139 Batch 0: Train Loss = 0.2492
2023-11-08 09:20:59,079 - __main__ - INFO - Epoch 139 Batch 20: Train Loss = 0.2712
2023-11-08 09:21:12,821 - __main__ - INFO - Epoch 139 Batch 40: Train Loss = 0.3042
2023-11-08 09:21:27,246 - __main__ - INFO - Epoch 139 Batch 60: Train Loss = 0.2724
2023-11-08 09:21:41,503 - __main__ - INFO - Epoch 139 Batch 80: Train Loss = 0.2131
2023-11-08 09:21:55,965 - __main__ - INFO - Epoch 139 Batch 100: Train Loss = 0.1969
2023-11-08 09:22:09,651 - __main__ - INFO - Epoch 139 Batch 120: Train Loss = 0.2253
2023-11-08 09:22:21,406 - __main__ - INFO - Epoch 140 Batch 0: Train Loss = 0.2558
2023-11-08 09:22:35,895 - __main__ - INFO - Epoch 140 Batch 20: Train Loss = 0.2605
2023-11-08 09:22:50,533 - __main__ - INFO - Epoch 140 Batch 40: Train Loss = 0.2318
2023-11-08 09:23:05,235 - __main__ - INFO - Epoch 140 Batch 60: Train Loss = 0.2650
2023-11-08 09:23:19,389 - __main__ - INFO - Epoch 140 Batch 80: Train Loss = 0.2089
2023-11-08 09:23:32,781 - __main__ - INFO - Epoch 140 Batch 100: Train Loss = 0.1905
2023-11-08 09:23:46,892 - __main__ - INFO - Epoch 140 Batch 120: Train Loss = 0.2127
2023-11-08 09:23:58,591 - __main__ - INFO - Epoch 141 Batch 0: Train Loss = 0.2396
2023-11-08 09:24:13,794 - __main__ - INFO - Epoch 141 Batch 20: Train Loss = 0.2495
2023-11-08 09:24:27,612 - __main__ - INFO - Epoch 141 Batch 40: Train Loss = 0.2628
2023-11-08 09:24:40,922 - __main__ - INFO - Epoch 141 Batch 60: Train Loss = 0.2711
2023-11-08 09:24:55,799 - __main__ - INFO - Epoch 141 Batch 80: Train Loss = 0.1990
2023-11-08 09:25:09,381 - __main__ - INFO - Epoch 141 Batch 100: Train Loss = 0.2042
2023-11-08 09:25:22,695 - __main__ - INFO - Epoch 141 Batch 120: Train Loss = 0.2311
2023-11-08 09:25:34,096 - __main__ - INFO - Epoch 142 Batch 0: Train Loss = 0.2766
2023-11-08 09:25:48,566 - __main__ - INFO - Epoch 142 Batch 20: Train Loss = 0.2944
2023-11-08 09:26:02,320 - __main__ - INFO - Epoch 142 Batch 40: Train Loss = 0.2655
2023-11-08 09:26:16,111 - __main__ - INFO - Epoch 142 Batch 60: Train Loss = 0.2541
2023-11-08 09:26:31,083 - __main__ - INFO - Epoch 142 Batch 80: Train Loss = 0.2589
2023-11-08 09:26:44,351 - __main__ - INFO - Epoch 142 Batch 100: Train Loss = 0.1910
2023-11-08 09:26:57,795 - __main__ - INFO - Epoch 142 Batch 120: Train Loss = 0.2204
2023-11-08 09:27:09,071 - __main__ - INFO - Epoch 143 Batch 0: Train Loss = 0.2637
2023-11-08 09:27:23,316 - __main__ - INFO - Epoch 143 Batch 20: Train Loss = 0.2499
2023-11-08 09:27:37,069 - __main__ - INFO - Epoch 143 Batch 40: Train Loss = 0.2458
2023-11-08 09:27:49,761 - __main__ - INFO - Epoch 143 Batch 60: Train Loss = 0.2745
2023-11-08 09:28:04,504 - __main__ - INFO - Epoch 143 Batch 80: Train Loss = 0.2374
2023-11-08 09:28:18,517 - __main__ - INFO - Epoch 143 Batch 100: Train Loss = 0.1761
2023-11-08 09:28:32,127 - __main__ - INFO - Epoch 143 Batch 120: Train Loss = 0.2179
2023-11-08 09:28:43,856 - __main__ - INFO - Epoch 144 Batch 0: Train Loss = 0.2856
2023-11-08 09:28:58,005 - __main__ - INFO - Epoch 144 Batch 20: Train Loss = 0.2451
2023-11-08 09:29:11,918 - __main__ - INFO - Epoch 144 Batch 40: Train Loss = 0.2576
2023-11-08 09:29:25,706 - __main__ - INFO - Epoch 144 Batch 60: Train Loss = 0.3045
2023-11-08 09:29:39,965 - __main__ - INFO - Epoch 144 Batch 80: Train Loss = 0.2069
2023-11-08 09:29:55,051 - __main__ - INFO - Epoch 144 Batch 100: Train Loss = 0.1695
2023-11-08 09:30:09,348 - __main__ - INFO - Epoch 144 Batch 120: Train Loss = 0.2127
2023-11-08 09:30:21,309 - __main__ - INFO - Epoch 145 Batch 0: Train Loss = 0.2382
2023-11-08 09:30:35,851 - __main__ - INFO - Epoch 145 Batch 20: Train Loss = 0.2473
2023-11-08 09:30:49,124 - __main__ - INFO - Epoch 145 Batch 40: Train Loss = 0.2822
2023-11-08 09:31:02,389 - __main__ - INFO - Epoch 145 Batch 60: Train Loss = 0.2943
2023-11-08 09:31:16,394 - __main__ - INFO - Epoch 145 Batch 80: Train Loss = 0.1992
2023-11-08 09:31:31,496 - __main__ - INFO - Epoch 145 Batch 100: Train Loss = 0.2110
2023-11-08 09:31:46,207 - __main__ - INFO - Epoch 145 Batch 120: Train Loss = 0.2260
2023-11-08 09:31:57,606 - __main__ - INFO - Epoch 146 Batch 0: Train Loss = 0.2594
2023-11-08 09:32:11,476 - __main__ - INFO - Epoch 146 Batch 20: Train Loss = 0.2719
2023-11-08 09:32:24,318 - __main__ - INFO - Epoch 146 Batch 40: Train Loss = 0.2615
2023-11-08 09:32:38,326 - __main__ - INFO - Epoch 146 Batch 60: Train Loss = 0.2811
2023-11-08 09:32:52,391 - __main__ - INFO - Epoch 146 Batch 80: Train Loss = 0.1918
2023-11-08 09:33:07,087 - __main__ - INFO - Epoch 146 Batch 100: Train Loss = 0.1731
2023-11-08 09:33:20,851 - __main__ - INFO - Epoch 146 Batch 120: Train Loss = 0.2063
2023-11-08 09:33:32,072 - __main__ - INFO - Epoch 147 Batch 0: Train Loss = 0.2368
2023-11-08 09:33:47,330 - __main__ - INFO - Epoch 147 Batch 20: Train Loss = 0.2518
2023-11-08 09:34:00,245 - __main__ - INFO - Epoch 147 Batch 40: Train Loss = 0.2510
2023-11-08 09:34:13,587 - __main__ - INFO - Epoch 147 Batch 60: Train Loss = 0.2738
2023-11-08 09:34:27,664 - __main__ - INFO - Epoch 147 Batch 80: Train Loss = 0.2812
2023-11-08 09:34:42,692 - __main__ - INFO - Epoch 147 Batch 100: Train Loss = 0.2063
2023-11-08 09:34:56,734 - __main__ - INFO - Epoch 147 Batch 120: Train Loss = 0.2436
2023-11-08 09:35:07,630 - __main__ - INFO - Epoch 148 Batch 0: Train Loss = 0.2423
2023-11-08 09:35:23,065 - __main__ - INFO - Epoch 148 Batch 20: Train Loss = 0.2568
2023-11-08 09:35:36,715 - __main__ - INFO - Epoch 148 Batch 40: Train Loss = 0.2441
2023-11-08 09:35:51,371 - __main__ - INFO - Epoch 148 Batch 60: Train Loss = 0.3016
2023-11-08 09:36:05,587 - __main__ - INFO - Epoch 148 Batch 80: Train Loss = 0.1923
2023-11-08 09:36:20,492 - __main__ - INFO - Epoch 148 Batch 100: Train Loss = 0.1763
2023-11-08 09:36:35,303 - __main__ - INFO - Epoch 148 Batch 120: Train Loss = 0.2622
2023-11-08 09:36:46,274 - __main__ - INFO - Epoch 149 Batch 0: Train Loss = 0.3403
2023-11-08 09:37:01,017 - __main__ - INFO - Epoch 149 Batch 20: Train Loss = 0.2748
2023-11-08 09:37:15,725 - __main__ - INFO - Epoch 149 Batch 40: Train Loss = 0.2559
2023-11-08 09:37:29,367 - __main__ - INFO - Epoch 149 Batch 60: Train Loss = 0.2709
2023-11-08 09:37:44,191 - __main__ - INFO - Epoch 149 Batch 80: Train Loss = 0.2272
2023-11-08 09:37:58,891 - __main__ - INFO - Epoch 149 Batch 100: Train Loss = 0.1927
2023-11-08 09:38:13,100 - __main__ - INFO - Epoch 149 Batch 120: Train Loss = 0.2473
2023-11-08 09:38:24,395 - __main__ - INFO - auroc 0.8556
2023-11-08 09:38:24,399 - __main__ - INFO - auprc 0.4704
2023-11-08 09:38:24,401 - __main__ - INFO - minpse 0.4555
2023-11-08 09:38:24,618 - __main__ - INFO - last saved model is in epoch 71
2023-11-08 09:38:25,095 - __main__ - INFO - Batch 0: Test Loss = 0.1674
2023-11-08 09:38:32,037 - __main__ - INFO - 
==>Predicting on test
2023-11-08 09:38:32,039 - __main__ - INFO - Test Loss = 0.1998
2023-11-08 09:38:32,098 - __main__ - INFO - Transfer Target Dataset & Model
2023-11-08 09:38:32,165 - __main__ - INFO - [[-0.43249781572948887, 0.7734225027254102, 0.11121635247274919, -1.1510759266125783, -0.5775173789921769, -0.894979488306686, 0.6418248141325145, -1.1001333024577382, -0.09626591113773565, 0.07837976568684282, 0.27346416059268763, -0.29216681084789653, -0.20501606288246071, -0.44880109579951655, -0.6600819004124745, -0.5342137257388455, -0.16268638728171492, 1.013400735854239, 1.0, 1.098202336859675, -0.6863245135763378, -0.4670502544450039, -0.41420989908626493, -0.2941064849292665, 0.2534399947977164, 0.6689953308348103, -0.2780944554864925, 1.6237612679911448, -0.016555502218216205, -0.411288788529273, -0.5683812105284773, -0.9512610100808374, -0.9403624977548863, 0.651423723145103, -0.43323556246024986, 0.6921334800493779, 2.9116903104103184, 0.2360670258723392, -0.2674665445678642, 0.4444334033508993, -0.21164668014859825, -0.16135240337065737, -0.7969887859671778, -0.8495988149987506, -0.705221190542043, -1.0534078933590525, 1.3058941843049534, -0.3135827086480343, -0.3167900638717034, -0.24477434302814813, 0.7655720433538256, -0.33782515371340677, -0.18389192944425495, 0.311524270793097, -0.03429036235566567, 1.6532381373162057, -0.6035840289958837, -0.4798165125287964, 1.3136602297615485, -0.38113328114517314, -0.438838853292927, -0.25994688807198424, 1.8115519460700398, 0.6532487251600559, -0.6813987150690228, -0.6881330975707701, -0.8608674113288034, 0.5282372331730917, -0.3927744234337778, -0.2332558526002517, -0.7532798583884561, -1.2230128823047004, 0.4609091575849162, -0.7849239794277927, -1.4109277280424448], [-0.43249781572948887, 0.7734225027254102, 0.11121635247274919, -1.1510759266125783, -0.5775173789921769, -0.894979488306686, 0.6418248141325145, -1.1001333024577382, -0.09626591113773565, 0.07837976568684282, 0.27346416059268763, -0.29216681084789653, -0.20501606288246071, -0.44880109579951655, -0.6600819004124745, -0.5342137257388455, -0.16268638728171492, 1.013400735854239, 1.0, 1.098202336859675, -0.6863245135763378, -0.4670502544450039, -0.41420989908626493, -0.2941064849292665, 0.2534399947977164, 0.6689953308348103, -0.2780944554864925, 1.6237612679911448, -0.016555502218216205, -0.411288788529273, -0.5683812105284773, -0.9512610100808374, -0.9403624977548863, 0.651423723145103, -0.43323556246024986, 0.6921334800493779, 2.9116903104103184, 0.2360670258723392, -0.2674665445678642, 0.4444334033508993, -0.21164668014859825, -0.16135240337065737, -0.7969887859671778, -0.8495988149987506, -0.705221190542043, -1.0534078933590525, 1.3058941843049534, -0.3135827086480343, -0.3167900638717034, -0.24477434302814813, 0.7655720433538256, -0.33782515371340677, -0.18389192944425495, 0.311524270793097, -0.03429036235566567, 1.6532381373162057, -0.6035840289958837, -0.4798165125287964, 1.3136602297615485, -0.38113328114517314, -0.438838853292927, -0.25994688807198424, 1.8115519460700398, 0.6532487251600559, -0.6813987150690228, -0.6881330975707701, -0.8608674113288034, 0.5282372331730917, -0.3927744234337778, -0.2332558526002517, -0.7532798583884561, -1.2230128823047004, 0.4609091575849162, -0.7849239794277927, -1.4109277280424448], [-0.43249781572948887, 1.0626655489546322, -0.2607021557915042, -0.8723303017698648, -0.6987602223992506, 0.42060382604722935, 0.7735163080078745, -0.4422053625222244, -0.09626591113773565, -0.5703381110324975, 0.27346416059268763, 0.4054213944799434, -0.6478420259418122, -0.652648910528702, 1.0582568698308186, 1.3623416964867365, -0.16268638728171492, 0.06953911374846142, 1.0, 1.098202336859675, -0.6863245135763378, -0.4670502544450039, -0.8301874079837086, -0.2941064849292665, 0.23429139098427385, -0.9597305030519863, -0.2780944554864925, 0.44841735124674564, -0.016555502218216205, -0.411288788529273, -0.49230413633903536, -1.2379951878994253, -0.8899655603378251, 0.4144052468488665, -0.43323556246024986, 0.6921334800493779, 2.9116903104103184, 0.07172496731033204, -0.2674665445678642, 1.0100887085497614, -0.21164668014859825, 1.943067011208341, -0.08881508319448383, -0.8495988149987506, 1.7124080848737049, -0.5711764060634537, 0.07961653401158347, -0.3135827086480343, -0.7495061187450142, -0.24477434302814813, 1.0712982894377368, -0.33782515371340677, -0.18389192944425495, 0.1906271826667854, 0.08009979585285867, 0.2295633031960889, -0.6035840289958837, -0.7241072055173139, 0.12678603163309085, -0.38113328114517314, -0.438838853292927, 0.37611362926166203, -0.07561878133196119, 0.3448973731726548, -0.45301547558529076, -0.6881330975707701, -0.8608674113288034, 0.6571388363948929, -0.9767247139266083, -0.2332558526002517, 0.4743340898355563, 0.31603580948316384, 0.4609091575849162, 0.5732318714191863, -0.40170913406717196], [-0.43249781572948887, 0.3395579333815775, -0.8951513757717067, -0.7678006924538472, 0.5406110657619471, -0.13459647175350561, 0.3257652288316494, 0.680026078568982, -0.738540973511107, -0.08843340261241621, 0.27346416059268763, -0.5014432724462485, 2.8390253475185365, -0.170826802986991, 0.7145891157821607, -0.36929586293662087, -0.43897479874595047, 0.28190797872226137, 1.0, 1.098202336859675, -0.5896780760673638, -0.4670502544450039, -0.96884657761619, -0.2941064849292665, 0.08110256047672398, -0.9597305030519863, -0.2780944554864925, -0.32395150832814545, -0.016555502218216205, -0.411288788529273, 2.0182393119125503, -0.9512610100808374, 0.3915565625531523, -0.5505985495001029, -0.43323556246024986, 0.572828751735985, 2.9116903104103184, 0.44736395830920606, -0.2674665445678642, -0.026946017648152392, -0.21164668014859825, 0.9439992083274025, -0.21802572370037915, -0.8495988149987506, -0.302282977972751, 0.721203979888751, -0.4108945261057649, -0.3135827086480343, -0.38890940635058796, 1.1668382976193643, -0.35542419228718275, -0.33782515371340677, -0.5335587241304727, -0.024300974002213524, -1.0066067071281226, 1.247958440011939, -0.6035840289958837, -0.942223895685633, -0.29927137282327887, -0.38113328114517314, -0.438838853292927, 1.0121741465953082, -0.43265108111071765, -0.8173500304721677, -0.583520183861709, -0.5895349223964507, -0.8608674113288034, 0.39933562995129274, -0.9767247139266083, -0.2332558526002517, -0.35194452916137237, -0.3835317776931382, 0.4609091575849162, -0.10584605400430319, -0.7071814287953199], [-0.43249781572948887, 0.2672471718242721, -0.9826616130103564, -0.6284278800324904, -0.40238882740418175, -0.43633576403651375, 0.03604394230585671, -0.017417052954250063, -0.738540973511107, -0.21817697795628416, 0.27346416059268763, 0.19614493288159143, -0.48681440301113876, -0.5229275738828567, 2.4016853629301207, 1.3623416964867365, -0.43897479874595047, 0.022346032643172534, 1.0, 1.098202336859675, -0.5896780760673638, -0.4670502544450039, 0.2790859490761416, -0.2941064849292665, 0.751303693947251, 0.6689953308348103, -0.2780944554864925, 1.0528799370010082, -0.016555502218216205, -0.411288788529273, 0.07827392008177957, -0.9512610100808374, -1.2931410596743127, 0.295896008700747, -0.43323556246024986, 0.572828751735985, 2.9116903104103184, 0.5647511429963535, -0.2674665445678642, 0.7272610559503303, -0.21164668014859825, 3.6648647140457036, -0.287600683972784, 0.972012082109433, 1.1080007660197713, -0.3718540579812729, 0.815383124187605, -0.3135827086480343, -0.3408298446979971, 1.1668382976193643, 1.3307023770240858, -0.33782515371340677, -0.5335587241304727, 1.2787009758035917, -0.7206313116068118, 1.3414845240052313, -0.6035840289958837, -0.9509485632923658, 0.9028191611786214, -0.38113328114517314, -0.438838853292927, 2.0192699657069153, 1.0974873465125259, -0.4615600089482426, -0.09412752782514046, -0.5895349223964507, 1.2819970892257146, 0.9793928444493915, -0.9915082655846547, -0.2332558526002517, -0.21029676590475738, -0.6633588125636588, 0.4609091575849162, -0.10584605400430319, -0.3243743759081472], [-0.43249781572948887, 0.411868694938883, -0.566987986126775, -0.2799958489790985, -0.8469459198967851, -0.46047490741915437, 0.3257652288316494, 0.10508004096767726, -0.8205335346651542, -0.292316163867066, 0.27346416059268763, 1.1378890100741745, -0.3908171662640067, -0.7082437690912071, 1.4644096700701428, 1.3623416964867365, -0.2662945415808034, 0.022346032643172534, 1.0, 1.098202336859675, -1.4111727948936457, -0.4670502544450039, 1.111040966871029, -0.2941064849292665, 1.0768299587757935, -0.14536758610858785, -0.2780944554864925, 0.6834861345956256, -0.016555502218216205, -0.411288788529273, -0.7966124330968032, -0.9512610100808374, -1.0483559350771596, 0.380545464520832, -0.43323556246024986, 2.1237902198100924, 2.9116903104103184, 0.7056157646209292, -0.2674665445678642, 0.25588163495127864, -0.21164668014859825, 3.452297096411461, -0.24287392379766656, 2.3382202549405706, 0.8057971065927999, -0.21753998204668118, 0.5701275941289319, -0.35234396942435836, -0.1485115980876371, -0.4706323655317493, 1.0712982894377368, -0.33782515371340677, -0.47452407048215023, 1.5742271912234647, -0.6062411533982874, 1.2167830786808416, -0.6035840289958837, -0.9160498928654347, 0.8571701535582954, -0.38113328114517314, -0.438838853292927, 1.754244750151229, 0.7914596609878773, -0.7461920261673826, -0.12675370489424503, -1.3783203237910062, 0.21056483894845573, 0.7860404396166896, -0.9915082655846547, -0.2332558526002517, 0.4271181687500135, -0.6633588125636588, 0.4609091575849162, -0.05360929051018861, -0.3243743759081472], [-0.43249781572948887, 0.411868694938883, -0.566987986126775, -0.2799958489790985, -0.8469459198967851, -0.46047490741915437, 0.3257652288316494, 0.10508004096767726, -0.8205335346651542, -0.292316163867066, 0.27346416059268763, 1.1378890100741745, -0.3908171662640067, -0.7082437690912071, 1.4644096700701428, 1.3623416964867365, -0.2662945415808034, 0.022346032643172534, 1.0, 1.098202336859675, -1.4111727948936457, -0.4670502544450039, 1.111040966871029, -0.2941064849292665, 1.0768299587757935, -0.14536758610858785, -0.2780944554864925, 0.6834861345956256, -0.016555502218216205, -0.411288788529273, -0.7966124330968032, -0.9512610100808374, -1.0483559350771596, 0.380545464520832, -0.43323556246024986, 2.1237902198100924, 2.9116903104103184, 0.7056157646209292, -0.2674665445678642, 0.25588163495127864, -0.21164668014859825, 3.452297096411461, -0.24287392379766656, 2.3382202549405706, 0.8057971065927999, -0.21753998204668118, 0.5701275941289319, -0.35234396942435836, -0.1485115980876371, -0.4706323655317493, 1.0712982894377368, -0.33782515371340677, -0.47452407048215023, 1.5742271912234647, -0.6062411533982874, 1.2167830786808416, -0.6035840289958837, -0.9160498928654347, 0.8571701535582954, -0.38113328114517314, -0.438838853292927, 1.754244750151229, 0.7914596609878773, -0.7461920261673826, -0.12675370489424503, -1.3783203237910062, 0.21056483894845573, 0.7860404396166896, -0.9915082655846547, -0.2332558526002517, 0.4271181687500135, -0.6633588125636588, 0.4609091575849162, -0.05360929051018861, -0.3243743759081472]]
2023-11-08 09:38:32,167 - __main__ - INFO - 75
2023-11-08 09:38:32,168 - __main__ - INFO - 361
2023-11-08 09:38:32,329 - __main__ - INFO - {'los_mean': 6.927731092436975, 'los_std': 5.1253246527009555, 'los_median': 6.0, 'large_los': 26.649999999999977, 'threshold': 4.9562737642585555}
2023-11-08 09:38:35,330 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 2.9328
2023-11-08 09:38:36,795 - __main__ - INFO - Fold 1, epoch 0: Loss = 2.4264 Valid loss = 2.2395 MSE = 38.9803 AUROC = 0.7297
2023-11-08 09:38:36,797 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 38.9803 ------------
2023-11-08 09:38:37,110 - __main__ - INFO - ------------ Save best model - MSE: 38.9803 ------------
2023-11-08 09:38:37,120 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.7297 ------------
2023-11-08 09:38:37,122 - __main__ - INFO - Fold 1, mse = 38.9803, mad = 4.6831
2023-11-08 09:38:37,423 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 2.2173
2023-11-08 09:38:38,865 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.0100 ------------
2023-11-08 09:38:39,179 - __main__ - INFO - ------------ Save best model - MSE: 37.0100 ------------
2023-11-08 09:38:39,181 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8064 ------------
2023-11-08 09:38:39,183 - __main__ - INFO - Fold 1, mse = 37.0100, mad = 4.5351
2023-11-08 09:38:39,448 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 2.1509
2023-11-08 09:38:41,001 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.9511 ------------
2023-11-08 09:38:41,505 - __main__ - INFO - ------------ Save best model - MSE: 35.9511 ------------
2023-11-08 09:38:41,507 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8453 ------------
2023-11-08 09:38:41,509 - __main__ - INFO - Fold 1, mse = 35.9511, mad = 4.4760
2023-11-08 09:38:41,930 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 2.0248
2023-11-08 09:38:43,653 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.2550 ------------
2023-11-08 09:38:43,974 - __main__ - INFO - ------------ Save best model - MSE: 35.2550 ------------
2023-11-08 09:38:43,977 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8788 ------------
2023-11-08 09:38:43,978 - __main__ - INFO - Fold 1, mse = 35.2550, mad = 4.4447
2023-11-08 09:38:44,254 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 2.3673
2023-11-08 09:38:46,007 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.9451 ------------
2023-11-08 09:38:46,282 - __main__ - INFO - ------------ Save best model - MSE: 34.9451 ------------
2023-11-08 09:38:46,283 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9100 ------------
2023-11-08 09:38:46,284 - __main__ - INFO - Fold 1, mse = 34.9451, mad = 4.4909
2023-11-08 09:38:46,489 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 1.7009
2023-11-08 09:38:48,087 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.3117 ------------
2023-11-08 09:38:48,430 - __main__ - INFO - ------------ Save best model - MSE: 34.3117 ------------
2023-11-08 09:38:48,432 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9351 ------------
2023-11-08 09:38:48,434 - __main__ - INFO - Fold 1, mse = 34.3117, mad = 4.3772
2023-11-08 09:38:48,732 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 1.7684
2023-11-08 09:38:50,333 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9430 ------------
2023-11-08 09:38:50,335 - __main__ - INFO - Fold 1, mse = 34.4055, mad = 4.4870
2023-11-08 09:38:50,612 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 2.0592
2023-11-08 09:38:52,299 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.1192 ------------
2023-11-08 09:38:52,586 - __main__ - INFO - ------------ Save best model - MSE: 34.1192 ------------
2023-11-08 09:38:52,588 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9483 ------------
2023-11-08 09:38:52,589 - __main__ - INFO - Fold 1, mse = 34.1192, mad = 4.4144
2023-11-08 09:38:52,926 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 1.9947
2023-11-08 09:38:54,332 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9525 ------------
2023-11-08 09:38:54,334 - __main__ - INFO - Fold 1, mse = 34.2717, mad = 4.4101
2023-11-08 09:38:54,627 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 1.7755
2023-11-08 09:38:56,183 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9557 ------------
2023-11-08 09:38:56,187 - __main__ - INFO - Fold 1, mse = 34.5667, mad = 4.5417
2023-11-08 09:38:56,457 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 1.5923
2023-11-08 09:38:57,796 - __main__ - INFO - Fold 1, epoch 10: Loss = 1.7175 Valid loss = 1.7823 MSE = 34.4514 AUROC = 0.9618
2023-11-08 09:38:57,798 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9618 ------------
2023-11-08 09:38:57,800 - __main__ - INFO - Fold 1, mse = 34.4514, mad = 4.4694
2023-11-08 09:38:58,048 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 1.6803
2023-11-08 09:38:59,565 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9643 ------------
2023-11-08 09:38:59,567 - __main__ - INFO - Fold 1, mse = 34.8167, mad = 4.4521
2023-11-08 09:38:59,856 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 1.8768
2023-11-08 09:39:01,591 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9684 ------------
2023-11-08 09:39:01,593 - __main__ - INFO - Fold 1, mse = 34.8291, mad = 4.5312
2023-11-08 09:39:01,850 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 1.9250
2023-11-08 09:39:03,250 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9689 ------------
2023-11-08 09:39:03,254 - __main__ - INFO - Fold 1, mse = 34.8424, mad = 4.4557
2023-11-08 09:39:03,468 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 1.7633
2023-11-08 09:39:05,098 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9703 ------------
2023-11-08 09:39:05,100 - __main__ - INFO - Fold 1, mse = 34.7223, mad = 4.4505
2023-11-08 09:39:05,390 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 1.7651
2023-11-08 09:39:06,791 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9718 ------------
2023-11-08 09:39:06,796 - __main__ - INFO - Fold 1, mse = 35.1285, mad = 4.5293
2023-11-08 09:39:07,065 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 1.4893
2023-11-08 09:39:08,561 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9728 ------------
2023-11-08 09:39:08,563 - __main__ - INFO - Fold 1, mse = 35.3826, mad = 4.4342
2023-11-08 09:39:08,834 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 1.6482
2023-11-08 09:39:10,532 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9733 ------------
2023-11-08 09:39:10,534 - __main__ - INFO - Fold 1, mse = 34.8959, mad = 4.4633
2023-11-08 09:39:10,795 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 1.5449
2023-11-08 09:39:12,416 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9736 ------------
2023-11-08 09:39:12,418 - __main__ - INFO - Fold 1, mse = 35.2102, mad = 4.5055
2023-11-08 09:39:12,652 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 1.6295
2023-11-08 09:39:14,448 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9739 ------------
2023-11-08 09:39:14,450 - __main__ - INFO - Fold 1, mse = 35.2157, mad = 4.4919
2023-11-08 09:39:14,679 - __main__ - INFO - Fold 1 Epoch 20 Batch 0: Train Loss = 1.6371
2023-11-08 09:39:16,467 - __main__ - INFO - Fold 1, epoch 20: Loss = 1.5495 Valid loss = 1.7323 MSE = 34.8675 AUROC = 0.9748
2023-11-08 09:39:16,469 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9748 ------------
2023-11-08 09:39:16,470 - __main__ - INFO - Fold 1, mse = 34.8675, mad = 4.4191
2023-11-08 09:39:16,728 - __main__ - INFO - Fold 1 Epoch 21 Batch 0: Train Loss = 1.4453
2023-11-08 09:39:18,462 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9753 ------------
2023-11-08 09:39:18,475 - __main__ - INFO - Fold 1, mse = 34.2688, mad = 4.3961
2023-11-08 09:39:18,727 - __main__ - INFO - Fold 1 Epoch 22 Batch 0: Train Loss = 1.5984
2023-11-08 09:39:19,883 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9760 ------------
2023-11-08 09:39:19,887 - __main__ - INFO - Fold 1, mse = 34.3843, mad = 4.4239
2023-11-08 09:39:20,090 - __main__ - INFO - Fold 1 Epoch 23 Batch 0: Train Loss = 1.8596
2023-11-08 09:39:21,395 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9764 ------------
2023-11-08 09:39:21,397 - __main__ - INFO - Fold 1, mse = 34.3878, mad = 4.4262
2023-11-08 09:39:21,606 - __main__ - INFO - Fold 1 Epoch 24 Batch 0: Train Loss = 1.6972
2023-11-08 09:39:22,944 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.8900 ------------
2023-11-08 09:39:23,291 - __main__ - INFO - ------------ Save best model - MSE: 33.8900 ------------
2023-11-08 09:39:23,293 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9775 ------------
2023-11-08 09:39:23,295 - __main__ - INFO - Fold 1, mse = 33.8900, mad = 4.4013
2023-11-08 09:39:23,602 - __main__ - INFO - Fold 1 Epoch 25 Batch 0: Train Loss = 1.1795
2023-11-08 09:39:25,306 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.7107 ------------
2023-11-08 09:39:25,547 - __main__ - INFO - ------------ Save best model - MSE: 33.7107 ------------
2023-11-08 09:39:25,548 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9782 ------------
2023-11-08 09:39:25,549 - __main__ - INFO - Fold 1, mse = 33.7107, mad = 4.3618
2023-11-08 09:39:25,892 - __main__ - INFO - Fold 1 Epoch 26 Batch 0: Train Loss = 1.6489
2023-11-08 09:39:27,611 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9785 ------------
2023-11-08 09:39:27,612 - __main__ - INFO - Fold 1, mse = 33.9450, mad = 4.3985
2023-11-08 09:39:28,031 - __main__ - INFO - Fold 1 Epoch 27 Batch 0: Train Loss = 1.3959
2023-11-08 09:39:29,800 - __main__ - INFO - Fold 1, mse = 34.2722, mad = 4.4675
2023-11-08 09:39:30,105 - __main__ - INFO - Fold 1 Epoch 28 Batch 0: Train Loss = 1.3122
2023-11-08 09:39:31,992 - __main__ - INFO - Fold 1, mse = 34.5149, mad = 4.4455
2023-11-08 09:39:32,359 - __main__ - INFO - Fold 1 Epoch 29 Batch 0: Train Loss = 1.1259
2023-11-08 09:39:34,096 - __main__ - INFO - Fold 1, mse = 34.1400, mad = 4.4505
2023-11-08 09:39:34,323 - __main__ - INFO - Fold 1 Epoch 30 Batch 0: Train Loss = 1.4818
2023-11-08 09:39:35,835 - __main__ - INFO - Fold 1, epoch 30: Loss = 1.4503 Valid loss = 1.6381 MSE = 33.8332 AUROC = 0.9787
2023-11-08 09:39:35,840 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9787 ------------
2023-11-08 09:39:35,842 - __main__ - INFO - Fold 1, mse = 33.8332, mad = 4.3394
2023-11-08 09:39:36,094 - __main__ - INFO - Fold 1 Epoch 31 Batch 0: Train Loss = 1.4435
2023-11-08 09:39:37,832 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.2635 ------------
2023-11-08 09:39:38,069 - __main__ - INFO - ------------ Save best model - MSE: 33.2635 ------------
2023-11-08 09:39:38,071 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9793 ------------
2023-11-08 09:39:38,073 - __main__ - INFO - Fold 1, mse = 33.2635, mad = 4.3012
2023-11-08 09:39:38,331 - __main__ - INFO - Fold 1 Epoch 32 Batch 0: Train Loss = 1.3780
2023-11-08 09:39:39,669 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9797 ------------
2023-11-08 09:39:39,671 - __main__ - INFO - Fold 1, mse = 33.4193, mad = 4.3437
2023-11-08 09:39:39,929 - __main__ - INFO - Fold 1 Epoch 33 Batch 0: Train Loss = 1.4522
2023-11-08 09:39:41,587 - __main__ - INFO - Fold 1, mse = 34.0700, mad = 4.3160
2023-11-08 09:39:41,855 - __main__ - INFO - Fold 1 Epoch 34 Batch 0: Train Loss = 1.3710
2023-11-08 09:39:43,381 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9799 ------------
2023-11-08 09:39:43,383 - __main__ - INFO - Fold 1, mse = 33.2840, mad = 4.3340
2023-11-08 09:39:43,672 - __main__ - INFO - Fold 1 Epoch 35 Batch 0: Train Loss = 1.4531
2023-11-08 09:39:45,549 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9802 ------------
2023-11-08 09:39:45,552 - __main__ - INFO - Fold 1, mse = 33.6541, mad = 4.3362
2023-11-08 09:39:45,858 - __main__ - INFO - Fold 1 Epoch 36 Batch 0: Train Loss = 1.1230
2023-11-08 09:39:47,517 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.2435 ------------
2023-11-08 09:39:47,831 - __main__ - INFO - ------------ Save best model - MSE: 33.2435 ------------
2023-11-08 09:39:47,835 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9805 ------------
2023-11-08 09:39:47,836 - __main__ - INFO - Fold 1, mse = 33.2435, mad = 4.3140
2023-11-08 09:39:48,170 - __main__ - INFO - Fold 1 Epoch 37 Batch 0: Train Loss = 1.2642
2023-11-08 09:39:49,778 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.8814 ------------
2023-11-08 09:39:50,119 - __main__ - INFO - ------------ Save best model - MSE: 32.8814 ------------
2023-11-08 09:39:50,124 - __main__ - INFO - Fold 1, mse = 32.8814, mad = 4.2663
2023-11-08 09:39:50,362 - __main__ - INFO - Fold 1 Epoch 38 Batch 0: Train Loss = 1.3029
2023-11-08 09:39:51,794 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.8032 ------------
2023-11-08 09:39:52,114 - __main__ - INFO - ------------ Save best model - MSE: 32.8032 ------------
2023-11-08 09:39:52,117 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9805 ------------
2023-11-08 09:39:52,127 - __main__ - INFO - Fold 1, mse = 32.8032, mad = 4.2636
2023-11-08 09:39:52,406 - __main__ - INFO - Fold 1 Epoch 39 Batch 0: Train Loss = 1.2798
2023-11-08 09:39:53,765 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9811 ------------
2023-11-08 09:39:53,767 - __main__ - INFO - Fold 1, mse = 33.1935, mad = 4.3385
2023-11-08 09:39:54,027 - __main__ - INFO - Fold 1 Epoch 40 Batch 0: Train Loss = 1.4180
2023-11-08 09:39:55,411 - __main__ - INFO - Fold 1, epoch 40: Loss = 1.4374 Valid loss = 1.6139 MSE = 33.3430 AUROC = 0.9814
2023-11-08 09:39:55,413 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9814 ------------
2023-11-08 09:39:55,415 - __main__ - INFO - Fold 1, mse = 33.3430, mad = 4.3395
2023-11-08 09:39:55,616 - __main__ - INFO - Fold 1 Epoch 41 Batch 0: Train Loss = 1.3235
2023-11-08 09:39:56,916 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9818 ------------
2023-11-08 09:39:56,918 - __main__ - INFO - Fold 1, mse = 33.1729, mad = 4.3208
2023-11-08 09:39:57,170 - __main__ - INFO - Fold 1 Epoch 42 Batch 0: Train Loss = 1.5739
2023-11-08 09:39:58,778 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.7097 ------------
2023-11-08 09:39:59,186 - __main__ - INFO - ------------ Save best model - MSE: 32.7097 ------------
2023-11-08 09:39:59,189 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9820 ------------
2023-11-08 09:39:59,191 - __main__ - INFO - Fold 1, mse = 32.7097, mad = 4.2652
2023-11-08 09:39:59,490 - __main__ - INFO - Fold 1 Epoch 43 Batch 0: Train Loss = 1.5092
2023-11-08 09:40:01,025 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9820 ------------
2023-11-08 09:40:01,027 - __main__ - INFO - Fold 1, mse = 32.7239, mad = 4.2347
2023-11-08 09:40:01,282 - __main__ - INFO - Fold 1 Epoch 44 Batch 0: Train Loss = 1.3930
2023-11-08 09:40:02,972 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9829 ------------
2023-11-08 09:40:02,981 - __main__ - INFO - Fold 1, mse = 33.1941, mad = 4.2838
2023-11-08 09:40:03,277 - __main__ - INFO - Fold 1 Epoch 45 Batch 0: Train Loss = 1.3311
2023-11-08 09:40:04,725 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9830 ------------
2023-11-08 09:40:04,727 - __main__ - INFO - Fold 1, mse = 33.3748, mad = 4.2877
2023-11-08 09:40:05,036 - __main__ - INFO - Fold 1 Epoch 46 Batch 0: Train Loss = 1.6735
2023-11-08 09:40:06,890 - __main__ - INFO - Fold 1, mse = 32.8756, mad = 4.3075
2023-11-08 09:40:07,188 - __main__ - INFO - Fold 1 Epoch 47 Batch 0: Train Loss = 1.2086
2023-11-08 09:40:08,537 - __main__ - INFO - Fold 1, mse = 32.7860, mad = 4.1819
2023-11-08 09:40:08,762 - __main__ - INFO - Fold 1 Epoch 48 Batch 0: Train Loss = 1.5317
2023-11-08 09:40:10,416 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.4171 ------------
2023-11-08 09:40:10,849 - __main__ - INFO - ------------ Save best model - MSE: 32.4171 ------------
2023-11-08 09:40:10,851 - __main__ - INFO - Fold 1, mse = 32.4171, mad = 4.2764
2023-11-08 09:40:11,195 - __main__ - INFO - Fold 1 Epoch 49 Batch 0: Train Loss = 1.5771
2023-11-08 09:40:12,809 - __main__ - INFO - Fold 1, mse = 32.8161, mad = 4.2232
2023-11-08 09:40:13,032 - __main__ - INFO - Fold 1 Epoch 50 Batch 0: Train Loss = 1.2592
2023-11-08 09:40:14,831 - __main__ - INFO - Fold 1, epoch 50: Loss = 1.4044 Valid loss = 1.5616 MSE = 32.4091 AUROC = 0.9825
2023-11-08 09:40:14,833 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.4091 ------------
2023-11-08 09:40:15,123 - __main__ - INFO - ------------ Save best model - MSE: 32.4091 ------------
2023-11-08 09:40:15,125 - __main__ - INFO - Fold 1, mse = 32.4091, mad = 4.2613
2023-11-08 09:40:15,405 - __main__ - INFO - Fold 1 Epoch 51 Batch 0: Train Loss = 1.4032
2023-11-08 09:40:17,060 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.3066 ------------
2023-11-08 09:40:17,405 - __main__ - INFO - ------------ Save best model - MSE: 32.3066 ------------
2023-11-08 09:40:17,408 - __main__ - INFO - Fold 1, mse = 32.3066, mad = 4.2577
2023-11-08 09:40:17,603 - __main__ - INFO - Fold 1 Epoch 52 Batch 0: Train Loss = 1.2332
2023-11-08 09:40:19,389 - __main__ - INFO - Fold 1, mse = 32.8671, mad = 4.2580
2023-11-08 09:40:19,683 - __main__ - INFO - Fold 1 Epoch 53 Batch 0: Train Loss = 1.5015
2023-11-08 09:40:21,327 - __main__ - INFO - Fold 1, mse = 32.6780, mad = 4.2882
2023-11-08 09:40:21,633 - __main__ - INFO - Fold 1 Epoch 54 Batch 0: Train Loss = 1.2411
2023-11-08 09:40:23,119 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.2753 ------------
2023-11-08 09:40:23,598 - __main__ - INFO - ------------ Save best model - MSE: 32.2753 ------------
2023-11-08 09:40:23,600 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9831 ------------
2023-11-08 09:40:23,601 - __main__ - INFO - Fold 1, mse = 32.2753, mad = 4.2113
2023-11-08 09:40:23,919 - __main__ - INFO - Fold 1 Epoch 55 Batch 0: Train Loss = 1.1966
2023-11-08 09:40:25,722 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.2686 ------------
2023-11-08 09:40:26,066 - __main__ - INFO - ------------ Save best model - MSE: 32.2686 ------------
2023-11-08 09:40:26,068 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9834 ------------
2023-11-08 09:40:26,070 - __main__ - INFO - Fold 1, mse = 32.2686, mad = 4.2558
2023-11-08 09:40:26,376 - __main__ - INFO - Fold 1 Epoch 56 Batch 0: Train Loss = 1.2681
2023-11-08 09:40:27,975 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9842 ------------
2023-11-08 09:40:27,977 - __main__ - INFO - Fold 1, mse = 33.0370, mad = 4.3142
2023-11-08 09:40:28,317 - __main__ - INFO - Fold 1 Epoch 57 Batch 0: Train Loss = 1.3153
2023-11-08 09:40:30,079 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9846 ------------
2023-11-08 09:40:30,081 - __main__ - INFO - Fold 1, mse = 32.9223, mad = 4.3190
2023-11-08 09:40:30,320 - __main__ - INFO - Fold 1 Epoch 58 Batch 0: Train Loss = 1.3983
2023-11-08 09:40:31,986 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9846 ------------
2023-11-08 09:40:31,988 - __main__ - INFO - Fold 1, mse = 32.6520, mad = 4.2299
2023-11-08 09:40:32,219 - __main__ - INFO - Fold 1 Epoch 59 Batch 0: Train Loss = 1.5285
2023-11-08 09:40:33,673 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.7343 ------------
2023-11-08 09:40:33,952 - __main__ - INFO - ------------ Save best model - MSE: 31.7343 ------------
2023-11-08 09:40:33,954 - __main__ - INFO - Fold 1, mse = 31.7343, mad = 4.2320
2023-11-08 09:40:34,198 - __main__ - INFO - Fold 1 Epoch 60 Batch 0: Train Loss = 1.1295
2023-11-08 09:40:35,437 - __main__ - INFO - Fold 1, epoch 60: Loss = 1.2672 Valid loss = 1.5773 MSE = 32.5281 AUROC = 0.9843
2023-11-08 09:40:35,439 - __main__ - INFO - Fold 1, mse = 32.5281, mad = 4.1996
2023-11-08 09:40:35,654 - __main__ - INFO - Fold 1 Epoch 61 Batch 0: Train Loss = 1.2263
2023-11-08 09:40:37,371 - __main__ - INFO - Fold 1, mse = 32.2449, mad = 4.2523
2023-11-08 09:40:37,631 - __main__ - INFO - Fold 1 Epoch 62 Batch 0: Train Loss = 1.1614
2023-11-08 09:40:39,037 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.7117 ------------
2023-11-08 09:40:39,364 - __main__ - INFO - ------------ Save best model - MSE: 31.7117 ------------
2023-11-08 09:40:39,367 - __main__ - INFO - Fold 1, mse = 31.7117, mad = 4.2247
2023-11-08 09:40:39,618 - __main__ - INFO - Fold 1 Epoch 63 Batch 0: Train Loss = 1.1784
2023-11-08 09:40:40,932 - __main__ - INFO - Fold 1, mse = 31.9402, mad = 4.1829
2023-11-08 09:40:41,154 - __main__ - INFO - Fold 1 Epoch 64 Batch 0: Train Loss = 1.2096
2023-11-08 09:40:42,628 - __main__ - INFO - Fold 1, mse = 31.9621, mad = 4.2369
2023-11-08 09:40:42,905 - __main__ - INFO - Fold 1 Epoch 65 Batch 0: Train Loss = 1.1862
2023-11-08 09:40:44,373 - __main__ - INFO - Fold 1, mse = 32.4345, mad = 4.1681
2023-11-08 09:40:44,635 - __main__ - INFO - Fold 1 Epoch 66 Batch 0: Train Loss = 1.3471
2023-11-08 09:40:46,020 - __main__ - INFO - Fold 1, mse = 31.9992, mad = 4.2532
2023-11-08 09:40:46,339 - __main__ - INFO - Fold 1 Epoch 67 Batch 0: Train Loss = 1.1573
2023-11-08 09:40:47,777 - __main__ - INFO - Fold 1, mse = 32.0324, mad = 4.2060
2023-11-08 09:40:48,041 - __main__ - INFO - Fold 1 Epoch 68 Batch 0: Train Loss = 1.2267
2023-11-08 09:40:49,953 - __main__ - INFO - Fold 1, mse = 31.9358, mad = 4.2172
2023-11-08 09:40:50,253 - __main__ - INFO - Fold 1 Epoch 69 Batch 0: Train Loss = 1.0880
2023-11-08 09:40:51,919 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9849 ------------
2023-11-08 09:40:51,924 - __main__ - INFO - Fold 1, mse = 31.8010, mad = 4.2153
2023-11-08 09:40:52,162 - __main__ - INFO - Fold 1 Epoch 70 Batch 0: Train Loss = 1.2067
2023-11-08 09:40:53,962 - __main__ - INFO - Fold 1, epoch 70: Loss = 1.2684 Valid loss = 1.5178 MSE = 32.0691 AUROC = 0.9856
2023-11-08 09:40:53,965 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9856 ------------
2023-11-08 09:40:53,968 - __main__ - INFO - Fold 1, mse = 32.0691, mad = 4.2580
2023-11-08 09:40:54,290 - __main__ - INFO - Fold 1 Epoch 71 Batch 0: Train Loss = 1.3950
2023-11-08 09:40:55,902 - __main__ - INFO - Fold 1, mse = 32.1272, mad = 4.2150
2023-11-08 09:40:56,162 - __main__ - INFO - Fold 1 Epoch 72 Batch 0: Train Loss = 1.1794
2023-11-08 09:40:57,870 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.4801 ------------
2023-11-08 09:40:58,185 - __main__ - INFO - ------------ Save best model - MSE: 31.4801 ------------
2023-11-08 09:40:58,193 - __main__ - INFO - Fold 1, mse = 31.4801, mad = 4.1624
2023-11-08 09:40:58,523 - __main__ - INFO - Fold 1 Epoch 73 Batch 0: Train Loss = 1.4988
2023-11-08 09:41:00,225 - __main__ - INFO - Fold 1, mse = 31.5626, mad = 4.2364
2023-11-08 09:41:00,482 - __main__ - INFO - Fold 1 Epoch 74 Batch 0: Train Loss = 1.2756
2023-11-08 09:41:02,261 - __main__ - INFO - Fold 1, mse = 31.8301, mad = 4.2034
2023-11-08 09:41:02,665 - __main__ - INFO - Fold 1 Epoch 75 Batch 0: Train Loss = 1.2538
2023-11-08 09:41:04,154 - __main__ - INFO - Fold 1, mse = 31.8380, mad = 4.2365
2023-11-08 09:41:04,362 - __main__ - INFO - Fold 1 Epoch 76 Batch 0: Train Loss = 1.2334
2023-11-08 09:41:05,661 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9858 ------------
2023-11-08 09:41:05,665 - __main__ - INFO - Fold 1, mse = 31.8277, mad = 4.2310
2023-11-08 09:41:05,878 - __main__ - INFO - Fold 1 Epoch 77 Batch 0: Train Loss = 1.1906
2023-11-08 09:41:07,246 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9866 ------------
2023-11-08 09:41:07,249 - __main__ - INFO - Fold 1, mse = 32.3994, mad = 4.2006
2023-11-08 09:41:07,460 - __main__ - INFO - Fold 1 Epoch 78 Batch 0: Train Loss = 1.3112
2023-11-08 09:41:08,749 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9868 ------------
2023-11-08 09:41:08,750 - __main__ - INFO - Fold 1, mse = 31.9479, mad = 4.2707
2023-11-08 09:41:09,007 - __main__ - INFO - Fold 1 Epoch 79 Batch 0: Train Loss = 1.3373
2023-11-08 09:41:10,708 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9871 ------------
2023-11-08 09:41:10,711 - __main__ - INFO - Fold 1, mse = 31.7516, mad = 4.1183
2023-11-08 09:41:11,003 - __main__ - INFO - Fold 1 Epoch 80 Batch 0: Train Loss = 1.1536
2023-11-08 09:41:12,765 - __main__ - INFO - Fold 1, epoch 80: Loss = 1.1981 Valid loss = 1.5183 MSE = 31.7094 AUROC = 0.9872
2023-11-08 09:41:12,769 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9872 ------------
2023-11-08 09:41:12,773 - __main__ - INFO - Fold 1, mse = 31.7094, mad = 4.1598
2023-11-08 09:41:13,050 - __main__ - INFO - Fold 1 Epoch 81 Batch 0: Train Loss = 1.2827
2023-11-08 09:41:14,767 - __main__ - INFO - Fold 1, mse = 31.9344, mad = 4.1918
2023-11-08 09:41:15,124 - __main__ - INFO - Fold 1 Epoch 82 Batch 0: Train Loss = 1.2671
2023-11-08 09:41:17,018 - __main__ - INFO - Fold 1, mse = 32.3251, mad = 4.2212
2023-11-08 09:41:17,363 - __main__ - INFO - Fold 1 Epoch 83 Batch 0: Train Loss = 1.1995
2023-11-08 09:41:19,398 - __main__ - INFO - Fold 1, mse = 31.7667, mad = 4.1876
2023-11-08 09:41:19,728 - __main__ - INFO - Fold 1 Epoch 84 Batch 0: Train Loss = 1.1397
2023-11-08 09:41:21,387 - __main__ - INFO - Fold 1, mse = 31.8645, mad = 4.1506
2023-11-08 09:41:21,688 - __main__ - INFO - Fold 1 Epoch 85 Batch 0: Train Loss = 1.1089
2023-11-08 09:41:23,483 - __main__ - INFO - Fold 1, mse = 31.8266, mad = 4.2472
2023-11-08 09:41:23,785 - __main__ - INFO - Fold 1 Epoch 86 Batch 0: Train Loss = 1.2278
2023-11-08 09:41:25,413 - __main__ - INFO - Fold 1, mse = 32.3182, mad = 4.1633
2023-11-08 09:41:25,635 - __main__ - INFO - Fold 1 Epoch 87 Batch 0: Train Loss = 1.1329
2023-11-08 09:41:27,088 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9873 ------------
2023-11-08 09:41:27,090 - __main__ - INFO - Fold 1, mse = 31.8271, mad = 4.2179
2023-11-08 09:41:27,304 - __main__ - INFO - Fold 1 Epoch 88 Batch 0: Train Loss = 1.1528
2023-11-08 09:41:28,705 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9877 ------------
2023-11-08 09:41:28,707 - __main__ - INFO - Fold 1, mse = 31.9569, mad = 4.1313
2023-11-08 09:41:28,975 - __main__ - INFO - Fold 1 Epoch 89 Batch 0: Train Loss = 1.3280
2023-11-08 09:41:30,562 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9879 ------------
2023-11-08 09:41:30,565 - __main__ - INFO - Fold 1, mse = 31.6642, mad = 4.1729
2023-11-08 09:41:30,780 - __main__ - INFO - Fold 1 Epoch 90 Batch 0: Train Loss = 1.2357
2023-11-08 09:41:32,328 - __main__ - INFO - Fold 1, epoch 90: Loss = 1.1641 Valid loss = 1.5589 MSE = 31.9805 AUROC = 0.9880
2023-11-08 09:41:32,332 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9880 ------------
2023-11-08 09:41:32,336 - __main__ - INFO - Fold 1, mse = 31.9805, mad = 4.1403
2023-11-08 09:41:32,569 - __main__ - INFO - Fold 1 Epoch 91 Batch 0: Train Loss = 1.0143
2023-11-08 09:41:33,936 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9881 ------------
2023-11-08 09:41:33,938 - __main__ - INFO - Fold 1, mse = 32.0000, mad = 4.1897
2023-11-08 09:41:34,197 - __main__ - INFO - Fold 1 Epoch 92 Batch 0: Train Loss = 0.9990
2023-11-08 09:41:36,020 - __main__ - INFO - Fold 1, mse = 32.1042, mad = 4.2061
2023-11-08 09:41:36,284 - __main__ - INFO - Fold 1 Epoch 93 Batch 0: Train Loss = 1.1761
2023-11-08 09:41:38,014 - __main__ - INFO - Fold 1, mse = 32.3016, mad = 4.1574
2023-11-08 09:41:38,270 - __main__ - INFO - Fold 1 Epoch 94 Batch 0: Train Loss = 1.0651
2023-11-08 09:41:39,896 - __main__ - INFO - Fold 1, mse = 31.8639, mad = 4.1460
2023-11-08 09:41:40,234 - __main__ - INFO - Fold 1 Epoch 95 Batch 0: Train Loss = 1.2282
2023-11-08 09:41:41,716 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9881 ------------
2023-11-08 09:41:41,718 - __main__ - INFO - Fold 1, mse = 31.9328, mad = 4.1641
2023-11-08 09:41:41,942 - __main__ - INFO - Fold 1 Epoch 96 Batch 0: Train Loss = 1.0296
2023-11-08 09:41:43,584 - __main__ - INFO - Fold 1, mse = 31.5856, mad = 4.2128
2023-11-08 09:41:43,864 - __main__ - INFO - Fold 1 Epoch 97 Batch 0: Train Loss = 1.0836
2023-11-08 09:41:45,497 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.0643 ------------
2023-11-08 09:41:45,828 - __main__ - INFO - ------------ Save best model - MSE: 31.0643 ------------
2023-11-08 09:41:45,831 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9882 ------------
2023-11-08 09:41:45,833 - __main__ - INFO - Fold 1, mse = 31.0643, mad = 4.0570
2023-11-08 09:41:46,110 - __main__ - INFO - Fold 1 Epoch 98 Batch 0: Train Loss = 1.3604
2023-11-08 09:41:47,800 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.7455 ------------
2023-11-08 09:41:48,105 - __main__ - INFO - ------------ Save best model - MSE: 30.7455 ------------
2023-11-08 09:41:48,108 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9884 ------------
2023-11-08 09:41:48,111 - __main__ - INFO - Fold 1, mse = 30.7455, mad = 4.1405
2023-11-08 09:41:48,324 - __main__ - INFO - Fold 1 Epoch 99 Batch 0: Train Loss = 1.0811
2023-11-08 09:41:49,750 - __main__ - INFO - Fold 1, mse = 30.9456, mad = 4.0888
2023-11-08 09:41:50,034 - __main__ - INFO - Fold 1 Epoch 100 Batch 0: Train Loss = 1.2061
2023-11-08 09:41:51,910 - __main__ - INFO - Fold 1, epoch 100: Loss = 1.1278 Valid loss = 1.5367 MSE = 31.8526 AUROC = 0.9885
2023-11-08 09:41:51,913 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9885 ------------
2023-11-08 09:41:51,915 - __main__ - INFO - Fold 1, mse = 31.8526, mad = 4.1643
2023-11-08 09:41:52,235 - __main__ - INFO - Fold 1 Epoch 101 Batch 0: Train Loss = 1.4752
2023-11-08 09:41:53,826 - __main__ - INFO - Fold 1, mse = 32.2875, mad = 4.2446
2023-11-08 09:41:54,077 - __main__ - INFO - Fold 1 Epoch 102 Batch 0: Train Loss = 1.2162
2023-11-08 09:41:55,862 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9885 ------------
2023-11-08 09:41:55,864 - __main__ - INFO - Fold 1, mse = 31.6364, mad = 4.1006
2023-11-08 09:41:56,127 - __main__ - INFO - Fold 1 Epoch 103 Batch 0: Train Loss = 1.0634
2023-11-08 09:41:57,845 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9888 ------------
2023-11-08 09:41:57,847 - __main__ - INFO - Fold 1, mse = 31.8800, mad = 4.1598
2023-11-08 09:41:58,105 - __main__ - INFO - Fold 1 Epoch 104 Batch 0: Train Loss = 1.3437
2023-11-08 09:41:59,769 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9894 ------------
2023-11-08 09:41:59,772 - __main__ - INFO - Fold 1, mse = 32.3280, mad = 4.1285
2023-11-08 09:42:00,081 - __main__ - INFO - Fold 1 Epoch 105 Batch 0: Train Loss = 1.1800
2023-11-08 09:42:01,579 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9894 ------------
2023-11-08 09:42:01,581 - __main__ - INFO - Fold 1, mse = 32.2979, mad = 4.2013
2023-11-08 09:42:01,843 - __main__ - INFO - Fold 1 Epoch 106 Batch 0: Train Loss = 1.2708
2023-11-08 09:42:03,579 - __main__ - INFO - Fold 1, mse = 32.4558, mad = 4.1393
2023-11-08 09:42:03,836 - __main__ - INFO - Fold 1 Epoch 107 Batch 0: Train Loss = 1.2294
2023-11-08 09:42:05,841 - __main__ - INFO - Fold 1, mse = 31.8460, mad = 4.1745
2023-11-08 09:42:06,068 - __main__ - INFO - Fold 1 Epoch 108 Batch 0: Train Loss = 1.1051
2023-11-08 09:42:07,682 - __main__ - INFO - Fold 1, mse = 31.8196, mad = 4.1082
2023-11-08 09:42:08,031 - __main__ - INFO - Fold 1 Epoch 109 Batch 0: Train Loss = 1.1658
2023-11-08 09:42:09,633 - __main__ - INFO - Fold 1, mse = 31.6216, mad = 4.1016
2023-11-08 09:42:09,923 - __main__ - INFO - Fold 1 Epoch 110 Batch 0: Train Loss = 1.1113
2023-11-08 09:42:11,610 - __main__ - INFO - Fold 1, epoch 110: Loss = 1.1966 Valid loss = 1.5235 MSE = 31.6877 AUROC = 0.9886
2023-11-08 09:42:11,613 - __main__ - INFO - Fold 1, mse = 31.6877, mad = 4.1119
2023-11-08 09:42:11,889 - __main__ - INFO - Fold 1 Epoch 111 Batch 0: Train Loss = 0.8814
2023-11-08 09:42:13,460 - __main__ - INFO - Fold 1, mse = 31.4001, mad = 4.1236
2023-11-08 09:42:13,755 - __main__ - INFO - Fold 1 Epoch 112 Batch 0: Train Loss = 1.1567
2023-11-08 09:42:15,490 - __main__ - INFO - Fold 1, mse = 31.5336, mad = 4.0627
2023-11-08 09:42:15,839 - __main__ - INFO - Fold 1 Epoch 113 Batch 0: Train Loss = 1.2948
2023-11-08 09:42:17,491 - __main__ - INFO - Fold 1, mse = 31.3704, mad = 4.1478
2023-11-08 09:42:17,710 - __main__ - INFO - Fold 1 Epoch 114 Batch 0: Train Loss = 1.0716
2023-11-08 09:42:19,347 - __main__ - INFO - Fold 1, mse = 31.5615, mad = 4.0550
2023-11-08 09:42:19,608 - __main__ - INFO - Fold 1 Epoch 115 Batch 0: Train Loss = 1.1127
2023-11-08 09:42:21,025 - __main__ - INFO - Fold 1, mse = 31.0111, mad = 4.1221
2023-11-08 09:42:21,277 - __main__ - INFO - Fold 1 Epoch 116 Batch 0: Train Loss = 1.2751
2023-11-08 09:42:22,908 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9898 ------------
2023-11-08 09:42:22,910 - __main__ - INFO - Fold 1, mse = 32.0466, mad = 4.1085
2023-11-08 09:42:23,278 - __main__ - INFO - Fold 1 Epoch 117 Batch 0: Train Loss = 1.1064
2023-11-08 09:42:24,414 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9901 ------------
2023-11-08 09:42:24,416 - __main__ - INFO - Fold 1, mse = 32.0787, mad = 4.2301
2023-11-08 09:42:24,627 - __main__ - INFO - Fold 1 Epoch 118 Batch 0: Train Loss = 1.2693
2023-11-08 09:42:25,858 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9905 ------------
2023-11-08 09:42:25,861 - __main__ - INFO - Fold 1, mse = 32.3381, mad = 4.0954
2023-11-08 09:42:26,097 - __main__ - INFO - Fold 1 Epoch 119 Batch 0: Train Loss = 1.3823
2023-11-08 09:42:27,569 - __main__ - INFO - Fold 1, mse = 31.9230, mad = 4.1435
2023-11-08 09:42:27,803 - __main__ - INFO - Fold 1 Epoch 120 Batch 0: Train Loss = 1.1503
2023-11-08 09:42:29,439 - __main__ - INFO - Fold 1, epoch 120: Loss = 1.1226 Valid loss = 1.5974 MSE = 32.4505 AUROC = 0.9904
2023-11-08 09:42:29,442 - __main__ - INFO - Fold 1, mse = 32.4505, mad = 4.1045
2023-11-08 09:42:29,700 - __main__ - INFO - Fold 1 Epoch 121 Batch 0: Train Loss = 0.9823
2023-11-08 09:42:31,438 - __main__ - INFO - Fold 1, mse = 31.9894, mad = 4.1173
2023-11-08 09:42:31,741 - __main__ - INFO - Fold 1 Epoch 122 Batch 0: Train Loss = 1.2333
2023-11-08 09:42:33,182 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9907 ------------
2023-11-08 09:42:33,184 - __main__ - INFO - Fold 1, mse = 32.0463, mad = 4.0973
2023-11-08 09:42:33,415 - __main__ - INFO - Fold 1 Epoch 123 Batch 0: Train Loss = 1.2395
2023-11-08 09:42:35,168 - __main__ - INFO - Fold 1, mse = 32.0098, mad = 4.0627
2023-11-08 09:42:35,462 - __main__ - INFO - Fold 1 Epoch 124 Batch 0: Train Loss = 1.1044
2023-11-08 09:42:36,886 - __main__ - INFO - Fold 1, mse = 31.6296, mad = 4.0658
2023-11-08 09:42:37,087 - __main__ - INFO - Fold 1 Epoch 125 Batch 0: Train Loss = 1.2045
2023-11-08 09:42:38,635 - __main__ - INFO - Fold 1, mse = 31.8404, mad = 4.0767
2023-11-08 09:42:38,840 - __main__ - INFO - Fold 1 Epoch 126 Batch 0: Train Loss = 1.3192
2023-11-08 09:42:40,515 - __main__ - INFO - Fold 1, mse = 31.5169, mad = 4.1293
2023-11-08 09:42:40,806 - __main__ - INFO - Fold 1 Epoch 127 Batch 0: Train Loss = 1.2255
2023-11-08 09:42:42,630 - __main__ - INFO - Fold 1, mse = 31.6170, mad = 4.1076
2023-11-08 09:42:42,904 - __main__ - INFO - Fold 1 Epoch 128 Batch 0: Train Loss = 1.1601
2023-11-08 09:42:44,648 - __main__ - INFO - Fold 1, mse = 31.7383, mad = 4.1772
2023-11-08 09:42:44,856 - __main__ - INFO - Fold 1 Epoch 129 Batch 0: Train Loss = 1.0055
2023-11-08 09:42:46,536 - __main__ - INFO - Fold 1, mse = 31.9974, mad = 4.1219
2023-11-08 09:42:46,766 - __main__ - INFO - Fold 1 Epoch 130 Batch 0: Train Loss = 0.9954
2023-11-08 09:42:48,650 - __main__ - INFO - Fold 1, epoch 130: Loss = 1.1024 Valid loss = 1.5182 MSE = 31.5077 AUROC = 0.9900
2023-11-08 09:42:48,654 - __main__ - INFO - Fold 1, mse = 31.5077, mad = 4.1244
2023-11-08 09:42:48,944 - __main__ - INFO - Fold 1 Epoch 131 Batch 0: Train Loss = 1.1263
2023-11-08 09:42:50,302 - __main__ - INFO - Fold 1, mse = 31.3553, mad = 4.0232
2023-11-08 09:42:50,515 - __main__ - INFO - Fold 1 Epoch 132 Batch 0: Train Loss = 1.1013
2023-11-08 09:42:51,899 - __main__ - INFO - Fold 1, mse = 31.1320, mad = 4.0557
2023-11-08 09:42:52,175 - __main__ - INFO - Fold 1 Epoch 133 Batch 0: Train Loss = 1.2197
2023-11-08 09:42:53,647 - __main__ - INFO - Fold 1, mse = 31.8444, mad = 4.1133
2023-11-08 09:42:53,889 - __main__ - INFO - Fold 1 Epoch 134 Batch 0: Train Loss = 1.1261
2023-11-08 09:42:55,245 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9909 ------------
2023-11-08 09:42:55,247 - __main__ - INFO - Fold 1, mse = 31.7632, mad = 4.1084
2023-11-08 09:42:55,533 - __main__ - INFO - Fold 1 Epoch 135 Batch 0: Train Loss = 0.9813
2023-11-08 09:42:57,210 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9910 ------------
2023-11-08 09:42:57,212 - __main__ - INFO - Fold 1, mse = 31.1930, mad = 4.0432
2023-11-08 09:42:57,486 - __main__ - INFO - Fold 1 Epoch 136 Batch 0: Train Loss = 0.9252
2023-11-08 09:42:59,061 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9911 ------------
2023-11-08 09:42:59,063 - __main__ - INFO - Fold 1, mse = 31.1682, mad = 4.0107
2023-11-08 09:42:59,323 - __main__ - INFO - Fold 1 Epoch 137 Batch 0: Train Loss = 1.1579
2023-11-08 09:43:00,960 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9911 ------------
2023-11-08 09:43:00,964 - __main__ - INFO - Fold 1, mse = 31.4724, mad = 4.0583
2023-11-08 09:43:01,171 - __main__ - INFO - Fold 1 Epoch 138 Batch 0: Train Loss = 1.0105
2023-11-08 09:43:02,338 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9911 ------------
2023-11-08 09:43:02,340 - __main__ - INFO - Fold 1, mse = 31.7849, mad = 4.1269
2023-11-08 09:43:02,554 - __main__ - INFO - Fold 1 Epoch 139 Batch 0: Train Loss = 1.1885
2023-11-08 09:43:04,292 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9914 ------------
2023-11-08 09:43:04,295 - __main__ - INFO - Fold 1, mse = 31.8946, mad = 4.1459
2023-11-08 09:43:04,596 - __main__ - INFO - Fold 1 Epoch 140 Batch 0: Train Loss = 1.0780
2023-11-08 09:43:06,242 - __main__ - INFO - Fold 1, epoch 140: Loss = 1.1069 Valid loss = 1.4850 MSE = 31.5843 AUROC = 0.9916
2023-11-08 09:43:06,244 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9916 ------------
2023-11-08 09:43:06,246 - __main__ - INFO - Fold 1, mse = 31.5843, mad = 4.1296
2023-11-08 09:43:06,462 - __main__ - INFO - Fold 1 Epoch 141 Batch 0: Train Loss = 0.9081
2023-11-08 09:43:08,076 - __main__ - INFO - Fold 1, mse = 32.3074, mad = 4.0994
2023-11-08 09:43:08,319 - __main__ - INFO - Fold 1 Epoch 142 Batch 0: Train Loss = 1.1647
2023-11-08 09:43:10,141 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9918 ------------
2023-11-08 09:43:10,143 - __main__ - INFO - Fold 1, mse = 32.2227, mad = 4.1672
2023-11-08 09:43:10,396 - __main__ - INFO - Fold 1 Epoch 143 Batch 0: Train Loss = 1.1757
2023-11-08 09:43:12,032 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9921 ------------
2023-11-08 09:43:12,034 - __main__ - INFO - Fold 1, mse = 32.2563, mad = 4.0847
2023-11-08 09:43:12,326 - __main__ - INFO - Fold 1 Epoch 144 Batch 0: Train Loss = 1.0026
2023-11-08 09:43:13,959 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9923 ------------
2023-11-08 09:43:13,961 - __main__ - INFO - Fold 1, mse = 32.3976, mad = 4.1516
2023-11-08 09:43:14,280 - __main__ - INFO - Fold 1 Epoch 145 Batch 0: Train Loss = 1.2336
2023-11-08 09:43:16,020 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9923 ------------
2023-11-08 09:43:16,026 - __main__ - INFO - Fold 1, mse = 32.4793, mad = 4.1301
2023-11-08 09:43:16,309 - __main__ - INFO - Fold 1 Epoch 146 Batch 0: Train Loss = 1.1205
2023-11-08 09:43:17,972 - __main__ - INFO - Fold 1, mse = 32.7154, mad = 4.1847
2023-11-08 09:43:18,220 - __main__ - INFO - Fold 1 Epoch 147 Batch 0: Train Loss = 1.0776
2023-11-08 09:43:19,736 - __main__ - INFO - Fold 1, mse = 32.4394, mad = 4.1854
2023-11-08 09:43:19,974 - __main__ - INFO - Fold 1 Epoch 148 Batch 0: Train Loss = 0.8359
2023-11-08 09:43:21,529 - __main__ - INFO - Fold 1, mse = 32.1158, mad = 4.0758
2023-11-08 09:43:21,787 - __main__ - INFO - Fold 1 Epoch 149 Batch 0: Train Loss = 1.0388
2023-11-08 09:43:23,536 - __main__ - INFO - Fold 1, mse = 31.9076, mad = 4.1532
2023-11-08 09:43:24,135 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 2.4787
2023-11-08 09:43:25,845 - __main__ - INFO - Fold 2, epoch 0: Loss = 2.4387 Valid loss = 2.0310 MSE = 46.8977 AUROC = 0.7934
2023-11-08 09:43:25,849 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 46.8977 ------------
2023-11-08 09:43:26,002 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.7934 ------------
2023-11-08 09:43:26,004 - __main__ - INFO - Fold 2, mse = 46.8977, mad = 4.9794
2023-11-08 09:43:26,259 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 2.2215
2023-11-08 09:43:27,910 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.5832 ------------
2023-11-08 09:43:28,087 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8227 ------------
2023-11-08 09:43:28,088 - __main__ - INFO - Fold 2, mse = 45.5832, mad = 4.8331
2023-11-08 09:43:28,353 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 2.0354
2023-11-08 09:43:30,077 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.2473 ------------
2023-11-08 09:43:30,272 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8541 ------------
2023-11-08 09:43:30,276 - __main__ - INFO - Fold 2, mse = 45.2473, mad = 4.8646
2023-11-08 09:43:30,507 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 1.9800
2023-11-08 09:43:31,951 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8768 ------------
2023-11-08 09:43:31,954 - __main__ - INFO - Fold 2, mse = 45.2925, mad = 4.8643
2023-11-08 09:43:32,275 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 1.9472
2023-11-08 09:43:33,919 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.1262 ------------
2023-11-08 09:43:34,235 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9032 ------------
2023-11-08 09:43:34,240 - __main__ - INFO - Fold 2, mse = 45.1262, mad = 4.8166
2023-11-08 09:43:34,541 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 1.7720
2023-11-08 09:43:36,419 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.6009 ------------
2023-11-08 09:43:36,567 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9230 ------------
2023-11-08 09:43:36,570 - __main__ - INFO - Fold 2, mse = 44.6009, mad = 4.8275
2023-11-08 09:43:36,845 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 2.0300
2023-11-08 09:43:38,455 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.8741 ------------
2023-11-08 09:43:38,641 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9398 ------------
2023-11-08 09:43:38,643 - __main__ - INFO - Fold 2, mse = 43.8741, mad = 4.7848
2023-11-08 09:43:38,904 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 1.8842
2023-11-08 09:43:40,420 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.3882 ------------
2023-11-08 09:43:40,578 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9533 ------------
2023-11-08 09:43:40,582 - __main__ - INFO - Fold 2, mse = 43.3882, mad = 4.7759
2023-11-08 09:43:40,821 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 1.6000
2023-11-08 09:43:42,484 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 42.9740 ------------
2023-11-08 09:43:42,652 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9584 ------------
2023-11-08 09:43:42,655 - __main__ - INFO - Fold 2, mse = 42.9740, mad = 4.8205
2023-11-08 09:43:42,998 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 1.7027
2023-11-08 09:43:44,644 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 42.5676 ------------
2023-11-08 09:43:44,808 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9618 ------------
2023-11-08 09:43:44,810 - __main__ - INFO - Fold 2, mse = 42.5676, mad = 4.7275
2023-11-08 09:43:45,085 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 1.5165
2023-11-08 09:43:46,770 - __main__ - INFO - Fold 2, epoch 10: Loss = 1.6205 Valid loss = 1.6670 MSE = 42.6679 AUROC = 0.9635
2023-11-08 09:43:46,772 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9635 ------------
2023-11-08 09:43:46,773 - __main__ - INFO - Fold 2, mse = 42.6679, mad = 4.7433
2023-11-08 09:43:47,033 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 1.3883
2023-11-08 09:43:48,465 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9648 ------------
2023-11-08 09:43:48,467 - __main__ - INFO - Fold 2, mse = 42.7679, mad = 4.8049
2023-11-08 09:43:48,754 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 1.5639
2023-11-08 09:43:50,437 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 42.2421 ------------
2023-11-08 09:43:50,635 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9660 ------------
2023-11-08 09:43:50,636 - __main__ - INFO - Fold 2, mse = 42.2421, mad = 4.7273
2023-11-08 09:43:50,885 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 1.5002
2023-11-08 09:43:52,564 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9667 ------------
2023-11-08 09:43:52,567 - __main__ - INFO - Fold 2, mse = 42.3489, mad = 4.7870
2023-11-08 09:43:52,855 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 1.6592
2023-11-08 09:43:54,366 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 42.1263 ------------
2023-11-08 09:43:54,577 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9674 ------------
2023-11-08 09:43:54,579 - __main__ - INFO - Fold 2, mse = 42.1263, mad = 4.6595
2023-11-08 09:43:54,792 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 1.4559
2023-11-08 09:43:56,023 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9678 ------------
2023-11-08 09:43:56,025 - __main__ - INFO - Fold 2, mse = 42.4982, mad = 4.7474
2023-11-08 09:43:56,300 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 1.3496
2023-11-08 09:43:57,985 - __main__ - INFO - Fold 2, mse = 42.8070, mad = 4.7994
2023-11-08 09:43:58,262 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 1.6663
2023-11-08 09:43:59,872 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9681 ------------
2023-11-08 09:43:59,874 - __main__ - INFO - Fold 2, mse = 42.3380, mad = 4.6476
2023-11-08 09:44:00,127 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 1.6323
2023-11-08 09:44:02,191 - __main__ - INFO - Fold 2, mse = 43.0606, mad = 4.8911
2023-11-08 09:44:02,441 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 1.3172
2023-11-08 09:44:04,121 - __main__ - INFO - Fold 2, mse = 42.4876, mad = 4.7271
2023-11-08 09:44:04,391 - __main__ - INFO - Fold 2 Epoch 20 Batch 0: Train Loss = 1.4908
2023-11-08 09:44:05,927 - __main__ - INFO - Fold 2, epoch 20: Loss = 1.4815 Valid loss = 1.6111 MSE = 42.3294 AUROC = 0.9666
2023-11-08 09:44:05,929 - __main__ - INFO - Fold 2, mse = 42.3294, mad = 4.7269
2023-11-08 09:44:06,156 - __main__ - INFO - Fold 2 Epoch 21 Batch 0: Train Loss = 1.4190
2023-11-08 09:44:07,758 - __main__ - INFO - Fold 2, mse = 42.4343, mad = 4.7785
2023-11-08 09:44:08,008 - __main__ - INFO - Fold 2 Epoch 22 Batch 0: Train Loss = 1.6267
2023-11-08 09:44:09,536 - __main__ - INFO - Fold 2, mse = 42.5070, mad = 4.7586
2023-11-08 09:44:09,784 - __main__ - INFO - Fold 2 Epoch 23 Batch 0: Train Loss = 1.4714
2023-11-08 09:44:11,422 - __main__ - INFO - Fold 2, mse = 42.5605, mad = 4.7407
2023-11-08 09:44:11,702 - __main__ - INFO - Fold 2 Epoch 24 Batch 0: Train Loss = 1.4749
2023-11-08 09:44:13,180 - __main__ - INFO - Fold 2, mse = 42.6768, mad = 4.8208
2023-11-08 09:44:13,461 - __main__ - INFO - Fold 2 Epoch 25 Batch 0: Train Loss = 1.3170
2023-11-08 09:44:15,028 - __main__ - INFO - Fold 2, mse = 42.5034, mad = 4.7042
2023-11-08 09:44:15,283 - __main__ - INFO - Fold 2 Epoch 26 Batch 0: Train Loss = 1.2827
2023-11-08 09:44:16,836 - __main__ - INFO - Fold 2, mse = 43.0308, mad = 4.8266
2023-11-08 09:44:17,060 - __main__ - INFO - Fold 2 Epoch 27 Batch 0: Train Loss = 1.1691
2023-11-08 09:44:18,648 - __main__ - INFO - Fold 2, mse = 42.7104, mad = 4.7918
2023-11-08 09:44:18,905 - __main__ - INFO - Fold 2 Epoch 28 Batch 0: Train Loss = 1.3785
2023-11-08 09:44:20,611 - __main__ - INFO - Fold 2, mse = 42.8823, mad = 4.8301
2023-11-08 09:44:20,867 - __main__ - INFO - Fold 2 Epoch 29 Batch 0: Train Loss = 1.5220
2023-11-08 09:44:22,465 - __main__ - INFO - Fold 2, mse = 42.4809, mad = 4.7330
2023-11-08 09:44:22,712 - __main__ - INFO - Fold 2 Epoch 30 Batch 0: Train Loss = 1.1790
2023-11-08 09:44:24,395 - __main__ - INFO - Fold 2, epoch 30: Loss = 1.3049 Valid loss = 1.6337 MSE = 42.4919 AUROC = 0.9658
2023-11-08 09:44:24,396 - __main__ - INFO - Fold 2, mse = 42.4919, mad = 4.7561
2023-11-08 09:44:24,706 - __main__ - INFO - Fold 2 Epoch 31 Batch 0: Train Loss = 1.2196
2023-11-08 09:44:26,658 - __main__ - INFO - Fold 2, mse = 42.7767, mad = 4.7797
2023-11-08 09:44:26,975 - __main__ - INFO - Fold 2 Epoch 32 Batch 0: Train Loss = 1.2954
2023-11-08 09:44:28,687 - __main__ - INFO - Fold 2, mse = 43.0304, mad = 4.7931
2023-11-08 09:44:28,918 - __main__ - INFO - Fold 2 Epoch 33 Batch 0: Train Loss = 1.3862
2023-11-08 09:44:30,551 - __main__ - INFO - Fold 2, mse = 43.5732, mad = 4.8936
2023-11-08 09:44:30,824 - __main__ - INFO - Fold 2 Epoch 34 Batch 0: Train Loss = 1.4254
2023-11-08 09:44:32,480 - __main__ - INFO - Fold 2, mse = 42.5038, mad = 4.6935
2023-11-08 09:44:32,727 - __main__ - INFO - Fold 2 Epoch 35 Batch 0: Train Loss = 1.1268
2023-11-08 09:44:34,441 - __main__ - INFO - Fold 2, mse = 42.6624, mad = 4.8211
2023-11-08 09:44:34,744 - __main__ - INFO - Fold 2 Epoch 36 Batch 0: Train Loss = 1.4234
2023-11-08 09:44:36,206 - __main__ - INFO - Fold 2, mse = 42.3806, mad = 4.8140
2023-11-08 09:44:36,476 - __main__ - INFO - Fold 2 Epoch 37 Batch 0: Train Loss = 1.1751
2023-11-08 09:44:38,249 - __main__ - INFO - Fold 2, mse = 42.3581, mad = 4.7496
2023-11-08 09:44:38,531 - __main__ - INFO - Fold 2 Epoch 38 Batch 0: Train Loss = 1.2738
2023-11-08 09:44:40,024 - __main__ - INFO - Fold 2, mse = 42.4744, mad = 4.7926
2023-11-08 09:44:40,237 - __main__ - INFO - Fold 2 Epoch 39 Batch 0: Train Loss = 1.3093
2023-11-08 09:44:41,856 - __main__ - INFO - Fold 2, mse = 42.3226, mad = 4.7815
2023-11-08 09:44:42,174 - __main__ - INFO - Fold 2 Epoch 40 Batch 0: Train Loss = 1.3726
2023-11-08 09:44:44,061 - __main__ - INFO - Fold 2, epoch 40: Loss = 1.2309 Valid loss = 1.6753 MSE = 42.8131 AUROC = 0.9638
2023-11-08 09:44:44,065 - __main__ - INFO - Fold 2, mse = 42.8131, mad = 4.8863
2023-11-08 09:44:44,411 - __main__ - INFO - Fold 2 Epoch 41 Batch 0: Train Loss = 1.3272
2023-11-08 09:44:45,742 - __main__ - INFO - Fold 2, mse = 42.3059, mad = 4.7447
2023-11-08 09:44:45,997 - __main__ - INFO - Fold 2 Epoch 42 Batch 0: Train Loss = 1.2504
2023-11-08 09:44:47,737 - __main__ - INFO - Fold 2, mse = 42.6024, mad = 4.8710
2023-11-08 09:44:47,987 - __main__ - INFO - Fold 2 Epoch 43 Batch 0: Train Loss = 1.2459
2023-11-08 09:44:49,696 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 41.9529 ------------
2023-11-08 09:44:49,849 - __main__ - INFO - Fold 2, mse = 41.9529, mad = 4.7602
2023-11-08 09:44:50,128 - __main__ - INFO - Fold 2 Epoch 44 Batch 0: Train Loss = 1.2002
2023-11-08 09:44:52,023 - __main__ - INFO - Fold 2, mse = 42.5121, mad = 4.8322
2023-11-08 09:44:52,296 - __main__ - INFO - Fold 2 Epoch 45 Batch 0: Train Loss = 1.2871
2023-11-08 09:44:54,273 - __main__ - INFO - Fold 2, mse = 43.0802, mad = 4.8084
2023-11-08 09:44:54,568 - __main__ - INFO - Fold 2 Epoch 46 Batch 0: Train Loss = 1.1545
2023-11-08 09:44:56,294 - __main__ - INFO - Fold 2, mse = 43.1360, mad = 4.8058
2023-11-08 09:44:56,571 - __main__ - INFO - Fold 2 Epoch 47 Batch 0: Train Loss = 1.0239
2023-11-08 09:44:58,538 - __main__ - INFO - Fold 2, mse = 43.5481, mad = 4.8534
2023-11-08 09:44:58,939 - __main__ - INFO - Fold 2 Epoch 48 Batch 0: Train Loss = 1.2470
2023-11-08 09:45:00,322 - __main__ - INFO - Fold 2, mse = 43.4567, mad = 4.8403
2023-11-08 09:45:00,597 - __main__ - INFO - Fold 2 Epoch 49 Batch 0: Train Loss = 0.9745
2023-11-08 09:45:02,160 - __main__ - INFO - Fold 2, mse = 42.9191, mad = 4.7341
2023-11-08 09:45:02,365 - __main__ - INFO - Fold 2 Epoch 50 Batch 0: Train Loss = 1.0633
2023-11-08 09:45:03,921 - __main__ - INFO - Fold 2, epoch 50: Loss = 1.1862 Valid loss = 1.6947 MSE = 43.5379 AUROC = 0.9631
2023-11-08 09:45:03,925 - __main__ - INFO - Fold 2, mse = 43.5379, mad = 4.8823
2023-11-08 09:45:04,192 - __main__ - INFO - Fold 2 Epoch 51 Batch 0: Train Loss = 1.1610
2023-11-08 09:45:05,566 - __main__ - INFO - Fold 2, mse = 43.1594, mad = 4.8262
2023-11-08 09:45:05,775 - __main__ - INFO - Fold 2 Epoch 52 Batch 0: Train Loss = 1.2687
2023-11-08 09:45:07,529 - __main__ - INFO - Fold 2, mse = 42.3926, mad = 4.7271
2023-11-08 09:45:07,819 - __main__ - INFO - Fold 2 Epoch 53 Batch 0: Train Loss = 1.2793
2023-11-08 09:45:09,628 - __main__ - INFO - Fold 2, mse = 42.7511, mad = 4.8574
2023-11-08 09:45:09,932 - __main__ - INFO - Fold 2 Epoch 54 Batch 0: Train Loss = 1.3053
2023-11-08 09:45:11,613 - __main__ - INFO - Fold 2, mse = 42.3288, mad = 4.7051
2023-11-08 09:45:11,940 - __main__ - INFO - Fold 2 Epoch 55 Batch 0: Train Loss = 1.1837
2023-11-08 09:45:13,512 - __main__ - INFO - Fold 2, mse = 43.2250, mad = 4.8932
2023-11-08 09:45:13,823 - __main__ - INFO - Fold 2 Epoch 56 Batch 0: Train Loss = 1.2142
2023-11-08 09:45:15,559 - __main__ - INFO - Fold 2, mse = 42.8848, mad = 4.8964
2023-11-08 09:45:15,825 - __main__ - INFO - Fold 2 Epoch 57 Batch 0: Train Loss = 1.1841
2023-11-08 09:45:17,628 - __main__ - INFO - Fold 2, mse = 42.7074, mad = 4.7924
2023-11-08 09:45:17,886 - __main__ - INFO - Fold 2 Epoch 58 Batch 0: Train Loss = 1.0773
2023-11-08 09:45:19,525 - __main__ - INFO - Fold 2, mse = 43.5826, mad = 4.8787
2023-11-08 09:45:19,729 - __main__ - INFO - Fold 2 Epoch 59 Batch 0: Train Loss = 1.2806
2023-11-08 09:45:21,699 - __main__ - INFO - Fold 2, mse = 43.4161, mad = 4.8330
2023-11-08 09:45:21,987 - __main__ - INFO - Fold 2 Epoch 60 Batch 0: Train Loss = 1.2946
2023-11-08 09:45:23,858 - __main__ - INFO - Fold 2, epoch 60: Loss = 1.1497 Valid loss = 1.7075 MSE = 43.8650 AUROC = 0.9660
2023-11-08 09:45:23,860 - __main__ - INFO - Fold 2, mse = 43.8650, mad = 4.9263
2023-11-08 09:45:24,067 - __main__ - INFO - Fold 2 Epoch 61 Batch 0: Train Loss = 1.2277
2023-11-08 09:45:25,481 - __main__ - INFO - Fold 2, mse = 43.9448, mad = 4.9463
2023-11-08 09:45:25,795 - __main__ - INFO - Fold 2 Epoch 62 Batch 0: Train Loss = 1.2083
2023-11-08 09:45:27,474 - __main__ - INFO - Fold 2, mse = 43.5392, mad = 4.8413
2023-11-08 09:45:27,683 - __main__ - INFO - Fold 2 Epoch 63 Batch 0: Train Loss = 1.3828
2023-11-08 09:45:29,331 - __main__ - INFO - Fold 2, mse = 43.9349, mad = 4.9031
2023-11-08 09:45:29,698 - __main__ - INFO - Fold 2 Epoch 64 Batch 0: Train Loss = 1.2003
2023-11-08 09:45:31,240 - __main__ - INFO - Fold 2, mse = 44.1661, mad = 4.9285
2023-11-08 09:45:31,613 - __main__ - INFO - Fold 2 Epoch 65 Batch 0: Train Loss = 1.4575
2023-11-08 09:45:33,421 - __main__ - INFO - Fold 2, mse = 44.0149, mad = 4.8482
2023-11-08 09:45:33,726 - __main__ - INFO - Fold 2 Epoch 66 Batch 0: Train Loss = 1.1243
2023-11-08 09:45:35,487 - __main__ - INFO - Fold 2, mse = 44.5149, mad = 4.9488
2023-11-08 09:45:35,814 - __main__ - INFO - Fold 2 Epoch 67 Batch 0: Train Loss = 1.1732
2023-11-08 09:45:37,445 - __main__ - INFO - Fold 2, mse = 43.9056, mad = 4.8376
2023-11-08 09:45:37,718 - __main__ - INFO - Fold 2 Epoch 68 Batch 0: Train Loss = 1.1443
2023-11-08 09:45:39,076 - __main__ - INFO - Fold 2, mse = 44.3564, mad = 4.9477
2023-11-08 09:45:39,315 - __main__ - INFO - Fold 2 Epoch 69 Batch 0: Train Loss = 1.1507
2023-11-08 09:45:40,679 - __main__ - INFO - Fold 2, mse = 44.1933, mad = 4.9452
2023-11-08 09:45:40,891 - __main__ - INFO - Fold 2 Epoch 70 Batch 0: Train Loss = 0.8958
2023-11-08 09:45:42,604 - __main__ - INFO - Fold 2, epoch 70: Loss = 1.1503 Valid loss = 1.7142 MSE = 43.9390 AUROC = 0.9679
2023-11-08 09:45:42,606 - __main__ - INFO - Fold 2, mse = 43.9390, mad = 4.9038
2023-11-08 09:45:42,848 - __main__ - INFO - Fold 2 Epoch 71 Batch 0: Train Loss = 1.1941
2023-11-08 09:45:44,414 - __main__ - INFO - Fold 2, mse = 43.9991, mad = 4.9028
2023-11-08 09:45:44,701 - __main__ - INFO - Fold 2 Epoch 72 Batch 0: Train Loss = 1.1985
2023-11-08 09:45:46,526 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9682 ------------
2023-11-08 09:45:46,528 - __main__ - INFO - Fold 2, mse = 44.0306, mad = 4.8981
2023-11-08 09:45:46,926 - __main__ - INFO - Fold 2 Epoch 73 Batch 0: Train Loss = 1.1946
2023-11-08 09:45:48,458 - __main__ - INFO - Fold 2, mse = 43.9595, mad = 4.9045
2023-11-08 09:45:48,696 - __main__ - INFO - Fold 2 Epoch 74 Batch 0: Train Loss = 1.0346
2023-11-08 09:45:50,238 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9685 ------------
2023-11-08 09:45:50,241 - __main__ - INFO - Fold 2, mse = 44.4725, mad = 4.9185
2023-11-08 09:45:50,490 - __main__ - INFO - Fold 2 Epoch 75 Batch 0: Train Loss = 1.1238
2023-11-08 09:45:52,200 - __main__ - INFO - Fold 2, mse = 45.3723, mad = 5.0077
2023-11-08 09:45:52,529 - __main__ - INFO - Fold 2 Epoch 76 Batch 0: Train Loss = 1.0173
2023-11-08 09:45:54,193 - __main__ - INFO - Fold 2, mse = 45.2502, mad = 4.9875
2023-11-08 09:45:54,570 - __main__ - INFO - Fold 2 Epoch 77 Batch 0: Train Loss = 1.1478
2023-11-08 09:45:56,241 - __main__ - INFO - Fold 2, mse = 44.8325, mad = 4.9925
2023-11-08 09:45:56,528 - __main__ - INFO - Fold 2 Epoch 78 Batch 0: Train Loss = 1.0518
2023-11-08 09:45:58,215 - __main__ - INFO - Fold 2, mse = 43.7192, mad = 4.7993
2023-11-08 09:45:58,595 - __main__ - INFO - Fold 2 Epoch 79 Batch 0: Train Loss = 1.2401
2023-11-08 09:46:00,093 - __main__ - INFO - Fold 2, mse = 44.9135, mad = 5.0288
2023-11-08 09:46:00,334 - __main__ - INFO - Fold 2 Epoch 80 Batch 0: Train Loss = 1.1253
2023-11-08 09:46:01,783 - __main__ - INFO - Fold 2, epoch 80: Loss = 1.1283 Valid loss = 1.7019 MSE = 44.1765 AUROC = 0.9677
2023-11-08 09:46:01,785 - __main__ - INFO - Fold 2, mse = 44.1765, mad = 4.8904
2023-11-08 09:46:02,036 - __main__ - INFO - Fold 2 Epoch 81 Batch 0: Train Loss = 1.1252
2023-11-08 09:46:03,745 - __main__ - INFO - Fold 2, mse = 44.1452, mad = 4.9084
2023-11-08 09:46:04,049 - __main__ - INFO - Fold 2 Epoch 82 Batch 0: Train Loss = 1.3270
2023-11-08 09:46:05,789 - __main__ - INFO - Fold 2, mse = 44.4580, mad = 4.9495
2023-11-08 09:46:06,039 - __main__ - INFO - Fold 2 Epoch 83 Batch 0: Train Loss = 1.1301
2023-11-08 09:46:07,583 - __main__ - INFO - Fold 2, mse = 44.3721, mad = 4.9212
2023-11-08 09:46:07,865 - __main__ - INFO - Fold 2 Epoch 84 Batch 0: Train Loss = 1.0668
2023-11-08 09:46:09,342 - __main__ - INFO - Fold 2, mse = 43.9215, mad = 4.8140
2023-11-08 09:46:09,571 - __main__ - INFO - Fold 2 Epoch 85 Batch 0: Train Loss = 1.1444
2023-11-08 09:46:10,835 - __main__ - INFO - Fold 2, mse = 44.4087, mad = 4.9732
2023-11-08 09:46:11,064 - __main__ - INFO - Fold 2 Epoch 86 Batch 0: Train Loss = 1.1030
2023-11-08 09:46:12,641 - __main__ - INFO - Fold 2, mse = 43.8924, mad = 4.8205
2023-11-08 09:46:12,902 - __main__ - INFO - Fold 2 Epoch 87 Batch 0: Train Loss = 1.0570
2023-11-08 09:46:14,616 - __main__ - INFO - Fold 2, mse = 45.2988, mad = 5.0499
2023-11-08 09:46:14,877 - __main__ - INFO - Fold 2 Epoch 88 Batch 0: Train Loss = 1.1302
2023-11-08 09:46:16,365 - __main__ - INFO - Fold 2, mse = 43.8837, mad = 4.8126
2023-11-08 09:46:16,630 - __main__ - INFO - Fold 2 Epoch 89 Batch 0: Train Loss = 1.1790
2023-11-08 09:46:18,184 - __main__ - INFO - Fold 2, mse = 44.1834, mad = 4.9098
2023-11-08 09:46:18,460 - __main__ - INFO - Fold 2 Epoch 90 Batch 0: Train Loss = 0.9611
2023-11-08 09:46:19,678 - __main__ - INFO - Fold 2, epoch 90: Loss = 1.0966 Valid loss = 1.6948 MSE = 44.3154 AUROC = 0.9682
2023-11-08 09:46:19,682 - __main__ - INFO - Fold 2, mse = 44.3154, mad = 4.8618
2023-11-08 09:46:19,901 - __main__ - INFO - Fold 2 Epoch 91 Batch 0: Train Loss = 1.2338
2023-11-08 09:46:21,457 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9695 ------------
2023-11-08 09:46:21,459 - __main__ - INFO - Fold 2, mse = 44.3559, mad = 4.8908
2023-11-08 09:46:21,728 - __main__ - INFO - Fold 2 Epoch 92 Batch 0: Train Loss = 1.2111
2023-11-08 09:46:23,253 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9699 ------------
2023-11-08 09:46:23,255 - __main__ - INFO - Fold 2, mse = 44.1195, mad = 4.8610
2023-11-08 09:46:23,558 - __main__ - INFO - Fold 2 Epoch 93 Batch 0: Train Loss = 0.9821
2023-11-08 09:46:25,048 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9702 ------------
2023-11-08 09:46:25,052 - __main__ - INFO - Fold 2, mse = 45.2031, mad = 5.0299
2023-11-08 09:46:25,372 - __main__ - INFO - Fold 2 Epoch 94 Batch 0: Train Loss = 1.0215
2023-11-08 09:46:26,930 - __main__ - INFO - Fold 2, mse = 44.7377, mad = 4.9536
2023-11-08 09:46:27,165 - __main__ - INFO - Fold 2 Epoch 95 Batch 0: Train Loss = 0.9069
2023-11-08 09:46:28,745 - __main__ - INFO - Fold 2, mse = 45.6287, mad = 5.0905
2023-11-08 09:46:29,017 - __main__ - INFO - Fold 2 Epoch 96 Batch 0: Train Loss = 1.1586
2023-11-08 09:46:30,726 - __main__ - INFO - Fold 2, mse = 44.7193, mad = 4.9256
2023-11-08 09:46:30,999 - __main__ - INFO - Fold 2 Epoch 97 Batch 0: Train Loss = 1.1765
2023-11-08 09:46:32,606 - __main__ - INFO - Fold 2, mse = 44.7733, mad = 4.9279
2023-11-08 09:46:32,850 - __main__ - INFO - Fold 2 Epoch 98 Batch 0: Train Loss = 1.2226
2023-11-08 09:46:34,553 - __main__ - INFO - Fold 2, mse = 44.8739, mad = 4.9454
2023-11-08 09:46:34,822 - __main__ - INFO - Fold 2 Epoch 99 Batch 0: Train Loss = 0.9034
2023-11-08 09:46:36,220 - __main__ - INFO - Fold 2, mse = 44.9716, mad = 4.9250
2023-11-08 09:46:36,533 - __main__ - INFO - Fold 2 Epoch 100 Batch 0: Train Loss = 0.9957
2023-11-08 09:46:38,113 - __main__ - INFO - Fold 2, epoch 100: Loss = 1.0388 Valid loss = 1.7651 MSE = 45.3362 AUROC = 0.9685
2023-11-08 09:46:38,115 - __main__ - INFO - Fold 2, mse = 45.3362, mad = 5.0000
2023-11-08 09:46:38,431 - __main__ - INFO - Fold 2 Epoch 101 Batch 0: Train Loss = 0.9493
2023-11-08 09:46:40,087 - __main__ - INFO - Fold 2, mse = 45.2966, mad = 4.9899
2023-11-08 09:46:40,400 - __main__ - INFO - Fold 2 Epoch 102 Batch 0: Train Loss = 0.9305
2023-11-08 09:46:41,968 - __main__ - INFO - Fold 2, mse = 45.7691, mad = 5.0679
2023-11-08 09:46:42,223 - __main__ - INFO - Fold 2 Epoch 103 Batch 0: Train Loss = 0.9724
2023-11-08 09:46:43,847 - __main__ - INFO - Fold 2, mse = 44.8742, mad = 4.9431
2023-11-08 09:46:44,119 - __main__ - INFO - Fold 2 Epoch 104 Batch 0: Train Loss = 0.9984
2023-11-08 09:46:45,737 - __main__ - INFO - Fold 2, mse = 44.9395, mad = 4.9823
2023-11-08 09:46:46,076 - __main__ - INFO - Fold 2 Epoch 105 Batch 0: Train Loss = 0.9094
2023-11-08 09:46:47,798 - __main__ - INFO - Fold 2, mse = 45.2506, mad = 5.0254
2023-11-08 09:46:48,097 - __main__ - INFO - Fold 2 Epoch 106 Batch 0: Train Loss = 0.9915
2023-11-08 09:46:49,708 - __main__ - INFO - Fold 2, mse = 44.9534, mad = 4.9525
2023-11-08 09:46:49,959 - __main__ - INFO - Fold 2 Epoch 107 Batch 0: Train Loss = 0.9520
2023-11-08 09:46:51,647 - __main__ - INFO - Fold 2, mse = 45.7675, mad = 5.0951
2023-11-08 09:46:51,912 - __main__ - INFO - Fold 2 Epoch 108 Batch 0: Train Loss = 1.1543
2023-11-08 09:46:53,840 - __main__ - INFO - Fold 2, mse = 44.7410, mad = 4.8640
2023-11-08 09:46:54,086 - __main__ - INFO - Fold 2 Epoch 109 Batch 0: Train Loss = 1.2575
2023-11-08 09:46:55,531 - __main__ - INFO - Fold 2, mse = 46.1286, mad = 5.1366
2023-11-08 09:46:55,819 - __main__ - INFO - Fold 2 Epoch 110 Batch 0: Train Loss = 1.0699
2023-11-08 09:46:57,547 - __main__ - INFO - Fold 2, epoch 110: Loss = 1.0354 Valid loss = 1.6998 MSE = 44.7160 AUROC = 0.9693
2023-11-08 09:46:57,549 - __main__ - INFO - Fold 2, mse = 44.7160, mad = 4.7980
2023-11-08 09:46:57,808 - __main__ - INFO - Fold 2 Epoch 111 Batch 0: Train Loss = 1.3871
2023-11-08 09:46:59,525 - __main__ - INFO - Fold 2, mse = 45.9605, mad = 5.1084
2023-11-08 09:46:59,804 - __main__ - INFO - Fold 2 Epoch 112 Batch 0: Train Loss = 1.0403
2023-11-08 09:47:01,163 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9703 ------------
2023-11-08 09:47:01,166 - __main__ - INFO - Fold 2, mse = 44.7228, mad = 4.8755
2023-11-08 09:47:01,502 - __main__ - INFO - Fold 2 Epoch 113 Batch 0: Train Loss = 1.1555
2023-11-08 09:47:03,062 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9703 ------------
2023-11-08 09:47:03,064 - __main__ - INFO - Fold 2, mse = 45.8595, mad = 5.0932
2023-11-08 09:47:03,370 - __main__ - INFO - Fold 2 Epoch 114 Batch 0: Train Loss = 1.0711
2023-11-08 09:47:05,121 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9705 ------------
2023-11-08 09:47:05,126 - __main__ - INFO - Fold 2, mse = 44.7192, mad = 4.8663
2023-11-08 09:47:05,426 - __main__ - INFO - Fold 2 Epoch 115 Batch 0: Train Loss = 0.9742
2023-11-08 09:47:07,042 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9705 ------------
2023-11-08 09:47:07,044 - __main__ - INFO - Fold 2, mse = 45.8523, mad = 5.0926
2023-11-08 09:47:07,389 - __main__ - INFO - Fold 2 Epoch 116 Batch 0: Train Loss = 1.0163
2023-11-08 09:47:09,148 - __main__ - INFO - Fold 2, mse = 45.0788, mad = 4.8903
2023-11-08 09:47:09,402 - __main__ - INFO - Fold 2 Epoch 117 Batch 0: Train Loss = 1.0006
2023-11-08 09:47:11,063 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9707 ------------
2023-11-08 09:47:11,065 - __main__ - INFO - Fold 2, mse = 46.8144, mad = 5.1608
2023-11-08 09:47:11,298 - __main__ - INFO - Fold 2 Epoch 118 Batch 0: Train Loss = 0.9366
2023-11-08 09:47:12,821 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9714 ------------
2023-11-08 09:47:12,825 - __main__ - INFO - Fold 2, mse = 45.8261, mad = 4.9906
2023-11-08 09:47:13,083 - __main__ - INFO - Fold 2 Epoch 119 Batch 0: Train Loss = 1.0586
2023-11-08 09:47:14,623 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9715 ------------
2023-11-08 09:47:14,627 - __main__ - INFO - Fold 2, mse = 46.5917, mad = 5.1363
2023-11-08 09:47:14,925 - __main__ - INFO - Fold 2 Epoch 120 Batch 0: Train Loss = 1.2248
2023-11-08 09:47:16,665 - __main__ - INFO - Fold 2, epoch 120: Loss = 0.9910 Valid loss = 1.7665 MSE = 46.1551 AUROC = 0.9713
2023-11-08 09:47:16,669 - __main__ - INFO - Fold 2, mse = 46.1551, mad = 5.0347
2023-11-08 09:47:16,925 - __main__ - INFO - Fold 2 Epoch 121 Batch 0: Train Loss = 1.0272
2023-11-08 09:47:18,483 - __main__ - INFO - Fold 2, mse = 45.9570, mad = 5.0079
2023-11-08 09:47:18,694 - __main__ - INFO - Fold 2 Epoch 122 Batch 0: Train Loss = 1.4533
2023-11-08 09:47:20,011 - __main__ - INFO - Fold 2, mse = 45.7225, mad = 4.9858
2023-11-08 09:47:20,228 - __main__ - INFO - Fold 2 Epoch 123 Batch 0: Train Loss = 0.9364
2023-11-08 09:47:21,772 - __main__ - INFO - Fold 2, mse = 45.1536, mad = 4.9262
2023-11-08 09:47:22,135 - __main__ - INFO - Fold 2 Epoch 124 Batch 0: Train Loss = 1.0498
2023-11-08 09:47:23,768 - __main__ - INFO - Fold 2, mse = 45.0815, mad = 4.9583
2023-11-08 09:47:24,034 - __main__ - INFO - Fold 2 Epoch 125 Batch 0: Train Loss = 1.1282
2023-11-08 09:47:25,593 - __main__ - INFO - Fold 2, mse = 44.7462, mad = 4.9349
2023-11-08 09:47:25,873 - __main__ - INFO - Fold 2 Epoch 126 Batch 0: Train Loss = 1.0962
2023-11-08 09:47:27,456 - __main__ - INFO - Fold 2, mse = 44.6689, mad = 4.9469
2023-11-08 09:47:27,745 - __main__ - INFO - Fold 2 Epoch 127 Batch 0: Train Loss = 1.1017
2023-11-08 09:47:29,393 - __main__ - INFO - Fold 2, mse = 44.1314, mad = 4.8758
2023-11-08 09:47:29,768 - __main__ - INFO - Fold 2 Epoch 128 Batch 0: Train Loss = 0.8960
2023-11-08 09:47:31,384 - __main__ - INFO - Fold 2, mse = 44.5380, mad = 4.9111
2023-11-08 09:47:31,687 - __main__ - INFO - Fold 2 Epoch 129 Batch 0: Train Loss = 1.0297
2023-11-08 09:47:33,671 - __main__ - INFO - Fold 2, mse = 44.8722, mad = 4.9324
2023-11-08 09:47:34,072 - __main__ - INFO - Fold 2 Epoch 130 Batch 0: Train Loss = 0.9158
2023-11-08 09:47:35,564 - __main__ - INFO - Fold 2, epoch 130: Loss = 0.9832 Valid loss = 1.7306 MSE = 44.8355 AUROC = 0.9695
2023-11-08 09:47:35,566 - __main__ - INFO - Fold 2, mse = 44.8355, mad = 4.9480
2023-11-08 09:47:35,833 - __main__ - INFO - Fold 2 Epoch 131 Batch 0: Train Loss = 1.1814
2023-11-08 09:47:37,596 - __main__ - INFO - Fold 2, mse = 44.1449, mad = 4.7476
2023-11-08 09:47:37,933 - __main__ - INFO - Fold 2 Epoch 132 Batch 0: Train Loss = 1.1184
2023-11-08 09:47:39,615 - __main__ - INFO - Fold 2, mse = 46.8969, mad = 5.2334
2023-11-08 09:47:39,876 - __main__ - INFO - Fold 2 Epoch 133 Batch 0: Train Loss = 0.9618
2023-11-08 09:47:41,472 - __main__ - INFO - Fold 2, mse = 44.4022, mad = 4.7826
2023-11-08 09:47:41,737 - __main__ - INFO - Fold 2 Epoch 134 Batch 0: Train Loss = 1.2559
2023-11-08 09:47:43,452 - __main__ - INFO - Fold 2, mse = 45.5682, mad = 5.0816
2023-11-08 09:47:43,690 - __main__ - INFO - Fold 2 Epoch 135 Batch 0: Train Loss = 0.9533
2023-11-08 09:47:45,237 - __main__ - INFO - Fold 2, mse = 44.4750, mad = 4.9077
2023-11-08 09:47:45,502 - __main__ - INFO - Fold 2 Epoch 136 Batch 0: Train Loss = 1.0379
2023-11-08 09:47:46,847 - __main__ - INFO - Fold 2, mse = 44.7332, mad = 4.9244
2023-11-08 09:47:47,085 - __main__ - INFO - Fold 2 Epoch 137 Batch 0: Train Loss = 1.0625
2023-11-08 09:47:48,681 - __main__ - INFO - Fold 2, mse = 45.3402, mad = 5.0223
2023-11-08 09:47:49,051 - __main__ - INFO - Fold 2 Epoch 138 Batch 0: Train Loss = 1.0964
2023-11-08 09:47:50,367 - __main__ - INFO - Fold 2, mse = 44.5094, mad = 4.8460
2023-11-08 09:47:50,596 - __main__ - INFO - Fold 2 Epoch 139 Batch 0: Train Loss = 1.0005
2023-11-08 09:47:51,989 - __main__ - INFO - Fold 2, mse = 45.7153, mad = 5.0668
2023-11-08 09:47:52,228 - __main__ - INFO - Fold 2 Epoch 140 Batch 0: Train Loss = 0.8827
2023-11-08 09:47:53,520 - __main__ - INFO - Fold 2, epoch 140: Loss = 1.0479 Valid loss = 1.7526 MSE = 45.1001 AUROC = 0.9690
2023-11-08 09:47:53,523 - __main__ - INFO - Fold 2, mse = 45.1001, mad = 4.9505
2023-11-08 09:47:53,740 - __main__ - INFO - Fold 2 Epoch 141 Batch 0: Train Loss = 1.1425
2023-11-08 09:47:55,280 - __main__ - INFO - Fold 2, mse = 45.4926, mad = 5.0083
2023-11-08 09:47:55,617 - __main__ - INFO - Fold 2 Epoch 142 Batch 0: Train Loss = 1.0602
2023-11-08 09:47:57,124 - __main__ - INFO - Fold 2, mse = 45.2434, mad = 4.9237
2023-11-08 09:47:57,375 - __main__ - INFO - Fold 2 Epoch 143 Batch 0: Train Loss = 0.8661
2023-11-08 09:47:58,986 - __main__ - INFO - Fold 2, mse = 45.7956, mad = 5.0421
2023-11-08 09:47:59,301 - __main__ - INFO - Fold 2 Epoch 144 Batch 0: Train Loss = 0.9326
2023-11-08 09:48:00,713 - __main__ - INFO - Fold 2, mse = 45.2249, mad = 4.9888
2023-11-08 09:48:00,927 - __main__ - INFO - Fold 2 Epoch 145 Batch 0: Train Loss = 1.0056
2023-11-08 09:48:02,590 - __main__ - INFO - Fold 2, mse = 45.5283, mad = 5.0300
2023-11-08 09:48:02,891 - __main__ - INFO - Fold 2 Epoch 146 Batch 0: Train Loss = 1.0232
2023-11-08 09:48:04,511 - __main__ - INFO - Fold 2, mse = 45.0768, mad = 4.9000
2023-11-08 09:48:04,820 - __main__ - INFO - Fold 2 Epoch 147 Batch 0: Train Loss = 1.1258
2023-11-08 09:48:06,632 - __main__ - INFO - Fold 2, mse = 44.9998, mad = 4.8965
2023-11-08 09:48:06,944 - __main__ - INFO - Fold 2 Epoch 148 Batch 0: Train Loss = 0.8520
2023-11-08 09:48:08,407 - __main__ - INFO - Fold 2, mse = 46.0183, mad = 5.0824
2023-11-08 09:48:08,609 - __main__ - INFO - Fold 2 Epoch 149 Batch 0: Train Loss = 1.1515
2023-11-08 09:48:10,229 - __main__ - INFO - Fold 2, mse = 45.0121, mad = 4.8852
2023-11-08 09:48:10,757 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 2.6782
2023-11-08 09:48:12,387 - __main__ - INFO - Fold 3, epoch 0: Loss = 2.4802 Valid loss = 2.2185 MSE = 42.9965 AUROC = 0.7917
2023-11-08 09:48:12,389 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 42.9965 ------------
2023-11-08 09:48:12,574 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.7917 ------------
2023-11-08 09:48:12,576 - __main__ - INFO - Fold 3, mse = 42.9965, mad = 5.0189
2023-11-08 09:48:12,854 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 2.6031
2023-11-08 09:48:14,277 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 40.5741 ------------
2023-11-08 09:48:14,441 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8282 ------------
2023-11-08 09:48:14,443 - __main__ - INFO - Fold 3, mse = 40.5741, mad = 4.8276
2023-11-08 09:48:14,841 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 2.1802
2023-11-08 09:48:16,675 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 39.2852 ------------
2023-11-08 09:48:16,830 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8520 ------------
2023-11-08 09:48:16,832 - __main__ - INFO - Fold 3, mse = 39.2852, mad = 4.6983
2023-11-08 09:48:17,085 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 2.1401
2023-11-08 09:48:18,913 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.9285 ------------
2023-11-08 09:48:19,075 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8907 ------------
2023-11-08 09:48:19,077 - __main__ - INFO - Fold 3, mse = 37.9285, mad = 4.7132
2023-11-08 09:48:19,329 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 1.8742
2023-11-08 09:48:20,958 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.4506 ------------
2023-11-08 09:48:21,105 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9219 ------------
2023-11-08 09:48:21,106 - __main__ - INFO - Fold 3, mse = 37.4506, mad = 4.6305
2023-11-08 09:48:21,379 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 1.9896
2023-11-08 09:48:23,041 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.2168 ------------
2023-11-08 09:48:23,278 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9411 ------------
2023-11-08 09:48:23,280 - __main__ - INFO - Fold 3, mse = 37.2168, mad = 4.6033
2023-11-08 09:48:23,575 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 1.9055
2023-11-08 09:48:25,147 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.6895 ------------
2023-11-08 09:48:25,303 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9486 ------------
2023-11-08 09:48:25,305 - __main__ - INFO - Fold 3, mse = 36.6895, mad = 4.6340
2023-11-08 09:48:25,575 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 1.8830
2023-11-08 09:48:27,106 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.6091 ------------
2023-11-08 09:48:27,358 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9525 ------------
2023-11-08 09:48:27,361 - __main__ - INFO - Fold 3, mse = 36.6091, mad = 4.6694
2023-11-08 09:48:27,688 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 1.7066
2023-11-08 09:48:29,067 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9540 ------------
2023-11-08 09:48:29,071 - __main__ - INFO - Fold 3, mse = 37.1229, mad = 4.6141
2023-11-08 09:48:29,280 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 1.6940
2023-11-08 09:48:30,790 - __main__ - INFO - Fold 3, mse = 37.0362, mad = 4.6818
2023-11-08 09:48:31,023 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 1.8060
2023-11-08 09:48:33,014 - __main__ - INFO - Fold 3, epoch 10: Loss = 1.6904 Valid loss = 1.7098 MSE = 37.1414 AUROC = 0.9557
2023-11-08 09:48:33,017 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9557 ------------
2023-11-08 09:48:33,019 - __main__ - INFO - Fold 3, mse = 37.1414, mad = 4.6655
2023-11-08 09:48:33,328 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 1.7845
2023-11-08 09:48:34,975 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9563 ------------
2023-11-08 09:48:34,977 - __main__ - INFO - Fold 3, mse = 37.2584, mad = 4.6871
2023-11-08 09:48:35,267 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 1.5602
2023-11-08 09:48:37,029 - __main__ - INFO - Fold 3, mse = 37.4157, mad = 4.6839
2023-11-08 09:48:37,450 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 1.5765
2023-11-08 09:48:39,186 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9585 ------------
2023-11-08 09:48:39,188 - __main__ - INFO - Fold 3, mse = 37.2715, mad = 4.7054
2023-11-08 09:48:39,438 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 1.4316
2023-11-08 09:48:40,784 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9606 ------------
2023-11-08 09:48:40,786 - __main__ - INFO - Fold 3, mse = 37.3684, mad = 4.6810
2023-11-08 09:48:40,988 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 1.6113
2023-11-08 09:48:42,398 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9639 ------------
2023-11-08 09:48:42,400 - __main__ - INFO - Fold 3, mse = 37.2290, mad = 4.6898
2023-11-08 09:48:42,752 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 1.4313
2023-11-08 09:48:44,436 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9660 ------------
2023-11-08 09:48:44,438 - __main__ - INFO - Fold 3, mse = 37.0161, mad = 4.6757
2023-11-08 09:48:44,697 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 1.7659
2023-11-08 09:48:46,327 - __main__ - INFO - Fold 3, mse = 37.0898, mad = 4.6439
2023-11-08 09:48:46,537 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 1.6183
2023-11-08 09:48:48,463 - __main__ - INFO - Fold 3, mse = 36.8870, mad = 4.6532
2023-11-08 09:48:48,784 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 1.5678
2023-11-08 09:48:50,512 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9664 ------------
2023-11-08 09:48:50,514 - __main__ - INFO - Fold 3, mse = 36.8745, mad = 4.6300
2023-11-08 09:48:50,842 - __main__ - INFO - Fold 3 Epoch 20 Batch 0: Train Loss = 1.5848
2023-11-08 09:48:52,776 - __main__ - INFO - Fold 3, epoch 20: Loss = 1.5197 Valid loss = 1.6601 MSE = 37.4744 AUROC = 0.9676
2023-11-08 09:48:52,777 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9676 ------------
2023-11-08 09:48:52,779 - __main__ - INFO - Fold 3, mse = 37.4744, mad = 4.6162
2023-11-08 09:48:53,036 - __main__ - INFO - Fold 3 Epoch 21 Batch 0: Train Loss = 1.5273
2023-11-08 09:48:54,824 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9696 ------------
2023-11-08 09:48:54,828 - __main__ - INFO - Fold 3, mse = 36.9040, mad = 4.7437
2023-11-08 09:48:55,049 - __main__ - INFO - Fold 3 Epoch 22 Batch 0: Train Loss = 1.7098
2023-11-08 09:48:56,741 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9709 ------------
2023-11-08 09:48:56,743 - __main__ - INFO - Fold 3, mse = 37.1158, mad = 4.6147
2023-11-08 09:48:57,002 - __main__ - INFO - Fold 3 Epoch 23 Batch 0: Train Loss = 1.6257
2023-11-08 09:48:58,673 - __main__ - INFO - Fold 3, mse = 36.8392, mad = 4.6557
2023-11-08 09:48:59,023 - __main__ - INFO - Fold 3 Epoch 24 Batch 0: Train Loss = 1.6210
2023-11-08 09:49:00,653 - __main__ - INFO - Fold 3, mse = 36.6591, mad = 4.6215
2023-11-08 09:49:00,880 - __main__ - INFO - Fold 3 Epoch 25 Batch 0: Train Loss = 1.6431
2023-11-08 09:49:02,621 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9712 ------------
2023-11-08 09:49:02,623 - __main__ - INFO - Fold 3, mse = 36.6921, mad = 4.5907
2023-11-08 09:49:02,836 - __main__ - INFO - Fold 3 Epoch 26 Batch 0: Train Loss = 1.5468
2023-11-08 09:49:04,054 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.6007 ------------
2023-11-08 09:49:04,213 - __main__ - INFO - Fold 3, mse = 36.6007, mad = 4.6050
2023-11-08 09:49:04,426 - __main__ - INFO - Fold 3 Epoch 27 Batch 0: Train Loss = 1.4649
2023-11-08 09:49:05,857 - __main__ - INFO - Fold 3, mse = 36.7899, mad = 4.6265
2023-11-08 09:49:06,161 - __main__ - INFO - Fold 3 Epoch 28 Batch 0: Train Loss = 1.4624
2023-11-08 09:49:07,617 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9717 ------------
2023-11-08 09:49:07,619 - __main__ - INFO - Fold 3, mse = 36.6331, mad = 4.6167
2023-11-08 09:49:07,961 - __main__ - INFO - Fold 3 Epoch 29 Batch 0: Train Loss = 1.3084
2023-11-08 09:49:09,654 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.5130 ------------
2023-11-08 09:49:09,833 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9720 ------------
2023-11-08 09:49:09,835 - __main__ - INFO - Fold 3, mse = 36.5130, mad = 4.6099
2023-11-08 09:49:10,172 - __main__ - INFO - Fold 3 Epoch 30 Batch 0: Train Loss = 1.6921
2023-11-08 09:49:11,674 - __main__ - INFO - Fold 3, epoch 30: Loss = 1.4560 Valid loss = 1.6633 MSE = 37.2068 AUROC = 0.9723
2023-11-08 09:49:11,678 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9723 ------------
2023-11-08 09:49:11,682 - __main__ - INFO - Fold 3, mse = 37.2068, mad = 4.6153
2023-11-08 09:49:11,893 - __main__ - INFO - Fold 3 Epoch 31 Batch 0: Train Loss = 1.4513
2023-11-08 09:49:13,479 - __main__ - INFO - Fold 3, mse = 37.1180, mad = 4.6514
2023-11-08 09:49:13,764 - __main__ - INFO - Fold 3 Epoch 32 Batch 0: Train Loss = 1.3062
2023-11-08 09:49:15,431 - __main__ - INFO - Fold 3, mse = 36.8376, mad = 4.6041
2023-11-08 09:49:15,803 - __main__ - INFO - Fold 3 Epoch 33 Batch 0: Train Loss = 1.1891
2023-11-08 09:49:17,405 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.4475 ------------
2023-11-08 09:49:17,569 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9737 ------------
2023-11-08 09:49:17,571 - __main__ - INFO - Fold 3, mse = 36.4475, mad = 4.5447
2023-11-08 09:49:17,879 - __main__ - INFO - Fold 3 Epoch 34 Batch 0: Train Loss = 1.1520
2023-11-08 09:49:19,649 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.0436 ------------
2023-11-08 09:49:19,882 - __main__ - INFO - Fold 3, mse = 36.0436, mad = 4.5366
2023-11-08 09:49:20,218 - __main__ - INFO - Fold 3 Epoch 35 Batch 0: Train Loss = 1.4829
2023-11-08 09:49:21,882 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9747 ------------
2023-11-08 09:49:21,886 - __main__ - INFO - Fold 3, mse = 36.4129, mad = 4.5542
2023-11-08 09:49:22,135 - __main__ - INFO - Fold 3 Epoch 36 Batch 0: Train Loss = 1.3177
2023-11-08 09:49:23,407 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9755 ------------
2023-11-08 09:49:23,412 - __main__ - INFO - Fold 3, mse = 36.3593, mad = 4.6047
2023-11-08 09:49:23,642 - __main__ - INFO - Fold 3 Epoch 37 Batch 0: Train Loss = 1.3693
2023-11-08 09:49:25,684 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9763 ------------
2023-11-08 09:49:25,686 - __main__ - INFO - Fold 3, mse = 36.5321, mad = 4.5477
2023-11-08 09:49:25,994 - __main__ - INFO - Fold 3 Epoch 38 Batch 0: Train Loss = 1.5321
2023-11-08 09:49:27,677 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9769 ------------
2023-11-08 09:49:27,680 - __main__ - INFO - Fold 3, mse = 36.2565, mad = 4.5246
2023-11-08 09:49:27,909 - __main__ - INFO - Fold 3 Epoch 39 Batch 0: Train Loss = 1.3404
2023-11-08 09:49:29,266 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9770 ------------
2023-11-08 09:49:29,269 - __main__ - INFO - Fold 3, mse = 36.1313, mad = 4.5237
2023-11-08 09:49:29,469 - __main__ - INFO - Fold 3 Epoch 40 Batch 0: Train Loss = 1.5768
2023-11-08 09:49:30,773 - __main__ - INFO - Fold 3, epoch 40: Loss = 1.3430 Valid loss = 1.6119 MSE = 36.0425 AUROC = 0.9769
2023-11-08 09:49:30,777 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.0425 ------------
2023-11-08 09:49:30,984 - __main__ - INFO - Fold 3, mse = 36.0425, mad = 4.4932
2023-11-08 09:49:31,342 - __main__ - INFO - Fold 3 Epoch 41 Batch 0: Train Loss = 1.2057
2023-11-08 09:49:32,911 - __main__ - INFO - Fold 3, mse = 36.0900, mad = 4.4773
2023-11-08 09:49:33,257 - __main__ - INFO - Fold 3 Epoch 42 Batch 0: Train Loss = 1.2198
2023-11-08 09:49:35,182 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.8962 ------------
2023-11-08 09:49:35,378 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9778 ------------
2023-11-08 09:49:35,383 - __main__ - INFO - Fold 3, mse = 35.8962, mad = 4.4833
2023-11-08 09:49:35,690 - __main__ - INFO - Fold 3 Epoch 43 Batch 0: Train Loss = 1.2388
2023-11-08 09:49:37,233 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9780 ------------
2023-11-08 09:49:37,236 - __main__ - INFO - Fold 3, mse = 35.9591, mad = 4.4846
2023-11-08 09:49:37,445 - __main__ - INFO - Fold 3 Epoch 44 Batch 0: Train Loss = 0.9856
2023-11-08 09:49:39,148 - __main__ - INFO - Fold 3, mse = 35.9664, mad = 4.4954
2023-11-08 09:49:39,470 - __main__ - INFO - Fold 3 Epoch 45 Batch 0: Train Loss = 1.1783
2023-11-08 09:49:41,120 - __main__ - INFO - Fold 3, mse = 36.0492, mad = 4.5188
2023-11-08 09:49:41,508 - __main__ - INFO - Fold 3 Epoch 46 Batch 0: Train Loss = 1.2764
2023-11-08 09:49:43,265 - __main__ - INFO - Fold 3, mse = 36.0167, mad = 4.5307
2023-11-08 09:49:43,526 - __main__ - INFO - Fold 3 Epoch 47 Batch 0: Train Loss = 1.4179
2023-11-08 09:49:45,211 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.5804 ------------
2023-11-08 09:49:45,369 - __main__ - INFO - Fold 3, mse = 35.5804, mad = 4.4644
2023-11-08 09:49:45,639 - __main__ - INFO - Fold 3 Epoch 48 Batch 0: Train Loss = 1.3032
2023-11-08 09:49:47,407 - __main__ - INFO - Fold 3, mse = 35.7013, mad = 4.4242
2023-11-08 09:49:47,735 - __main__ - INFO - Fold 3 Epoch 49 Batch 0: Train Loss = 1.1933
2023-11-08 09:49:49,389 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.3429 ------------
2023-11-08 09:49:49,564 - __main__ - INFO - Fold 3, mse = 35.3429, mad = 4.4755
2023-11-08 09:49:49,881 - __main__ - INFO - Fold 3 Epoch 50 Batch 0: Train Loss = 1.1795
2023-11-08 09:49:51,514 - __main__ - INFO - Fold 3, epoch 50: Loss = 1.2443 Valid loss = 1.6562 MSE = 35.8836 AUROC = 0.9772
2023-11-08 09:49:51,516 - __main__ - INFO - Fold 3, mse = 35.8836, mad = 4.4906
2023-11-08 09:49:51,750 - __main__ - INFO - Fold 3 Epoch 51 Batch 0: Train Loss = 1.1820
2023-11-08 09:49:53,432 - __main__ - INFO - Fold 3, mse = 35.9506, mad = 4.5370
2023-11-08 09:49:53,683 - __main__ - INFO - Fold 3 Epoch 52 Batch 0: Train Loss = 1.3654
2023-11-08 09:49:55,315 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9790 ------------
2023-11-08 09:49:55,318 - __main__ - INFO - Fold 3, mse = 36.8581, mad = 4.4672
2023-11-08 09:49:55,678 - __main__ - INFO - Fold 3 Epoch 53 Batch 0: Train Loss = 1.2305
2023-11-08 09:49:57,521 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9797 ------------
2023-11-08 09:49:57,523 - __main__ - INFO - Fold 3, mse = 36.1840, mad = 4.5486
2023-11-08 09:49:57,883 - __main__ - INFO - Fold 3 Epoch 54 Batch 0: Train Loss = 1.3303
2023-11-08 09:49:59,492 - __main__ - INFO - Fold 3, mse = 36.8087, mad = 4.5119
2023-11-08 09:49:59,747 - __main__ - INFO - Fold 3 Epoch 55 Batch 0: Train Loss = 1.2984
2023-11-08 09:50:01,607 - __main__ - INFO - Fold 3, mse = 36.0538, mad = 4.5622
2023-11-08 09:50:02,005 - __main__ - INFO - Fold 3 Epoch 56 Batch 0: Train Loss = 1.4377
2023-11-08 09:50:03,693 - __main__ - INFO - Fold 3, mse = 36.1379, mad = 4.4903
2023-11-08 09:50:04,027 - __main__ - INFO - Fold 3 Epoch 57 Batch 0: Train Loss = 0.9957
2023-11-08 09:50:05,702 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9802 ------------
2023-11-08 09:50:05,705 - __main__ - INFO - Fold 3, mse = 35.6463, mad = 4.4982
2023-11-08 09:50:06,056 - __main__ - INFO - Fold 3 Epoch 58 Batch 0: Train Loss = 1.1362
2023-11-08 09:50:07,767 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9809 ------------
2023-11-08 09:50:07,774 - __main__ - INFO - Fold 3, mse = 35.6305, mad = 4.4381
2023-11-08 09:50:08,140 - __main__ - INFO - Fold 3 Epoch 59 Batch 0: Train Loss = 1.0652
2023-11-08 09:50:09,508 - __main__ - INFO - Fold 3, mse = 35.6031, mad = 4.4536
2023-11-08 09:50:09,732 - __main__ - INFO - Fold 3 Epoch 60 Batch 0: Train Loss = 1.1468
2023-11-08 09:50:11,364 - __main__ - INFO - Fold 3, epoch 60: Loss = 1.2365 Valid loss = 1.5849 MSE = 35.3860 AUROC = 0.9790
2023-11-08 09:50:11,366 - __main__ - INFO - Fold 3, mse = 35.3860, mad = 4.4766
2023-11-08 09:50:11,717 - __main__ - INFO - Fold 3 Epoch 61 Batch 0: Train Loss = 1.1519
2023-11-08 09:50:13,325 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.3268 ------------
2023-11-08 09:50:13,538 - __main__ - INFO - Fold 3, mse = 35.3268, mad = 4.4277
2023-11-08 09:50:13,833 - __main__ - INFO - Fold 3 Epoch 62 Batch 0: Train Loss = 1.3915
2023-11-08 09:50:15,585 - __main__ - INFO - Fold 3, mse = 35.8754, mad = 4.4680
2023-11-08 09:50:15,899 - __main__ - INFO - Fold 3 Epoch 63 Batch 0: Train Loss = 1.1341
2023-11-08 09:50:17,473 - __main__ - INFO - Fold 3, mse = 35.8775, mad = 4.5730
2023-11-08 09:50:17,773 - __main__ - INFO - Fold 3 Epoch 64 Batch 0: Train Loss = 1.2733
2023-11-08 09:50:19,386 - __main__ - INFO - Fold 3, mse = 35.6754, mad = 4.4249
2023-11-08 09:50:19,762 - __main__ - INFO - Fold 3 Epoch 65 Batch 0: Train Loss = 1.1226
2023-11-08 09:50:21,411 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.9531 ------------
2023-11-08 09:50:21,627 - __main__ - INFO - Fold 3, mse = 34.9531, mad = 4.4200
2023-11-08 09:50:22,029 - __main__ - INFO - Fold 3 Epoch 66 Batch 0: Train Loss = 1.1551
2023-11-08 09:50:23,863 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.9373 ------------
2023-11-08 09:50:24,019 - __main__ - INFO - Fold 3, mse = 34.9373, mad = 4.3572
2023-11-08 09:50:24,358 - __main__ - INFO - Fold 3 Epoch 67 Batch 0: Train Loss = 1.3711
2023-11-08 09:50:25,989 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.5584 ------------
2023-11-08 09:50:26,249 - __main__ - INFO - Fold 3, mse = 34.5584, mad = 4.3811
2023-11-08 09:50:26,526 - __main__ - INFO - Fold 3 Epoch 68 Batch 0: Train Loss = 1.3013
2023-11-08 09:50:28,120 - __main__ - INFO - Fold 3, mse = 35.0041, mad = 4.4002
2023-11-08 09:50:28,487 - __main__ - INFO - Fold 3 Epoch 69 Batch 0: Train Loss = 0.8919
2023-11-08 09:50:30,020 - __main__ - INFO - Fold 3, mse = 34.8386, mad = 4.4425
2023-11-08 09:50:30,364 - __main__ - INFO - Fold 3 Epoch 70 Batch 0: Train Loss = 1.1531
2023-11-08 09:50:32,241 - __main__ - INFO - Fold 3, epoch 70: Loss = 1.1758 Valid loss = 1.6194 MSE = 35.5928 AUROC = 0.9782
2023-11-08 09:50:32,244 - __main__ - INFO - Fold 3, mse = 35.5928, mad = 4.3874
2023-11-08 09:50:32,560 - __main__ - INFO - Fold 3 Epoch 71 Batch 0: Train Loss = 0.8753
2023-11-08 09:50:34,293 - __main__ - INFO - Fold 3, mse = 34.9599, mad = 4.4084
2023-11-08 09:50:34,589 - __main__ - INFO - Fold 3 Epoch 72 Batch 0: Train Loss = 1.5553
2023-11-08 09:50:36,234 - __main__ - INFO - Fold 3, mse = 35.7154, mad = 4.3654
2023-11-08 09:50:36,485 - __main__ - INFO - Fold 3 Epoch 73 Batch 0: Train Loss = 0.9584
2023-11-08 09:50:37,999 - __main__ - INFO - Fold 3, mse = 34.9939, mad = 4.3793
2023-11-08 09:50:38,255 - __main__ - INFO - Fold 3 Epoch 74 Batch 0: Train Loss = 1.0371
2023-11-08 09:50:39,906 - __main__ - INFO - Fold 3, mse = 34.9063, mad = 4.3833
2023-11-08 09:50:40,116 - __main__ - INFO - Fold 3 Epoch 75 Batch 0: Train Loss = 1.2065
2023-11-08 09:50:41,876 - __main__ - INFO - Fold 3, mse = 35.0646, mad = 4.3635
2023-11-08 09:50:42,152 - __main__ - INFO - Fold 3 Epoch 76 Batch 0: Train Loss = 1.1303
2023-11-08 09:50:43,889 - __main__ - INFO - Fold 3, mse = 34.9232, mad = 4.4059
2023-11-08 09:50:44,140 - __main__ - INFO - Fold 3 Epoch 77 Batch 0: Train Loss = 1.4162
2023-11-08 09:50:45,721 - __main__ - INFO - Fold 3, mse = 35.0732, mad = 4.3451
2023-11-08 09:50:46,045 - __main__ - INFO - Fold 3 Epoch 78 Batch 0: Train Loss = 1.1057
2023-11-08 09:50:47,774 - __main__ - INFO - Fold 3, mse = 34.6678, mad = 4.3682
2023-11-08 09:50:48,135 - __main__ - INFO - Fold 3 Epoch 79 Batch 0: Train Loss = 1.1939
2023-11-08 09:50:49,795 - __main__ - INFO - Fold 3, mse = 34.7112, mad = 4.3598
2023-11-08 09:50:50,028 - __main__ - INFO - Fold 3 Epoch 80 Batch 0: Train Loss = 1.2665
2023-11-08 09:50:51,669 - __main__ - INFO - Fold 3, epoch 80: Loss = 1.1355 Valid loss = 1.5549 MSE = 34.3355 AUROC = 0.9793
2023-11-08 09:50:51,672 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.3355 ------------
2023-11-08 09:50:51,857 - __main__ - INFO - Fold 3, mse = 34.3355, mad = 4.3950
2023-11-08 09:50:52,125 - __main__ - INFO - Fold 3 Epoch 81 Batch 0: Train Loss = 1.2507
2023-11-08 09:50:53,480 - __main__ - INFO - Fold 3, mse = 34.8572, mad = 4.3555
2023-11-08 09:50:53,785 - __main__ - INFO - Fold 3 Epoch 82 Batch 0: Train Loss = 1.2235
2023-11-08 09:50:55,474 - __main__ - INFO - Fold 3, mse = 34.6196, mad = 4.3594
2023-11-08 09:50:55,813 - __main__ - INFO - Fold 3 Epoch 83 Batch 0: Train Loss = 0.9804
2023-11-08 09:50:57,397 - __main__ - INFO - Fold 3, mse = 34.5041, mad = 4.4405
2023-11-08 09:50:57,663 - __main__ - INFO - Fold 3 Epoch 84 Batch 0: Train Loss = 1.1087
2023-11-08 09:50:59,191 - __main__ - INFO - Fold 3, mse = 34.8933, mad = 4.3554
2023-11-08 09:50:59,427 - __main__ - INFO - Fold 3 Epoch 85 Batch 0: Train Loss = 1.3522
2023-11-08 09:51:00,902 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.2764 ------------
2023-11-08 09:51:01,070 - __main__ - INFO - Fold 3, mse = 34.2764, mad = 4.3379
2023-11-08 09:51:01,466 - __main__ - INFO - Fold 3 Epoch 86 Batch 0: Train Loss = 1.1968
2023-11-08 09:51:03,218 - __main__ - INFO - Fold 3, mse = 34.3880, mad = 4.3349
2023-11-08 09:51:03,509 - __main__ - INFO - Fold 3 Epoch 87 Batch 0: Train Loss = 1.0780
2023-11-08 09:51:05,050 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.8464 ------------
2023-11-08 09:51:05,221 - __main__ - INFO - Fold 3, mse = 33.8464, mad = 4.3660
2023-11-08 09:51:05,515 - __main__ - INFO - Fold 3 Epoch 88 Batch 0: Train Loss = 1.0662
2023-11-08 09:51:07,096 - __main__ - INFO - Fold 3, mse = 33.9670, mad = 4.3144
2023-11-08 09:51:07,406 - __main__ - INFO - Fold 3 Epoch 89 Batch 0: Train Loss = 1.2171
2023-11-08 09:51:08,804 - __main__ - INFO - Fold 3, mse = 34.3498, mad = 4.3211
2023-11-08 09:51:09,035 - __main__ - INFO - Fold 3 Epoch 90 Batch 0: Train Loss = 1.1926
2023-11-08 09:51:10,644 - __main__ - INFO - Fold 3, epoch 90: Loss = 1.1252 Valid loss = 1.5649 MSE = 34.1190 AUROC = 0.9792
2023-11-08 09:51:10,646 - __main__ - INFO - Fold 3, mse = 34.1190, mad = 4.3713
2023-11-08 09:51:10,865 - __main__ - INFO - Fold 3 Epoch 91 Batch 0: Train Loss = 0.9967
2023-11-08 09:51:12,645 - __main__ - INFO - Fold 3, mse = 34.6578, mad = 4.3408
2023-11-08 09:51:12,870 - __main__ - INFO - Fold 3 Epoch 92 Batch 0: Train Loss = 1.1818
2023-11-08 09:51:14,426 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9811 ------------
2023-11-08 09:51:14,428 - __main__ - INFO - Fold 3, mse = 34.3707, mad = 4.3232
2023-11-08 09:51:14,791 - __main__ - INFO - Fold 3 Epoch 93 Batch 0: Train Loss = 1.0733
2023-11-08 09:51:16,303 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9822 ------------
2023-11-08 09:51:16,305 - __main__ - INFO - Fold 3, mse = 33.9056, mad = 4.3087
2023-11-08 09:51:16,519 - __main__ - INFO - Fold 3 Epoch 94 Batch 0: Train Loss = 1.2266
2023-11-08 09:51:18,157 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9823 ------------
2023-11-08 09:51:18,159 - __main__ - INFO - Fold 3, mse = 34.1060, mad = 4.3352
2023-11-08 09:51:18,445 - __main__ - INFO - Fold 3 Epoch 95 Batch 0: Train Loss = 1.1593
2023-11-08 09:51:20,402 - __main__ - INFO - Fold 3, mse = 33.8806, mad = 4.3352
2023-11-08 09:51:20,675 - __main__ - INFO - Fold 3 Epoch 96 Batch 0: Train Loss = 1.0087
2023-11-08 09:51:22,268 - __main__ - INFO - Fold 3, mse = 34.4737, mad = 4.3001
2023-11-08 09:51:22,532 - __main__ - INFO - Fold 3 Epoch 97 Batch 0: Train Loss = 1.0349
2023-11-08 09:51:24,032 - __main__ - INFO - Fold 3, mse = 34.0374, mad = 4.3653
2023-11-08 09:51:24,261 - __main__ - INFO - Fold 3 Epoch 98 Batch 0: Train Loss = 1.0350
2023-11-08 09:51:25,878 - __main__ - INFO - Fold 3, mse = 35.5878, mad = 4.3750
2023-11-08 09:51:26,232 - __main__ - INFO - Fold 3 Epoch 99 Batch 0: Train Loss = 1.2736
2023-11-08 09:51:27,749 - __main__ - INFO - Fold 3, mse = 34.5263, mad = 4.3884
2023-11-08 09:51:28,002 - __main__ - INFO - Fold 3 Epoch 100 Batch 0: Train Loss = 1.0843
2023-11-08 09:51:29,331 - __main__ - INFO - Fold 3, epoch 100: Loss = 1.0683 Valid loss = 1.5945 MSE = 35.1739 AUROC = 0.9826
2023-11-08 09:51:29,333 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9826 ------------
2023-11-08 09:51:29,335 - __main__ - INFO - Fold 3, mse = 35.1739, mad = 4.3438
2023-11-08 09:51:29,579 - __main__ - INFO - Fold 3 Epoch 101 Batch 0: Train Loss = 1.1231
2023-11-08 09:51:31,188 - __main__ - INFO - Fold 3, mse = 34.5884, mad = 4.3557
2023-11-08 09:51:31,433 - __main__ - INFO - Fold 3 Epoch 102 Batch 0: Train Loss = 1.0993
2023-11-08 09:51:33,019 - __main__ - INFO - Fold 3, mse = 34.6910, mad = 4.3911
2023-11-08 09:51:33,343 - __main__ - INFO - Fold 3 Epoch 103 Batch 0: Train Loss = 1.2401
2023-11-08 09:51:35,261 - __main__ - INFO - Fold 3, mse = 34.5585, mad = 4.3524
2023-11-08 09:51:35,637 - __main__ - INFO - Fold 3 Epoch 104 Batch 0: Train Loss = 1.1119
2023-11-08 09:51:37,213 - __main__ - INFO - Fold 3, mse = 34.1921, mad = 4.3357
2023-11-08 09:51:37,498 - __main__ - INFO - Fold 3 Epoch 105 Batch 0: Train Loss = 1.2412
2023-11-08 09:51:39,239 - __main__ - INFO - Fold 3, mse = 34.7315, mad = 4.3213
2023-11-08 09:51:39,631 - __main__ - INFO - Fold 3 Epoch 106 Batch 0: Train Loss = 1.1379
2023-11-08 09:51:41,287 - __main__ - INFO - Fold 3, mse = 34.3513, mad = 4.3317
2023-11-08 09:51:41,568 - __main__ - INFO - Fold 3 Epoch 107 Batch 0: Train Loss = 1.0072
2023-11-08 09:51:43,238 - __main__ - INFO - Fold 3, mse = 34.2319, mad = 4.3311
2023-11-08 09:51:43,510 - __main__ - INFO - Fold 3 Epoch 108 Batch 0: Train Loss = 1.1647
2023-11-08 09:51:45,237 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.6464 ------------
2023-11-08 09:51:45,393 - __main__ - INFO - Fold 3, mse = 33.6464, mad = 4.3119
2023-11-08 09:51:45,626 - __main__ - INFO - Fold 3 Epoch 109 Batch 0: Train Loss = 1.1833
2023-11-08 09:51:47,154 - __main__ - INFO - Fold 3, mse = 33.9211, mad = 4.3128
2023-11-08 09:51:47,372 - __main__ - INFO - Fold 3 Epoch 110 Batch 0: Train Loss = 0.9692
2023-11-08 09:51:48,751 - __main__ - INFO - Fold 3, epoch 110: Loss = 1.0575 Valid loss = 1.5700 MSE = 33.4948 AUROC = 0.9805
2023-11-08 09:51:48,755 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.4948 ------------
2023-11-08 09:51:48,952 - __main__ - INFO - Fold 3, mse = 33.4948, mad = 4.3149
2023-11-08 09:51:49,185 - __main__ - INFO - Fold 3 Epoch 111 Batch 0: Train Loss = 1.0389
2023-11-08 09:51:51,036 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.1131 ------------
2023-11-08 09:51:51,181 - __main__ - INFO - Fold 3, mse = 33.1131, mad = 4.2933
2023-11-08 09:51:51,453 - __main__ - INFO - Fold 3 Epoch 112 Batch 0: Train Loss = 1.0555
2023-11-08 09:51:52,901 - __main__ - INFO - Fold 3, mse = 33.7602, mad = 4.2771
2023-11-08 09:51:53,133 - __main__ - INFO - Fold 3 Epoch 113 Batch 0: Train Loss = 0.8926
2023-11-08 09:51:54,734 - __main__ - INFO - Fold 3, mse = 33.5802, mad = 4.3303
2023-11-08 09:51:55,005 - __main__ - INFO - Fold 3 Epoch 114 Batch 0: Train Loss = 1.0195
2023-11-08 09:51:56,672 - __main__ - INFO - Fold 3, mse = 33.8334, mad = 4.3025
2023-11-08 09:51:56,927 - __main__ - INFO - Fold 3 Epoch 115 Batch 0: Train Loss = 0.9473
2023-11-08 09:51:58,603 - __main__ - INFO - Fold 3, mse = 33.5849, mad = 4.2738
2023-11-08 09:51:58,900 - __main__ - INFO - Fold 3 Epoch 116 Batch 0: Train Loss = 1.0990
2023-11-08 09:52:00,646 - __main__ - INFO - Fold 3, mse = 33.3827, mad = 4.2927
2023-11-08 09:52:00,942 - __main__ - INFO - Fold 3 Epoch 117 Batch 0: Train Loss = 1.1347
2023-11-08 09:52:02,354 - __main__ - INFO - Fold 3, mse = 33.6054, mad = 4.3245
2023-11-08 09:52:02,570 - __main__ - INFO - Fold 3 Epoch 118 Batch 0: Train Loss = 1.0363
2023-11-08 09:52:04,237 - __main__ - INFO - Fold 3, mse = 33.8823, mad = 4.3232
2023-11-08 09:52:04,539 - __main__ - INFO - Fold 3 Epoch 119 Batch 0: Train Loss = 1.1540
2023-11-08 09:52:06,280 - __main__ - INFO - Fold 3, mse = 33.7296, mad = 4.3114
2023-11-08 09:52:06,550 - __main__ - INFO - Fold 3 Epoch 120 Batch 0: Train Loss = 1.2070
2023-11-08 09:52:08,338 - __main__ - INFO - Fold 3, epoch 120: Loss = 1.0807 Valid loss = 1.5413 MSE = 34.0270 AUROC = 0.9814
2023-11-08 09:52:08,342 - __main__ - INFO - Fold 3, mse = 34.0270, mad = 4.3236
2023-11-08 09:52:08,591 - __main__ - INFO - Fold 3 Epoch 121 Batch 0: Train Loss = 1.1057
2023-11-08 09:52:10,257 - __main__ - INFO - Fold 3, mse = 33.9863, mad = 4.3305
2023-11-08 09:52:10,627 - __main__ - INFO - Fold 3 Epoch 122 Batch 0: Train Loss = 1.1092
2023-11-08 09:52:12,330 - __main__ - INFO - Fold 3, mse = 33.3083, mad = 4.2909
2023-11-08 09:52:12,598 - __main__ - INFO - Fold 3 Epoch 123 Batch 0: Train Loss = 0.9624
2023-11-08 09:52:14,078 - __main__ - INFO - Fold 3, mse = 34.2079, mad = 4.2965
2023-11-08 09:52:14,384 - __main__ - INFO - Fold 3 Epoch 124 Batch 0: Train Loss = 1.1061
2023-11-08 09:52:16,028 - __main__ - INFO - Fold 3, mse = 33.2393, mad = 4.2932
2023-11-08 09:52:16,350 - __main__ - INFO - Fold 3 Epoch 125 Batch 0: Train Loss = 1.4837
2023-11-08 09:52:17,934 - __main__ - INFO - Fold 3, mse = 33.9540, mad = 4.2931
2023-11-08 09:52:18,204 - __main__ - INFO - Fold 3 Epoch 126 Batch 0: Train Loss = 0.8854
2023-11-08 09:52:19,805 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.0793 ------------
2023-11-08 09:52:19,969 - __main__ - INFO - Fold 3, mse = 33.0793, mad = 4.2783
2023-11-08 09:52:20,191 - __main__ - INFO - Fold 3 Epoch 127 Batch 0: Train Loss = 0.8518
2023-11-08 09:52:21,738 - __main__ - INFO - Fold 3, mse = 33.9820, mad = 4.2722
2023-11-08 09:52:22,077 - __main__ - INFO - Fold 3 Epoch 128 Batch 0: Train Loss = 1.2382
2023-11-08 09:52:23,951 - __main__ - INFO - Fold 3, mse = 33.3156, mad = 4.2586
2023-11-08 09:52:24,217 - __main__ - INFO - Fold 3 Epoch 129 Batch 0: Train Loss = 0.8243
2023-11-08 09:52:25,864 - __main__ - INFO - Fold 3, mse = 33.5759, mad = 4.2676
2023-11-08 09:52:26,110 - __main__ - INFO - Fold 3 Epoch 130 Batch 0: Train Loss = 0.8368
2023-11-08 09:52:27,881 - __main__ - INFO - Fold 3, epoch 130: Loss = 1.0519 Valid loss = 1.5472 MSE = 33.6419 AUROC = 0.9824
2023-11-08 09:52:27,884 - __main__ - INFO - Fold 3, mse = 33.6419, mad = 4.2748
2023-11-08 09:52:28,211 - __main__ - INFO - Fold 3 Epoch 131 Batch 0: Train Loss = 1.0765
2023-11-08 09:52:30,131 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9827 ------------
2023-11-08 09:52:30,133 - __main__ - INFO - Fold 3, mse = 34.0697, mad = 4.2879
2023-11-08 09:52:30,418 - __main__ - INFO - Fold 3 Epoch 132 Batch 0: Train Loss = 1.1179
2023-11-08 09:52:31,794 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9828 ------------
2023-11-08 09:52:31,796 - __main__ - INFO - Fold 3, mse = 33.6138, mad = 4.3055
2023-11-08 09:52:32,025 - __main__ - INFO - Fold 3 Epoch 133 Batch 0: Train Loss = 1.0769
2023-11-08 09:52:33,690 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9834 ------------
2023-11-08 09:52:33,692 - __main__ - INFO - Fold 3, mse = 34.4520, mad = 4.3176
2023-11-08 09:52:33,955 - __main__ - INFO - Fold 3 Epoch 134 Batch 0: Train Loss = 0.8716
2023-11-08 09:52:35,470 - __main__ - INFO - Fold 3, mse = 33.8023, mad = 4.3304
2023-11-08 09:52:35,805 - __main__ - INFO - Fold 3 Epoch 135 Batch 0: Train Loss = 1.1711
2023-11-08 09:52:37,690 - __main__ - INFO - Fold 3, mse = 34.0460, mad = 4.2849
2023-11-08 09:52:38,072 - __main__ - INFO - Fold 3 Epoch 136 Batch 0: Train Loss = 1.1049
2023-11-08 09:52:39,712 - __main__ - INFO - Fold 3, mse = 33.5370, mad = 4.2853
2023-11-08 09:52:40,030 - __main__ - INFO - Fold 3 Epoch 137 Batch 0: Train Loss = 1.1625
2023-11-08 09:52:41,789 - __main__ - INFO - Fold 3, mse = 34.1330, mad = 4.2977
2023-11-08 09:52:42,094 - __main__ - INFO - Fold 3 Epoch 138 Batch 0: Train Loss = 1.2152
2023-11-08 09:52:43,914 - __main__ - INFO - Fold 3, mse = 33.8895, mad = 4.2747
2023-11-08 09:52:44,278 - __main__ - INFO - Fold 3 Epoch 139 Batch 0: Train Loss = 1.0247
2023-11-08 09:52:45,676 - __main__ - INFO - Fold 3, mse = 33.6552, mad = 4.2373
2023-11-08 09:52:45,948 - __main__ - INFO - Fold 3 Epoch 140 Batch 0: Train Loss = 0.9662
2023-11-08 09:52:47,347 - __main__ - INFO - Fold 3, epoch 140: Loss = 1.0517 Valid loss = 1.5286 MSE = 33.3717 AUROC = 0.9820
2023-11-08 09:52:47,349 - __main__ - INFO - Fold 3, mse = 33.3717, mad = 4.2499
2023-11-08 09:52:47,677 - __main__ - INFO - Fold 3 Epoch 141 Batch 0: Train Loss = 1.1631
2023-11-08 09:52:49,261 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.0250 ------------
2023-11-08 09:52:49,421 - __main__ - INFO - Fold 3, mse = 33.0250, mad = 4.2556
2023-11-08 09:52:49,640 - __main__ - INFO - Fold 3 Epoch 142 Batch 0: Train Loss = 1.0824
2023-11-08 09:52:51,689 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.8415 ------------
2023-11-08 09:52:51,845 - __main__ - INFO - Fold 3, mse = 32.8415, mad = 4.2361
2023-11-08 09:52:52,194 - __main__ - INFO - Fold 3 Epoch 143 Batch 0: Train Loss = 1.1479
2023-11-08 09:52:53,811 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.3311 ------------
2023-11-08 09:52:53,998 - __main__ - INFO - Fold 3, mse = 32.3311, mad = 4.2072
2023-11-08 09:52:54,252 - __main__ - INFO - Fold 3 Epoch 144 Batch 0: Train Loss = 1.0204
2023-11-08 09:52:55,917 - __main__ - INFO - Fold 3, mse = 33.1094, mad = 4.2440
2023-11-08 09:52:56,183 - __main__ - INFO - Fold 3 Epoch 145 Batch 0: Train Loss = 1.1743
2023-11-08 09:52:58,049 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9836 ------------
2023-11-08 09:52:58,053 - __main__ - INFO - Fold 3, mse = 32.4269, mad = 4.2284
2023-11-08 09:52:58,414 - __main__ - INFO - Fold 3 Epoch 146 Batch 0: Train Loss = 1.0422
2023-11-08 09:53:00,357 - __main__ - INFO - Fold 3, mse = 33.2331, mad = 4.2366
2023-11-08 09:53:00,795 - __main__ - INFO - Fold 3 Epoch 147 Batch 0: Train Loss = 1.0723
2023-11-08 09:53:02,535 - __main__ - INFO - Fold 3, mse = 33.0282, mad = 4.2549
2023-11-08 09:53:02,758 - __main__ - INFO - Fold 3 Epoch 148 Batch 0: Train Loss = 1.1050
2023-11-08 09:53:04,402 - __main__ - INFO - Fold 3, mse = 33.4245, mad = 4.2752
2023-11-08 09:53:04,742 - __main__ - INFO - Fold 3 Epoch 149 Batch 0: Train Loss = 0.9091
2023-11-08 09:53:06,529 - __main__ - INFO - Fold 3, mse = 33.2314, mad = 4.3020
2023-11-08 09:53:07,007 - __main__ - INFO - Fold 4 Epoch 0 Batch 0: Train Loss = 2.6486
2023-11-08 09:53:08,633 - __main__ - INFO - Fold 4, epoch 0: Loss = 2.4949 Valid loss = 2.1509 MSE = 39.0742 AUROC = 0.7554
2023-11-08 09:53:08,635 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 39.0742 ------------
2023-11-08 09:53:08,817 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.7554 ------------
2023-11-08 09:53:08,820 - __main__ - INFO - Fold 4, mse = 39.0742, mad = 4.6787
2023-11-08 09:53:09,152 - __main__ - INFO - Fold 4 Epoch 1 Batch 0: Train Loss = 2.5184
2023-11-08 09:53:10,497 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.1127 ------------
2023-11-08 09:53:10,652 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.7791 ------------
2023-11-08 09:53:10,654 - __main__ - INFO - Fold 4, mse = 37.1127, mad = 4.6006
2023-11-08 09:53:11,010 - __main__ - INFO - Fold 4 Epoch 2 Batch 0: Train Loss = 2.3928
2023-11-08 09:53:12,684 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.8829 ------------
2023-11-08 09:53:12,857 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8062 ------------
2023-11-08 09:53:12,859 - __main__ - INFO - Fold 4, mse = 35.8829, mad = 4.5476
2023-11-08 09:53:13,070 - __main__ - INFO - Fold 4 Epoch 3 Batch 0: Train Loss = 2.1290
2023-11-08 09:53:14,584 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.3057 ------------
2023-11-08 09:53:14,905 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8362 ------------
2023-11-08 09:53:14,908 - __main__ - INFO - Fold 4, mse = 35.3057, mad = 4.4577
2023-11-08 09:53:15,263 - __main__ - INFO - Fold 4 Epoch 4 Batch 0: Train Loss = 2.1919
2023-11-08 09:53:17,013 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.9771 ------------
2023-11-08 09:53:17,174 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8714 ------------
2023-11-08 09:53:17,177 - __main__ - INFO - Fold 4, mse = 34.9771, mad = 4.4868
2023-11-08 09:53:17,552 - __main__ - INFO - Fold 4 Epoch 5 Batch 0: Train Loss = 1.8305
2023-11-08 09:53:18,968 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.8293 ------------
2023-11-08 09:53:19,178 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9085 ------------
2023-11-08 09:53:19,180 - __main__ - INFO - Fold 4, mse = 34.8293, mad = 4.5147
2023-11-08 09:53:19,543 - __main__ - INFO - Fold 4 Epoch 6 Batch 0: Train Loss = 1.6303
2023-11-08 09:53:21,196 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9448 ------------
2023-11-08 09:53:21,198 - __main__ - INFO - Fold 4, mse = 34.9687, mad = 4.5762
2023-11-08 09:53:21,459 - __main__ - INFO - Fold 4 Epoch 7 Batch 0: Train Loss = 1.6895
2023-11-08 09:53:23,388 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9602 ------------
2023-11-08 09:53:23,392 - __main__ - INFO - Fold 4, mse = 35.0121, mad = 4.5330
2023-11-08 09:53:23,649 - __main__ - INFO - Fold 4 Epoch 8 Batch 0: Train Loss = 1.7982
2023-11-08 09:53:25,336 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9653 ------------
2023-11-08 09:53:25,339 - __main__ - INFO - Fold 4, mse = 34.9621, mad = 4.6022
2023-11-08 09:53:25,634 - __main__ - INFO - Fold 4 Epoch 9 Batch 0: Train Loss = 1.8964
2023-11-08 09:53:27,077 - __main__ - INFO - Fold 4, mse = 35.1654, mad = 4.6151
2023-11-08 09:53:27,412 - __main__ - INFO - Fold 4 Epoch 10 Batch 0: Train Loss = 1.7470
2023-11-08 09:53:28,936 - __main__ - INFO - Fold 4, epoch 10: Loss = 1.6519 Valid loss = 1.7528 MSE = 35.7692 AUROC = 0.9629
2023-11-08 09:53:28,938 - __main__ - INFO - Fold 4, mse = 35.7692, mad = 4.5616
2023-11-08 09:53:29,196 - __main__ - INFO - Fold 4 Epoch 11 Batch 0: Train Loss = 1.7757
2023-11-08 09:53:31,012 - __main__ - INFO - Fold 4, mse = 35.9454, mad = 4.6482
2023-11-08 09:53:31,347 - __main__ - INFO - Fold 4 Epoch 12 Batch 0: Train Loss = 1.5661
2023-11-08 09:53:33,228 - __main__ - INFO - Fold 4, mse = 35.8019, mad = 4.6570
2023-11-08 09:53:33,522 - __main__ - INFO - Fold 4 Epoch 13 Batch 0: Train Loss = 1.6134
2023-11-08 09:53:34,831 - __main__ - INFO - Fold 4, mse = 35.6779, mad = 4.5816
2023-11-08 09:53:35,147 - __main__ - INFO - Fold 4 Epoch 14 Batch 0: Train Loss = 1.5914
2023-11-08 09:53:36,608 - __main__ - INFO - Fold 4, mse = 35.8733, mad = 4.7058
2023-11-08 09:53:36,924 - __main__ - INFO - Fold 4 Epoch 15 Batch 0: Train Loss = 1.3655
2023-11-08 09:53:38,483 - __main__ - INFO - Fold 4, mse = 35.3831, mad = 4.5237
2023-11-08 09:53:38,795 - __main__ - INFO - Fold 4 Epoch 16 Batch 0: Train Loss = 1.5080
2023-11-08 09:53:40,503 - __main__ - INFO - Fold 4, mse = 35.3604, mad = 4.6359
2023-11-08 09:53:40,758 - __main__ - INFO - Fold 4 Epoch 17 Batch 0: Train Loss = 1.9037
2023-11-08 09:53:42,494 - __main__ - INFO - Fold 4, mse = 35.5277, mad = 4.6395
2023-11-08 09:53:42,873 - __main__ - INFO - Fold 4 Epoch 18 Batch 0: Train Loss = 1.7020
2023-11-08 09:53:44,566 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9655 ------------
2023-11-08 09:53:44,568 - __main__ - INFO - Fold 4, mse = 35.0335, mad = 4.5452
2023-11-08 09:53:44,815 - __main__ - INFO - Fold 4 Epoch 19 Batch 0: Train Loss = 1.6969
2023-11-08 09:53:46,444 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.5927 ------------
2023-11-08 09:53:46,660 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9662 ------------
2023-11-08 09:53:46,662 - __main__ - INFO - Fold 4, mse = 34.5927, mad = 4.5948
2023-11-08 09:53:46,892 - __main__ - INFO - Fold 4 Epoch 20 Batch 0: Train Loss = 1.5647
2023-11-08 09:53:48,502 - __main__ - INFO - Fold 4, epoch 20: Loss = 1.4916 Valid loss = 1.6450 MSE = 34.2166 AUROC = 0.9669
2023-11-08 09:53:48,504 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.2166 ------------
2023-11-08 09:53:48,670 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9669 ------------
2023-11-08 09:53:48,672 - __main__ - INFO - Fold 4, mse = 34.2166, mad = 4.5112
2023-11-08 09:53:48,950 - __main__ - INFO - Fold 4 Epoch 21 Batch 0: Train Loss = 1.5119
2023-11-08 09:53:50,592 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9675 ------------
2023-11-08 09:53:50,594 - __main__ - INFO - Fold 4, mse = 34.6339, mad = 4.6194
2023-11-08 09:53:50,825 - __main__ - INFO - Fold 4 Epoch 22 Batch 0: Train Loss = 1.5322
2023-11-08 09:53:52,361 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9682 ------------
2023-11-08 09:53:52,363 - __main__ - INFO - Fold 4, mse = 34.6119, mad = 4.5882
2023-11-08 09:53:52,664 - __main__ - INFO - Fold 4 Epoch 23 Batch 0: Train Loss = 1.2355
2023-11-08 09:53:54,388 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9685 ------------
2023-11-08 09:53:54,390 - __main__ - INFO - Fold 4, mse = 34.9878, mad = 4.6469
2023-11-08 09:53:54,724 - __main__ - INFO - Fold 4 Epoch 24 Batch 0: Train Loss = 1.3078
2023-11-08 09:53:56,642 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9693 ------------
2023-11-08 09:53:56,644 - __main__ - INFO - Fold 4, mse = 35.6860, mad = 4.7254
2023-11-08 09:53:56,941 - __main__ - INFO - Fold 4 Epoch 25 Batch 0: Train Loss = 1.5211
2023-11-08 09:53:58,795 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9697 ------------
2023-11-08 09:53:58,799 - __main__ - INFO - Fold 4, mse = 35.5332, mad = 4.6124
2023-11-08 09:53:59,059 - __main__ - INFO - Fold 4 Epoch 26 Batch 0: Train Loss = 1.4303
2023-11-08 09:54:00,756 - __main__ - INFO - Fold 4, mse = 35.7155, mad = 4.7096
2023-11-08 09:54:01,009 - __main__ - INFO - Fold 4 Epoch 27 Batch 0: Train Loss = 1.5332
2023-11-08 09:54:02,538 - __main__ - INFO - Fold 4, mse = 35.5737, mad = 4.6316
2023-11-08 09:54:02,859 - __main__ - INFO - Fold 4 Epoch 28 Batch 0: Train Loss = 1.4859
2023-11-08 09:54:04,679 - __main__ - INFO - Fold 4, mse = 35.3645, mad = 4.6167
2023-11-08 09:54:04,979 - __main__ - INFO - Fold 4 Epoch 29 Batch 0: Train Loss = 1.4336
2023-11-08 09:54:06,743 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9700 ------------
2023-11-08 09:54:06,745 - __main__ - INFO - Fold 4, mse = 35.5185, mad = 4.6408
2023-11-08 09:54:07,044 - __main__ - INFO - Fold 4 Epoch 30 Batch 0: Train Loss = 1.4216
2023-11-08 09:54:08,913 - __main__ - INFO - Fold 4, epoch 30: Loss = 1.3689 Valid loss = 1.6845 MSE = 35.8714 AUROC = 0.9701
2023-11-08 09:54:08,914 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9701 ------------
2023-11-08 09:54:08,917 - __main__ - INFO - Fold 4, mse = 35.8714, mad = 4.6547
2023-11-08 09:54:09,201 - __main__ - INFO - Fold 4 Epoch 31 Batch 0: Train Loss = 1.2214
2023-11-08 09:54:10,867 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9707 ------------
2023-11-08 09:54:10,869 - __main__ - INFO - Fold 4, mse = 36.1949, mad = 4.7582
2023-11-08 09:54:11,151 - __main__ - INFO - Fold 4 Epoch 32 Batch 0: Train Loss = 1.6401
2023-11-08 09:54:12,695 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9712 ------------
2023-11-08 09:54:12,697 - __main__ - INFO - Fold 4, mse = 35.2248, mad = 4.5648
2023-11-08 09:54:12,958 - __main__ - INFO - Fold 4 Epoch 33 Batch 0: Train Loss = 1.2555
2023-11-08 09:54:14,718 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9718 ------------
2023-11-08 09:54:14,723 - __main__ - INFO - Fold 4, mse = 35.5665, mad = 4.6545
2023-11-08 09:54:15,095 - __main__ - INFO - Fold 4 Epoch 34 Batch 0: Train Loss = 1.5008
2023-11-08 09:54:16,886 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9728 ------------
2023-11-08 09:54:16,888 - __main__ - INFO - Fold 4, mse = 35.6252, mad = 4.6476
2023-11-08 09:54:17,264 - __main__ - INFO - Fold 4 Epoch 35 Batch 0: Train Loss = 1.2895
2023-11-08 09:54:18,582 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9737 ------------
2023-11-08 09:54:18,584 - __main__ - INFO - Fold 4, mse = 35.2797, mad = 4.6361
2023-11-08 09:54:18,923 - __main__ - INFO - Fold 4 Epoch 36 Batch 0: Train Loss = 1.4399
2023-11-08 09:54:20,460 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9740 ------------
2023-11-08 09:54:20,462 - __main__ - INFO - Fold 4, mse = 34.9272, mad = 4.5582
2023-11-08 09:54:20,871 - __main__ - INFO - Fold 4 Epoch 37 Batch 0: Train Loss = 1.4609
2023-11-08 09:54:22,367 - __main__ - INFO - Fold 4, mse = 34.8691, mad = 4.5112
2023-11-08 09:54:22,662 - __main__ - INFO - Fold 4 Epoch 38 Batch 0: Train Loss = 1.5307
2023-11-08 09:54:24,349 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9750 ------------
2023-11-08 09:54:24,351 - __main__ - INFO - Fold 4, mse = 34.9677, mad = 4.6056
2023-11-08 09:54:24,626 - __main__ - INFO - Fold 4 Epoch 39 Batch 0: Train Loss = 1.3431
2023-11-08 09:54:25,985 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9761 ------------
2023-11-08 09:54:25,987 - __main__ - INFO - Fold 4, mse = 35.1415, mad = 4.5989
2023-11-08 09:54:26,247 - __main__ - INFO - Fold 4 Epoch 40 Batch 0: Train Loss = 1.3262
2023-11-08 09:54:27,541 - __main__ - INFO - Fold 4, epoch 40: Loss = 1.3073 Valid loss = 1.7237 MSE = 34.8561 AUROC = 0.9766
2023-11-08 09:54:27,546 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9766 ------------
2023-11-08 09:54:27,549 - __main__ - INFO - Fold 4, mse = 34.8561, mad = 4.5494
2023-11-08 09:54:27,833 - __main__ - INFO - Fold 4 Epoch 41 Batch 0: Train Loss = 1.4743
2023-11-08 09:54:29,245 - __main__ - INFO - Fold 4, mse = 35.3301, mad = 4.6035
2023-11-08 09:54:29,553 - __main__ - INFO - Fold 4 Epoch 42 Batch 0: Train Loss = 1.1221
2023-11-08 09:54:31,106 - __main__ - INFO - Fold 4, mse = 35.5368, mad = 4.6522
2023-11-08 09:54:31,372 - __main__ - INFO - Fold 4 Epoch 43 Batch 0: Train Loss = 1.1885
2023-11-08 09:54:32,991 - __main__ - INFO - Fold 4, mse = 35.5714, mad = 4.6155
2023-11-08 09:54:33,202 - __main__ - INFO - Fold 4 Epoch 44 Batch 0: Train Loss = 1.2690
2023-11-08 09:54:34,707 - __main__ - INFO - Fold 4, mse = 36.1619, mad = 4.7063
2023-11-08 09:54:35,045 - __main__ - INFO - Fold 4 Epoch 45 Batch 0: Train Loss = 1.3174
2023-11-08 09:54:36,881 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9767 ------------
2023-11-08 09:54:36,883 - __main__ - INFO - Fold 4, mse = 36.2191, mad = 4.6570
2023-11-08 09:54:37,179 - __main__ - INFO - Fold 4 Epoch 46 Batch 0: Train Loss = 1.2092
2023-11-08 09:54:38,829 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9768 ------------
2023-11-08 09:54:38,831 - __main__ - INFO - Fold 4, mse = 35.9999, mad = 4.6907
2023-11-08 09:54:39,115 - __main__ - INFO - Fold 4 Epoch 47 Batch 0: Train Loss = 1.3702
2023-11-08 09:54:41,239 - __main__ - INFO - Fold 4, mse = 35.5340, mad = 4.6168
2023-11-08 09:54:41,536 - __main__ - INFO - Fold 4 Epoch 48 Batch 0: Train Loss = 1.5713
2023-11-08 09:54:43,292 - __main__ - INFO - Fold 4, mse = 35.5923, mad = 4.6409
2023-11-08 09:54:43,608 - __main__ - INFO - Fold 4 Epoch 49 Batch 0: Train Loss = 1.1187
2023-11-08 09:54:45,481 - __main__ - INFO - Fold 4, mse = 35.4851, mad = 4.5744
2023-11-08 09:54:45,856 - __main__ - INFO - Fold 4 Epoch 50 Batch 0: Train Loss = 1.2964
2023-11-08 09:54:47,704 - __main__ - INFO - Fold 4, epoch 50: Loss = 1.2242 Valid loss = 1.6750 MSE = 35.4668 AUROC = 0.9767
2023-11-08 09:54:47,707 - __main__ - INFO - Fold 4, mse = 35.4668, mad = 4.5939
2023-11-08 09:54:47,996 - __main__ - INFO - Fold 4 Epoch 51 Batch 0: Train Loss = 1.0401
2023-11-08 09:54:49,944 - __main__ - INFO - Fold 4, mse = 35.7600, mad = 4.6714
2023-11-08 09:54:50,335 - __main__ - INFO - Fold 4 Epoch 52 Batch 0: Train Loss = 1.1044
2023-11-08 09:54:52,059 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9770 ------------
2023-11-08 09:54:52,061 - __main__ - INFO - Fold 4, mse = 35.2867, mad = 4.5751
2023-11-08 09:54:52,282 - __main__ - INFO - Fold 4 Epoch 53 Batch 0: Train Loss = 1.2736
2023-11-08 09:54:53,590 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9779 ------------
2023-11-08 09:54:53,592 - __main__ - INFO - Fold 4, mse = 36.0405, mad = 4.6971
2023-11-08 09:54:53,860 - __main__ - INFO - Fold 4 Epoch 54 Batch 0: Train Loss = 1.1787
2023-11-08 09:54:55,130 - __main__ - INFO - Fold 4, mse = 35.8804, mad = 4.6505
2023-11-08 09:54:55,364 - __main__ - INFO - Fold 4 Epoch 55 Batch 0: Train Loss = 1.2081
2023-11-08 09:54:56,735 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9787 ------------
2023-11-08 09:54:56,737 - __main__ - INFO - Fold 4, mse = 36.4309, mad = 4.7115
2023-11-08 09:54:57,075 - __main__ - INFO - Fold 4 Epoch 56 Batch 0: Train Loss = 1.1996
2023-11-08 09:54:58,901 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9793 ------------
2023-11-08 09:54:58,904 - __main__ - INFO - Fold 4, mse = 36.5021, mad = 4.7252
2023-11-08 09:54:59,199 - __main__ - INFO - Fold 4 Epoch 57 Batch 0: Train Loss = 1.2443
2023-11-08 09:55:00,712 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9796 ------------
2023-11-08 09:55:00,714 - __main__ - INFO - Fold 4, mse = 35.6462, mad = 4.5937
2023-11-08 09:55:01,022 - __main__ - INFO - Fold 4 Epoch 58 Batch 0: Train Loss = 1.2154
2023-11-08 09:55:02,650 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9800 ------------
2023-11-08 09:55:02,652 - __main__ - INFO - Fold 4, mse = 35.8906, mad = 4.6738
2023-11-08 09:55:02,898 - __main__ - INFO - Fold 4 Epoch 59 Batch 0: Train Loss = 1.3170
2023-11-08 09:55:04,563 - __main__ - INFO - Fold 4, mse = 35.6584, mad = 4.5704
2023-11-08 09:55:04,843 - __main__ - INFO - Fold 4 Epoch 60 Batch 0: Train Loss = 1.2832
2023-11-08 09:55:06,621 - __main__ - INFO - Fold 4, epoch 60: Loss = 1.2170 Valid loss = 1.7131 MSE = 35.5275 AUROC = 0.9796
2023-11-08 09:55:06,623 - __main__ - INFO - Fold 4, mse = 35.5275, mad = 4.6601
2023-11-08 09:55:06,876 - __main__ - INFO - Fold 4 Epoch 61 Batch 0: Train Loss = 0.9740
2023-11-08 09:55:08,809 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9800 ------------
2023-11-08 09:55:08,811 - __main__ - INFO - Fold 4, mse = 35.2947, mad = 4.5725
2023-11-08 09:55:09,199 - __main__ - INFO - Fold 4 Epoch 62 Batch 0: Train Loss = 1.1731
2023-11-08 09:55:10,840 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9806 ------------
2023-11-08 09:55:10,842 - __main__ - INFO - Fold 4, mse = 35.8790, mad = 4.6745
2023-11-08 09:55:11,114 - __main__ - INFO - Fold 4 Epoch 63 Batch 0: Train Loss = 1.2380
2023-11-08 09:55:12,841 - __main__ - INFO - Fold 4, mse = 35.8421, mad = 4.6366
2023-11-08 09:55:13,092 - __main__ - INFO - Fold 4 Epoch 64 Batch 0: Train Loss = 1.0911
2023-11-08 09:55:14,745 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9809 ------------
2023-11-08 09:55:14,747 - __main__ - INFO - Fold 4, mse = 36.1804, mad = 4.7011
2023-11-08 09:55:15,044 - __main__ - INFO - Fold 4 Epoch 65 Batch 0: Train Loss = 1.1990
2023-11-08 09:55:16,741 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9814 ------------
2023-11-08 09:55:16,743 - __main__ - INFO - Fold 4, mse = 35.8179, mad = 4.6456
2023-11-08 09:55:16,985 - __main__ - INFO - Fold 4 Epoch 66 Batch 0: Train Loss = 1.1404
2023-11-08 09:55:18,831 - __main__ - INFO - Fold 4, mse = 35.6723, mad = 4.6371
2023-11-08 09:55:19,151 - __main__ - INFO - Fold 4 Epoch 67 Batch 0: Train Loss = 1.3335
2023-11-08 09:55:20,926 - __main__ - INFO - Fold 4, mse = 35.4569, mad = 4.6043
2023-11-08 09:55:21,172 - __main__ - INFO - Fold 4 Epoch 68 Batch 0: Train Loss = 1.2652
2023-11-08 09:55:23,072 - __main__ - INFO - Fold 4, mse = 36.3987, mad = 4.7229
2023-11-08 09:55:23,455 - __main__ - INFO - Fold 4 Epoch 69 Batch 0: Train Loss = 1.3323
2023-11-08 09:55:25,029 - __main__ - INFO - Fold 4, mse = 35.4695, mad = 4.5955
2023-11-08 09:55:25,327 - __main__ - INFO - Fold 4 Epoch 70 Batch 0: Train Loss = 1.2701
2023-11-08 09:55:26,922 - __main__ - INFO - Fold 4, epoch 70: Loss = 1.2155 Valid loss = 1.7557 MSE = 36.0783 AUROC = 0.9805
2023-11-08 09:55:26,925 - __main__ - INFO - Fold 4, mse = 36.0783, mad = 4.6786
2023-11-08 09:55:27,208 - __main__ - INFO - Fold 4 Epoch 71 Batch 0: Train Loss = 1.2311
2023-11-08 09:55:28,663 - __main__ - INFO - Fold 4, mse = 36.3222, mad = 4.6833
2023-11-08 09:55:28,933 - __main__ - INFO - Fold 4 Epoch 72 Batch 0: Train Loss = 1.1508
2023-11-08 09:55:30,647 - __main__ - INFO - Fold 4, mse = 36.0456, mad = 4.6477
2023-11-08 09:55:30,982 - __main__ - INFO - Fold 4 Epoch 73 Batch 0: Train Loss = 1.1270
2023-11-08 09:55:32,699 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9816 ------------
2023-11-08 09:55:32,702 - __main__ - INFO - Fold 4, mse = 36.5917, mad = 4.7723
2023-11-08 09:55:33,024 - __main__ - INFO - Fold 4 Epoch 74 Batch 0: Train Loss = 1.2546
2023-11-08 09:55:34,778 - __main__ - INFO - Fold 4, mse = 35.9961, mad = 4.5807
2023-11-08 09:55:35,116 - __main__ - INFO - Fold 4 Epoch 75 Batch 0: Train Loss = 1.1529
2023-11-08 09:55:36,914 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9817 ------------
2023-11-08 09:55:36,916 - __main__ - INFO - Fold 4, mse = 35.9575, mad = 4.6895
2023-11-08 09:55:37,176 - __main__ - INFO - Fold 4 Epoch 76 Batch 0: Train Loss = 1.2692
2023-11-08 09:55:38,918 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9817 ------------
2023-11-08 09:55:38,921 - __main__ - INFO - Fold 4, mse = 35.5704, mad = 4.5982
2023-11-08 09:55:39,262 - __main__ - INFO - Fold 4 Epoch 77 Batch 0: Train Loss = 1.3749
2023-11-08 09:55:40,874 - __main__ - INFO - Fold 4, mse = 36.0741, mad = 4.6724
2023-11-08 09:55:41,163 - __main__ - INFO - Fold 4 Epoch 78 Batch 0: Train Loss = 1.3340
2023-11-08 09:55:42,877 - __main__ - INFO - Fold 4, mse = 36.3089, mad = 4.7159
2023-11-08 09:55:43,091 - __main__ - INFO - Fold 4 Epoch 79 Batch 0: Train Loss = 1.0268
2023-11-08 09:55:44,768 - __main__ - INFO - Fold 4, mse = 35.9039, mad = 4.6417
2023-11-08 09:55:45,015 - __main__ - INFO - Fold 4 Epoch 80 Batch 0: Train Loss = 1.1607
2023-11-08 09:55:46,770 - __main__ - INFO - Fold 4, epoch 80: Loss = 1.1646 Valid loss = 1.7272 MSE = 35.8010 AUROC = 0.9820
2023-11-08 09:55:46,774 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9820 ------------
2023-11-08 09:55:46,778 - __main__ - INFO - Fold 4, mse = 35.8010, mad = 4.6960
2023-11-08 09:55:47,013 - __main__ - INFO - Fold 4 Epoch 81 Batch 0: Train Loss = 1.3691
2023-11-08 09:55:48,905 - __main__ - INFO - Fold 4, mse = 35.2620, mad = 4.4898
2023-11-08 09:55:49,223 - __main__ - INFO - Fold 4 Epoch 82 Batch 0: Train Loss = 1.6008
2023-11-08 09:55:50,991 - __main__ - INFO - Fold 4, mse = 36.7894, mad = 4.8134
2023-11-08 09:55:51,282 - __main__ - INFO - Fold 4 Epoch 83 Batch 0: Train Loss = 1.1132
2023-11-08 09:55:53,036 - __main__ - INFO - Fold 4, mse = 36.1414, mad = 4.6555
2023-11-08 09:55:53,481 - __main__ - INFO - Fold 4 Epoch 84 Batch 0: Train Loss = 1.0177
2023-11-08 09:55:55,277 - __main__ - INFO - Fold 4, mse = 36.7155, mad = 4.7485
2023-11-08 09:55:55,534 - __main__ - INFO - Fold 4 Epoch 85 Batch 0: Train Loss = 1.3350
2023-11-08 09:55:57,278 - __main__ - INFO - Fold 4, mse = 36.1034, mad = 4.6170
2023-11-08 09:55:57,507 - __main__ - INFO - Fold 4 Epoch 86 Batch 0: Train Loss = 1.3682
2023-11-08 09:55:59,311 - __main__ - INFO - Fold 4, mse = 36.4400, mad = 4.7014
2023-11-08 09:55:59,709 - __main__ - INFO - Fold 4 Epoch 87 Batch 0: Train Loss = 1.3539
2023-11-08 09:56:01,670 - __main__ - INFO - Fold 4, mse = 36.5225, mad = 4.6340
2023-11-08 09:56:02,006 - __main__ - INFO - Fold 4 Epoch 88 Batch 0: Train Loss = 1.3294
2023-11-08 09:56:03,730 - __main__ - INFO - Fold 4, mse = 36.8835, mad = 4.7224
2023-11-08 09:56:03,943 - __main__ - INFO - Fold 4 Epoch 89 Batch 0: Train Loss = 0.9986
2023-11-08 09:56:05,245 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9820 ------------
2023-11-08 09:56:05,249 - __main__ - INFO - Fold 4, mse = 36.3695, mad = 4.6712
2023-11-08 09:56:05,483 - __main__ - INFO - Fold 4 Epoch 90 Batch 0: Train Loss = 1.1075
2023-11-08 09:56:07,307 - __main__ - INFO - Fold 4, epoch 90: Loss = 1.1871 Valid loss = 1.7193 MSE = 36.0734 AUROC = 0.9825
2023-11-08 09:56:07,309 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9825 ------------
2023-11-08 09:56:07,311 - __main__ - INFO - Fold 4, mse = 36.0734, mad = 4.6807
2023-11-08 09:56:07,601 - __main__ - INFO - Fold 4 Epoch 91 Batch 0: Train Loss = 1.0747
2023-11-08 09:56:09,018 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9826 ------------
2023-11-08 09:56:09,022 - __main__ - INFO - Fold 4, mse = 36.4317, mad = 4.6965
2023-11-08 09:56:09,290 - __main__ - INFO - Fold 4 Epoch 92 Batch 0: Train Loss = 1.0572
2023-11-08 09:56:11,205 - __main__ - INFO - Fold 4, mse = 36.9062, mad = 4.6910
2023-11-08 09:56:11,502 - __main__ - INFO - Fold 4 Epoch 93 Batch 0: Train Loss = 1.2781
2023-11-08 09:56:13,241 - __main__ - INFO - Fold 4, mse = 36.7832, mad = 4.7144
2023-11-08 09:56:13,525 - __main__ - INFO - Fold 4 Epoch 94 Batch 0: Train Loss = 1.0881
2023-11-08 09:56:15,315 - __main__ - INFO - Fold 4, mse = 36.3950, mad = 4.6717
2023-11-08 09:56:15,695 - __main__ - INFO - Fold 4 Epoch 95 Batch 0: Train Loss = 0.9077
2023-11-08 09:56:17,254 - __main__ - INFO - Fold 4, mse = 35.8440, mad = 4.6027
2023-11-08 09:56:17,490 - __main__ - INFO - Fold 4 Epoch 96 Batch 0: Train Loss = 1.1328
2023-11-08 09:56:19,313 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9829 ------------
2023-11-08 09:56:19,315 - __main__ - INFO - Fold 4, mse = 36.3782, mad = 4.6772
2023-11-08 09:56:19,656 - __main__ - INFO - Fold 4 Epoch 97 Batch 0: Train Loss = 0.9500
2023-11-08 09:56:21,558 - __main__ - INFO - Fold 4, mse = 36.4294, mad = 4.7092
2023-11-08 09:56:21,787 - __main__ - INFO - Fold 4 Epoch 98 Batch 0: Train Loss = 1.4429
2023-11-08 09:56:23,703 - __main__ - INFO - Fold 4, mse = 35.7713, mad = 4.6015
2023-11-08 09:56:24,032 - __main__ - INFO - Fold 4 Epoch 99 Batch 0: Train Loss = 1.1383
2023-11-08 09:56:25,768 - __main__ - INFO - Fold 4, mse = 36.2886, mad = 4.6993
2023-11-08 09:56:26,088 - __main__ - INFO - Fold 4 Epoch 100 Batch 0: Train Loss = 0.9010
2023-11-08 09:56:27,733 - __main__ - INFO - Fold 4, epoch 100: Loss = 1.1622 Valid loss = 1.7843 MSE = 36.5668 AUROC = 0.9814
2023-11-08 09:56:27,735 - __main__ - INFO - Fold 4, mse = 36.5668, mad = 4.7080
2023-11-08 09:56:28,054 - __main__ - INFO - Fold 4 Epoch 101 Batch 0: Train Loss = 1.1458
2023-11-08 09:56:29,555 - __main__ - INFO - Fold 4, mse = 36.5772, mad = 4.7124
2023-11-08 09:56:29,849 - __main__ - INFO - Fold 4 Epoch 102 Batch 0: Train Loss = 1.0494
2023-11-08 09:56:31,576 - __main__ - INFO - Fold 4, mse = 36.5914, mad = 4.7223
2023-11-08 09:56:31,826 - __main__ - INFO - Fold 4 Epoch 103 Batch 0: Train Loss = 0.9904
2023-11-08 09:56:33,507 - __main__ - INFO - Fold 4, mse = 36.8380, mad = 4.7602
2023-11-08 09:56:33,767 - __main__ - INFO - Fold 4 Epoch 104 Batch 0: Train Loss = 0.9671
2023-11-08 09:56:35,246 - __main__ - INFO - Fold 4, mse = 36.1003, mad = 4.6625
2023-11-08 09:56:35,593 - __main__ - INFO - Fold 4 Epoch 105 Batch 0: Train Loss = 1.1866
2023-11-08 09:56:37,034 - __main__ - INFO - Fold 4, mse = 35.9392, mad = 4.6489
2023-11-08 09:56:37,326 - __main__ - INFO - Fold 4 Epoch 106 Batch 0: Train Loss = 0.9079
2023-11-08 09:56:39,131 - __main__ - INFO - Fold 4, mse = 36.1773, mad = 4.6619
2023-11-08 09:56:39,417 - __main__ - INFO - Fold 4 Epoch 107 Batch 0: Train Loss = 0.9771
2023-11-08 09:56:41,049 - __main__ - INFO - Fold 4, mse = 36.8274, mad = 4.7594
2023-11-08 09:56:41,375 - __main__ - INFO - Fold 4 Epoch 108 Batch 0: Train Loss = 1.0574
2023-11-08 09:56:43,052 - __main__ - INFO - Fold 4, mse = 36.5145, mad = 4.6844
2023-11-08 09:56:43,309 - __main__ - INFO - Fold 4 Epoch 109 Batch 0: Train Loss = 1.1520
2023-11-08 09:56:44,908 - __main__ - INFO - Fold 4, mse = 36.5797, mad = 4.6973
2023-11-08 09:56:45,190 - __main__ - INFO - Fold 4 Epoch 110 Batch 0: Train Loss = 0.9270
2023-11-08 09:56:46,886 - __main__ - INFO - Fold 4, epoch 110: Loss = 1.1255 Valid loss = 1.8406 MSE = 36.4451 AUROC = 0.9817
2023-11-08 09:56:46,887 - __main__ - INFO - Fold 4, mse = 36.4451, mad = 4.6719
2023-11-08 09:56:47,207 - __main__ - INFO - Fold 4 Epoch 111 Batch 0: Train Loss = 1.1083
2023-11-08 09:56:48,452 - __main__ - INFO - Fold 4, mse = 37.0600, mad = 4.7445
2023-11-08 09:56:48,754 - __main__ - INFO - Fold 4 Epoch 112 Batch 0: Train Loss = 1.1512
2023-11-08 09:56:50,321 - __main__ - INFO - Fold 4, mse = 36.8829, mad = 4.6577
2023-11-08 09:56:50,604 - __main__ - INFO - Fold 4 Epoch 113 Batch 0: Train Loss = 1.0893
2023-11-08 09:56:52,508 - __main__ - INFO - Fold 4, mse = 37.9058, mad = 4.8426
2023-11-08 09:56:52,875 - __main__ - INFO - Fold 4 Epoch 114 Batch 0: Train Loss = 1.0145
2023-11-08 09:56:54,306 - __main__ - INFO - Fold 4, mse = 36.9037, mad = 4.6772
2023-11-08 09:56:54,526 - __main__ - INFO - Fold 4 Epoch 115 Batch 0: Train Loss = 1.2108
2023-11-08 09:56:56,186 - __main__ - INFO - Fold 4, mse = 36.8981, mad = 4.7007
2023-11-08 09:56:56,519 - __main__ - INFO - Fold 4 Epoch 116 Batch 0: Train Loss = 1.1003
2023-11-08 09:56:58,131 - __main__ - INFO - Fold 4, mse = 36.5837, mad = 4.7148
2023-11-08 09:56:58,483 - __main__ - INFO - Fold 4 Epoch 117 Batch 0: Train Loss = 1.1348
2023-11-08 09:57:00,198 - __main__ - INFO - Fold 4, mse = 36.2399, mad = 4.6324
2023-11-08 09:57:00,485 - __main__ - INFO - Fold 4 Epoch 118 Batch 0: Train Loss = 0.9089
2023-11-08 09:57:02,096 - __main__ - INFO - Fold 4, mse = 36.5298, mad = 4.7165
2023-11-08 09:57:02,404 - __main__ - INFO - Fold 4 Epoch 119 Batch 0: Train Loss = 0.9141
2023-11-08 09:57:04,284 - __main__ - INFO - Fold 4, mse = 36.4538, mad = 4.6699
2023-11-08 09:57:04,600 - __main__ - INFO - Fold 4 Epoch 120 Batch 0: Train Loss = 1.0957
2023-11-08 09:57:06,499 - __main__ - INFO - Fold 4, epoch 120: Loss = 1.0957 Valid loss = 1.8293 MSE = 36.3329 AUROC = 0.9819
2023-11-08 09:57:06,501 - __main__ - INFO - Fold 4, mse = 36.3329, mad = 4.6849
2023-11-08 09:57:06,784 - __main__ - INFO - Fold 4 Epoch 121 Batch 0: Train Loss = 1.0679
2023-11-08 09:57:08,693 - __main__ - INFO - Fold 4, mse = 35.9959, mad = 4.6152
2023-11-08 09:57:08,947 - __main__ - INFO - Fold 4 Epoch 122 Batch 0: Train Loss = 1.1232
2023-11-08 09:57:10,767 - __main__ - INFO - Fold 4, mse = 36.5546, mad = 4.6827
2023-11-08 09:57:11,052 - __main__ - INFO - Fold 4 Epoch 123 Batch 0: Train Loss = 1.0019
2023-11-08 09:57:12,845 - __main__ - INFO - Fold 4, mse = 37.1362, mad = 4.7160
2023-11-08 09:57:13,074 - __main__ - INFO - Fold 4 Epoch 124 Batch 0: Train Loss = 0.8903
2023-11-08 09:57:14,862 - __main__ - INFO - Fold 4, mse = 37.3819, mad = 4.7697
2023-11-08 09:57:15,119 - __main__ - INFO - Fold 4 Epoch 125 Batch 0: Train Loss = 1.0782
2023-11-08 09:57:16,829 - __main__ - INFO - Fold 4, mse = 37.5493, mad = 4.7893
2023-11-08 09:57:17,204 - __main__ - INFO - Fold 4 Epoch 126 Batch 0: Train Loss = 1.1484
2023-11-08 09:57:19,270 - __main__ - INFO - Fold 4, mse = 36.7160, mad = 4.7174
2023-11-08 09:57:19,571 - __main__ - INFO - Fold 4 Epoch 127 Batch 0: Train Loss = 1.1105
2023-11-08 09:57:21,121 - __main__ - INFO - Fold 4, mse = 36.1873, mad = 4.6128
2023-11-08 09:57:21,392 - __main__ - INFO - Fold 4 Epoch 128 Batch 0: Train Loss = 1.1263
2023-11-08 09:57:23,191 - __main__ - INFO - Fold 4, mse = 37.0106, mad = 4.7370
2023-11-08 09:57:23,463 - __main__ - INFO - Fold 4 Epoch 129 Batch 0: Train Loss = 1.1496
2023-11-08 09:57:24,797 - __main__ - INFO - Fold 4, mse = 37.1077, mad = 4.7055
2023-11-08 09:57:25,081 - __main__ - INFO - Fold 4 Epoch 130 Batch 0: Train Loss = 1.0174
2023-11-08 09:57:26,969 - __main__ - INFO - Fold 4, epoch 130: Loss = 1.0327 Valid loss = 1.8454 MSE = 37.2196 AUROC = 0.9817
2023-11-08 09:57:26,970 - __main__ - INFO - Fold 4, mse = 37.2196, mad = 4.7413
2023-11-08 09:57:27,299 - __main__ - INFO - Fold 4 Epoch 131 Batch 0: Train Loss = 0.9943
2023-11-08 09:57:28,989 - __main__ - INFO - Fold 4, mse = 36.2267, mad = 4.6155
2023-11-08 09:57:29,266 - __main__ - INFO - Fold 4 Epoch 132 Batch 0: Train Loss = 1.0369
2023-11-08 09:57:31,005 - __main__ - INFO - Fold 4, mse = 37.1035, mad = 4.7572
2023-11-08 09:57:31,242 - __main__ - INFO - Fold 4 Epoch 133 Batch 0: Train Loss = 1.0291
2023-11-08 09:57:32,871 - __main__ - INFO - Fold 4, mse = 37.0511, mad = 4.6991
2023-11-08 09:57:33,079 - __main__ - INFO - Fold 4 Epoch 134 Batch 0: Train Loss = 1.1731
2023-11-08 09:57:34,777 - __main__ - INFO - Fold 4, mse = 37.6680, mad = 4.8199
2023-11-08 09:57:35,094 - __main__ - INFO - Fold 4 Epoch 135 Batch 0: Train Loss = 0.9626
2023-11-08 09:57:36,892 - __main__ - INFO - Fold 4, mse = 36.9199, mad = 4.6777
2023-11-08 09:57:37,151 - __main__ - INFO - Fold 4 Epoch 136 Batch 0: Train Loss = 1.0103
2023-11-08 09:57:38,595 - __main__ - INFO - Fold 4, mse = 37.3979, mad = 4.7819
2023-11-08 09:57:38,859 - __main__ - INFO - Fold 4 Epoch 137 Batch 0: Train Loss = 1.0905
2023-11-08 09:57:40,503 - __main__ - INFO - Fold 4, mse = 36.3436, mad = 4.6171
2023-11-08 09:57:40,814 - __main__ - INFO - Fold 4 Epoch 138 Batch 0: Train Loss = 1.0648
2023-11-08 09:57:42,601 - __main__ - INFO - Fold 4, mse = 37.1475, mad = 4.7698
2023-11-08 09:57:42,898 - __main__ - INFO - Fold 4 Epoch 139 Batch 0: Train Loss = 1.2659
2023-11-08 09:57:44,733 - __main__ - INFO - Fold 4, mse = 36.5862, mad = 4.6147
2023-11-08 09:57:45,025 - __main__ - INFO - Fold 4 Epoch 140 Batch 0: Train Loss = 0.8968
2023-11-08 09:57:46,568 - __main__ - INFO - Fold 4, epoch 140: Loss = 1.0365 Valid loss = 1.8606 MSE = 36.5606 AUROC = 0.9824
2023-11-08 09:57:46,570 - __main__ - INFO - Fold 4, mse = 36.5606, mad = 4.6917
2023-11-08 09:57:46,855 - __main__ - INFO - Fold 4 Epoch 141 Batch 0: Train Loss = 1.0781
2023-11-08 09:57:48,634 - __main__ - INFO - Fold 4, mse = 35.9219, mad = 4.5622
2023-11-08 09:57:48,922 - __main__ - INFO - Fold 4 Epoch 142 Batch 0: Train Loss = 1.0126
2023-11-08 09:57:50,643 - __main__ - INFO - Fold 4, mse = 36.6051, mad = 4.6388
2023-11-08 09:57:51,010 - __main__ - INFO - Fold 4 Epoch 143 Batch 0: Train Loss = 1.0132
2023-11-08 09:57:52,544 - __main__ - INFO - Fold 4, mse = 36.9382, mad = 4.6866
2023-11-08 09:57:52,782 - __main__ - INFO - Fold 4 Epoch 144 Batch 0: Train Loss = 0.9183
2023-11-08 09:57:54,348 - __main__ - INFO - Fold 4, mse = 36.4285, mad = 4.6383
2023-11-08 09:57:54,608 - __main__ - INFO - Fold 4 Epoch 145 Batch 0: Train Loss = 0.9256
2023-11-08 09:57:56,148 - __main__ - INFO - Fold 4, mse = 36.0719, mad = 4.5943
2023-11-08 09:57:56,356 - __main__ - INFO - Fold 4 Epoch 146 Batch 0: Train Loss = 1.1458
2023-11-08 09:57:57,889 - __main__ - INFO - Fold 4, mse = 36.1182, mad = 4.6046
2023-11-08 09:57:58,213 - __main__ - INFO - Fold 4 Epoch 147 Batch 0: Train Loss = 1.0998
2023-11-08 09:57:59,738 - __main__ - INFO - Fold 4, mse = 37.0565, mad = 4.7223
2023-11-08 09:58:00,031 - __main__ - INFO - Fold 4 Epoch 148 Batch 0: Train Loss = 0.9802
2023-11-08 09:58:01,614 - __main__ - INFO - Fold 4, mse = 37.0458, mad = 4.6910
2023-11-08 09:58:01,818 - __main__ - INFO - Fold 4 Epoch 149 Batch 0: Train Loss = 1.1812
2023-11-08 09:58:03,353 - __main__ - INFO - Fold 4, mse = 36.9577, mad = 4.6542
2023-11-08 09:58:03,896 - __main__ - INFO - Fold 5 Epoch 0 Batch 0: Train Loss = 2.3331
2023-11-08 09:58:05,887 - __main__ - INFO - Fold 5, epoch 0: Loss = 2.3864 Valid loss = 3.0573 MSE = 52.3130 AUROC = 0.5277
2023-11-08 09:58:05,889 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 52.3130 ------------
2023-11-08 09:58:06,065 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.5277 ------------
2023-11-08 09:58:06,070 - __main__ - INFO - Fold 5, mse = 52.3130, mad = 5.1131
2023-11-08 09:58:06,351 - __main__ - INFO - Fold 5 Epoch 1 Batch 0: Train Loss = 2.5513
2023-11-08 09:58:08,161 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 50.0828 ------------
2023-11-08 09:58:08,351 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.8702 ------------
2023-11-08 09:58:08,355 - __main__ - INFO - Fold 5, mse = 50.0828, mad = 4.9999
2023-11-08 09:58:08,648 - __main__ - INFO - Fold 5 Epoch 2 Batch 0: Train Loss = 2.2458
2023-11-08 09:58:10,355 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 48.4071 ------------
2023-11-08 09:58:10,586 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9275 ------------
2023-11-08 09:58:10,589 - __main__ - INFO - Fold 5, mse = 48.4071, mad = 4.8292
2023-11-08 09:58:10,800 - __main__ - INFO - Fold 5 Epoch 3 Batch 0: Train Loss = 2.2605
2023-11-08 09:58:12,336 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 47.1106 ------------
2023-11-08 09:58:12,520 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9506 ------------
2023-11-08 09:58:12,522 - __main__ - INFO - Fold 5, mse = 47.1106, mad = 4.7456
2023-11-08 09:58:12,804 - __main__ - INFO - Fold 5 Epoch 4 Batch 0: Train Loss = 1.7635
2023-11-08 09:58:14,272 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 46.3357 ------------
2023-11-08 09:58:14,483 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9666 ------------
2023-11-08 09:58:14,486 - __main__ - INFO - Fold 5, mse = 46.3357, mad = 4.7781
2023-11-08 09:58:14,799 - __main__ - INFO - Fold 5 Epoch 5 Batch 0: Train Loss = 2.0514
2023-11-08 09:58:16,463 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.6979 ------------
2023-11-08 09:58:16,661 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9779 ------------
2023-11-08 09:58:16,663 - __main__ - INFO - Fold 5, mse = 45.6979, mad = 4.6877
2023-11-08 09:58:16,989 - __main__ - INFO - Fold 5 Epoch 6 Batch 0: Train Loss = 2.0180
2023-11-08 09:58:18,606 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.2809 ------------
2023-11-08 09:58:18,789 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9858 ------------
2023-11-08 09:58:18,791 - __main__ - INFO - Fold 5, mse = 45.2809, mad = 4.5911
2023-11-08 09:58:19,132 - __main__ - INFO - Fold 5 Epoch 7 Batch 0: Train Loss = 1.8112
2023-11-08 09:58:20,793 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 45.1193 ------------
2023-11-08 09:58:20,940 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9905 ------------
2023-11-08 09:58:20,943 - __main__ - INFO - Fold 5, mse = 45.1193, mad = 4.6437
2023-11-08 09:58:21,323 - __main__ - INFO - Fold 5 Epoch 8 Batch 0: Train Loss = 1.4664
2023-11-08 09:58:22,955 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.7928 ------------
2023-11-08 09:58:23,138 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9922 ------------
2023-11-08 09:58:23,141 - __main__ - INFO - Fold 5, mse = 44.7928, mad = 4.5843
2023-11-08 09:58:23,444 - __main__ - INFO - Fold 5 Epoch 9 Batch 0: Train Loss = 1.7463
2023-11-08 09:58:25,336 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.7214 ------------
2023-11-08 09:58:25,478 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9922 ------------
2023-11-08 09:58:25,481 - __main__ - INFO - Fold 5, mse = 44.7214, mad = 4.6380
2023-11-08 09:58:25,766 - __main__ - INFO - Fold 5 Epoch 10 Batch 0: Train Loss = 1.4983
2023-11-08 09:58:27,235 - __main__ - INFO - Fold 5, epoch 10: Loss = 1.6136 Valid loss = 2.5126 MSE = 44.7585 AUROC = 0.9926
2023-11-08 09:58:27,237 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9926 ------------
2023-11-08 09:58:27,239 - __main__ - INFO - Fold 5, mse = 44.7585, mad = 4.5437
2023-11-08 09:58:27,531 - __main__ - INFO - Fold 5 Epoch 11 Batch 0: Train Loss = 1.6591
2023-11-08 09:58:29,307 - __main__ - INFO - Fold 5, mse = 45.0117, mad = 4.5893
2023-11-08 09:58:29,576 - __main__ - INFO - Fold 5 Epoch 12 Batch 0: Train Loss = 1.3497
2023-11-08 09:58:31,062 - __main__ - INFO - Fold 5, mse = 45.2621, mad = 4.6740
2023-11-08 09:58:31,434 - __main__ - INFO - Fold 5 Epoch 13 Batch 0: Train Loss = 1.5472
2023-11-08 09:58:33,433 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9928 ------------
2023-11-08 09:58:33,435 - __main__ - INFO - Fold 5, mse = 45.0992, mad = 4.5735
2023-11-08 09:58:33,816 - __main__ - INFO - Fold 5 Epoch 14 Batch 0: Train Loss = 1.6191
2023-11-08 09:58:35,467 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9929 ------------
2023-11-08 09:58:35,469 - __main__ - INFO - Fold 5, mse = 45.5008, mad = 4.7043
2023-11-08 09:58:35,845 - __main__ - INFO - Fold 5 Epoch 15 Batch 0: Train Loss = 1.2640
2023-11-08 09:58:37,593 - __main__ - INFO - ------------ auroc-BEST model - auroc: 0.9929 ------------
2023-11-08 09:58:37,595 - __main__ - INFO - Fold 5, mse = 45.3964, mad = 4.6931
2023-11-08 09:58:37,869 - __main__ - INFO - Fold 5 Epoch 16 Batch 0: Train Loss = 1.3380
2023-11-08 09:58:39,492 - __main__ - INFO - Fold 5, mse = 45.2795, mad = 4.6143
2023-11-08 09:58:39,876 - __main__ - INFO - Fold 5 Epoch 17 Batch 0: Train Loss = 1.3180
2023-11-08 09:58:41,743 - __main__ - INFO - Fold 5, mse = 45.7156, mad = 4.6767
2023-11-08 09:58:42,053 - __main__ - INFO - Fold 5 Epoch 18 Batch 0: Train Loss = 1.4511
2023-11-08 09:58:43,742 - __main__ - INFO - Fold 5, mse = 45.7284, mad = 4.5989
2023-11-08 09:58:43,966 - __main__ - INFO - Fold 5 Epoch 19 Batch 0: Train Loss = 1.2628
2023-11-08 09:58:45,194 - __main__ - INFO - Fold 5, mse = 45.7640, mad = 4.6621
2023-11-08 09:58:45,481 - __main__ - INFO - Fold 5 Epoch 20 Batch 0: Train Loss = 1.1408
2023-11-08 09:58:47,045 - __main__ - INFO - Fold 5, epoch 20: Loss = 1.4215 Valid loss = 2.4794 MSE = 45.8000 AUROC = 0.9929
2023-11-08 09:58:47,047 - __main__ - INFO - Fold 5, mse = 45.8000, mad = 4.6045
2023-11-08 09:58:47,336 - __main__ - INFO - Fold 5 Epoch 21 Batch 0: Train Loss = 1.2879
2023-11-08 09:58:48,824 - __main__ - INFO - Fold 5, mse = 45.7515, mad = 4.5403
2023-11-08 09:58:49,158 - __main__ - INFO - Fold 5 Epoch 22 Batch 0: Train Loss = 1.4319
2023-11-08 09:58:50,827 - __main__ - INFO - Fold 5, mse = 45.7058, mad = 4.6798
2023-11-08 09:58:51,143 - __main__ - INFO - Fold 5 Epoch 23 Batch 0: Train Loss = 1.0779
2023-11-08 09:58:52,947 - __main__ - INFO - Fold 5, mse = 45.1872, mad = 4.6033
2023-11-08 09:58:53,288 - __main__ - INFO - Fold 5 Epoch 24 Batch 0: Train Loss = 1.2776
2023-11-08 09:58:55,146 - __main__ - INFO - Fold 5, mse = 45.0321, mad = 4.6591
2023-11-08 09:58:55,457 - __main__ - INFO - Fold 5 Epoch 25 Batch 0: Train Loss = 1.2546
2023-11-08 09:58:57,195 - __main__ - INFO - Fold 5, mse = 45.1754, mad = 4.6705
2023-11-08 09:58:57,513 - __main__ - INFO - Fold 5 Epoch 26 Batch 0: Train Loss = 1.2635
2023-11-08 09:58:59,262 - __main__ - INFO - Fold 5, mse = 45.2626, mad = 4.5629
2023-11-08 09:58:59,521 - __main__ - INFO - Fold 5 Epoch 27 Batch 0: Train Loss = 1.2371
2023-11-08 09:59:01,281 - __main__ - INFO - Fold 5, mse = 45.5747, mad = 4.6336
2023-11-08 09:59:01,573 - __main__ - INFO - Fold 5 Epoch 28 Batch 0: Train Loss = 1.2687
2023-11-08 09:59:03,051 - __main__ - INFO - Fold 5, mse = 45.2838, mad = 4.5952
2023-11-08 09:59:03,275 - __main__ - INFO - Fold 5 Epoch 29 Batch 0: Train Loss = 1.2421
2023-11-08 09:59:04,939 - __main__ - INFO - Fold 5, mse = 44.9642, mad = 4.6057
2023-11-08 09:59:05,178 - __main__ - INFO - Fold 5 Epoch 30 Batch 0: Train Loss = 1.1408
2023-11-08 09:59:06,818 - __main__ - INFO - Fold 5, epoch 30: Loss = 1.3310 Valid loss = 2.4468 MSE = 45.4186 AUROC = 0.9917
2023-11-08 09:59:06,820 - __main__ - INFO - Fold 5, mse = 45.4186, mad = 4.6880
2023-11-08 09:59:07,079 - __main__ - INFO - Fold 5 Epoch 31 Batch 0: Train Loss = 1.0452
2023-11-08 09:59:08,822 - __main__ - INFO - Fold 5, mse = 45.0848, mad = 4.6175
2023-11-08 09:59:09,133 - __main__ - INFO - Fold 5 Epoch 32 Batch 0: Train Loss = 1.1186
2023-11-08 09:59:11,052 - __main__ - INFO - Fold 5, mse = 44.8199, mad = 4.5097
2023-11-08 09:59:11,360 - __main__ - INFO - Fold 5 Epoch 33 Batch 0: Train Loss = 1.4496
2023-11-08 09:59:12,877 - __main__ - INFO - Fold 5, mse = 44.9003, mad = 4.6110
2023-11-08 09:59:13,219 - __main__ - INFO - Fold 5 Epoch 34 Batch 0: Train Loss = 1.1568
2023-11-08 09:59:14,886 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.6192 ------------
2023-11-08 09:59:15,149 - __main__ - INFO - Fold 5, mse = 44.6192, mad = 4.5438
2023-11-08 09:59:15,389 - __main__ - INFO - Fold 5 Epoch 35 Batch 0: Train Loss = 1.1478
2023-11-08 09:59:16,906 - __main__ - INFO - Fold 5, mse = 44.6561, mad = 4.5084
2023-11-08 09:59:17,132 - __main__ - INFO - Fold 5 Epoch 36 Batch 0: Train Loss = 1.5590
2023-11-08 09:59:18,719 - __main__ - INFO - Fold 5, mse = 44.7280, mad = 4.5514
2023-11-08 09:59:19,018 - __main__ - INFO - Fold 5 Epoch 37 Batch 0: Train Loss = 0.9884
2023-11-08 09:59:20,600 - __main__ - INFO - Fold 5, mse = 44.7506, mad = 4.5671
2023-11-08 09:59:20,855 - __main__ - INFO - Fold 5 Epoch 38 Batch 0: Train Loss = 1.2482
2023-11-08 09:59:22,486 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.4714 ------------
2023-11-08 09:59:22,720 - __main__ - INFO - Fold 5, mse = 44.4714, mad = 4.5154
2023-11-08 09:59:22,971 - __main__ - INFO - Fold 5 Epoch 39 Batch 0: Train Loss = 1.1506
2023-11-08 09:59:24,651 - __main__ - INFO - Fold 5, mse = 44.6024, mad = 4.5568
2023-11-08 09:59:24,946 - __main__ - INFO - Fold 5 Epoch 40 Batch 0: Train Loss = 1.1901
2023-11-08 09:59:26,645 - __main__ - INFO - Fold 5, epoch 40: Loss = 1.2136 Valid loss = 2.4346 MSE = 44.6353 AUROC = 0.9913
2023-11-08 09:59:26,647 - __main__ - INFO - Fold 5, mse = 44.6353, mad = 4.5002
2023-11-08 09:59:26,930 - __main__ - INFO - Fold 5 Epoch 41 Batch 0: Train Loss = 1.0949
2023-11-08 09:59:28,480 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.3021 ------------
2023-11-08 09:59:28,699 - __main__ - INFO - Fold 5, mse = 44.3021, mad = 4.4885
2023-11-08 09:59:28,934 - __main__ - INFO - Fold 5 Epoch 42 Batch 0: Train Loss = 1.1082
2023-11-08 09:59:30,854 - __main__ - INFO - Fold 5, mse = 44.3238, mad = 4.5243
2023-11-08 09:59:31,225 - __main__ - INFO - Fold 5 Epoch 43 Batch 0: Train Loss = 1.1254
2023-11-08 09:59:32,841 - __main__ - INFO - Fold 5, mse = 44.4639, mad = 4.4843
2023-11-08 09:59:33,174 - __main__ - INFO - Fold 5 Epoch 44 Batch 0: Train Loss = 1.2831
2023-11-08 09:59:34,692 - __main__ - INFO - Fold 5, mse = 44.3765, mad = 4.4989
2023-11-08 09:59:35,078 - __main__ - INFO - Fold 5 Epoch 45 Batch 0: Train Loss = 1.0776
2023-11-08 09:59:36,872 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 44.1326 ------------
2023-11-08 09:59:37,044 - __main__ - INFO - Fold 5, mse = 44.1326, mad = 4.4874
2023-11-08 09:59:37,389 - __main__ - INFO - Fold 5 Epoch 46 Batch 0: Train Loss = 1.1946
2023-11-08 09:59:39,207 - __main__ - INFO - Fold 5, mse = 44.7237, mad = 4.5453
2023-11-08 09:59:39,563 - __main__ - INFO - Fold 5 Epoch 47 Batch 0: Train Loss = 1.2454
2023-11-08 09:59:41,193 - __main__ - INFO - Fold 5, mse = 44.9977, mad = 4.4577
2023-11-08 09:59:41,487 - __main__ - INFO - Fold 5 Epoch 48 Batch 0: Train Loss = 1.2107
2023-11-08 09:59:43,151 - __main__ - INFO - Fold 5, mse = 44.7979, mad = 4.5048
2023-11-08 09:59:43,471 - __main__ - INFO - Fold 5 Epoch 49 Batch 0: Train Loss = 1.1962
2023-11-08 09:59:44,852 - __main__ - INFO - Fold 5, mse = 44.7222, mad = 4.5602
2023-11-08 09:59:45,163 - __main__ - INFO - Fold 5 Epoch 50 Batch 0: Train Loss = 1.2920
2023-11-08 09:59:47,004 - __main__ - INFO - Fold 5, epoch 50: Loss = 1.1971 Valid loss = 2.4009 MSE = 44.4865 AUROC = 0.9907
2023-11-08 09:59:47,006 - __main__ - INFO - Fold 5, mse = 44.4865, mad = 4.4585
2023-11-08 09:59:47,281 - __main__ - INFO - Fold 5 Epoch 51 Batch 0: Train Loss = 1.2257
2023-11-08 09:59:49,127 - __main__ - INFO - Fold 5, mse = 44.7194, mad = 4.5037
2023-11-08 09:59:49,352 - __main__ - INFO - Fold 5 Epoch 52 Batch 0: Train Loss = 1.1575
2023-11-08 09:59:51,183 - __main__ - INFO - Fold 5, mse = 44.5024, mad = 4.5708
2023-11-08 09:59:51,440 - __main__ - INFO - Fold 5 Epoch 53 Batch 0: Train Loss = 1.2597
2023-11-08 09:59:53,067 - __main__ - INFO - Fold 5, mse = 44.4150, mad = 4.5051
2023-11-08 09:59:53,355 - __main__ - INFO - Fold 5 Epoch 54 Batch 0: Train Loss = 0.9865
2023-11-08 09:59:55,026 - __main__ - INFO - Fold 5, mse = 44.6586, mad = 4.5870
2023-11-08 09:59:55,311 - __main__ - INFO - Fold 5 Epoch 55 Batch 0: Train Loss = 1.3029
2023-11-08 09:59:57,003 - __main__ - INFO - Fold 5, mse = 44.3996, mad = 4.4333
2023-11-08 09:59:57,249 - __main__ - INFO - Fold 5 Epoch 56 Batch 0: Train Loss = 1.1104
2023-11-08 09:59:58,804 - __main__ - INFO - Fold 5, mse = 44.8535, mad = 4.6835
2023-11-08 09:59:59,132 - __main__ - INFO - Fold 5 Epoch 57 Batch 0: Train Loss = 1.1907
2023-11-08 10:00:00,821 - __main__ - INFO - Fold 5, mse = 44.5286, mad = 4.4104
2023-11-08 10:00:01,105 - __main__ - INFO - Fold 5 Epoch 58 Batch 0: Train Loss = 1.3675
2023-11-08 10:00:02,735 - __main__ - INFO - Fold 5, mse = 45.0055, mad = 4.6755
2023-11-08 10:00:03,045 - __main__ - INFO - Fold 5 Epoch 59 Batch 0: Train Loss = 1.3891
2023-11-08 10:00:04,460 - __main__ - INFO - Fold 5, mse = 44.6584, mad = 4.4219
2023-11-08 10:00:04,664 - __main__ - INFO - Fold 5 Epoch 60 Batch 0: Train Loss = 1.3683
2023-11-08 10:00:06,243 - __main__ - INFO - Fold 5, epoch 60: Loss = 1.1809 Valid loss = 2.3788 MSE = 44.5563 AUROC = 0.9904
2023-11-08 10:00:06,247 - __main__ - INFO - Fold 5, mse = 44.5563, mad = 4.5194
2023-11-08 10:00:06,542 - __main__ - INFO - Fold 5 Epoch 61 Batch 0: Train Loss = 1.0947
2023-11-08 10:00:07,808 - __main__ - INFO - Fold 5, mse = 44.3944, mad = 4.4469
2023-11-08 10:00:08,044 - __main__ - INFO - Fold 5 Epoch 62 Batch 0: Train Loss = 1.1610
2023-11-08 10:00:09,652 - __main__ - INFO - Fold 5, mse = 44.8992, mad = 4.6005
2023-11-08 10:00:09,879 - __main__ - INFO - Fold 5 Epoch 63 Batch 0: Train Loss = 1.2232
2023-11-08 10:00:11,507 - __main__ - INFO - Fold 5, mse = 44.7786, mad = 4.4884
2023-11-08 10:00:11,988 - __main__ - INFO - Fold 5 Epoch 64 Batch 0: Train Loss = 1.1012
2023-11-08 10:00:13,581 - __main__ - INFO - Fold 5, mse = 44.4013, mad = 4.4955
2023-11-08 10:00:13,824 - __main__ - INFO - Fold 5 Epoch 65 Batch 0: Train Loss = 1.1430
2023-11-08 10:00:15,772 - __main__ - INFO - Fold 5, mse = 44.2860, mad = 4.5430
2023-11-08 10:00:16,162 - __main__ - INFO - Fold 5 Epoch 66 Batch 0: Train Loss = 0.9764
2023-11-08 10:00:17,984 - __main__ - INFO - Fold 5, mse = 44.1940, mad = 4.4950
2023-11-08 10:00:18,322 - __main__ - INFO - Fold 5 Epoch 67 Batch 0: Train Loss = 1.1985
2023-11-08 10:00:19,846 - __main__ - INFO - Fold 5, mse = 44.1551, mad = 4.5338
2023-11-08 10:00:20,123 - __main__ - INFO - Fold 5 Epoch 68 Batch 0: Train Loss = 0.9633
2023-11-08 10:00:21,450 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.9232 ------------
2023-11-08 10:00:21,606 - __main__ - INFO - Fold 5, mse = 43.9232, mad = 4.4682
2023-11-08 10:00:21,824 - __main__ - INFO - Fold 5 Epoch 69 Batch 0: Train Loss = 1.0357
2023-11-08 10:00:23,351 - __main__ - INFO - Fold 5, mse = 44.1388, mad = 4.4929
2023-11-08 10:00:23,744 - __main__ - INFO - Fold 5 Epoch 70 Batch 0: Train Loss = 1.1931
2023-11-08 10:00:25,291 - __main__ - INFO - Fold 5, epoch 70: Loss = 1.1275 Valid loss = 2.3989 MSE = 44.5765 AUROC = 0.9896
2023-11-08 10:00:25,294 - __main__ - INFO - Fold 5, mse = 44.5765, mad = 4.5657
2023-11-08 10:00:25,548 - __main__ - INFO - Fold 5 Epoch 71 Batch 0: Train Loss = 1.0075
2023-11-08 10:00:27,129 - __main__ - INFO - Fold 5, mse = 44.2017, mad = 4.4996
2023-11-08 10:00:27,345 - __main__ - INFO - Fold 5 Epoch 72 Batch 0: Train Loss = 1.1739
2023-11-08 10:00:29,042 - __main__ - INFO - Fold 5, mse = 44.3978, mad = 4.6078
2023-11-08 10:00:29,330 - __main__ - INFO - Fold 5 Epoch 73 Batch 0: Train Loss = 1.2315
2023-11-08 10:00:30,897 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 43.2205 ------------
2023-11-08 10:00:31,075 - __main__ - INFO - Fold 5, mse = 43.2205, mad = 4.4191
2023-11-08 10:00:31,349 - __main__ - INFO - Fold 5 Epoch 74 Batch 0: Train Loss = 1.0737
2023-11-08 10:00:32,684 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 42.7018 ------------
2023-11-08 10:00:32,841 - __main__ - INFO - Fold 5, mse = 42.7018, mad = 4.4404
2023-11-08 10:00:33,240 - __main__ - INFO - Fold 5 Epoch 75 Batch 0: Train Loss = 1.2232
2023-11-08 10:00:34,855 - __main__ - INFO - Fold 5, mse = 42.9688, mad = 4.4938
2023-11-08 10:00:35,218 - __main__ - INFO - Fold 5 Epoch 76 Batch 0: Train Loss = 1.0239
2023-11-08 10:00:36,935 - __main__ - INFO - Fold 5, mse = 43.2598, mad = 4.3779
2023-11-08 10:00:37,175 - __main__ - INFO - Fold 5 Epoch 77 Batch 0: Train Loss = 1.1479
2023-11-08 10:00:39,017 - __main__ - INFO - Fold 5, mse = 43.8500, mad = 4.5557
2023-11-08 10:00:39,246 - __main__ - INFO - Fold 5 Epoch 78 Batch 0: Train Loss = 1.1385
2023-11-08 10:00:40,805 - __main__ - INFO - Fold 5, mse = 43.7892, mad = 4.4746
2023-11-08 10:00:41,113 - __main__ - INFO - Fold 5 Epoch 79 Batch 0: Train Loss = 1.0129
2023-11-08 10:00:42,588 - __main__ - INFO - Fold 5, mse = 44.0814, mad = 4.5006
2023-11-08 10:00:42,810 - __main__ - INFO - Fold 5 Epoch 80 Batch 0: Train Loss = 1.1345
2023-11-08 10:00:44,453 - __main__ - INFO - Fold 5, epoch 80: Loss = 1.1321 Valid loss = 2.4009 MSE = 44.2961 AUROC = 0.9889
2023-11-08 10:00:44,455 - __main__ - INFO - Fold 5, mse = 44.2961, mad = 4.4966
2023-11-08 10:00:44,768 - __main__ - INFO - Fold 5 Epoch 81 Batch 0: Train Loss = 1.1962
2023-11-08 10:00:46,357 - __main__ - INFO - Fold 5, mse = 44.1377, mad = 4.5212
2023-11-08 10:00:46,608 - __main__ - INFO - Fold 5 Epoch 82 Batch 0: Train Loss = 1.1868
2023-11-08 10:00:48,095 - __main__ - INFO - Fold 5, mse = 43.9110, mad = 4.5930
2023-11-08 10:00:48,412 - __main__ - INFO - Fold 5 Epoch 83 Batch 0: Train Loss = 1.0189
2023-11-08 10:00:50,216 - __main__ - INFO - Fold 5, mse = 43.7393, mad = 4.5113
2023-11-08 10:00:50,650 - __main__ - INFO - Fold 5 Epoch 84 Batch 0: Train Loss = 0.9553
2023-11-08 10:00:52,289 - __main__ - INFO - Fold 5, mse = 44.0688, mad = 4.5599
2023-11-08 10:00:52,628 - __main__ - INFO - Fold 5 Epoch 85 Batch 0: Train Loss = 1.0833
2023-11-08 10:00:54,174 - __main__ - INFO - Fold 5, mse = 44.1045, mad = 4.5400
2023-11-08 10:00:54,376 - __main__ - INFO - Fold 5 Epoch 86 Batch 0: Train Loss = 1.0359
2023-11-08 10:00:55,855 - __main__ - INFO - Fold 5, mse = 44.1118, mad = 4.4210
2023-11-08 10:00:56,138 - __main__ - INFO - Fold 5 Epoch 87 Batch 0: Train Loss = 1.0982
2023-11-08 10:00:57,775 - __main__ - INFO - Fold 5, mse = 44.4902, mad = 4.6074
2023-11-08 10:00:58,034 - __main__ - INFO - Fold 5 Epoch 88 Batch 0: Train Loss = 1.1511
2023-11-08 10:00:59,712 - __main__ - INFO - Fold 5, mse = 44.0958, mad = 4.5080
2023-11-08 10:00:59,969 - __main__ - INFO - Fold 5 Epoch 89 Batch 0: Train Loss = 1.1017
2023-11-08 10:01:01,755 - __main__ - INFO - Fold 5, mse = 44.4403, mad = 4.5246
2023-11-08 10:01:02,101 - __main__ - INFO - Fold 5 Epoch 90 Batch 0: Train Loss = 1.1339
2023-11-08 10:01:03,831 - __main__ - INFO - Fold 5, epoch 90: Loss = 1.0920 Valid loss = 2.3832 MSE = 44.1976 AUROC = 0.9888
2023-11-08 10:01:03,832 - __main__ - INFO - Fold 5, mse = 44.1976, mad = 4.5259
2023-11-08 10:01:04,125 - __main__ - INFO - Fold 5 Epoch 91 Batch 0: Train Loss = 1.1084
2023-11-08 10:01:05,981 - __main__ - INFO - Fold 5, mse = 43.9579, mad = 4.5203
2023-11-08 10:01:06,321 - __main__ - INFO - Fold 5 Epoch 92 Batch 0: Train Loss = 0.8713
2023-11-08 10:01:08,092 - __main__ - INFO - Fold 5, mse = 43.7817, mad = 4.4872
2023-11-08 10:01:08,358 - __main__ - INFO - Fold 5 Epoch 93 Batch 0: Train Loss = 1.0835
2023-11-08 10:01:09,598 - __main__ - INFO - Fold 5, mse = 43.9167, mad = 4.6225
2023-11-08 10:01:09,918 - __main__ - INFO - Fold 5 Epoch 94 Batch 0: Train Loss = 1.0542
2023-11-08 10:01:11,338 - __main__ - INFO - Fold 5, mse = 43.8280, mad = 4.5199
2023-11-08 10:01:11,602 - __main__ - INFO - Fold 5 Epoch 95 Batch 0: Train Loss = 1.1612
2023-11-08 10:01:13,206 - __main__ - INFO - Fold 5, mse = 43.8154, mad = 4.5424
2023-11-08 10:01:13,538 - __main__ - INFO - Fold 5 Epoch 96 Batch 0: Train Loss = 1.3407
2023-11-08 10:01:15,211 - __main__ - INFO - Fold 5, mse = 43.3662, mad = 4.4487
2023-11-08 10:01:15,605 - __main__ - INFO - Fold 5 Epoch 97 Batch 0: Train Loss = 1.0422
2023-11-08 10:01:17,254 - __main__ - INFO - Fold 5, mse = 43.2512, mad = 4.4876
2023-11-08 10:01:17,581 - __main__ - INFO - Fold 5 Epoch 98 Batch 0: Train Loss = 1.2691
2023-11-08 10:01:19,102 - __main__ - INFO - Fold 5, mse = 43.3421, mad = 4.5576
2023-11-08 10:01:19,392 - __main__ - INFO - Fold 5 Epoch 99 Batch 0: Train Loss = 1.1125
2023-11-08 10:01:21,014 - __main__ - INFO - Fold 5, mse = 43.2376, mad = 4.4469
2023-11-08 10:01:21,276 - __main__ - INFO - Fold 5 Epoch 100 Batch 0: Train Loss = 1.0416
2023-11-08 10:01:22,865 - __main__ - INFO - Fold 5, epoch 100: Loss = 1.1075 Valid loss = 2.3394 MSE = 43.5970 AUROC = 0.9889
2023-11-08 10:01:22,867 - __main__ - INFO - Fold 5, mse = 43.5970, mad = 4.5307
2023-11-08 10:01:23,181 - __main__ - INFO - Fold 5 Epoch 101 Batch 0: Train Loss = 0.9799
2023-11-08 10:01:24,555 - __main__ - INFO - Fold 5, mse = 43.8309, mad = 4.5442
2023-11-08 10:01:24,853 - __main__ - INFO - Fold 5 Epoch 102 Batch 0: Train Loss = 0.9114
2023-11-08 10:01:26,277 - __main__ - INFO - Fold 5, mse = 43.8511, mad = 4.4837
2023-11-08 10:01:26,553 - __main__ - INFO - Fold 5 Epoch 103 Batch 0: Train Loss = 1.1307
2023-11-08 10:01:28,173 - __main__ - INFO - Fold 5, mse = 44.2484, mad = 4.6140
2023-11-08 10:01:28,393 - __main__ - INFO - Fold 5 Epoch 104 Batch 0: Train Loss = 1.1656
2023-11-08 10:01:30,064 - __main__ - INFO - Fold 5, mse = 43.9570, mad = 4.4192
2023-11-08 10:01:30,269 - __main__ - INFO - Fold 5 Epoch 105 Batch 0: Train Loss = 1.2769
2023-11-08 10:01:32,039 - __main__ - INFO - Fold 5, mse = 43.7449, mad = 4.4862
2023-11-08 10:01:32,335 - __main__ - INFO - Fold 5 Epoch 106 Batch 0: Train Loss = 1.3401
2023-11-08 10:01:33,947 - __main__ - INFO - Fold 5, mse = 43.8005, mad = 4.4373
2023-11-08 10:01:34,279 - __main__ - INFO - Fold 5 Epoch 107 Batch 0: Train Loss = 1.2697
2023-11-08 10:01:35,800 - __main__ - INFO - Fold 5, mse = 44.4434, mad = 4.5140
2023-11-08 10:01:36,109 - __main__ - INFO - Fold 5 Epoch 108 Batch 0: Train Loss = 0.9702
2023-11-08 10:01:37,736 - __main__ - INFO - Fold 5, mse = 44.0036, mad = 4.4447
2023-11-08 10:01:38,005 - __main__ - INFO - Fold 5 Epoch 109 Batch 0: Train Loss = 1.1483
2023-11-08 10:01:39,559 - __main__ - INFO - Fold 5, mse = 43.6212, mad = 4.5825
2023-11-08 10:01:39,888 - __main__ - INFO - Fold 5 Epoch 110 Batch 0: Train Loss = 1.1135
2023-11-08 10:01:41,422 - __main__ - INFO - Fold 5, epoch 110: Loss = 1.0433 Valid loss = 2.3503 MSE = 43.3164 AUROC = 0.9895
2023-11-08 10:01:41,424 - __main__ - INFO - Fold 5, mse = 43.3164, mad = 4.4500
2023-11-08 10:01:41,708 - __main__ - INFO - Fold 5 Epoch 111 Batch 0: Train Loss = 0.9549
2023-11-08 10:01:43,059 - __main__ - INFO - Fold 5, mse = 43.4267, mad = 4.4388
2023-11-08 10:01:43,320 - __main__ - INFO - Fold 5 Epoch 112 Batch 0: Train Loss = 1.0704
2023-11-08 10:01:45,054 - __main__ - INFO - Fold 5, mse = 42.9816, mad = 4.4281
2023-11-08 10:01:45,315 - __main__ - INFO - Fold 5 Epoch 113 Batch 0: Train Loss = 0.9648
2023-11-08 10:01:46,719 - __main__ - INFO - Fold 5, mse = 43.0538, mad = 4.4159
2023-11-08 10:01:46,985 - __main__ - INFO - Fold 5 Epoch 114 Batch 0: Train Loss = 1.1378
2023-11-08 10:01:48,516 - __main__ - INFO - Fold 5, mse = 43.1144, mad = 4.4557
2023-11-08 10:01:48,886 - __main__ - INFO - Fold 5 Epoch 115 Batch 0: Train Loss = 0.9567
2023-11-08 10:01:50,465 - __main__ - INFO - Fold 5, mse = 42.7622, mad = 4.4327
2023-11-08 10:01:50,740 - __main__ - INFO - Fold 5 Epoch 116 Batch 0: Train Loss = 1.1307
2023-11-08 10:01:52,539 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 42.6334 ------------
2023-11-08 10:01:52,773 - __main__ - INFO - Fold 5, mse = 42.6334, mad = 4.4676
2023-11-08 10:01:53,183 - __main__ - INFO - Fold 5 Epoch 117 Batch 0: Train Loss = 1.0305
2023-11-08 10:01:54,680 - __main__ - INFO - Fold 5, mse = 43.3667, mad = 4.4712
2023-11-08 10:01:54,956 - __main__ - INFO - Fold 5 Epoch 118 Batch 0: Train Loss = 0.9816
2023-11-08 10:01:56,426 - __main__ - INFO - Fold 5, mse = 43.4388, mad = 4.4871
2023-11-08 10:01:56,734 - __main__ - INFO - Fold 5 Epoch 119 Batch 0: Train Loss = 1.0129
2023-11-08 10:01:58,431 - __main__ - INFO - Fold 5, mse = 43.3905, mad = 4.5064
2023-11-08 10:01:58,644 - __main__ - INFO - Fold 5 Epoch 120 Batch 0: Train Loss = 0.9977
2023-11-08 10:02:00,217 - __main__ - INFO - Fold 5, epoch 120: Loss = 1.0159 Valid loss = 2.3689 MSE = 43.5780 AUROC = 0.9890
2023-11-08 10:02:00,219 - __main__ - INFO - Fold 5, mse = 43.5780, mad = 4.4822
2023-11-08 10:02:00,604 - __main__ - INFO - Fold 5 Epoch 121 Batch 0: Train Loss = 1.1208
2023-11-08 10:02:02,290 - __main__ - INFO - Fold 5, mse = 43.7062, mad = 4.4695
2023-11-08 10:02:02,521 - __main__ - INFO - Fold 5 Epoch 122 Batch 0: Train Loss = 0.9066
2023-11-08 10:02:03,930 - __main__ - INFO - Fold 5, mse = 43.6139, mad = 4.5076
2023-11-08 10:02:04,175 - __main__ - INFO - Fold 5 Epoch 123 Batch 0: Train Loss = 1.0766
2023-11-08 10:02:06,011 - __main__ - INFO - Fold 5, mse = 43.4970, mad = 4.5228
2023-11-08 10:02:06,395 - __main__ - INFO - Fold 5 Epoch 124 Batch 0: Train Loss = 1.1955
2023-11-08 10:02:07,933 - __main__ - INFO - Fold 5, mse = 43.1033, mad = 4.4437
2023-11-08 10:02:08,149 - __main__ - INFO - Fold 5 Epoch 125 Batch 0: Train Loss = 0.9318
2023-11-08 10:02:09,507 - __main__ - INFO - Fold 5, mse = 43.0490, mad = 4.4020
2023-11-08 10:02:09,764 - __main__ - INFO - Fold 5 Epoch 126 Batch 0: Train Loss = 0.9056
2023-11-08 10:02:11,312 - __main__ - INFO - Fold 5, mse = 43.1265, mad = 4.5233
2023-11-08 10:02:11,574 - __main__ - INFO - Fold 5 Epoch 127 Batch 0: Train Loss = 0.9873
2023-11-08 10:02:13,445 - __main__ - INFO - Fold 5, mse = 42.9195, mad = 4.4230
2023-11-08 10:02:13,709 - __main__ - INFO - Fold 5 Epoch 128 Batch 0: Train Loss = 1.2292
2023-11-08 10:02:15,431 - __main__ - INFO - Fold 5, mse = 42.9750, mad = 4.5356
2023-11-08 10:02:15,684 - __main__ - INFO - Fold 5 Epoch 129 Batch 0: Train Loss = 1.1459
2023-11-08 10:02:17,306 - __main__ - INFO - Fold 5, mse = 43.3929, mad = 4.5039
2023-11-08 10:02:17,574 - __main__ - INFO - Fold 5 Epoch 130 Batch 0: Train Loss = 0.9892
2023-11-08 10:02:19,313 - __main__ - INFO - Fold 5, epoch 130: Loss = 1.0409 Valid loss = 2.4235 MSE = 44.3344 AUROC = 0.9890
2023-11-08 10:02:19,315 - __main__ - INFO - Fold 5, mse = 44.3344, mad = 4.5300
2023-11-08 10:02:19,616 - __main__ - INFO - Fold 5 Epoch 131 Batch 0: Train Loss = 1.0009
2023-11-08 10:02:21,182 - __main__ - INFO - Fold 5, mse = 44.2982, mad = 4.6129
2023-11-08 10:02:21,481 - __main__ - INFO - Fold 5 Epoch 132 Batch 0: Train Loss = 0.9382
2023-11-08 10:02:23,463 - __main__ - INFO - Fold 5, mse = 43.5518, mad = 4.4713
2023-11-08 10:02:23,799 - __main__ - INFO - Fold 5 Epoch 133 Batch 0: Train Loss = 1.0402
2023-11-08 10:02:25,476 - __main__ - INFO - Fold 5, mse = 43.8460, mad = 4.5672
2023-11-08 10:02:25,804 - __main__ - INFO - Fold 5 Epoch 134 Batch 0: Train Loss = 0.8498
2023-11-08 10:02:27,509 - __main__ - INFO - Fold 5, mse = 43.5101, mad = 4.4383
2023-11-08 10:02:27,858 - __main__ - INFO - Fold 5 Epoch 135 Batch 0: Train Loss = 1.1929
2023-11-08 10:02:29,587 - __main__ - INFO - Fold 5, mse = 43.4432, mad = 4.4844
2023-11-08 10:02:29,820 - __main__ - INFO - Fold 5 Epoch 136 Batch 0: Train Loss = 0.7128
2023-11-08 10:02:31,168 - __main__ - INFO - Fold 5, mse = 43.4708, mad = 4.5546
2023-11-08 10:02:31,375 - __main__ - INFO - Fold 5 Epoch 137 Batch 0: Train Loss = 1.2683
2023-11-08 10:02:33,035 - __main__ - INFO - Fold 5, mse = 43.3111, mad = 4.4839
2023-11-08 10:02:33,395 - __main__ - INFO - Fold 5 Epoch 138 Batch 0: Train Loss = 1.0393
2023-11-08 10:02:35,102 - __main__ - INFO - Fold 5, mse = 43.4489, mad = 4.4782
2023-11-08 10:02:35,427 - __main__ - INFO - Fold 5 Epoch 139 Batch 0: Train Loss = 1.2206
2023-11-08 10:02:37,145 - __main__ - INFO - Fold 5, mse = 43.2969, mad = 4.4550
2023-11-08 10:02:37,397 - __main__ - INFO - Fold 5 Epoch 140 Batch 0: Train Loss = 1.0684
2023-11-08 10:02:38,901 - __main__ - INFO - Fold 5, epoch 140: Loss = 1.0275 Valid loss = 2.3499 MSE = 43.1911 AUROC = 0.9885
2023-11-08 10:02:38,903 - __main__ - INFO - Fold 5, mse = 43.1911, mad = 4.4771
2023-11-08 10:02:39,142 - __main__ - INFO - Fold 5 Epoch 141 Batch 0: Train Loss = 0.9595
2023-11-08 10:02:40,629 - __main__ - INFO - Fold 5, mse = 43.8297, mad = 4.5158
2023-11-08 10:02:40,888 - __main__ - INFO - Fold 5 Epoch 142 Batch 0: Train Loss = 0.9375
2023-11-08 10:02:42,445 - __main__ - INFO - Fold 5, mse = 43.1690, mad = 4.4461
2023-11-08 10:02:42,693 - __main__ - INFO - Fold 5 Epoch 143 Batch 0: Train Loss = 0.9931
2023-11-08 10:02:44,115 - __main__ - INFO - Fold 5, mse = 43.1793, mad = 4.5471
2023-11-08 10:02:44,426 - __main__ - INFO - Fold 5 Epoch 144 Batch 0: Train Loss = 1.0159
2023-11-08 10:02:46,317 - __main__ - INFO - Fold 5, mse = 43.2333, mad = 4.4291
2023-11-08 10:02:46,609 - __main__ - INFO - Fold 5 Epoch 145 Batch 0: Train Loss = 1.0546
2023-11-08 10:02:48,089 - __main__ - INFO - Fold 5, mse = 43.2315, mad = 4.5131
2023-11-08 10:02:48,333 - __main__ - INFO - Fold 5 Epoch 146 Batch 0: Train Loss = 0.9373
2023-11-08 10:02:50,010 - __main__ - INFO - Fold 5, mse = 42.8421, mad = 4.4644
2023-11-08 10:02:50,391 - __main__ - INFO - Fold 5 Epoch 147 Batch 0: Train Loss = 1.0749
2023-11-08 10:02:52,067 - __main__ - INFO - Fold 5, mse = 43.4923, mad = 4.6048
2023-11-08 10:02:52,372 - __main__ - INFO - Fold 5 Epoch 148 Batch 0: Train Loss = 1.0198
2023-11-08 10:02:53,832 - __main__ - INFO - Fold 5, mse = 42.9565, mad = 4.4268
2023-11-08 10:02:54,117 - __main__ - INFO - Fold 5 Epoch 149 Batch 0: Train Loss = 1.1432
2023-11-08 10:02:55,807 - __main__ - INFO - Fold 5, mse = 42.7681, mad = 4.5210
2023-11-08 10:02:55,810 - __main__ - INFO - mse 36.3759(4.9595)
2023-11-08 10:02:55,811 - __main__ - INFO - mad 4.4173(0.2235)
2023-11-08 10:02:55,812 - __main__ - INFO - auroc 0.9846(0.0078)
2023-11-08 10:02:55,814 - __main__ - INFO - auprc 0.9775(0.0084)
2023-11-08 10:02:55,825 - __main__ - INFO - mse 36.3759(4.9595)
2023-11-08 10:02:55,826 - __main__ - INFO - mad 4.4173(0.2235)
2023-11-08 10:02:55,828 - __main__ - INFO - auroc 0.9846(0.0078)
2023-11-08 10:02:55,829 - __main__ - INFO - auprc 0.9775(0.0084)
