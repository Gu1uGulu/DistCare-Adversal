2023-11-08 11:39:43,177 - __main__ - INFO - 这是希望输出的info内容
2023-11-08 11:39:43,181 - __main__ - WARNING - 这是希望输出的warning内容
2023-11-08 11:40:03,372 - __main__ - INFO - [[-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 1.2605692444279462, 0.585371321489137, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.9546043520029015, 0.2987563180527675, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 1.0769903089729194, 0.4420638197709522, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.566534136852991, 1.803485086093707, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, 0.46506052412282983, -0.05951243624269434, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, 0.281481588667803, -0.2744736888199714, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.8934113735178925, 0.9436400757845987, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592]]
2023-11-08 11:40:03,376 - __main__ - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
2023-11-08 11:40:03,378 - __main__ - INFO - 110609
2023-11-08 11:40:03,379 - __main__ - INFO - [[-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8962118618028821, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8617355345389544, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8272592072750267, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7927828800110989, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7583065527471711, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7238302254832434, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6893538982193156, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6548775709553878, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.62040124369146, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5859249164275323, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5514485891636045, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5169722618996767, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.48249593463574897, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.4480196073718212, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.41354328010789343, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3790669528439657, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3445906255800379, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.31011429831611015, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.27563797105218235, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.2411616437882546, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.20668531652432684, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.17220898926039907, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.1377326619964713, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.10325633473254354, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.06878000746861578, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.034303680204688006, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.0001726470592397616, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.034648974323167527, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.06912530158709529, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.10360162885102306, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.13807795611495083, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.1725542833788786, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.20703061064280637, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.24150693790673414, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.2759832651706619, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.31045959243458965, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.34493591969851745, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.3794122469624452, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.413888574226373, '0']]
2023-11-08 11:40:22,844 - __main__ - INFO - [[-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.38817852813712, 0.585371321489137, 0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.2605692444279462, 0.6420908338073934, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.043819195154697, 0.2987563180527675, 0.6590707727034506, -0.4065324680479176, -1.009369603802189, 0.9546043520029015, 0.6420908338073934, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.38817852813712, 0.4420638197709522, 0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.0769903089729194, 0.8382388140070928, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.4742683613827257, 1.803485086093707, 1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.566534136852991, 0.8382388140070928, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.44119036243545645, -0.05951243624269434, 0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.46506052412282983, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.6564149455494709, -0.2744736888199714, 0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.281481588667803, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.09683102945303342, 0.9436400757845987, 0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.8934113735178925, 1.4266827546061909, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.3981454458126536, -0.05951243624269434, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.3981454458126536, -0.05951243624269434, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.37666305339779826, -0.5610886922563408, 0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.26925521769727756, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.37666305339779826, -0.5610886922563408, 0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.26925521769727756, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.1299090284003026, 0.585371321489137, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.0157973304879104, 0.44594285360769403, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.1299090284003026, 0.585371321489137, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.0157973304879104, 0.44594285360769403, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.613370028926668, 1.6601775843755224, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 1.1993762659429372, -0.9270930077902015, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.613370028926668, 1.6601775843755224, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 1.1993762659429372, -0.9270930077902015, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327]]
2023-11-08 11:40:22,851 - __main__ - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
2023-11-08 11:40:22,853 - __main__ - INFO - 34
2023-11-08 11:40:24,742 - __main__ - INFO - 32269
2023-11-08 11:40:24,744 - __main__ - INFO - 4034
2023-11-08 11:40:24,745 - __main__ - INFO - 4033
2023-11-08 11:40:33,710 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 0.6808
2023-11-08 11:40:46,183 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 0.4999
2023-11-08 11:40:58,978 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 0.2446
2023-11-08 11:41:11,901 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 0.2429
2023-11-08 11:41:24,679 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 0.2754
2023-11-08 11:41:37,387 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 0.2965
2023-11-08 11:41:50,539 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 0.2876
2023-11-08 11:42:00,621 - __main__ - INFO - Epoch 0: Loss = 0.3324 Valid loss = 0.2427 roc = 0.7840
2023-11-08 11:42:01,462 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 0.1582
2023-11-08 11:42:14,411 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 0.2439
2023-11-08 11:42:27,416 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 0.2760
2023-11-08 11:42:39,931 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 0.2402
2023-11-08 11:42:53,054 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 0.2449
2023-11-08 11:43:06,564 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 0.2678
2023-11-08 11:43:20,390 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 0.2198
2023-11-08 11:43:30,614 - __main__ - INFO - Epoch 1: Loss = 0.2463 Valid loss = 0.2316 roc = 0.7967
2023-11-08 11:43:31,350 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 0.1713
2023-11-08 11:43:43,942 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 0.2360
2023-11-08 11:43:56,534 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 0.3260
2023-11-08 11:44:09,215 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 0.2090
2023-11-08 11:44:22,335 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 0.2575
2023-11-08 11:44:36,610 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 0.2657
2023-11-08 11:44:51,192 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 0.2165
2023-11-08 11:45:00,662 - __main__ - INFO - Epoch 2: Loss = 0.2377 Valid loss = 0.2251 roc = 0.8182
2023-11-08 11:45:01,491 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 0.2491
2023-11-08 11:45:14,806 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 0.2855
2023-11-08 11:45:28,513 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 0.2296
2023-11-08 11:45:42,110 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 0.2714
2023-11-08 11:45:57,801 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 0.2521
2023-11-08 11:46:13,047 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 0.2506
2023-11-08 11:46:27,097 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 0.1608
2023-11-08 11:46:37,610 - __main__ - INFO - Epoch 3: Loss = 0.2359 Valid loss = 0.2130 roc = 0.8437
2023-11-08 11:46:38,342 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 0.1998
2023-11-08 11:46:53,245 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 0.2392
2023-11-08 11:47:06,659 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 0.2199
2023-11-08 11:47:20,553 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 0.2691
2023-11-08 11:47:34,181 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 0.2417
2023-11-08 11:47:48,534 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 0.2220
2023-11-08 11:48:02,189 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 0.2499
2023-11-08 11:48:12,890 - __main__ - INFO - Epoch 4: Loss = 0.2276 Valid loss = 0.2039 roc = 0.8561
2023-11-08 11:48:13,628 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 0.2781
2023-11-08 11:48:27,499 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 0.2354
2023-11-08 11:48:41,157 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 0.2568
2023-11-08 11:48:55,020 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 0.2312
2023-11-08 11:49:08,215 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 0.2030
2023-11-08 11:49:20,950 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 0.2164
2023-11-08 11:49:33,356 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 0.1825
2023-11-08 11:49:43,901 - __main__ - INFO - Epoch 5: Loss = 0.2180 Valid loss = 0.1930 roc = 0.8853
2023-11-08 11:49:44,550 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 0.2449
2023-11-08 11:49:58,515 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 0.2428
2023-11-08 11:50:12,164 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 0.2123
2023-11-08 11:50:25,301 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 0.2073
2023-11-08 11:50:38,860 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 0.3216
2023-11-08 11:50:52,476 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 0.2235
2023-11-08 11:51:06,015 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 0.1941
2023-11-08 11:51:16,255 - __main__ - INFO - Epoch 6: Loss = 0.2114 Valid loss = 0.1864 roc = 0.8846
2023-11-08 11:51:16,901 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 0.2285
2023-11-08 11:51:30,753 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 0.1803
2023-11-08 11:51:43,456 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 0.2132
2023-11-08 11:51:55,915 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 0.1914
2023-11-08 11:52:09,201 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 0.1551
2023-11-08 11:52:22,609 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 0.2247
2023-11-08 11:52:35,315 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 0.2422
2023-11-08 11:52:45,129 - __main__ - INFO - Epoch 7: Loss = 0.2019 Valid loss = 0.1765 roc = 0.9029
2023-11-08 11:52:45,710 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 0.2371
2023-11-08 11:52:59,051 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 0.1523
2023-11-08 11:53:12,550 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 0.1752
2023-11-08 11:53:24,807 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 0.1609
2023-11-08 11:53:36,952 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 0.2099
2023-11-08 11:53:50,736 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 0.2223
2023-11-08 11:54:04,421 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 0.2348
2023-11-08 11:54:15,805 - __main__ - INFO - Epoch 8: Loss = 0.1981 Valid loss = 0.1683 roc = 0.9039
2023-11-08 11:54:16,706 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 0.2188
2023-11-08 11:54:31,974 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 0.2142
2023-11-08 11:54:46,804 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 0.1987
2023-11-08 11:55:03,288 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 0.2035
2023-11-08 11:55:17,413 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 0.1700
2023-11-08 11:55:30,545 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 0.1498
2023-11-08 11:55:43,702 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 0.1345
2023-11-08 11:55:53,665 - __main__ - INFO - Epoch 9: Loss = 0.1932 Valid loss = 0.1725 roc = 0.9011
2023-11-08 11:55:54,372 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 0.1341
2023-11-08 11:56:09,038 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 0.2501
2023-11-08 11:56:22,314 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 0.1331
2023-11-08 11:56:35,289 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 0.2376
2023-11-08 11:56:47,237 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 0.2369
2023-11-08 11:56:59,101 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 0.1186
2023-11-08 11:57:11,945 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 0.1720
2023-11-08 11:57:21,754 - __main__ - INFO - Epoch 10: Loss = 0.1885 Valid loss = 0.1660 roc = 0.8912
2023-11-08 11:57:22,442 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 0.1261
2023-11-08 11:57:34,638 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 0.1907
2023-11-08 11:57:46,888 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 0.2247
2023-11-08 11:57:59,610 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 0.1740
2023-11-08 11:58:12,564 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 0.2186
2023-11-08 11:58:25,825 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 0.1970
2023-11-08 11:58:38,902 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 0.1989
2023-11-08 11:58:48,868 - __main__ - INFO - Epoch 11: Loss = 0.1860 Valid loss = 0.1590 roc = 0.9053
2023-11-08 11:58:49,673 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 0.3109
2023-11-08 11:59:02,730 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 0.1454
2023-11-08 11:59:15,760 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 0.2114
2023-11-08 11:59:28,255 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 0.1977
2023-11-08 11:59:41,833 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 0.1376
2023-11-08 11:59:54,721 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 0.1977
2023-11-08 12:00:07,482 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 0.1945
2023-11-08 12:00:17,639 - __main__ - INFO - Epoch 12: Loss = 0.1865 Valid loss = 0.1610 roc = 0.9019
2023-11-08 12:00:18,184 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 0.2058
2023-11-08 12:00:30,707 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 0.2137
2023-11-08 12:00:43,990 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 0.1930
2023-11-08 12:00:58,249 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 0.1794
2023-11-08 12:01:11,898 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 0.1908
2023-11-08 12:01:25,562 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 0.2018
2023-11-08 12:01:37,891 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 0.2336
2023-11-08 12:01:47,840 - __main__ - INFO - Epoch 13: Loss = 0.1940 Valid loss = 0.1618 roc = 0.9073
2023-11-08 12:01:48,583 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 0.1541
2023-11-08 12:02:01,441 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 0.2563
2023-11-08 12:02:14,084 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 0.2016
2023-11-08 12:02:27,094 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 0.2101
2023-11-08 12:02:40,307 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 0.1961
2023-11-08 12:02:53,602 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 0.1995
2023-11-08 12:03:06,131 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 0.1404
2023-11-08 12:03:16,764 - __main__ - INFO - Epoch 14: Loss = 0.1930 Valid loss = 0.1574 roc = 0.9111
2023-11-08 12:03:17,424 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 0.2595
2023-11-08 12:03:30,441 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 0.2258
2023-11-08 12:03:43,115 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 0.2414
2023-11-08 12:03:55,574 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 0.2082
2023-11-08 12:04:09,028 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 0.1853
2023-11-08 12:04:23,979 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 0.2478
2023-11-08 12:04:39,579 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 0.1594
2023-11-08 12:04:51,525 - __main__ - INFO - Epoch 15: Loss = 0.1918 Valid loss = 0.1624 roc = 0.9125
2023-11-08 12:04:52,405 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 0.2613
2023-11-08 12:05:08,019 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 0.2168
2023-11-08 12:05:22,146 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 0.1494
2023-11-08 12:05:34,587 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 0.1643
2023-11-08 12:05:48,007 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 0.2665
2023-11-08 12:06:00,523 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 0.1348
2023-11-08 12:06:13,169 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 0.1934
2023-11-08 12:06:23,660 - __main__ - INFO - Epoch 16: Loss = 0.1921 Valid loss = 0.1576 roc = 0.9090
2023-11-08 12:06:24,252 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 0.1813
2023-11-08 12:06:38,731 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 0.1758
2023-11-08 12:06:51,796 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 0.1693
2023-11-08 12:07:04,209 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 0.1643
2023-11-08 12:07:15,924 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 0.1533
2023-11-08 12:07:28,784 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 0.2085
2023-11-08 12:07:41,722 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 0.2055
2023-11-08 12:07:51,563 - __main__ - INFO - Epoch 17: Loss = 0.1916 Valid loss = 0.1586 roc = 0.9074
2023-11-08 12:07:52,016 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 0.2329
2023-11-08 12:08:04,562 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 0.2777
2023-11-08 12:08:18,373 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 0.1320
2023-11-08 12:08:30,293 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 0.1897
2023-11-08 12:08:42,706 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 0.1385
2023-11-08 12:08:55,962 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 0.1755
2023-11-08 12:09:09,065 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 0.1679
2023-11-08 12:09:18,968 - __main__ - INFO - Epoch 18: Loss = 0.1863 Valid loss = 0.1590 roc = 0.9101
2023-11-08 12:09:19,470 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 0.1713
2023-11-08 12:09:32,589 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 0.1965
2023-11-08 12:09:45,145 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 0.2154
2023-11-08 12:09:59,001 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 0.1909
2023-11-08 12:10:14,162 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 0.2065
2023-11-08 12:10:28,843 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 0.2025
2023-11-08 12:10:43,442 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 0.2421
2023-11-08 12:10:54,738 - __main__ - INFO - Epoch 19: Loss = 0.1849 Valid loss = 0.1556 roc = 0.9099
2023-11-08 12:10:55,538 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 0.1600
2023-11-08 12:11:09,522 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 0.2116
2023-11-08 12:11:23,471 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 0.1445
2023-11-08 12:11:38,135 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 0.2213
2023-11-08 12:11:53,035 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 0.1932
2023-11-08 12:12:07,428 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 0.1569
2023-11-08 12:12:21,511 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 0.1407
2023-11-08 12:12:32,043 - __main__ - INFO - Epoch 20: Loss = 0.1844 Valid loss = 0.1595 roc = 0.9112
2023-11-08 12:12:32,656 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 0.1671
2023-11-08 12:12:46,464 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 0.2223
2023-11-08 12:13:00,397 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 0.1499
2023-11-08 12:13:13,557 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 0.1695
2023-11-08 12:13:26,612 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 0.1728
2023-11-08 12:13:40,471 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 0.1451
2023-11-08 12:13:53,687 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 0.2684
2023-11-08 12:14:04,295 - __main__ - INFO - Epoch 21: Loss = 0.1837 Valid loss = 0.1554 roc = 0.9081
2023-11-08 12:14:04,895 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 0.2226
2023-11-08 12:14:18,269 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 0.2353
2023-11-08 12:14:32,166 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 0.1256
2023-11-08 12:14:46,111 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 0.1670
2023-11-08 12:14:59,523 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 0.1866
2023-11-08 12:15:13,325 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 0.1613
2023-11-08 12:15:27,802 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 0.1813
2023-11-08 12:15:39,034 - __main__ - INFO - Epoch 22: Loss = 0.1846 Valid loss = 0.1550 roc = 0.9130
2023-11-08 12:15:39,955 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 0.1993
2023-11-08 12:15:55,149 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 0.2041
2023-11-08 12:16:08,715 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 0.1823
2023-11-08 12:16:22,593 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 0.2057
2023-11-08 12:16:36,830 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 0.2710
2023-11-08 12:16:49,721 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 0.2262
2023-11-08 12:17:03,031 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 0.1497
2023-11-08 12:17:13,916 - __main__ - INFO - Epoch 23: Loss = 0.1828 Valid loss = 0.1584 roc = 0.9078
2023-11-08 12:17:14,531 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 0.1743
2023-11-08 12:17:28,924 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 0.1444
2023-11-08 12:17:43,007 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 0.1735
2023-11-08 12:17:58,487 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 0.2271
2023-11-08 12:18:13,783 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 0.2426
2023-11-08 12:18:27,378 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 0.2060
2023-11-08 12:18:40,986 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 0.1841
2023-11-08 12:18:51,688 - __main__ - INFO - Epoch 24: Loss = 0.1805 Valid loss = 0.1523 roc = 0.9169
2023-11-08 12:18:52,814 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 0.1738
2023-11-08 12:19:06,117 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 0.2032
2023-11-08 12:19:20,867 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 0.1768
2023-11-08 12:19:34,715 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 0.1730
2023-11-08 12:19:49,241 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 0.2055
2023-11-08 12:20:03,350 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 0.1957
2023-11-08 12:20:16,674 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 0.1453
2023-11-08 12:20:27,015 - __main__ - INFO - Epoch 25: Loss = 0.1841 Valid loss = 0.1542 roc = 0.9115
2023-11-08 12:20:27,717 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 0.1741
2023-11-08 12:20:41,567 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 0.1477
2023-11-08 12:20:55,729 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 0.1583
2023-11-08 12:21:09,397 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 0.1650
2023-11-08 12:21:23,365 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 0.2349
2023-11-08 12:21:36,814 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 0.1916
2023-11-08 12:21:51,066 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 0.1982
2023-11-08 12:22:01,619 - __main__ - INFO - Epoch 26: Loss = 0.1822 Valid loss = 0.1516 roc = 0.9128
2023-11-08 12:22:02,372 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 0.1590
2023-11-08 12:22:16,354 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 0.1885
2023-11-08 12:22:30,710 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 0.1729
2023-11-08 12:22:44,866 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 0.1401
2023-11-08 12:22:58,525 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 0.2267
2023-11-08 12:23:12,559 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 0.1853
2023-11-08 12:23:26,895 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 0.1845
2023-11-08 12:23:37,289 - __main__ - INFO - Epoch 27: Loss = 0.1821 Valid loss = 0.1516 roc = 0.9153
2023-11-08 12:23:38,283 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 0.1861
2023-11-08 12:23:52,963 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 0.0862
2023-11-08 12:24:06,023 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 0.1870
2023-11-08 12:24:19,721 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 0.1750
2023-11-08 12:24:33,000 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 0.2891
2023-11-08 12:24:46,367 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 0.1972
2023-11-08 12:25:01,546 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 0.1711
2023-11-08 12:25:11,641 - __main__ - INFO - Epoch 28: Loss = 0.1757 Valid loss = 0.1483 roc = 0.9125
2023-11-08 12:25:12,249 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 0.1805
2023-11-08 12:25:25,070 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 0.2363
2023-11-08 12:25:38,589 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 0.2268
2023-11-08 12:25:52,636 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 0.1830
2023-11-08 12:26:07,083 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 0.1871
2023-11-08 12:26:20,722 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 0.1116
2023-11-08 12:26:34,785 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 0.1486
2023-11-08 12:26:45,373 - __main__ - INFO - Epoch 29: Loss = 0.1755 Valid loss = 0.1515 roc = 0.9119
2023-11-08 12:26:46,119 - __main__ - INFO - Epoch 30 Batch 0: Train Loss = 0.1761
2023-11-08 12:26:59,829 - __main__ - INFO - Epoch 30 Batch 20: Train Loss = 0.1897
2023-11-08 12:27:13,674 - __main__ - INFO - Epoch 30 Batch 40: Train Loss = 0.1542
2023-11-08 12:27:27,869 - __main__ - INFO - Epoch 30 Batch 60: Train Loss = 0.1251
2023-11-08 12:27:42,338 - __main__ - INFO - Epoch 30 Batch 80: Train Loss = 0.2020
2023-11-08 12:27:56,362 - __main__ - INFO - Epoch 30 Batch 100: Train Loss = 0.1817
2023-11-08 12:28:10,529 - __main__ - INFO - Epoch 30 Batch 120: Train Loss = 0.1841
2023-11-08 12:28:21,108 - __main__ - INFO - Epoch 30: Loss = 0.1749 Valid loss = 0.1535 roc = 0.9057
2023-11-08 12:28:21,639 - __main__ - INFO - Epoch 31 Batch 0: Train Loss = 0.2152
2023-11-08 12:28:34,917 - __main__ - INFO - Epoch 31 Batch 20: Train Loss = 0.2044
2023-11-08 12:28:49,592 - __main__ - INFO - Epoch 31 Batch 40: Train Loss = 0.1927
2023-11-08 12:29:04,185 - __main__ - INFO - Epoch 31 Batch 60: Train Loss = 0.1624
2023-11-08 12:29:19,030 - __main__ - INFO - Epoch 31 Batch 80: Train Loss = 0.1396
2023-11-08 12:29:33,083 - __main__ - INFO - Epoch 31 Batch 100: Train Loss = 0.1996
2023-11-08 12:29:47,149 - __main__ - INFO - Epoch 31 Batch 120: Train Loss = 0.0999
2023-11-08 12:29:57,433 - __main__ - INFO - Epoch 31: Loss = 0.1726 Valid loss = 0.1491 roc = 0.9119
2023-11-08 12:29:58,111 - __main__ - INFO - Epoch 32 Batch 0: Train Loss = 0.1374
2023-11-08 12:30:12,147 - __main__ - INFO - Epoch 32 Batch 20: Train Loss = 0.1629
2023-11-08 12:30:25,794 - __main__ - INFO - Epoch 32 Batch 40: Train Loss = 0.1501
2023-11-08 12:30:39,550 - __main__ - INFO - Epoch 32 Batch 60: Train Loss = 0.1685
2023-11-08 12:30:53,196 - __main__ - INFO - Epoch 32 Batch 80: Train Loss = 0.2318
2023-11-08 12:31:06,644 - __main__ - INFO - Epoch 32 Batch 100: Train Loss = 0.1532
2023-11-08 12:31:20,996 - __main__ - INFO - Epoch 32 Batch 120: Train Loss = 0.1526
2023-11-08 12:31:31,625 - __main__ - INFO - Epoch 32: Loss = 0.1736 Valid loss = 0.1493 roc = 0.9107
2023-11-08 12:31:32,316 - __main__ - INFO - Epoch 33 Batch 0: Train Loss = 0.0973
2023-11-08 12:31:46,212 - __main__ - INFO - Epoch 33 Batch 20: Train Loss = 0.1705
2023-11-08 12:31:59,871 - __main__ - INFO - Epoch 33 Batch 40: Train Loss = 0.1479
2023-11-08 12:32:14,327 - __main__ - INFO - Epoch 33 Batch 60: Train Loss = 0.2133
2023-11-08 12:32:28,358 - __main__ - INFO - Epoch 33 Batch 80: Train Loss = 0.1973
2023-11-08 12:32:42,642 - __main__ - INFO - Epoch 33 Batch 100: Train Loss = 0.1546
2023-11-08 12:32:56,729 - __main__ - INFO - Epoch 33 Batch 120: Train Loss = 0.1689
2023-11-08 12:33:07,288 - __main__ - INFO - Epoch 33: Loss = 0.1731 Valid loss = 0.1502 roc = 0.9149
2023-11-08 12:33:08,193 - __main__ - INFO - Epoch 34 Batch 0: Train Loss = 0.1262
2023-11-08 12:33:22,177 - __main__ - INFO - Epoch 34 Batch 20: Train Loss = 0.2572
2023-11-08 12:33:35,853 - __main__ - INFO - Epoch 34 Batch 40: Train Loss = 0.0931
2023-11-08 12:33:49,655 - __main__ - INFO - Epoch 34 Batch 60: Train Loss = 0.1724
2023-11-08 12:34:03,210 - __main__ - INFO - Epoch 34 Batch 80: Train Loss = 0.1821
2023-11-08 12:34:16,446 - __main__ - INFO - Epoch 34 Batch 100: Train Loss = 0.1487
2023-11-08 12:34:30,713 - __main__ - INFO - Epoch 34 Batch 120: Train Loss = 0.1317
2023-11-08 12:34:41,705 - __main__ - INFO - Epoch 34: Loss = 0.1697 Valid loss = 0.1472 roc = 0.9157
2023-11-08 12:34:42,365 - __main__ - INFO - Epoch 35 Batch 0: Train Loss = 0.1578
2023-11-08 12:34:56,545 - __main__ - INFO - Epoch 35 Batch 20: Train Loss = 0.1818
2023-11-08 12:35:11,129 - __main__ - INFO - Epoch 35 Batch 40: Train Loss = 0.2183
2023-11-08 12:35:25,192 - __main__ - INFO - Epoch 35 Batch 60: Train Loss = 0.1532
2023-11-08 12:35:39,288 - __main__ - INFO - Epoch 35 Batch 80: Train Loss = 0.1388
2023-11-08 12:35:53,118 - __main__ - INFO - Epoch 35 Batch 100: Train Loss = 0.1722
2023-11-08 12:36:06,939 - __main__ - INFO - Epoch 35 Batch 120: Train Loss = 0.1874
2023-11-08 12:36:18,472 - __main__ - INFO - Epoch 35: Loss = 0.1707 Valid loss = 0.1482 roc = 0.9144
2023-11-08 12:36:19,318 - __main__ - INFO - Epoch 36 Batch 0: Train Loss = 0.0789
2023-11-08 12:36:33,588 - __main__ - INFO - Epoch 36 Batch 20: Train Loss = 0.2401
2023-11-08 12:36:47,308 - __main__ - INFO - Epoch 36 Batch 40: Train Loss = 0.1562
2023-11-08 12:37:00,601 - __main__ - INFO - Epoch 36 Batch 60: Train Loss = 0.1952
2023-11-08 12:37:14,306 - __main__ - INFO - Epoch 36 Batch 80: Train Loss = 0.2365
2023-11-08 12:37:28,353 - __main__ - INFO - Epoch 36 Batch 100: Train Loss = 0.1492
2023-11-08 12:37:42,673 - __main__ - INFO - Epoch 36 Batch 120: Train Loss = 0.2077
2023-11-08 12:37:54,270 - __main__ - INFO - Epoch 36: Loss = 0.1713 Valid loss = 0.1481 roc = 0.9154
2023-11-08 12:37:54,993 - __main__ - INFO - Epoch 37 Batch 0: Train Loss = 0.1678
2023-11-08 12:38:09,925 - __main__ - INFO - Epoch 37 Batch 20: Train Loss = 0.1279
2023-11-08 12:38:23,341 - __main__ - INFO - Epoch 37 Batch 40: Train Loss = 0.1259
2023-11-08 12:38:37,064 - __main__ - INFO - Epoch 37 Batch 60: Train Loss = 0.1768
2023-11-08 12:38:51,171 - __main__ - INFO - Epoch 37 Batch 80: Train Loss = 0.2601
2023-11-08 12:39:05,499 - __main__ - INFO - Epoch 37 Batch 100: Train Loss = 0.1024
2023-11-08 12:39:18,975 - __main__ - INFO - Epoch 37 Batch 120: Train Loss = 0.1821
2023-11-08 12:39:29,494 - __main__ - INFO - Epoch 37: Loss = 0.1686 Valid loss = 0.1454 roc = 0.9131
2023-11-08 12:39:30,142 - __main__ - INFO - Epoch 38 Batch 0: Train Loss = 0.1655
2023-11-08 12:39:43,636 - __main__ - INFO - Epoch 38 Batch 20: Train Loss = 0.2368
2023-11-08 12:39:56,309 - __main__ - INFO - Epoch 38 Batch 40: Train Loss = 0.1205
2023-11-08 12:40:10,547 - __main__ - INFO - Epoch 38 Batch 60: Train Loss = 0.1775
2023-11-08 12:40:25,215 - __main__ - INFO - Epoch 38 Batch 80: Train Loss = 0.1351
2023-11-08 12:40:38,497 - __main__ - INFO - Epoch 38 Batch 100: Train Loss = 0.1705
2023-11-08 12:40:52,359 - __main__ - INFO - Epoch 38 Batch 120: Train Loss = 0.1805
2023-11-08 12:41:03,249 - __main__ - INFO - Epoch 38: Loss = 0.1697 Valid loss = 0.1458 roc = 0.9129
2023-11-08 12:41:03,931 - __main__ - INFO - Epoch 39 Batch 0: Train Loss = 0.1716
2023-11-08 12:41:18,073 - __main__ - INFO - Epoch 39 Batch 20: Train Loss = 0.1855
2023-11-08 12:41:31,121 - __main__ - INFO - Epoch 39 Batch 40: Train Loss = 0.1552
2023-11-08 12:41:44,225 - __main__ - INFO - Epoch 39 Batch 60: Train Loss = 0.1470
2023-11-08 12:41:57,892 - __main__ - INFO - Epoch 39 Batch 80: Train Loss = 0.1773
2023-11-08 12:42:11,015 - __main__ - INFO - Epoch 39 Batch 100: Train Loss = 0.1657
2023-11-08 12:42:24,432 - __main__ - INFO - Epoch 39 Batch 120: Train Loss = 0.2029
2023-11-08 12:42:34,283 - __main__ - INFO - Epoch 39: Loss = 0.1706 Valid loss = 0.1465 roc = 0.9147
2023-11-08 12:42:35,067 - __main__ - INFO - Epoch 40 Batch 0: Train Loss = 0.1540
2023-11-08 12:42:48,257 - __main__ - INFO - Epoch 40 Batch 20: Train Loss = 0.1285
2023-11-08 12:43:01,066 - __main__ - INFO - Epoch 40 Batch 40: Train Loss = 0.1329
2023-11-08 12:43:14,119 - __main__ - INFO - Epoch 40 Batch 60: Train Loss = 0.1737
2023-11-08 12:43:27,963 - __main__ - INFO - Epoch 40 Batch 80: Train Loss = 0.1205
2023-11-08 12:43:41,959 - __main__ - INFO - Epoch 40 Batch 100: Train Loss = 0.1452
2023-11-08 12:43:55,598 - __main__ - INFO - Epoch 40 Batch 120: Train Loss = 0.1582
2023-11-08 12:44:05,998 - __main__ - INFO - Epoch 40: Loss = 0.1707 Valid loss = 0.1478 roc = 0.9169
2023-11-08 12:44:06,791 - __main__ - INFO - Epoch 41 Batch 0: Train Loss = 0.2057
2023-11-08 12:44:20,810 - __main__ - INFO - Epoch 41 Batch 20: Train Loss = 0.1738
2023-11-08 12:44:35,324 - __main__ - INFO - Epoch 41 Batch 40: Train Loss = 0.1150
2023-11-08 12:44:49,506 - __main__ - INFO - Epoch 41 Batch 60: Train Loss = 0.1678
2023-11-08 12:45:03,120 - __main__ - INFO - Epoch 41 Batch 80: Train Loss = 0.1989
2023-11-08 12:45:15,938 - __main__ - INFO - Epoch 41 Batch 100: Train Loss = 0.1117
2023-11-08 12:45:30,213 - __main__ - INFO - Epoch 41 Batch 120: Train Loss = 0.1454
2023-11-08 12:45:40,542 - __main__ - INFO - Epoch 41: Loss = 0.1698 Valid loss = 0.1475 roc = 0.9117
2023-11-08 12:45:41,337 - __main__ - INFO - Epoch 42 Batch 0: Train Loss = 0.1855
2023-11-08 12:45:55,522 - __main__ - INFO - Epoch 42 Batch 20: Train Loss = 0.2091
2023-11-08 12:46:10,432 - __main__ - INFO - Epoch 42 Batch 40: Train Loss = 0.1616
2023-11-08 12:46:24,644 - __main__ - INFO - Epoch 42 Batch 60: Train Loss = 0.2111
2023-11-08 12:46:39,182 - __main__ - INFO - Epoch 42 Batch 80: Train Loss = 0.1903
2023-11-08 12:46:53,381 - __main__ - INFO - Epoch 42 Batch 100: Train Loss = 0.1813
2023-11-08 12:47:07,388 - __main__ - INFO - Epoch 42 Batch 120: Train Loss = 0.2300
2023-11-08 12:47:17,055 - __main__ - INFO - Epoch 42: Loss = 0.1709 Valid loss = 0.1481 roc = 0.9104
2023-11-08 12:47:17,763 - __main__ - INFO - Epoch 43 Batch 0: Train Loss = 0.1688
2023-11-08 12:47:30,559 - __main__ - INFO - Epoch 43 Batch 20: Train Loss = 0.1935
2023-11-08 12:47:44,570 - __main__ - INFO - Epoch 43 Batch 40: Train Loss = 0.1635
2023-11-08 12:47:59,238 - __main__ - INFO - Epoch 43 Batch 60: Train Loss = 0.1537
2023-11-08 12:48:12,981 - __main__ - INFO - Epoch 43 Batch 80: Train Loss = 0.1180
2023-11-08 12:48:26,401 - __main__ - INFO - Epoch 43 Batch 100: Train Loss = 0.1406
2023-11-08 12:48:39,760 - __main__ - INFO - Epoch 43 Batch 120: Train Loss = 0.1996
2023-11-08 12:48:49,806 - __main__ - INFO - Epoch 43: Loss = 0.1722 Valid loss = 0.1491 roc = 0.9113
2023-11-08 12:48:50,520 - __main__ - INFO - Epoch 44 Batch 0: Train Loss = 0.2005
2023-11-08 12:49:03,891 - __main__ - INFO - Epoch 44 Batch 20: Train Loss = 0.1566
2023-11-08 12:49:18,408 - __main__ - INFO - Epoch 44 Batch 40: Train Loss = 0.1224
2023-11-08 12:49:32,456 - __main__ - INFO - Epoch 44 Batch 60: Train Loss = 0.1948
2023-11-08 12:49:45,327 - __main__ - INFO - Epoch 44 Batch 80: Train Loss = 0.1383
2023-11-08 12:49:59,380 - __main__ - INFO - Epoch 44 Batch 100: Train Loss = 0.1319
2023-11-08 12:50:12,568 - __main__ - INFO - Epoch 44 Batch 120: Train Loss = 0.1594
2023-11-08 12:50:22,651 - __main__ - INFO - Epoch 44: Loss = 0.1711 Valid loss = 0.1463 roc = 0.9150
2023-11-08 12:50:23,280 - __main__ - INFO - Epoch 45 Batch 0: Train Loss = 0.1576
2023-11-08 12:50:37,260 - __main__ - INFO - Epoch 45 Batch 20: Train Loss = 0.1628
2023-11-08 12:50:50,777 - __main__ - INFO - Epoch 45 Batch 40: Train Loss = 0.1403
2023-11-08 12:51:03,916 - __main__ - INFO - Epoch 45 Batch 60: Train Loss = 0.1741
2023-11-08 12:51:17,230 - __main__ - INFO - Epoch 45 Batch 80: Train Loss = 0.1805
2023-11-08 12:51:31,317 - __main__ - INFO - Epoch 45 Batch 100: Train Loss = 0.1550
2023-11-08 12:51:44,993 - __main__ - INFO - Epoch 45 Batch 120: Train Loss = 0.1442
2023-11-08 12:51:55,493 - __main__ - INFO - Epoch 45: Loss = 0.1685 Valid loss = 0.1469 roc = 0.9167
2023-11-08 12:51:56,159 - __main__ - INFO - Epoch 46 Batch 0: Train Loss = 0.2382
2023-11-08 12:52:10,461 - __main__ - INFO - Epoch 46 Batch 20: Train Loss = 0.1255
2023-11-08 12:52:24,539 - __main__ - INFO - Epoch 46 Batch 40: Train Loss = 0.2215
2023-11-08 12:52:38,226 - __main__ - INFO - Epoch 46 Batch 60: Train Loss = 0.1210
2023-11-08 12:52:52,274 - __main__ - INFO - Epoch 46 Batch 80: Train Loss = 0.1099
2023-11-08 12:53:05,065 - __main__ - INFO - Epoch 46 Batch 100: Train Loss = 0.1592
2023-11-08 12:53:19,415 - __main__ - INFO - Epoch 46 Batch 120: Train Loss = 0.1351
2023-11-08 12:53:30,615 - __main__ - INFO - Epoch 46: Loss = 0.1654 Valid loss = 0.1439 roc = 0.9169
2023-11-08 12:53:31,356 - __main__ - INFO - Epoch 47 Batch 0: Train Loss = 0.2134
2023-11-08 12:53:44,444 - __main__ - INFO - Epoch 47 Batch 20: Train Loss = 0.1327
2023-11-08 12:53:58,103 - __main__ - INFO - Epoch 47 Batch 40: Train Loss = 0.2104
2023-11-08 12:54:12,549 - __main__ - INFO - Epoch 47 Batch 60: Train Loss = 0.1171
2023-11-08 12:54:26,565 - __main__ - INFO - Epoch 47 Batch 80: Train Loss = 0.1997
2023-11-08 12:54:40,223 - __main__ - INFO - Epoch 47 Batch 100: Train Loss = 0.1230
2023-11-08 12:54:53,872 - __main__ - INFO - Epoch 47 Batch 120: Train Loss = 0.1722
2023-11-08 12:55:03,861 - __main__ - INFO - Epoch 47: Loss = 0.1675 Valid loss = 0.1452 roc = 0.9186
2023-11-08 12:55:04,762 - __main__ - INFO - Epoch 48 Batch 0: Train Loss = 0.1463
2023-11-08 12:55:18,514 - __main__ - INFO - Epoch 48 Batch 20: Train Loss = 0.1519
2023-11-08 12:55:32,718 - __main__ - INFO - Epoch 48 Batch 40: Train Loss = 0.1814
2023-11-08 12:55:46,179 - __main__ - INFO - Epoch 48 Batch 60: Train Loss = 0.1572
2023-11-08 12:55:59,382 - __main__ - INFO - Epoch 48 Batch 80: Train Loss = 0.1945
2023-11-08 12:56:13,131 - __main__ - INFO - Epoch 48 Batch 100: Train Loss = 0.1802
2023-11-08 12:56:26,917 - __main__ - INFO - Epoch 48 Batch 120: Train Loss = 0.1270
2023-11-08 12:56:37,713 - __main__ - INFO - Epoch 48: Loss = 0.1673 Valid loss = 0.1468 roc = 0.9141
2023-11-08 12:56:38,334 - __main__ - INFO - Epoch 49 Batch 0: Train Loss = 0.1296
2023-11-08 12:56:53,110 - __main__ - INFO - Epoch 49 Batch 20: Train Loss = 0.2149
2023-11-08 12:57:06,761 - __main__ - INFO - Epoch 49 Batch 40: Train Loss = 0.1448
2023-11-08 12:57:21,426 - __main__ - INFO - Epoch 49 Batch 60: Train Loss = 0.1468
2023-11-08 12:57:36,183 - __main__ - INFO - Epoch 49 Batch 80: Train Loss = 0.1678
2023-11-08 12:57:50,957 - __main__ - INFO - Epoch 49 Batch 100: Train Loss = 0.1229
2023-11-08 12:58:04,818 - __main__ - INFO - Epoch 49 Batch 120: Train Loss = 0.1768
2023-11-08 12:58:15,324 - __main__ - INFO - Epoch 49: Loss = 0.1702 Valid loss = 0.1449 roc = 0.9163
2023-11-08 12:58:15,979 - __main__ - INFO - Epoch 50 Batch 0: Train Loss = 0.2289
2023-11-08 12:58:31,116 - __main__ - INFO - Epoch 50 Batch 20: Train Loss = 0.1479
2023-11-08 12:58:45,277 - __main__ - INFO - Epoch 50 Batch 40: Train Loss = 0.1891
2023-11-08 12:58:58,959 - __main__ - INFO - Epoch 50 Batch 60: Train Loss = 0.2568
2023-11-08 12:59:13,732 - __main__ - INFO - Epoch 50 Batch 80: Train Loss = 0.1412
2023-11-08 12:59:28,215 - __main__ - INFO - Epoch 50 Batch 100: Train Loss = 0.1494
2023-11-08 12:59:42,327 - __main__ - INFO - Epoch 50 Batch 120: Train Loss = 0.1419
2023-11-08 12:59:52,683 - __main__ - INFO - Epoch 50: Loss = 0.1632 Valid loss = 0.1452 roc = 0.9173
2023-11-08 12:59:53,364 - __main__ - INFO - Epoch 51 Batch 0: Train Loss = 0.1421
2023-11-08 13:00:08,709 - __main__ - INFO - Epoch 51 Batch 20: Train Loss = 0.1818
2023-11-08 13:00:22,259 - __main__ - INFO - Epoch 51 Batch 40: Train Loss = 0.1812
2023-11-08 13:00:36,541 - __main__ - INFO - Epoch 51 Batch 60: Train Loss = 0.1805
2023-11-08 13:00:49,853 - __main__ - INFO - Epoch 51 Batch 80: Train Loss = 0.1421
2023-11-08 13:01:03,358 - __main__ - INFO - Epoch 51 Batch 100: Train Loss = 0.1738
2023-11-08 13:01:17,403 - __main__ - INFO - Epoch 51 Batch 120: Train Loss = 0.1088
2023-11-08 13:01:28,993 - __main__ - INFO - Epoch 51: Loss = 0.1652 Valid loss = 0.1455 roc = 0.9155
2023-11-08 13:01:29,707 - __main__ - INFO - Epoch 52 Batch 0: Train Loss = 0.1395
2023-11-08 13:01:42,942 - __main__ - INFO - Epoch 52 Batch 20: Train Loss = 0.1774
2023-11-08 13:01:57,590 - __main__ - INFO - Epoch 52 Batch 40: Train Loss = 0.1840
2023-11-08 13:02:11,758 - __main__ - INFO - Epoch 52 Batch 60: Train Loss = 0.1816
2023-11-08 13:02:24,704 - __main__ - INFO - Epoch 52 Batch 80: Train Loss = 0.0824
2023-11-08 13:02:37,222 - __main__ - INFO - Epoch 52 Batch 100: Train Loss = 0.1359
2023-11-08 13:02:50,689 - __main__ - INFO - Epoch 52 Batch 120: Train Loss = 0.1367
2023-11-08 13:03:01,579 - __main__ - INFO - Epoch 52: Loss = 0.1643 Valid loss = 0.1458 roc = 0.9186
2023-11-08 13:03:02,377 - __main__ - INFO - Epoch 53 Batch 0: Train Loss = 0.1586
2023-11-08 13:03:15,367 - __main__ - INFO - Epoch 53 Batch 20: Train Loss = 0.2078
2023-11-08 13:03:28,577 - __main__ - INFO - Epoch 53 Batch 40: Train Loss = 0.1433
2023-11-08 13:03:41,684 - __main__ - INFO - Epoch 53 Batch 60: Train Loss = 0.1477
2023-11-08 13:03:54,578 - __main__ - INFO - Epoch 53 Batch 80: Train Loss = 0.1845
2023-11-08 13:04:07,547 - __main__ - INFO - Epoch 53 Batch 100: Train Loss = 0.1525
2023-11-08 13:04:20,530 - __main__ - INFO - Epoch 53 Batch 120: Train Loss = 0.1481
2023-11-08 13:04:30,359 - __main__ - INFO - Epoch 53: Loss = 0.1644 Valid loss = 0.1435 roc = 0.9169
2023-11-08 13:04:31,052 - __main__ - INFO - Epoch 54 Batch 0: Train Loss = 0.1626
2023-11-08 13:04:44,024 - __main__ - INFO - Epoch 54 Batch 20: Train Loss = 0.1828
2023-11-08 13:04:56,154 - __main__ - INFO - Epoch 54 Batch 40: Train Loss = 0.2045
2023-11-08 13:05:08,780 - __main__ - INFO - Epoch 54 Batch 60: Train Loss = 0.1257
2023-11-08 13:05:21,428 - __main__ - INFO - Epoch 54 Batch 80: Train Loss = 0.1560
2023-11-08 13:05:33,950 - __main__ - INFO - Epoch 54 Batch 100: Train Loss = 0.1692
2023-11-08 13:05:47,434 - __main__ - INFO - Epoch 54 Batch 120: Train Loss = 0.1423
2023-11-08 13:05:57,816 - __main__ - INFO - Epoch 54: Loss = 0.1605 Valid loss = 0.1445 roc = 0.9163
2023-11-08 13:05:58,370 - __main__ - INFO - Epoch 55 Batch 0: Train Loss = 0.1472
2023-11-08 13:06:10,365 - __main__ - INFO - Epoch 55 Batch 20: Train Loss = 0.1230
2023-11-08 13:06:22,760 - __main__ - INFO - Epoch 55 Batch 40: Train Loss = 0.1976
2023-11-08 13:06:34,790 - __main__ - INFO - Epoch 55 Batch 60: Train Loss = 0.1589
2023-11-08 13:06:47,334 - __main__ - INFO - Epoch 55 Batch 80: Train Loss = 0.1145
2023-11-08 13:07:00,824 - __main__ - INFO - Epoch 55 Batch 100: Train Loss = 0.1074
2023-11-08 13:07:13,508 - __main__ - INFO - Epoch 55 Batch 120: Train Loss = 0.1723
2023-11-08 13:07:23,048 - __main__ - INFO - Epoch 55: Loss = 0.1594 Valid loss = 0.1421 roc = 0.9181
2023-11-08 13:07:23,682 - __main__ - INFO - Epoch 56 Batch 0: Train Loss = 0.1744
2023-11-08 13:07:36,483 - __main__ - INFO - Epoch 56 Batch 20: Train Loss = 0.1831
2023-11-08 13:07:49,971 - __main__ - INFO - Epoch 56 Batch 40: Train Loss = 0.1710
2023-11-08 13:08:04,158 - __main__ - INFO - Epoch 56 Batch 60: Train Loss = 0.1546
2023-11-08 13:08:17,767 - __main__ - INFO - Epoch 56 Batch 80: Train Loss = 0.0892
2023-11-08 13:08:31,056 - __main__ - INFO - Epoch 56 Batch 100: Train Loss = 0.1176
2023-11-08 13:08:44,611 - __main__ - INFO - Epoch 56 Batch 120: Train Loss = 0.1133
2023-11-08 13:08:54,216 - __main__ - INFO - Epoch 56: Loss = 0.1632 Valid loss = 0.1459 roc = 0.9191
2023-11-08 13:08:54,865 - __main__ - INFO - Epoch 57 Batch 0: Train Loss = 0.1552
2023-11-08 13:09:07,803 - __main__ - INFO - Epoch 57 Batch 20: Train Loss = 0.1460
2023-11-08 13:09:20,215 - __main__ - INFO - Epoch 57 Batch 40: Train Loss = 0.1842
2023-11-08 13:09:32,998 - __main__ - INFO - Epoch 57 Batch 60: Train Loss = 0.1014
2023-11-08 13:09:46,746 - __main__ - INFO - Epoch 57 Batch 80: Train Loss = 0.1542
2023-11-08 13:09:59,237 - __main__ - INFO - Epoch 57 Batch 100: Train Loss = 0.1851
2023-11-08 13:10:11,915 - __main__ - INFO - Epoch 57 Batch 120: Train Loss = 0.1820
2023-11-08 13:10:20,861 - __main__ - INFO - Epoch 57: Loss = 0.1611 Valid loss = 0.1426 roc = 0.9176
2023-11-08 13:10:21,549 - __main__ - INFO - Epoch 58 Batch 0: Train Loss = 0.1832
2023-11-08 13:10:34,709 - __main__ - INFO - Epoch 58 Batch 20: Train Loss = 0.1157
2023-11-08 13:10:47,037 - __main__ - INFO - Epoch 58 Batch 40: Train Loss = 0.1679
2023-11-08 13:11:00,463 - __main__ - INFO - Epoch 58 Batch 60: Train Loss = 0.1942
2023-11-08 13:11:13,154 - __main__ - INFO - Epoch 58 Batch 80: Train Loss = 0.1651
2023-11-08 13:11:24,946 - __main__ - INFO - Epoch 58 Batch 100: Train Loss = 0.0897
2023-11-08 13:11:38,337 - __main__ - INFO - Epoch 58 Batch 120: Train Loss = 0.2232
2023-11-08 13:11:48,048 - __main__ - INFO - Epoch 58: Loss = 0.1611 Valid loss = 0.1424 roc = 0.9190
2023-11-08 13:11:48,519 - __main__ - INFO - Epoch 59 Batch 0: Train Loss = 0.1632
2023-11-08 13:12:01,351 - __main__ - INFO - Epoch 59 Batch 20: Train Loss = 0.1235
2023-11-08 13:12:13,524 - __main__ - INFO - Epoch 59 Batch 40: Train Loss = 0.1686
2023-11-08 13:12:25,733 - __main__ - INFO - Epoch 59 Batch 60: Train Loss = 0.1167
2023-11-08 13:12:38,421 - __main__ - INFO - Epoch 59 Batch 80: Train Loss = 0.1476
2023-11-08 13:12:51,205 - __main__ - INFO - Epoch 59 Batch 100: Train Loss = 0.1553
2023-11-08 13:13:03,782 - __main__ - INFO - Epoch 59 Batch 120: Train Loss = 0.1175
2023-11-08 13:13:13,445 - __main__ - INFO - Epoch 59: Loss = 0.1605 Valid loss = 0.1408 roc = 0.9216
2023-11-08 13:13:14,265 - __main__ - INFO - Epoch 60 Batch 0: Train Loss = 0.1149
2023-11-08 13:13:27,402 - __main__ - INFO - Epoch 60 Batch 20: Train Loss = 0.1661
2023-11-08 13:13:41,712 - __main__ - INFO - Epoch 60 Batch 40: Train Loss = 0.0979
2023-11-08 13:13:55,248 - __main__ - INFO - Epoch 60 Batch 60: Train Loss = 0.1621
2023-11-08 13:14:07,795 - __main__ - INFO - Epoch 60 Batch 80: Train Loss = 0.1101
2023-11-08 13:14:20,281 - __main__ - INFO - Epoch 60 Batch 100: Train Loss = 0.1806
2023-11-08 13:14:33,318 - __main__ - INFO - Epoch 60 Batch 120: Train Loss = 0.1945
2023-11-08 13:14:43,137 - __main__ - INFO - Epoch 60: Loss = 0.1621 Valid loss = 0.1442 roc = 0.9182
2023-11-08 13:14:43,848 - __main__ - INFO - Epoch 61 Batch 0: Train Loss = 0.2017
2023-11-08 13:14:55,944 - __main__ - INFO - Epoch 61 Batch 20: Train Loss = 0.1314
2023-11-08 13:15:08,404 - __main__ - INFO - Epoch 61 Batch 40: Train Loss = 0.1325
2023-11-08 13:15:21,431 - __main__ - INFO - Epoch 61 Batch 60: Train Loss = 0.1551
2023-11-08 13:15:34,951 - __main__ - INFO - Epoch 61 Batch 80: Train Loss = 0.1560
2023-11-08 13:15:47,748 - __main__ - INFO - Epoch 61 Batch 100: Train Loss = 0.1739
2023-11-08 13:15:59,826 - __main__ - INFO - Epoch 61 Batch 120: Train Loss = 0.2087
2023-11-08 13:16:09,819 - __main__ - INFO - Epoch 61: Loss = 0.1621 Valid loss = 0.1425 roc = 0.9183
2023-11-08 13:16:10,370 - __main__ - INFO - Epoch 62 Batch 0: Train Loss = 0.1340
2023-11-08 13:16:22,590 - __main__ - INFO - Epoch 62 Batch 20: Train Loss = 0.1423
2023-11-08 13:16:35,353 - __main__ - INFO - Epoch 62 Batch 40: Train Loss = 0.1927
2023-11-08 13:16:47,947 - __main__ - INFO - Epoch 62 Batch 60: Train Loss = 0.1675
2023-11-08 13:17:00,825 - __main__ - INFO - Epoch 62 Batch 80: Train Loss = 0.1510
2023-11-08 13:17:13,952 - __main__ - INFO - Epoch 62 Batch 100: Train Loss = 0.1630
2023-11-08 13:17:27,086 - __main__ - INFO - Epoch 62 Batch 120: Train Loss = 0.0978
2023-11-08 13:17:37,204 - __main__ - INFO - Epoch 62: Loss = 0.1615 Valid loss = 0.1420 roc = 0.9234
2023-11-08 13:17:37,901 - __main__ - INFO - Epoch 63 Batch 0: Train Loss = 0.1233
2023-11-08 13:17:50,320 - __main__ - INFO - Epoch 63 Batch 20: Train Loss = 0.1918
2023-11-08 13:18:03,189 - __main__ - INFO - Epoch 63 Batch 40: Train Loss = 0.1170
2023-11-08 13:18:16,060 - __main__ - INFO - Epoch 63 Batch 60: Train Loss = 0.1616
2023-11-08 13:18:28,452 - __main__ - INFO - Epoch 63 Batch 80: Train Loss = 0.1879
2023-11-08 13:18:42,174 - __main__ - INFO - Epoch 63 Batch 100: Train Loss = 0.2304
2023-11-08 13:18:55,468 - __main__ - INFO - Epoch 63 Batch 120: Train Loss = 0.0852
2023-11-08 13:19:07,064 - __main__ - INFO - Epoch 63: Loss = 0.1611 Valid loss = 0.1424 roc = 0.9234
2023-11-08 13:19:07,641 - __main__ - INFO - Epoch 64 Batch 0: Train Loss = 0.2304
2023-11-08 13:19:20,756 - __main__ - INFO - Epoch 64 Batch 20: Train Loss = 0.1934
2023-11-08 13:19:33,512 - __main__ - INFO - Epoch 64 Batch 40: Train Loss = 0.1243
2023-11-08 13:19:46,137 - __main__ - INFO - Epoch 64 Batch 60: Train Loss = 0.1682
2023-11-08 13:19:58,599 - __main__ - INFO - Epoch 64 Batch 80: Train Loss = 0.1779
2023-11-08 13:20:10,801 - __main__ - INFO - Epoch 64 Batch 100: Train Loss = 0.1193
2023-11-08 13:20:23,550 - __main__ - INFO - Epoch 64 Batch 120: Train Loss = 0.1402
2023-11-08 13:20:33,096 - __main__ - INFO - Epoch 64: Loss = 0.1595 Valid loss = 0.1400 roc = 0.9221
2023-11-08 13:20:33,707 - __main__ - INFO - Epoch 65 Batch 0: Train Loss = 0.1899
2023-11-08 13:20:46,188 - __main__ - INFO - Epoch 65 Batch 20: Train Loss = 0.1691
2023-11-08 13:20:58,323 - __main__ - INFO - Epoch 65 Batch 40: Train Loss = 0.0948
2023-11-08 13:21:10,460 - __main__ - INFO - Epoch 65 Batch 60: Train Loss = 0.1257
2023-11-08 13:21:22,957 - __main__ - INFO - Epoch 65 Batch 80: Train Loss = 0.1632
2023-11-08 13:21:36,604 - __main__ - INFO - Epoch 65 Batch 100: Train Loss = 0.1663
2023-11-08 13:21:49,001 - __main__ - INFO - Epoch 65 Batch 120: Train Loss = 0.1428
2023-11-08 13:21:58,791 - __main__ - INFO - Epoch 65: Loss = 0.1641 Valid loss = 0.1408 roc = 0.9212
2023-11-08 13:21:59,389 - __main__ - INFO - Epoch 66 Batch 0: Train Loss = 0.1795
2023-11-08 13:22:12,860 - __main__ - INFO - Epoch 66 Batch 20: Train Loss = 0.1706
2023-11-08 13:22:25,744 - __main__ - INFO - Epoch 66 Batch 40: Train Loss = 0.1434
2023-11-08 13:22:38,296 - __main__ - INFO - Epoch 66 Batch 60: Train Loss = 0.1624
2023-11-08 13:22:51,206 - __main__ - INFO - Epoch 66 Batch 80: Train Loss = 0.2004
2023-11-08 13:23:03,947 - __main__ - INFO - Epoch 66 Batch 100: Train Loss = 0.1587
2023-11-08 13:23:16,594 - __main__ - INFO - Epoch 66 Batch 120: Train Loss = 0.0954
2023-11-08 13:23:27,169 - __main__ - INFO - Epoch 66: Loss = 0.1616 Valid loss = 0.1415 roc = 0.9199
2023-11-08 13:23:27,865 - __main__ - INFO - Epoch 67 Batch 0: Train Loss = 0.1230
2023-11-08 13:23:40,878 - __main__ - INFO - Epoch 67 Batch 20: Train Loss = 0.1513
2023-11-08 13:23:54,249 - __main__ - INFO - Epoch 67 Batch 40: Train Loss = 0.1403
2023-11-08 13:24:07,857 - __main__ - INFO - Epoch 67 Batch 60: Train Loss = 0.1933
2023-11-08 13:24:20,978 - __main__ - INFO - Epoch 67 Batch 80: Train Loss = 0.1503
2023-11-08 13:24:33,259 - __main__ - INFO - Epoch 67 Batch 100: Train Loss = 0.1327
2023-11-08 13:24:45,897 - __main__ - INFO - Epoch 67 Batch 120: Train Loss = 0.2108
2023-11-08 13:24:55,969 - __main__ - INFO - Epoch 67: Loss = 0.1607 Valid loss = 0.1412 roc = 0.9210
2023-11-08 13:24:56,440 - __main__ - INFO - Epoch 68 Batch 0: Train Loss = 0.1325
2023-11-08 13:25:08,649 - __main__ - INFO - Epoch 68 Batch 20: Train Loss = 0.1986
2023-11-08 13:25:20,893 - __main__ - INFO - Epoch 68 Batch 40: Train Loss = 0.1955
2023-11-08 13:25:33,077 - __main__ - INFO - Epoch 68 Batch 60: Train Loss = 0.1494
2023-11-08 13:25:45,510 - __main__ - INFO - Epoch 68 Batch 80: Train Loss = 0.2536
2023-11-08 13:25:58,126 - __main__ - INFO - Epoch 68 Batch 100: Train Loss = 0.2080
2023-11-08 13:26:10,844 - __main__ - INFO - Epoch 68 Batch 120: Train Loss = 0.2209
2023-11-08 13:26:20,477 - __main__ - INFO - Epoch 68: Loss = 0.1611 Valid loss = 0.1416 roc = 0.9211
2023-11-08 13:26:21,254 - __main__ - INFO - Epoch 69 Batch 0: Train Loss = 0.1701
2023-11-08 13:26:33,737 - __main__ - INFO - Epoch 69 Batch 20: Train Loss = 0.1723
2023-11-08 13:26:46,906 - __main__ - INFO - Epoch 69 Batch 40: Train Loss = 0.1910
2023-11-08 13:26:59,574 - __main__ - INFO - Epoch 69 Batch 60: Train Loss = 0.1545
2023-11-08 13:27:12,062 - __main__ - INFO - Epoch 69 Batch 80: Train Loss = 0.1672
2023-11-08 13:27:25,045 - __main__ - INFO - Epoch 69 Batch 100: Train Loss = 0.1883
2023-11-08 13:27:37,639 - __main__ - INFO - Epoch 69 Batch 120: Train Loss = 0.1291
2023-11-08 13:27:47,482 - __main__ - INFO - Epoch 69: Loss = 0.1592 Valid loss = 0.1429 roc = 0.9173
2023-11-08 13:27:48,092 - __main__ - INFO - Epoch 70 Batch 0: Train Loss = 0.2047
2023-11-08 13:28:00,658 - __main__ - INFO - Epoch 70 Batch 20: Train Loss = 0.1822
2023-11-08 13:28:13,630 - __main__ - INFO - Epoch 70 Batch 40: Train Loss = 0.1638
2023-11-08 13:28:26,673 - __main__ - INFO - Epoch 70 Batch 60: Train Loss = 0.1856
2023-11-08 13:28:39,020 - __main__ - INFO - Epoch 70 Batch 80: Train Loss = 0.1845
2023-11-08 13:28:52,686 - __main__ - INFO - Epoch 70 Batch 100: Train Loss = 0.1515
2023-11-08 13:29:05,746 - __main__ - INFO - Epoch 70 Batch 120: Train Loss = 0.1205
2023-11-08 13:29:15,867 - __main__ - INFO - Epoch 70: Loss = 0.1601 Valid loss = 0.1448 roc = 0.9185
2023-11-08 13:29:16,505 - __main__ - INFO - Epoch 71 Batch 0: Train Loss = 0.1434
2023-11-08 13:29:30,392 - __main__ - INFO - Epoch 71 Batch 20: Train Loss = 0.1254
2023-11-08 13:29:43,029 - __main__ - INFO - Epoch 71 Batch 40: Train Loss = 0.1674
2023-11-08 13:29:55,995 - __main__ - INFO - Epoch 71 Batch 60: Train Loss = 0.1659
2023-11-08 13:30:08,537 - __main__ - INFO - Epoch 71 Batch 80: Train Loss = 0.1607
2023-11-08 13:30:21,862 - __main__ - INFO - Epoch 71 Batch 100: Train Loss = 0.1951
2023-11-08 13:30:33,894 - __main__ - INFO - Epoch 71 Batch 120: Train Loss = 0.1727
2023-11-08 13:30:43,743 - __main__ - INFO - Epoch 71: Loss = 0.1579 Valid loss = 0.1424 roc = 0.9226
2023-11-08 13:30:44,484 - __main__ - INFO - Epoch 72 Batch 0: Train Loss = 0.1570
2023-11-08 13:30:57,105 - __main__ - INFO - Epoch 72 Batch 20: Train Loss = 0.2115
2023-11-08 13:31:09,574 - __main__ - INFO - Epoch 72 Batch 40: Train Loss = 0.1936
2023-11-08 13:31:21,866 - __main__ - INFO - Epoch 72 Batch 60: Train Loss = 0.1152
2023-11-08 13:31:34,540 - __main__ - INFO - Epoch 72 Batch 80: Train Loss = 0.1098
2023-11-08 13:31:48,281 - __main__ - INFO - Epoch 72 Batch 100: Train Loss = 0.2156
2023-11-08 13:32:00,559 - __main__ - INFO - Epoch 72 Batch 120: Train Loss = 0.1636
2023-11-08 13:32:10,009 - __main__ - INFO - Epoch 72: Loss = 0.1594 Valid loss = 0.1401 roc = 0.9212
2023-11-08 13:32:10,701 - __main__ - INFO - Epoch 73 Batch 0: Train Loss = 0.1596
2023-11-08 13:32:22,999 - __main__ - INFO - Epoch 73 Batch 20: Train Loss = 0.1615
2023-11-08 13:32:34,875 - __main__ - INFO - Epoch 73 Batch 40: Train Loss = 0.1794
2023-11-08 13:32:47,839 - __main__ - INFO - Epoch 73 Batch 60: Train Loss = 0.0939
2023-11-08 13:33:00,051 - __main__ - INFO - Epoch 73 Batch 80: Train Loss = 0.1670
2023-11-08 13:33:12,167 - __main__ - INFO - Epoch 73 Batch 100: Train Loss = 0.1053
2023-11-08 13:33:24,515 - __main__ - INFO - Epoch 73 Batch 120: Train Loss = 0.1509
2023-11-08 13:33:34,423 - __main__ - INFO - Epoch 73: Loss = 0.1575 Valid loss = 0.1407 roc = 0.9196
2023-11-08 13:33:34,997 - __main__ - INFO - Epoch 74 Batch 0: Train Loss = 0.1266
2023-11-08 13:33:47,679 - __main__ - INFO - Epoch 74 Batch 20: Train Loss = 0.2133
2023-11-08 13:34:01,243 - __main__ - INFO - Epoch 74 Batch 40: Train Loss = 0.1930
2023-11-08 13:34:14,467 - __main__ - INFO - Epoch 74 Batch 60: Train Loss = 0.2092
2023-11-08 13:34:27,204 - __main__ - INFO - Epoch 74 Batch 80: Train Loss = 0.1742
2023-11-08 13:34:39,471 - __main__ - INFO - Epoch 74 Batch 100: Train Loss = 0.1368
2023-11-08 13:34:52,257 - __main__ - INFO - Epoch 74 Batch 120: Train Loss = 0.2448
2023-11-08 13:35:02,118 - __main__ - INFO - Epoch 74: Loss = 0.1577 Valid loss = 0.1393 roc = 0.9251
2023-11-08 13:35:02,769 - __main__ - INFO - Epoch 75 Batch 0: Train Loss = 0.2097
2023-11-08 13:35:15,573 - __main__ - INFO - Epoch 75 Batch 20: Train Loss = 0.1848
2023-11-08 13:35:28,239 - __main__ - INFO - Epoch 75 Batch 40: Train Loss = 0.2322
2023-11-08 13:35:40,256 - __main__ - INFO - Epoch 75 Batch 60: Train Loss = 0.1260
2023-11-08 13:35:53,034 - __main__ - INFO - Epoch 75 Batch 80: Train Loss = 0.1259
2023-11-08 13:36:05,268 - __main__ - INFO - Epoch 75 Batch 100: Train Loss = 0.1402
2023-11-08 13:36:17,962 - __main__ - INFO - Epoch 75 Batch 120: Train Loss = 0.1385
2023-11-08 13:36:27,570 - __main__ - INFO - Epoch 75: Loss = 0.1547 Valid loss = 0.1386 roc = 0.9227
2023-11-08 13:36:28,131 - __main__ - INFO - Epoch 76 Batch 0: Train Loss = 0.1502
2023-11-08 13:36:40,918 - __main__ - INFO - Epoch 76 Batch 20: Train Loss = 0.1455
2023-11-08 13:36:53,509 - __main__ - INFO - Epoch 76 Batch 40: Train Loss = 0.1775
2023-11-08 13:37:06,264 - __main__ - INFO - Epoch 76 Batch 60: Train Loss = 0.1428
2023-11-08 13:37:19,075 - __main__ - INFO - Epoch 76 Batch 80: Train Loss = 0.1361
2023-11-08 13:37:32,407 - __main__ - INFO - Epoch 76 Batch 100: Train Loss = 0.1334
2023-11-08 13:37:45,334 - __main__ - INFO - Epoch 76 Batch 120: Train Loss = 0.1662
2023-11-08 13:37:54,438 - __main__ - INFO - Epoch 76: Loss = 0.1576 Valid loss = 0.1390 roc = 0.9224
2023-11-08 13:37:54,914 - __main__ - INFO - Epoch 77 Batch 0: Train Loss = 0.1159
2023-11-08 13:38:07,382 - __main__ - INFO - Epoch 77 Batch 20: Train Loss = 0.2160
2023-11-08 13:38:20,478 - __main__ - INFO - Epoch 77 Batch 40: Train Loss = 0.2380
2023-11-08 13:38:32,691 - __main__ - INFO - Epoch 77 Batch 60: Train Loss = 0.1951
2023-11-08 13:38:45,700 - __main__ - INFO - Epoch 77 Batch 80: Train Loss = 0.1540
2023-11-08 13:38:58,813 - __main__ - INFO - Epoch 77 Batch 100: Train Loss = 0.1214
2023-11-08 13:39:12,551 - __main__ - INFO - Epoch 77 Batch 120: Train Loss = 0.0855
2023-11-08 13:39:22,926 - __main__ - INFO - Epoch 77: Loss = 0.1552 Valid loss = 0.1385 roc = 0.9253
2023-11-08 13:39:23,693 - __main__ - INFO - Epoch 78 Batch 0: Train Loss = 0.0918
2023-11-08 13:39:37,396 - __main__ - INFO - Epoch 78 Batch 20: Train Loss = 0.1718
2023-11-08 13:39:50,859 - __main__ - INFO - Epoch 78 Batch 40: Train Loss = 0.2258
2023-11-08 13:40:03,942 - __main__ - INFO - Epoch 78 Batch 60: Train Loss = 0.1491
2023-11-08 13:40:16,013 - __main__ - INFO - Epoch 78 Batch 80: Train Loss = 0.1976
2023-11-08 13:40:28,161 - __main__ - INFO - Epoch 78 Batch 100: Train Loss = 0.1201
2023-11-08 13:40:40,427 - __main__ - INFO - Epoch 78 Batch 120: Train Loss = 0.1746
2023-11-08 13:40:50,759 - __main__ - INFO - Epoch 78: Loss = 0.1676 Valid loss = 0.1429 roc = 0.9265
2023-11-08 13:40:51,522 - __main__ - INFO - Epoch 79 Batch 0: Train Loss = 0.1624
2023-11-08 13:41:03,950 - __main__ - INFO - Epoch 79 Batch 20: Train Loss = 0.1360
2023-11-08 13:41:17,163 - __main__ - INFO - Epoch 79 Batch 40: Train Loss = 0.1362
2023-11-08 13:41:29,722 - __main__ - INFO - Epoch 79 Batch 60: Train Loss = 0.0942
2023-11-08 13:41:42,064 - __main__ - INFO - Epoch 79 Batch 80: Train Loss = 0.1719
2023-11-08 13:41:54,901 - __main__ - INFO - Epoch 79 Batch 100: Train Loss = 0.1897
2023-11-08 13:42:08,746 - __main__ - INFO - Epoch 79 Batch 120: Train Loss = 0.1059
2023-11-08 13:42:18,380 - __main__ - INFO - Epoch 79: Loss = 0.1627 Valid loss = 0.1436 roc = 0.9247
2023-11-08 13:42:18,930 - __main__ - INFO - Epoch 80 Batch 0: Train Loss = 0.1813
2023-11-08 13:42:31,480 - __main__ - INFO - Epoch 80 Batch 20: Train Loss = 0.2132
2023-11-08 13:42:44,224 - __main__ - INFO - Epoch 80 Batch 40: Train Loss = 0.1200
2023-11-08 13:42:56,347 - __main__ - INFO - Epoch 80 Batch 60: Train Loss = 0.2239
2023-11-08 13:43:08,905 - __main__ - INFO - Epoch 80 Batch 80: Train Loss = 0.1675
2023-11-08 13:43:21,964 - __main__ - INFO - Epoch 80 Batch 100: Train Loss = 0.1400
2023-11-08 13:43:34,282 - __main__ - INFO - Epoch 80 Batch 120: Train Loss = 0.1357
2023-11-08 13:43:43,927 - __main__ - INFO - Epoch 80: Loss = 0.1615 Valid loss = 0.1431 roc = 0.9224
2023-11-08 13:43:44,625 - __main__ - INFO - Epoch 81 Batch 0: Train Loss = 0.1278
2023-11-08 13:43:58,090 - __main__ - INFO - Epoch 81 Batch 20: Train Loss = 0.1610
2023-11-08 13:44:13,012 - __main__ - INFO - Epoch 81 Batch 40: Train Loss = 0.1978
2023-11-08 13:44:26,567 - __main__ - INFO - Epoch 81 Batch 60: Train Loss = 0.2074
2023-11-08 13:44:40,276 - __main__ - INFO - Epoch 81 Batch 80: Train Loss = 0.1913
2023-11-08 13:44:53,165 - __main__ - INFO - Epoch 81 Batch 100: Train Loss = 0.1337
2023-11-08 13:45:05,514 - __main__ - INFO - Epoch 81 Batch 120: Train Loss = 0.1621
2023-11-08 13:45:15,971 - __main__ - INFO - Epoch 81: Loss = 0.1588 Valid loss = 0.1414 roc = 0.9229
2023-11-08 13:45:16,503 - __main__ - INFO - Epoch 82 Batch 0: Train Loss = 0.1429
2023-11-08 13:45:29,059 - __main__ - INFO - Epoch 82 Batch 20: Train Loss = 0.1331
2023-11-08 13:45:40,924 - __main__ - INFO - Epoch 82 Batch 40: Train Loss = 0.1268
2023-11-08 13:45:53,218 - __main__ - INFO - Epoch 82 Batch 60: Train Loss = 0.1777
2023-11-08 13:46:05,775 - __main__ - INFO - Epoch 82 Batch 80: Train Loss = 0.1600
2023-11-08 13:46:17,943 - __main__ - INFO - Epoch 82 Batch 100: Train Loss = 0.1385
2023-11-08 13:46:30,735 - __main__ - INFO - Epoch 82 Batch 120: Train Loss = 0.1632
2023-11-08 13:46:40,941 - __main__ - INFO - Epoch 82: Loss = 0.1555 Valid loss = 0.1389 roc = 0.9259
2023-11-08 13:46:41,753 - __main__ - INFO - Epoch 83 Batch 0: Train Loss = 0.1695
2023-11-08 13:46:53,742 - __main__ - INFO - Epoch 83 Batch 20: Train Loss = 0.1606
2023-11-08 13:47:06,152 - __main__ - INFO - Epoch 83 Batch 40: Train Loss = 0.1537
2023-11-08 13:47:18,966 - __main__ - INFO - Epoch 83 Batch 60: Train Loss = 0.1509
2023-11-08 13:47:31,168 - __main__ - INFO - Epoch 83 Batch 80: Train Loss = 0.1671
2023-11-08 13:47:43,943 - __main__ - INFO - Epoch 83 Batch 100: Train Loss = 0.1398
2023-11-08 13:47:56,995 - __main__ - INFO - Epoch 83 Batch 120: Train Loss = 0.1335
2023-11-08 13:48:07,364 - __main__ - INFO - Epoch 83: Loss = 0.1589 Valid loss = 0.1378 roc = 0.9295
2023-11-08 13:48:08,037 - __main__ - INFO - Epoch 84 Batch 0: Train Loss = 0.1138
2023-11-08 13:48:20,653 - __main__ - INFO - Epoch 84 Batch 20: Train Loss = 0.1554
2023-11-08 13:48:33,286 - __main__ - INFO - Epoch 84 Batch 40: Train Loss = 0.1793
2023-11-08 13:48:45,068 - __main__ - INFO - Epoch 84 Batch 60: Train Loss = 0.1029
2023-11-08 13:48:57,651 - __main__ - INFO - Epoch 84 Batch 80: Train Loss = 0.1594
2023-11-08 13:49:10,324 - __main__ - INFO - Epoch 84 Batch 100: Train Loss = 0.1851
2023-11-08 13:49:23,917 - __main__ - INFO - Epoch 84 Batch 120: Train Loss = 0.2537
2023-11-08 13:49:34,166 - __main__ - INFO - Epoch 84: Loss = 0.1558 Valid loss = 0.1383 roc = 0.9254
2023-11-08 13:49:34,691 - __main__ - INFO - Epoch 85 Batch 0: Train Loss = 0.1592
2023-11-08 13:49:47,021 - __main__ - INFO - Epoch 85 Batch 20: Train Loss = 0.1381
2023-11-08 13:50:00,061 - __main__ - INFO - Epoch 85 Batch 40: Train Loss = 0.1998
2023-11-08 13:50:11,677 - __main__ - INFO - Epoch 85 Batch 60: Train Loss = 0.1702
2023-11-08 13:50:23,893 - __main__ - INFO - Epoch 85 Batch 80: Train Loss = 0.1560
2023-11-08 13:50:36,912 - __main__ - INFO - Epoch 85 Batch 100: Train Loss = 0.1651
2023-11-08 13:50:49,559 - __main__ - INFO - Epoch 85 Batch 120: Train Loss = 0.1956
2023-11-08 13:50:59,436 - __main__ - INFO - Epoch 85: Loss = 0.1581 Valid loss = 0.1424 roc = 0.9200
2023-11-08 13:51:00,056 - __main__ - INFO - Epoch 86 Batch 0: Train Loss = 0.1349
2023-11-08 13:51:12,382 - __main__ - INFO - Epoch 86 Batch 20: Train Loss = 0.1518
2023-11-08 13:51:24,637 - __main__ - INFO - Epoch 86 Batch 40: Train Loss = 0.2271
2023-11-08 13:51:36,799 - __main__ - INFO - Epoch 86 Batch 60: Train Loss = 0.2028
2023-11-08 13:51:49,638 - __main__ - INFO - Epoch 86 Batch 80: Train Loss = 0.1190
2023-11-08 13:52:02,937 - __main__ - INFO - Epoch 86 Batch 100: Train Loss = 0.1944
2023-11-08 13:52:15,333 - __main__ - INFO - Epoch 86 Batch 120: Train Loss = 0.1521
2023-11-08 13:52:24,793 - __main__ - INFO - Epoch 86: Loss = 0.1608 Valid loss = 0.1375 roc = 0.9247
2023-11-08 13:52:25,245 - __main__ - INFO - Epoch 87 Batch 0: Train Loss = 0.1038
2023-11-08 13:52:37,762 - __main__ - INFO - Epoch 87 Batch 20: Train Loss = 0.1495
2023-11-08 13:52:51,203 - __main__ - INFO - Epoch 87 Batch 40: Train Loss = 0.1356
2023-11-08 13:53:03,835 - __main__ - INFO - Epoch 87 Batch 60: Train Loss = 0.1391
2023-11-08 13:53:15,629 - __main__ - INFO - Epoch 87 Batch 80: Train Loss = 0.1729
2023-11-08 13:53:28,780 - __main__ - INFO - Epoch 87 Batch 100: Train Loss = 0.2032
2023-11-08 13:53:41,802 - __main__ - INFO - Epoch 87 Batch 120: Train Loss = 0.1590
2023-11-08 13:53:51,598 - __main__ - INFO - Epoch 87: Loss = 0.1537 Valid loss = 0.1384 roc = 0.9239
2023-11-08 13:53:52,146 - __main__ - INFO - Epoch 88 Batch 0: Train Loss = 0.1525
2023-11-08 13:54:04,470 - __main__ - INFO - Epoch 88 Batch 20: Train Loss = 0.1684
2023-11-08 13:54:17,765 - __main__ - INFO - Epoch 88 Batch 40: Train Loss = 0.1371
2023-11-08 13:54:31,137 - __main__ - INFO - Epoch 88 Batch 60: Train Loss = 0.1531
2023-11-08 13:54:45,555 - __main__ - INFO - Epoch 88 Batch 80: Train Loss = 0.1491
2023-11-08 13:54:59,051 - __main__ - INFO - Epoch 88 Batch 100: Train Loss = 0.1031
2023-11-08 13:55:11,164 - __main__ - INFO - Epoch 88 Batch 120: Train Loss = 0.1540
2023-11-08 13:55:20,763 - __main__ - INFO - Epoch 88: Loss = 0.1555 Valid loss = 0.1388 roc = 0.9226
2023-11-08 13:55:21,501 - __main__ - INFO - Epoch 89 Batch 0: Train Loss = 0.1364
2023-11-08 13:55:34,340 - __main__ - INFO - Epoch 89 Batch 20: Train Loss = 0.2402
2023-11-08 13:55:46,970 - __main__ - INFO - Epoch 89 Batch 40: Train Loss = 0.1447
2023-11-08 13:55:59,582 - __main__ - INFO - Epoch 89 Batch 60: Train Loss = 0.1193
2023-11-08 13:56:11,788 - __main__ - INFO - Epoch 89 Batch 80: Train Loss = 0.1537
2023-11-08 13:56:24,658 - __main__ - INFO - Epoch 89 Batch 100: Train Loss = 0.1699
2023-11-08 13:56:38,016 - __main__ - INFO - Epoch 89 Batch 120: Train Loss = 0.2287
2023-11-08 13:56:47,620 - __main__ - INFO - Epoch 89: Loss = 0.1553 Valid loss = 0.1414 roc = 0.9146
2023-11-08 13:56:48,262 - __main__ - INFO - Epoch 90 Batch 0: Train Loss = 0.1330
2023-11-08 13:57:01,028 - __main__ - INFO - Epoch 90 Batch 20: Train Loss = 0.1224
2023-11-08 13:57:13,981 - __main__ - INFO - Epoch 90 Batch 40: Train Loss = 0.1945
2023-11-08 13:57:27,315 - __main__ - INFO - Epoch 90 Batch 60: Train Loss = 0.1393
2023-11-08 13:57:39,572 - __main__ - INFO - Epoch 90 Batch 80: Train Loss = 0.1510
2023-11-08 13:57:51,916 - __main__ - INFO - Epoch 90 Batch 100: Train Loss = 0.1345
2023-11-08 13:58:04,800 - __main__ - INFO - Epoch 90 Batch 120: Train Loss = 0.1595
2023-11-08 13:58:14,514 - __main__ - INFO - Epoch 90: Loss = 0.1549 Valid loss = 0.1374 roc = 0.9218
2023-11-08 13:58:15,275 - __main__ - INFO - Epoch 91 Batch 0: Train Loss = 0.1492
2023-11-08 13:58:28,695 - __main__ - INFO - Epoch 91 Batch 20: Train Loss = 0.1255
2023-11-08 13:58:41,521 - __main__ - INFO - Epoch 91 Batch 40: Train Loss = 0.2039
2023-11-08 13:58:54,312 - __main__ - INFO - Epoch 91 Batch 60: Train Loss = 0.1472
2023-11-08 13:59:06,760 - __main__ - INFO - Epoch 91 Batch 80: Train Loss = 0.1313
2023-11-08 13:59:19,193 - __main__ - INFO - Epoch 91 Batch 100: Train Loss = 0.2222
2023-11-08 13:59:32,886 - __main__ - INFO - Epoch 91 Batch 120: Train Loss = 0.0990
2023-11-08 13:59:43,307 - __main__ - INFO - Epoch 91: Loss = 0.1510 Valid loss = 0.1371 roc = 0.9239
2023-11-08 13:59:43,997 - __main__ - INFO - Epoch 92 Batch 0: Train Loss = 0.1573
2023-11-08 13:59:56,766 - __main__ - INFO - Epoch 92 Batch 20: Train Loss = 0.1440
2023-11-08 14:00:10,872 - __main__ - INFO - Epoch 92 Batch 40: Train Loss = 0.1276
2023-11-08 14:00:24,230 - __main__ - INFO - Epoch 92 Batch 60: Train Loss = 0.1318
2023-11-08 14:00:37,106 - __main__ - INFO - Epoch 92 Batch 80: Train Loss = 0.1023
2023-11-08 14:00:49,909 - __main__ - INFO - Epoch 92 Batch 100: Train Loss = 0.1736
2023-11-08 14:01:02,815 - __main__ - INFO - Epoch 92 Batch 120: Train Loss = 0.1778
2023-11-08 14:01:12,230 - __main__ - INFO - Epoch 92: Loss = 0.1497 Valid loss = 0.1372 roc = 0.9226
2023-11-08 14:01:12,945 - __main__ - INFO - Epoch 93 Batch 0: Train Loss = 0.1659
2023-11-08 14:01:25,596 - __main__ - INFO - Epoch 93 Batch 20: Train Loss = 0.1380
2023-11-08 14:01:39,302 - __main__ - INFO - Epoch 93 Batch 40: Train Loss = 0.1761
2023-11-08 14:01:51,328 - __main__ - INFO - Epoch 93 Batch 60: Train Loss = 0.0947
2023-11-08 14:02:04,746 - __main__ - INFO - Epoch 93 Batch 80: Train Loss = 0.1326
2023-11-08 14:02:17,162 - __main__ - INFO - Epoch 93 Batch 100: Train Loss = 0.1563
2023-11-08 14:02:29,630 - __main__ - INFO - Epoch 93 Batch 120: Train Loss = 0.2074
2023-11-08 14:02:39,791 - __main__ - INFO - Epoch 93: Loss = 0.1560 Valid loss = 0.1385 roc = 0.9237
2023-11-08 14:02:40,389 - __main__ - INFO - Epoch 94 Batch 0: Train Loss = 0.1407
2023-11-08 14:02:52,872 - __main__ - INFO - Epoch 94 Batch 20: Train Loss = 0.1278
2023-11-08 14:03:04,493 - __main__ - INFO - Epoch 94 Batch 40: Train Loss = 0.1287
2023-11-08 14:03:16,320 - __main__ - INFO - Epoch 94 Batch 60: Train Loss = 0.1130
2023-11-08 14:03:28,534 - __main__ - INFO - Epoch 94 Batch 80: Train Loss = 0.1584
2023-11-08 14:03:41,789 - __main__ - INFO - Epoch 94 Batch 100: Train Loss = 0.1261
2023-11-08 14:03:54,869 - __main__ - INFO - Epoch 94 Batch 120: Train Loss = 0.1139
2023-11-08 14:04:05,188 - __main__ - INFO - Epoch 94: Loss = 0.1502 Valid loss = 0.1379 roc = 0.9237
2023-11-08 14:04:05,915 - __main__ - INFO - Epoch 95 Batch 0: Train Loss = 0.1261
2023-11-08 14:04:19,080 - __main__ - INFO - Epoch 95 Batch 20: Train Loss = 0.1357
2023-11-08 14:04:31,728 - __main__ - INFO - Epoch 95 Batch 40: Train Loss = 0.1528
2023-11-08 14:04:45,557 - __main__ - INFO - Epoch 95 Batch 60: Train Loss = 0.1763
2023-11-08 14:04:59,780 - __main__ - INFO - Epoch 95 Batch 80: Train Loss = 0.1664
2023-11-08 14:05:12,946 - __main__ - INFO - Epoch 95 Batch 100: Train Loss = 0.1176
2023-11-08 14:05:25,990 - __main__ - INFO - Epoch 95 Batch 120: Train Loss = 0.1372
2023-11-08 14:05:36,223 - __main__ - INFO - Epoch 95: Loss = 0.1513 Valid loss = 0.1386 roc = 0.9240
2023-11-08 14:05:36,756 - __main__ - INFO - Epoch 96 Batch 0: Train Loss = 0.1653
2023-11-08 14:05:49,592 - __main__ - INFO - Epoch 96 Batch 20: Train Loss = 0.1952
2023-11-08 14:06:02,217 - __main__ - INFO - Epoch 96 Batch 40: Train Loss = 0.2323
2023-11-08 14:06:14,330 - __main__ - INFO - Epoch 96 Batch 60: Train Loss = 0.1498
2023-11-08 14:06:26,576 - __main__ - INFO - Epoch 96 Batch 80: Train Loss = 0.1494
2023-11-08 14:06:39,291 - __main__ - INFO - Epoch 96 Batch 100: Train Loss = 0.1839
2023-11-08 14:06:51,570 - __main__ - INFO - Epoch 96 Batch 120: Train Loss = 0.1910
2023-11-08 14:07:01,605 - __main__ - INFO - Epoch 96: Loss = 0.1528 Valid loss = 0.1378 roc = 0.9257
2023-11-08 14:07:02,325 - __main__ - INFO - Epoch 97 Batch 0: Train Loss = 0.1255
2023-11-08 14:07:14,519 - __main__ - INFO - Epoch 97 Batch 20: Train Loss = 0.1432
2023-11-08 14:07:27,525 - __main__ - INFO - Epoch 97 Batch 40: Train Loss = 0.1141
2023-11-08 14:07:40,163 - __main__ - INFO - Epoch 97 Batch 60: Train Loss = 0.1665
2023-11-08 14:07:53,053 - __main__ - INFO - Epoch 97 Batch 80: Train Loss = 0.1776
2023-11-08 14:08:05,845 - __main__ - INFO - Epoch 97 Batch 100: Train Loss = 0.1342
2023-11-08 14:08:18,703 - __main__ - INFO - Epoch 97 Batch 120: Train Loss = 0.0973
2023-11-08 14:08:27,882 - __main__ - INFO - Epoch 97: Loss = 0.1520 Valid loss = 0.1389 roc = 0.9230
2023-11-08 14:08:28,556 - __main__ - INFO - Epoch 98 Batch 0: Train Loss = 0.1118
2023-11-08 14:08:40,956 - __main__ - INFO - Epoch 98 Batch 20: Train Loss = 0.1490
2023-11-08 14:08:53,293 - __main__ - INFO - Epoch 98 Batch 40: Train Loss = 0.1268
2023-11-08 14:09:05,375 - __main__ - INFO - Epoch 98 Batch 60: Train Loss = 0.1569
2023-11-08 14:09:18,302 - __main__ - INFO - Epoch 98 Batch 80: Train Loss = 0.1438
2023-11-08 14:09:32,374 - __main__ - INFO - Epoch 98 Batch 100: Train Loss = 0.1417
2023-11-08 14:09:44,994 - __main__ - INFO - Epoch 98 Batch 120: Train Loss = 0.2011
2023-11-08 14:09:55,698 - __main__ - INFO - Epoch 98: Loss = 0.1511 Valid loss = 0.1394 roc = 0.9220
2023-11-08 14:09:56,350 - __main__ - INFO - Epoch 99 Batch 0: Train Loss = 0.1466
2023-11-08 14:10:09,957 - __main__ - INFO - Epoch 99 Batch 20: Train Loss = 0.1998
2023-11-08 14:10:23,436 - __main__ - INFO - Epoch 99 Batch 40: Train Loss = 0.1168
2023-11-08 14:10:36,932 - __main__ - INFO - Epoch 99 Batch 60: Train Loss = 0.1258
2023-11-08 14:10:48,898 - __main__ - INFO - Epoch 99 Batch 80: Train Loss = 0.1461
2023-11-08 14:11:01,652 - __main__ - INFO - Epoch 99 Batch 100: Train Loss = 0.1402
2023-11-08 14:11:14,177 - __main__ - INFO - Epoch 99 Batch 120: Train Loss = 0.1552
2023-11-08 14:11:23,992 - __main__ - INFO - Epoch 99: Loss = 0.1527 Valid loss = 0.1367 roc = 0.9257
2023-11-08 14:11:24,026 - __main__ - INFO - auroc 0.9295
2023-11-08 14:11:24,027 - __main__ - INFO - auprc 0.7015
2023-11-08 14:11:24,028 - __main__ - INFO - minpse 0.6233
2023-11-08 14:11:24,158 - __main__ - INFO - last saved model is in epoch 83
2023-11-08 14:11:24,728 - __main__ - INFO - Batch 0: Test Loss = 0.1067
2023-11-08 14:11:31,202 - __main__ - INFO - 
==>Predicting on test
2023-11-08 14:11:31,204 - __main__ - INFO - Test Loss = 0.1322
2023-11-08 14:11:31,276 - __main__ - INFO - load target data
2023-11-08 14:11:31,303 - __main__ - INFO - [[-0.84276482  0.37440202  0.67961237 -1.39897541 -0.48314192 -0.21203008
   1.58875966  0.79457896 -0.86126933 -0.48197292 -0.67452243  0.71372084
  -1.44708995 -0.77101282 -1.42318156 -0.58514053]
 [-0.84276482  0.37440202  0.67961237 -1.39897541 -0.48314192 -0.21203008
   1.58875966  0.79457896 -0.86126933 -0.48197292 -0.67452243  0.71372084
  -1.44708995 -0.77101282 -1.42318156 -0.58514053]
 [-0.53383302  0.43115698  0.89646606 -1.34242175 -0.69460513 -0.21203008
   0.89527972  0.03870413 -0.86126933  0.07263348 -0.50511021 -0.28657272
   0.39888266 -1.37943777 -0.82370897 -0.92211195]
 [-0.71138003  0.09062723  0.55763217 -1.39897541 -0.8637757  -0.21203008
   0.46852283  0.33565496 -0.86126933 -0.57440732 -0.50142734 -0.17115423
  -0.25396131 -0.77101282 -0.67384083 -2.43848334]
 [-0.71138003  0.09062723  0.55763217 -1.39897541 -0.8637757  -0.21203008
   0.46852283  0.33565496 -0.86126933 -0.57440732 -0.50142734 -0.17115423
  -0.25396131  0.64864541  0.0754999  -0.58514053]
 [-0.51252738  0.60142185  0.50341875 -1.45552908 -0.52543456 -0.41638034
   0.7885905   0.2816639  -0.58901728  0.59026612 -0.34306374 -0.74824667
  -0.18642573  0.64864541  0.0754999  -0.58514053]
 [-0.46281422 -0.24990251 -0.37754938 -1.11620709 -1.17392174  0.19667043
   0.68190128  0.4706326  -0.60716742 -0.14920908 -0.335698   -0.44046404
  -1.2444832   0.39513501 -0.52397268 -0.83786909]
 [-0.46281422 -0.24990251 -0.37754938 -1.11620709 -1.17392174  0.19667043
   0.68190128  0.4706326  -0.60716742 -0.14920908 -0.335698   -0.44046404
  -1.2444832   0.69934749 -0.67384083 -0.92211195]
 [-0.64391217  0.03387228 -0.17424904 -1.37069858 -0.55362966  0.40102068
   1.24201969  0.4706326  -0.60716742 -0.87019741 -0.4351356   0.02120991
  -1.06438832  0.69934749 -0.67384083 -0.92211195]
 [-0.64391217  0.03387228 -0.17424904 -1.37069858 -0.55362966  0.40102068
   1.24201969  0.4706326  -0.60716742 -0.87019741 -0.4351356   0.02120991
  -1.06438832  1.05426204 -0.67384083 -0.0796834 ]
 [-1.03806653  0.26089211  0.16458485 -1.45552908 -1.6391408   0.19667043
   1.24201969  0.4706326  -0.66161783 -0.5004598  -0.46091572  0.02120991
  -1.17694762  0.09092253 -1.34824748 -1.25908337]
 [-0.89957987  0.48791194  0.15103149 -1.41311383 -0.65231249  0.19667043
   0.14845517  0.4706326  -0.75236851 -0.94414493 -0.950738    0.17510123
  -1.04187645  0.54724125 -0.67384083 -1.0063548 ]
 [-0.5764443   0.31764706 -0.12003562 -1.3000065  -0.63821494  0.19667043
   0.86860741  0.4706326  -0.80681892 -0.24164348 -1.00229824 -4.7109481
   6.99485797  0.54724125 -0.67384083 -1.0063548 ]
 [-0.5764443   0.31764706 -0.12003562 -1.3000065  -0.63821494  0.19667043
   0.86860741  0.4706326  -0.80681892 -0.24164348 -1.00229824 -4.7109481
   6.99485797  0.24302877  0.82484063 -1.42756908]
 [-0.2320031  -0.64718721 -0.41820945 -1.15862233 -1.32899476 -0.21203008
   0.25514439  1.1995119  -0.75236851 -0.33407788 -0.93600651 -0.09420857
  -0.09637829  0.24302877  0.82484063 -1.42756908]
 [-0.2320031  -0.64718721 -0.41820945 -1.15862233 -1.32899476 -0.21203008
   0.25514439  1.1995119  -0.75236851 -0.33407788 -0.93600651 -0.09420857
  -0.09637829 -1.07522529 -1.04851119  2.27911655]
 [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043
  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097
   0.62400127 -1.07522529 -1.04851119  2.27911655]
 [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043
  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097
   0.62400127 -0.26399202 -0.67384083  2.27911655]
 [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043
  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097
   0.62400127 -0.77101282 -0.67384083 -0.66938338]
 [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043
  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097
   0.62400127 -0.77101282 -0.67384083  0.08880231]]
2023-11-08 14:11:31,312 - __main__ - INFO - 16
2023-11-08 14:11:31,313 - __main__ - INFO - 325
2023-11-08 14:11:31,815 - __main__ - INFO - Batch 0: Test Loss = 0.1479
2023-11-08 14:11:39,647 - __main__ - INFO - Batch 20: Test Loss = 0.1729
2023-11-08 14:11:47,763 - __main__ - INFO - Batch 40: Test Loss = 0.1553
2023-11-08 14:11:55,230 - __main__ - INFO - Batch 60: Test Loss = 0.1434
2023-11-08 14:12:02,893 - __main__ - INFO - Batch 80: Test Loss = 0.0945
2023-11-08 14:12:10,760 - __main__ - INFO - Batch 100: Test Loss = 0.0808
2023-11-08 14:12:18,265 - __main__ - INFO - Batch 120: Test Loss = 0.1037
2023-11-08 14:12:20,516 - __main__ - INFO - Training Student
2023-11-08 14:12:21,286 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 0.8190
2023-11-08 14:12:36,342 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 0.6159
2023-11-08 14:12:50,557 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 0.4856
2023-11-08 14:13:04,773 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 0.4410
2023-11-08 14:13:19,860 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 0.3073
2023-11-08 14:13:34,343 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 0.2774
2023-11-08 14:13:49,199 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 0.3056
2023-11-08 14:13:59,387 - __main__ - INFO - ------------ Save best model - AUROC: 0.6974 ------------
2023-11-08 14:14:00,082 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 0.3385
2023-11-08 14:14:14,746 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 0.3160
2023-11-08 14:14:29,301 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 0.3291
2023-11-08 14:14:43,732 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 0.3535
2023-11-08 14:14:58,679 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 0.2812
2023-11-08 14:15:15,080 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 0.2486
2023-11-08 14:15:30,213 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 0.2931
2023-11-08 14:15:40,744 - __main__ - INFO - ------------ Save best model - AUROC: 0.7162 ------------
2023-11-08 14:15:41,643 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 0.3378
2023-11-08 14:15:57,001 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 0.3159
2023-11-08 14:16:10,769 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 0.3231
2023-11-08 14:16:25,020 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 0.3503
2023-11-08 14:16:40,161 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 0.2509
2023-11-08 14:16:54,892 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 0.2357
2023-11-08 14:17:10,297 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 0.2852
2023-11-08 14:17:21,059 - __main__ - INFO - ------------ Save best model - AUROC: 0.7667 ------------
2023-11-08 14:17:21,672 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 0.3118
2023-11-08 14:17:37,274 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 0.2840
2023-11-08 14:17:51,684 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 0.2819
2023-11-08 14:18:05,746 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 0.3175
2023-11-08 14:18:20,677 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 0.2569
2023-11-08 14:18:35,791 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 0.2193
2023-11-08 14:18:50,737 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 0.2580
2023-11-08 14:19:00,917 - __main__ - INFO - ------------ Save best model - AUROC: 0.7952 ------------
2023-11-08 14:19:01,647 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 0.2732
2023-11-08 14:19:15,946 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 0.2820
2023-11-08 14:19:30,244 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 0.3043
2023-11-08 14:19:44,605 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 0.3062
2023-11-08 14:20:00,194 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 0.2524
2023-11-08 14:20:16,073 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 0.2225
2023-11-08 14:20:31,409 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 0.2576
2023-11-08 14:20:43,447 - __main__ - INFO - ------------ Save best model - AUROC: 0.8069 ------------
2023-11-08 14:20:44,169 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 0.2703
2023-11-08 14:20:59,629 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 0.2872
2023-11-08 14:21:14,025 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 0.3125
2023-11-08 14:21:29,073 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 0.2920
2023-11-08 14:21:43,507 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 0.2291
2023-11-08 14:21:58,351 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 0.2162
2023-11-08 14:22:12,814 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 0.2799
2023-11-08 14:22:23,334 - __main__ - INFO - ------------ Save best model - AUROC: 0.8134 ------------
2023-11-08 14:22:23,976 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 0.2749
2023-11-08 14:22:40,106 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 0.2999
2023-11-08 14:22:54,409 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 0.2968
2023-11-08 14:23:08,947 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 0.3004
2023-11-08 14:23:23,672 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 0.2043
2023-11-08 14:23:38,680 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 0.2010
2023-11-08 14:23:53,750 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 0.2113
2023-11-08 14:24:04,472 - __main__ - INFO - ------------ Save best model - AUROC: 0.8528 ------------
2023-11-08 14:24:05,134 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 0.2521
2023-11-08 14:24:20,628 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 0.2381
2023-11-08 14:24:34,976 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 0.2716
2023-11-08 14:24:50,399 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 0.2288
2023-11-08 14:25:05,681 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 0.1840
2023-11-08 14:25:21,693 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 0.1388
2023-11-08 14:25:37,265 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 0.1703
2023-11-08 14:25:48,969 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 0.2370
2023-11-08 14:26:04,315 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 0.2625
2023-11-08 14:26:18,984 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 0.2536
2023-11-08 14:26:32,926 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 0.2303
2023-11-08 14:26:47,697 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 0.1805
2023-11-08 14:27:02,674 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 0.1607
2023-11-08 14:27:17,013 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 0.1672
2023-11-08 14:27:27,490 - __main__ - INFO - ------------ Save best model - AUROC: 0.8616 ------------
2023-11-08 14:27:28,198 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 0.2591
2023-11-08 14:27:43,431 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 0.2424
2023-11-08 14:27:58,648 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 0.2499
2023-11-08 14:28:14,385 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 0.2284
2023-11-08 14:28:29,788 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 0.1685
2023-11-08 14:28:44,956 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 0.1465
2023-11-08 14:28:59,624 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 0.1655
2023-11-08 14:29:10,401 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 0.2141
2023-11-08 14:29:25,317 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 0.2453
2023-11-08 14:29:39,820 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 0.2406
2023-11-08 14:29:54,658 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 0.2277
2023-11-08 14:30:11,141 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 0.1853
2023-11-08 14:30:27,427 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 0.1443
2023-11-08 14:30:43,182 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 0.1817
2023-11-08 14:30:55,397 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 0.2349
2023-11-08 14:31:11,446 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 0.2641
2023-11-08 14:31:26,244 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 0.2402
2023-11-08 14:31:40,639 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 0.2328
2023-11-08 14:31:56,724 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 0.1550
2023-11-08 14:32:11,114 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 0.1456
2023-11-08 14:32:26,425 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 0.1642
2023-11-08 14:32:37,028 - __main__ - INFO - ------------ Save best model - AUROC: 0.8648 ------------
2023-11-08 14:32:37,708 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 0.2518
2023-11-08 14:32:53,024 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 0.2613
2023-11-08 14:33:07,827 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 0.2861
2023-11-08 14:33:22,816 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 0.2838
2023-11-08 14:33:37,720 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 0.2118
2023-11-08 14:33:52,988 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 0.1874
2023-11-08 14:34:07,961 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 0.2417
2023-11-08 14:34:19,245 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 0.2420
2023-11-08 14:34:34,824 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 0.2514
2023-11-08 14:34:49,714 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 0.2386
2023-11-08 14:35:04,839 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 0.2368
2023-11-08 14:35:19,846 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 0.1554
2023-11-08 14:35:35,586 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 0.1488
2023-11-08 14:35:51,487 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 0.1759
2023-11-08 14:36:02,719 - __main__ - INFO - ------------ Save best model - AUROC: 0.8700 ------------
2023-11-08 14:36:03,362 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 0.2305
2023-11-08 14:36:19,195 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 0.2284
2023-11-08 14:36:34,118 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 0.2320
2023-11-08 14:36:49,154 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 0.2284
2023-11-08 14:37:04,162 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 0.1658
2023-11-08 14:37:18,963 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 0.1407
2023-11-08 14:37:33,768 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 0.1531
2023-11-08 14:37:44,531 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 0.2324
2023-11-08 14:37:59,488 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 0.2393
2023-11-08 14:38:13,842 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 0.2390
2023-11-08 14:38:28,603 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 0.2294
2023-11-08 14:38:43,487 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 0.1705
2023-11-08 14:38:58,474 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 0.1377
2023-11-08 14:39:13,740 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 0.1602
2023-11-08 14:39:23,563 - __main__ - INFO - ------------ Save best model - AUROC: 0.8728 ------------
2023-11-08 14:39:24,259 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 0.2249
2023-11-08 14:39:39,653 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 0.2343
2023-11-08 14:39:54,000 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 0.2218
2023-11-08 14:40:08,329 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 0.2104
2023-11-08 14:40:23,445 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 0.1579
2023-11-08 14:40:38,576 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 0.1206
2023-11-08 14:40:54,012 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 0.1641
2023-11-08 14:41:05,990 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 0.2313
2023-11-08 14:41:21,540 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 0.2152
2023-11-08 14:41:35,806 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 0.2299
2023-11-08 14:41:51,056 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 0.2124
2023-11-08 14:42:06,728 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 0.1663
2023-11-08 14:42:21,851 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 0.1263
2023-11-08 14:42:37,075 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 0.1474
2023-11-08 14:42:47,628 - __main__ - INFO - ------------ Save best model - AUROC: 0.8741 ------------
2023-11-08 14:42:48,416 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 0.2207
2023-11-08 14:43:04,920 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 0.2499
2023-11-08 14:43:19,089 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 0.2441
2023-11-08 14:43:33,532 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 0.2337
2023-11-08 14:43:48,879 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 0.1686
2023-11-08 14:44:04,511 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 0.1435
2023-11-08 14:44:19,071 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 0.1704
2023-11-08 14:44:30,660 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 0.2013
2023-11-08 14:44:46,241 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 0.2533
2023-11-08 14:45:01,292 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 0.2407
2023-11-08 14:45:15,875 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 0.2163
2023-11-08 14:45:31,349 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 0.1631
2023-11-08 14:45:47,054 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 0.1395
2023-11-08 14:46:03,173 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 0.1626
2023-11-08 14:46:14,247 - __main__ - INFO - ------------ Save best model - AUROC: 0.8759 ------------
2023-11-08 14:46:15,099 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 0.2415
2023-11-08 14:46:30,422 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 0.2310
2023-11-08 14:46:45,256 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 0.2254
2023-11-08 14:47:00,078 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 0.2241
2023-11-08 14:47:14,856 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 0.1757
2023-11-08 14:47:29,794 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 0.1292
2023-11-08 14:47:45,100 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 0.1796
2023-11-08 14:47:55,260 - __main__ - INFO - ------------ Save best model - AUROC: 0.8783 ------------
2023-11-08 14:47:56,029 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 0.2299
2023-11-08 14:48:10,907 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 0.2407
2023-11-08 14:48:25,611 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 0.2253
2023-11-08 14:48:40,102 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 0.2162
2023-11-08 14:48:54,795 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 0.1742
2023-11-08 14:49:09,923 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 0.1251
2023-11-08 14:49:24,328 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 0.1760
2023-11-08 14:49:35,149 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 0.2199
2023-11-08 14:49:50,746 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 0.2208
2023-11-08 14:50:05,331 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 0.2034
2023-11-08 14:50:20,723 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 0.2287
2023-11-08 14:50:35,326 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 0.1558
2023-11-08 14:50:53,042 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 0.1245
2023-11-08 14:51:07,917 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 0.1575
2023-11-08 14:51:18,665 - __main__ - INFO - ------------ Save best model - AUROC: 0.8795 ------------
2023-11-08 14:51:19,358 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 0.2401
2023-11-08 14:51:35,520 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 0.2449
2023-11-08 14:51:49,971 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 0.2236
2023-11-08 14:52:04,735 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 0.2216
2023-11-08 14:52:20,500 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 0.1716
2023-11-08 14:52:35,710 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 0.1269
2023-11-08 14:52:49,716 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 0.1648
2023-11-08 14:53:00,484 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 0.2085
2023-11-08 14:53:14,965 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 0.2278
2023-11-08 14:53:29,251 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 0.2183
2023-11-08 14:53:43,826 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 0.2152
2023-11-08 14:53:59,125 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 0.1706
2023-11-08 14:54:14,385 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 0.1313
2023-11-08 14:54:29,289 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 0.1501
2023-11-08 14:54:39,891 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 0.2185
2023-11-08 14:54:54,570 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 0.2290
2023-11-08 14:55:08,999 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 0.2125
2023-11-08 14:55:23,869 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 0.2087
2023-11-08 14:55:38,750 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 0.1570
2023-11-08 14:55:54,011 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 0.1383
2023-11-08 14:56:09,589 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 0.1742
2023-11-08 14:56:20,587 - __main__ - INFO - ------------ Save best model - AUROC: 0.8811 ------------
2023-11-08 14:56:21,348 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 0.2267
2023-11-08 14:56:36,924 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 0.2520
2023-11-08 14:56:53,619 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 0.2259
2023-11-08 14:57:07,244 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 0.2259
2023-11-08 14:57:21,741 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 0.1664
2023-11-08 14:57:38,855 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 0.1305
2023-11-08 14:57:53,196 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 0.1704
2023-11-08 14:58:04,027 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 0.2190
2023-11-08 14:58:18,071 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 0.2393
2023-11-08 14:58:31,893 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 0.2417
2023-11-08 14:58:46,369 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 0.2397
2023-11-08 14:59:00,351 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 0.1667
2023-11-08 14:59:15,295 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 0.1293
2023-11-08 14:59:30,111 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 0.1744
2023-11-08 14:59:40,686 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 0.2133
2023-11-08 14:59:54,896 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 0.2494
2023-11-08 15:00:09,584 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 0.2318
2023-11-08 15:00:23,839 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 0.2428
2023-11-08 15:00:38,409 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 0.1733
2023-11-08 15:00:53,034 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 0.1153
2023-11-08 15:01:08,086 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 0.1491
2023-11-08 15:01:19,468 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 0.2187
2023-11-08 15:01:35,665 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 0.2452
2023-11-08 15:01:50,670 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 0.2111
2023-11-08 15:02:04,697 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 0.1922
2023-11-08 15:02:19,774 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 0.1606
2023-11-08 15:02:34,308 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 0.1341
2023-11-08 15:02:48,548 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 0.1468
2023-11-08 15:02:58,939 - __main__ - INFO - Epoch 30 Batch 0: Train Loss = 0.2018
2023-11-08 15:03:13,702 - __main__ - INFO - Epoch 30 Batch 20: Train Loss = 0.2524
2023-11-08 15:03:28,293 - __main__ - INFO - Epoch 30 Batch 40: Train Loss = 0.2097
2023-11-08 15:03:42,138 - __main__ - INFO - Epoch 30 Batch 60: Train Loss = 0.2170
2023-11-08 15:03:56,890 - __main__ - INFO - Epoch 30 Batch 80: Train Loss = 0.1650
2023-11-08 15:04:11,457 - __main__ - INFO - Epoch 30 Batch 100: Train Loss = 0.1348
2023-11-08 15:04:25,736 - __main__ - INFO - Epoch 30 Batch 120: Train Loss = 0.1753
2023-11-08 15:04:35,604 - __main__ - INFO - ------------ Save best model - AUROC: 0.8859 ------------
2023-11-08 15:04:36,197 - __main__ - INFO - Epoch 31 Batch 0: Train Loss = 0.2105
2023-11-08 15:04:51,193 - __main__ - INFO - Epoch 31 Batch 20: Train Loss = 0.2387
2023-11-08 15:05:05,832 - __main__ - INFO - Epoch 31 Batch 40: Train Loss = 0.2172
2023-11-08 15:05:19,670 - __main__ - INFO - Epoch 31 Batch 60: Train Loss = 0.2388
2023-11-08 15:05:34,043 - __main__ - INFO - Epoch 31 Batch 80: Train Loss = 0.1647
2023-11-08 15:05:48,319 - __main__ - INFO - Epoch 31 Batch 100: Train Loss = 0.1437
2023-11-08 15:06:02,095 - __main__ - INFO - Epoch 31 Batch 120: Train Loss = 0.1504
2023-11-08 15:06:12,966 - __main__ - INFO - Epoch 32 Batch 0: Train Loss = 0.2087
2023-11-08 15:06:28,860 - __main__ - INFO - Epoch 32 Batch 20: Train Loss = 0.2157
2023-11-08 15:06:43,975 - __main__ - INFO - Epoch 32 Batch 40: Train Loss = 0.2147
2023-11-08 15:06:59,668 - __main__ - INFO - Epoch 32 Batch 60: Train Loss = 0.2353
2023-11-08 15:07:15,270 - __main__ - INFO - Epoch 32 Batch 80: Train Loss = 0.1773
2023-11-08 15:07:30,001 - __main__ - INFO - Epoch 32 Batch 100: Train Loss = 0.1394
2023-11-08 15:07:45,210 - __main__ - INFO - Epoch 32 Batch 120: Train Loss = 0.1622
2023-11-08 15:07:55,673 - __main__ - INFO - ------------ Save best model - AUROC: 0.8862 ------------
2023-11-08 15:07:56,278 - __main__ - INFO - Epoch 33 Batch 0: Train Loss = 0.2238
2023-11-08 15:08:10,910 - __main__ - INFO - Epoch 33 Batch 20: Train Loss = 0.2605
2023-11-08 15:08:25,062 - __main__ - INFO - Epoch 33 Batch 40: Train Loss = 0.2058
2023-11-08 15:08:39,179 - __main__ - INFO - Epoch 33 Batch 60: Train Loss = 0.2139
2023-11-08 15:08:53,935 - __main__ - INFO - Epoch 33 Batch 80: Train Loss = 0.1686
2023-11-08 15:09:08,476 - __main__ - INFO - Epoch 33 Batch 100: Train Loss = 0.1349
2023-11-08 15:09:22,665 - __main__ - INFO - Epoch 33 Batch 120: Train Loss = 0.1483
2023-11-08 15:09:32,976 - __main__ - INFO - ------------ Save best model - AUROC: 0.8875 ------------
2023-11-08 15:09:33,550 - __main__ - INFO - Epoch 34 Batch 0: Train Loss = 0.2201
2023-11-08 15:09:48,217 - __main__ - INFO - Epoch 34 Batch 20: Train Loss = 0.2327
2023-11-08 15:10:02,411 - __main__ - INFO - Epoch 34 Batch 40: Train Loss = 0.2228
2023-11-08 15:10:17,075 - __main__ - INFO - Epoch 34 Batch 60: Train Loss = 0.2360
2023-11-08 15:10:31,849 - __main__ - INFO - Epoch 34 Batch 80: Train Loss = 0.1593
2023-11-08 15:10:46,904 - __main__ - INFO - Epoch 34 Batch 100: Train Loss = 0.1346
2023-11-08 15:11:01,881 - __main__ - INFO - Epoch 34 Batch 120: Train Loss = 0.1569
2023-11-08 15:11:12,123 - __main__ - INFO - ------------ Save best model - AUROC: 0.8882 ------------
2023-11-08 15:11:12,956 - __main__ - INFO - Epoch 35 Batch 0: Train Loss = 0.2408
2023-11-08 15:11:28,941 - __main__ - INFO - Epoch 35 Batch 20: Train Loss = 0.2344
2023-11-08 15:11:43,776 - __main__ - INFO - Epoch 35 Batch 40: Train Loss = 0.2162
2023-11-08 15:12:00,084 - __main__ - INFO - Epoch 35 Batch 60: Train Loss = 0.2039
2023-11-08 15:12:16,586 - __main__ - INFO - Epoch 35 Batch 80: Train Loss = 0.1586
2023-11-08 15:12:30,977 - __main__ - INFO - Epoch 35 Batch 100: Train Loss = 0.1255
2023-11-08 15:12:45,482 - __main__ - INFO - Epoch 35 Batch 120: Train Loss = 0.1527
2023-11-08 15:12:55,919 - __main__ - INFO - Epoch 36 Batch 0: Train Loss = 0.2416
2023-11-08 15:13:10,897 - __main__ - INFO - Epoch 36 Batch 20: Train Loss = 0.2325
2023-11-08 15:13:24,657 - __main__ - INFO - Epoch 36 Batch 40: Train Loss = 0.2403
2023-11-08 15:13:38,750 - __main__ - INFO - Epoch 36 Batch 60: Train Loss = 0.2137
2023-11-08 15:13:54,306 - __main__ - INFO - Epoch 36 Batch 80: Train Loss = 0.1687
2023-11-08 15:14:09,685 - __main__ - INFO - Epoch 36 Batch 100: Train Loss = 0.1154
2023-11-08 15:14:24,755 - __main__ - INFO - Epoch 36 Batch 120: Train Loss = 0.1580
2023-11-08 15:14:35,900 - __main__ - INFO - Epoch 37 Batch 0: Train Loss = 0.2093
2023-11-08 15:14:50,358 - __main__ - INFO - Epoch 37 Batch 20: Train Loss = 0.2387
2023-11-08 15:15:05,889 - __main__ - INFO - Epoch 37 Batch 40: Train Loss = 0.2060
2023-11-08 15:15:24,126 - __main__ - INFO - Epoch 37 Batch 60: Train Loss = 0.2313
2023-11-08 15:15:40,205 - __main__ - INFO - Epoch 37 Batch 80: Train Loss = 0.1535
2023-11-08 15:15:55,059 - __main__ - INFO - Epoch 37 Batch 100: Train Loss = 0.1106
2023-11-08 15:16:10,460 - __main__ - INFO - Epoch 37 Batch 120: Train Loss = 0.1477
2023-11-08 15:16:21,566 - __main__ - INFO - Epoch 38 Batch 0: Train Loss = 0.2203
2023-11-08 15:16:37,854 - __main__ - INFO - Epoch 38 Batch 20: Train Loss = 0.2341
2023-11-08 15:16:52,724 - __main__ - INFO - Epoch 38 Batch 40: Train Loss = 0.2406
2023-11-08 15:17:08,186 - __main__ - INFO - Epoch 38 Batch 60: Train Loss = 0.1871
2023-11-08 15:17:22,455 - __main__ - INFO - Epoch 38 Batch 80: Train Loss = 0.1464
2023-11-08 15:17:37,078 - __main__ - INFO - Epoch 38 Batch 100: Train Loss = 0.1200
2023-11-08 15:17:52,163 - __main__ - INFO - Epoch 38 Batch 120: Train Loss = 0.1587
2023-11-08 15:18:03,678 - __main__ - INFO - Epoch 39 Batch 0: Train Loss = 0.2251
2023-11-08 15:18:18,769 - __main__ - INFO - Epoch 39 Batch 20: Train Loss = 0.2297
2023-11-08 15:18:33,113 - __main__ - INFO - Epoch 39 Batch 40: Train Loss = 0.2232
2023-11-08 15:18:46,951 - __main__ - INFO - Epoch 39 Batch 60: Train Loss = 0.2346
2023-11-08 15:19:04,150 - __main__ - INFO - Epoch 39 Batch 80: Train Loss = 0.1384
2023-11-08 15:19:22,121 - __main__ - INFO - Epoch 39 Batch 100: Train Loss = 0.1239
2023-11-08 15:19:36,884 - __main__ - INFO - Epoch 39 Batch 120: Train Loss = 0.1636
2023-11-08 15:19:47,752 - __main__ - INFO - Epoch 40 Batch 0: Train Loss = 0.2299
2023-11-08 15:20:02,363 - __main__ - INFO - Epoch 40 Batch 20: Train Loss = 0.2446
2023-11-08 15:20:16,584 - __main__ - INFO - Epoch 40 Batch 40: Train Loss = 0.2228
2023-11-08 15:20:31,018 - __main__ - INFO - Epoch 40 Batch 60: Train Loss = 0.2033
2023-11-08 15:20:45,812 - __main__ - INFO - Epoch 40 Batch 80: Train Loss = 0.1590
2023-11-08 15:20:59,952 - __main__ - INFO - Epoch 40 Batch 100: Train Loss = 0.1209
2023-11-08 15:21:14,688 - __main__ - INFO - Epoch 40 Batch 120: Train Loss = 0.1519
2023-11-08 15:21:25,517 - __main__ - INFO - Epoch 41 Batch 0: Train Loss = 0.2195
2023-11-08 15:21:41,521 - __main__ - INFO - Epoch 41 Batch 20: Train Loss = 0.2299
2023-11-08 15:21:57,275 - __main__ - INFO - Epoch 41 Batch 40: Train Loss = 0.2074
2023-11-08 15:22:13,967 - __main__ - INFO - Epoch 41 Batch 60: Train Loss = 0.2227
2023-11-08 15:22:28,285 - __main__ - INFO - Epoch 41 Batch 80: Train Loss = 0.1668
2023-11-08 15:22:42,596 - __main__ - INFO - Epoch 41 Batch 100: Train Loss = 0.1349
2023-11-08 15:22:56,314 - __main__ - INFO - Epoch 41 Batch 120: Train Loss = 0.1725
2023-11-08 15:23:05,924 - __main__ - INFO - ------------ Save best model - AUROC: 0.8916 ------------
2023-11-08 15:23:06,526 - __main__ - INFO - Epoch 42 Batch 0: Train Loss = 0.2173
2023-11-08 15:23:20,709 - __main__ - INFO - Epoch 42 Batch 20: Train Loss = 0.2389
2023-11-08 15:23:34,685 - __main__ - INFO - Epoch 42 Batch 40: Train Loss = 0.2503
2023-11-08 15:23:48,659 - __main__ - INFO - Epoch 42 Batch 60: Train Loss = 0.2186
2023-11-08 15:24:03,449 - __main__ - INFO - Epoch 42 Batch 80: Train Loss = 0.1760
2023-11-08 15:24:17,346 - __main__ - INFO - Epoch 42 Batch 100: Train Loss = 0.1246
2023-11-08 15:24:31,105 - __main__ - INFO - Epoch 42 Batch 120: Train Loss = 0.1579
2023-11-08 15:24:41,340 - __main__ - INFO - Epoch 43 Batch 0: Train Loss = 0.2307
2023-11-08 15:24:55,422 - __main__ - INFO - Epoch 43 Batch 20: Train Loss = 0.2334
2023-11-08 15:25:09,121 - __main__ - INFO - Epoch 43 Batch 40: Train Loss = 0.2203
2023-11-08 15:25:22,764 - __main__ - INFO - Epoch 43 Batch 60: Train Loss = 0.2337
2023-11-08 15:25:36,856 - __main__ - INFO - Epoch 43 Batch 80: Train Loss = 0.1643
2023-11-08 15:25:51,451 - __main__ - INFO - Epoch 43 Batch 100: Train Loss = 0.1281
2023-11-08 15:26:06,458 - __main__ - INFO - Epoch 43 Batch 120: Train Loss = 0.1521
2023-11-08 15:26:18,318 - __main__ - INFO - Epoch 44 Batch 0: Train Loss = 0.2129
2023-11-08 15:26:34,521 - __main__ - INFO - Epoch 44 Batch 20: Train Loss = 0.2074
2023-11-08 15:26:48,938 - __main__ - INFO - Epoch 44 Batch 40: Train Loss = 0.2197
2023-11-08 15:27:04,035 - __main__ - INFO - Epoch 44 Batch 60: Train Loss = 0.2134
2023-11-08 15:27:19,288 - __main__ - INFO - Epoch 44 Batch 80: Train Loss = 0.1457
2023-11-08 15:27:34,325 - __main__ - INFO - Epoch 44 Batch 100: Train Loss = 0.1289
2023-11-08 15:27:47,908 - __main__ - INFO - Epoch 44 Batch 120: Train Loss = 0.1668
2023-11-08 15:27:58,188 - __main__ - INFO - Epoch 45 Batch 0: Train Loss = 0.2132
2023-11-08 15:28:11,940 - __main__ - INFO - Epoch 45 Batch 20: Train Loss = 0.2389
2023-11-08 15:28:26,206 - __main__ - INFO - Epoch 45 Batch 40: Train Loss = 0.2299
2023-11-08 15:28:40,529 - __main__ - INFO - Epoch 45 Batch 60: Train Loss = 0.2397
2023-11-08 15:28:55,535 - __main__ - INFO - Epoch 45 Batch 80: Train Loss = 0.1503
2023-11-08 15:29:09,563 - __main__ - INFO - Epoch 45 Batch 100: Train Loss = 0.1230
2023-11-08 15:29:23,133 - __main__ - INFO - Epoch 45 Batch 120: Train Loss = 0.1438
2023-11-08 15:29:33,237 - __main__ - INFO - Epoch 46 Batch 0: Train Loss = 0.2212
2023-11-08 15:29:47,014 - __main__ - INFO - Epoch 46 Batch 20: Train Loss = 0.2309
2023-11-08 15:30:00,794 - __main__ - INFO - Epoch 46 Batch 40: Train Loss = 0.2107
2023-11-08 15:30:14,310 - __main__ - INFO - Epoch 46 Batch 60: Train Loss = 0.2220
2023-11-08 15:30:28,859 - __main__ - INFO - Epoch 46 Batch 80: Train Loss = 0.1630
2023-11-08 15:30:43,798 - __main__ - INFO - Epoch 46 Batch 100: Train Loss = 0.1277
2023-11-08 15:30:58,432 - __main__ - INFO - Epoch 46 Batch 120: Train Loss = 0.1533
2023-11-08 15:31:08,815 - __main__ - INFO - ------------ Save best model - AUROC: 0.8917 ------------
2023-11-08 15:31:09,516 - __main__ - INFO - Epoch 47 Batch 0: Train Loss = 0.2005
2023-11-08 15:31:25,126 - __main__ - INFO - Epoch 47 Batch 20: Train Loss = 0.2431
2023-11-08 15:31:39,709 - __main__ - INFO - Epoch 47 Batch 40: Train Loss = 0.2284
2023-11-08 15:31:54,107 - __main__ - INFO - Epoch 47 Batch 60: Train Loss = 0.2110
2023-11-08 15:32:07,899 - __main__ - INFO - Epoch 47 Batch 80: Train Loss = 0.1619
2023-11-08 15:32:21,629 - __main__ - INFO - Epoch 47 Batch 100: Train Loss = 0.1403
2023-11-08 15:32:35,068 - __main__ - INFO - Epoch 47 Batch 120: Train Loss = 0.1504
2023-11-08 15:32:45,436 - __main__ - INFO - Epoch 48 Batch 0: Train Loss = 0.2180
2023-11-08 15:32:59,930 - __main__ - INFO - Epoch 48 Batch 20: Train Loss = 0.2158
2023-11-08 15:33:13,646 - __main__ - INFO - Epoch 48 Batch 40: Train Loss = 0.2261
2023-11-08 15:33:27,590 - __main__ - INFO - Epoch 48 Batch 60: Train Loss = 0.2316
2023-11-08 15:33:41,459 - __main__ - INFO - Epoch 48 Batch 80: Train Loss = 0.1634
2023-11-08 15:33:55,468 - __main__ - INFO - Epoch 48 Batch 100: Train Loss = 0.1322
2023-11-08 15:34:09,814 - __main__ - INFO - Epoch 48 Batch 120: Train Loss = 0.1526
2023-11-08 15:34:20,077 - __main__ - INFO - Epoch 49 Batch 0: Train Loss = 0.2401
2023-11-08 15:34:34,469 - __main__ - INFO - Epoch 49 Batch 20: Train Loss = 0.2418
2023-11-08 15:34:47,544 - __main__ - INFO - Epoch 49 Batch 40: Train Loss = 0.2292
2023-11-08 15:35:01,712 - __main__ - INFO - Epoch 49 Batch 60: Train Loss = 0.2312
2023-11-08 15:35:16,650 - __main__ - INFO - Epoch 49 Batch 80: Train Loss = 0.1576
2023-11-08 15:35:32,280 - __main__ - INFO - Epoch 49 Batch 100: Train Loss = 0.1259
2023-11-08 15:35:47,752 - __main__ - INFO - Epoch 49 Batch 120: Train Loss = 0.1477
2023-11-08 15:35:59,046 - __main__ - INFO - Epoch 50 Batch 0: Train Loss = 0.1929
2023-11-08 15:36:13,936 - __main__ - INFO - Epoch 50 Batch 20: Train Loss = 0.2438
2023-11-08 15:36:28,890 - __main__ - INFO - Epoch 50 Batch 40: Train Loss = 0.2295
2023-11-08 15:36:43,256 - __main__ - INFO - Epoch 50 Batch 60: Train Loss = 0.2161
2023-11-08 15:36:57,725 - __main__ - INFO - Epoch 50 Batch 80: Train Loss = 0.1567
2023-11-08 15:37:12,098 - __main__ - INFO - Epoch 50 Batch 100: Train Loss = 0.1298
2023-11-08 15:37:25,769 - __main__ - INFO - Epoch 50 Batch 120: Train Loss = 0.1678
2023-11-08 15:37:36,185 - __main__ - INFO - Epoch 51 Batch 0: Train Loss = 0.2446
2023-11-08 15:37:51,436 - __main__ - INFO - Epoch 51 Batch 20: Train Loss = 0.2437
2023-11-08 15:38:04,695 - __main__ - INFO - Epoch 51 Batch 40: Train Loss = 0.2347
2023-11-08 15:38:19,110 - __main__ - INFO - Epoch 51 Batch 60: Train Loss = 0.2310
2023-11-08 15:38:33,261 - __main__ - INFO - Epoch 51 Batch 80: Train Loss = 0.1526
2023-11-08 15:38:47,495 - __main__ - INFO - Epoch 51 Batch 100: Train Loss = 0.1344
2023-11-08 15:39:01,048 - __main__ - INFO - Epoch 51 Batch 120: Train Loss = 0.1701
2023-11-08 15:39:11,484 - __main__ - INFO - Epoch 52 Batch 0: Train Loss = 0.2366
2023-11-08 15:39:26,050 - __main__ - INFO - Epoch 52 Batch 20: Train Loss = 0.2438
2023-11-08 15:39:40,343 - __main__ - INFO - Epoch 52 Batch 40: Train Loss = 0.2056
2023-11-08 15:39:55,263 - __main__ - INFO - Epoch 52 Batch 60: Train Loss = 0.2389
2023-11-08 15:40:09,950 - __main__ - INFO - Epoch 52 Batch 80: Train Loss = 0.1682
2023-11-08 15:40:25,037 - __main__ - INFO - Epoch 52 Batch 100: Train Loss = 0.1325
2023-11-08 15:40:39,504 - __main__ - INFO - Epoch 52 Batch 120: Train Loss = 0.1615
2023-11-08 15:40:50,091 - __main__ - INFO - Epoch 53 Batch 0: Train Loss = 0.2335
2023-11-08 15:41:05,192 - __main__ - INFO - Epoch 53 Batch 20: Train Loss = 0.2390
2023-11-08 15:41:19,206 - __main__ - INFO - Epoch 53 Batch 40: Train Loss = 0.2201
2023-11-08 15:41:34,641 - __main__ - INFO - Epoch 53 Batch 60: Train Loss = 0.2493
2023-11-08 15:41:49,272 - __main__ - INFO - Epoch 53 Batch 80: Train Loss = 0.1609
2023-11-08 15:42:03,056 - __main__ - INFO - Epoch 53 Batch 100: Train Loss = 0.1068
2023-11-08 15:42:16,705 - __main__ - INFO - Epoch 53 Batch 120: Train Loss = 0.1575
2023-11-08 15:42:27,431 - __main__ - INFO - Epoch 54 Batch 0: Train Loss = 0.2335
2023-11-08 15:42:41,294 - __main__ - INFO - Epoch 54 Batch 20: Train Loss = 0.2409
2023-11-08 15:42:54,520 - __main__ - INFO - Epoch 54 Batch 40: Train Loss = 0.2112
2023-11-08 15:43:07,895 - __main__ - INFO - Epoch 54 Batch 60: Train Loss = 0.2360
2023-11-08 15:43:22,137 - __main__ - INFO - Epoch 54 Batch 80: Train Loss = 0.1403
2023-11-08 15:43:36,409 - __main__ - INFO - Epoch 54 Batch 100: Train Loss = 0.1321
2023-11-08 15:43:49,801 - __main__ - INFO - Epoch 54 Batch 120: Train Loss = 0.1648
2023-11-08 15:43:59,931 - __main__ - INFO - Epoch 55 Batch 0: Train Loss = 0.2227
2023-11-08 15:44:14,403 - __main__ - INFO - Epoch 55 Batch 20: Train Loss = 0.2531
2023-11-08 15:44:28,095 - __main__ - INFO - Epoch 55 Batch 40: Train Loss = 0.2324
2023-11-08 15:44:43,137 - __main__ - INFO - Epoch 55 Batch 60: Train Loss = 0.2305
2023-11-08 15:44:57,919 - __main__ - INFO - Epoch 55 Batch 80: Train Loss = 0.1524
2023-11-08 15:45:12,387 - __main__ - INFO - Epoch 55 Batch 100: Train Loss = 0.1301
2023-11-08 15:45:25,681 - __main__ - INFO - Epoch 55 Batch 120: Train Loss = 0.1650
2023-11-08 15:45:35,912 - __main__ - INFO - Epoch 56 Batch 0: Train Loss = 0.2223
2023-11-08 15:45:51,372 - __main__ - INFO - Epoch 56 Batch 20: Train Loss = 0.2253
2023-11-08 15:46:06,499 - __main__ - INFO - Epoch 56 Batch 40: Train Loss = 0.2102
2023-11-08 15:46:20,480 - __main__ - INFO - Epoch 56 Batch 60: Train Loss = 0.2095
2023-11-08 15:46:35,313 - __main__ - INFO - Epoch 56 Batch 80: Train Loss = 0.1650
2023-11-08 15:46:48,931 - __main__ - INFO - Epoch 56 Batch 100: Train Loss = 0.1320
2023-11-08 15:47:02,830 - __main__ - INFO - Epoch 56 Batch 120: Train Loss = 0.1654
2023-11-08 15:47:13,543 - __main__ - INFO - Epoch 57 Batch 0: Train Loss = 0.2556
2023-11-08 15:47:27,060 - __main__ - INFO - Epoch 57 Batch 20: Train Loss = 0.2249
2023-11-08 15:47:40,719 - __main__ - INFO - Epoch 57 Batch 40: Train Loss = 0.2438
2023-11-08 15:47:54,555 - __main__ - INFO - Epoch 57 Batch 60: Train Loss = 0.2189
2023-11-08 15:48:08,531 - __main__ - INFO - Epoch 57 Batch 80: Train Loss = 0.1723
2023-11-08 15:48:22,693 - __main__ - INFO - Epoch 57 Batch 100: Train Loss = 0.1336
2023-11-08 15:48:36,246 - __main__ - INFO - Epoch 57 Batch 120: Train Loss = 0.1885
2023-11-08 15:48:46,525 - __main__ - INFO - Epoch 58 Batch 0: Train Loss = 0.2093
2023-11-08 15:49:01,611 - __main__ - INFO - Epoch 58 Batch 20: Train Loss = 0.2345
2023-11-08 15:49:16,139 - __main__ - INFO - Epoch 58 Batch 40: Train Loss = 0.2443
2023-11-08 15:49:31,046 - __main__ - INFO - Epoch 58 Batch 60: Train Loss = 0.2171
2023-11-08 15:49:45,581 - __main__ - INFO - Epoch 58 Batch 80: Train Loss = 0.1628
2023-11-08 15:49:59,515 - __main__ - INFO - Epoch 58 Batch 100: Train Loss = 0.1385
2023-11-08 15:50:13,448 - __main__ - INFO - Epoch 58 Batch 120: Train Loss = 0.1545
2023-11-08 15:50:23,571 - __main__ - INFO - ------------ Save best model - AUROC: 0.8922 ------------
2023-11-08 15:50:24,216 - __main__ - INFO - Epoch 59 Batch 0: Train Loss = 0.2031
2023-11-08 15:50:39,281 - __main__ - INFO - Epoch 59 Batch 20: Train Loss = 0.2204
2023-11-08 15:50:53,477 - __main__ - INFO - Epoch 59 Batch 40: Train Loss = 0.2209
2023-11-08 15:51:09,057 - __main__ - INFO - Epoch 59 Batch 60: Train Loss = 0.2234
2023-11-08 15:51:24,484 - __main__ - INFO - Epoch 59 Batch 80: Train Loss = 0.1653
2023-11-08 15:51:38,999 - __main__ - INFO - Epoch 59 Batch 100: Train Loss = 0.1379
2023-11-08 15:51:52,674 - __main__ - INFO - Epoch 59 Batch 120: Train Loss = 0.1770
2023-11-08 15:52:03,583 - __main__ - INFO - Epoch 60 Batch 0: Train Loss = 0.2078
2023-11-08 15:52:17,984 - __main__ - INFO - Epoch 60 Batch 20: Train Loss = 0.2230
2023-11-08 15:52:31,761 - __main__ - INFO - Epoch 60 Batch 40: Train Loss = 0.2189
2023-11-08 15:52:45,649 - __main__ - INFO - Epoch 60 Batch 60: Train Loss = 0.2730
2023-11-08 15:52:59,158 - __main__ - INFO - Epoch 60 Batch 80: Train Loss = 0.1581
2023-11-08 15:53:13,379 - __main__ - INFO - Epoch 60 Batch 100: Train Loss = 0.1574
2023-11-08 15:53:27,205 - __main__ - INFO - Epoch 60 Batch 120: Train Loss = 0.1518
2023-11-08 15:53:38,321 - __main__ - INFO - Epoch 61 Batch 0: Train Loss = 0.2210
2023-11-08 15:53:54,236 - __main__ - INFO - Epoch 61 Batch 20: Train Loss = 0.2319
2023-11-08 15:54:09,249 - __main__ - INFO - Epoch 61 Batch 40: Train Loss = 0.2345
2023-11-08 15:54:23,661 - __main__ - INFO - Epoch 61 Batch 60: Train Loss = 0.2456
2023-11-08 15:54:38,220 - __main__ - INFO - Epoch 61 Batch 80: Train Loss = 0.1643
2023-11-08 15:54:52,426 - __main__ - INFO - Epoch 61 Batch 100: Train Loss = 0.1223
2023-11-08 15:55:06,056 - __main__ - INFO - Epoch 61 Batch 120: Train Loss = 0.1683
2023-11-08 15:55:16,277 - __main__ - INFO - Epoch 62 Batch 0: Train Loss = 0.2002
2023-11-08 15:55:30,951 - __main__ - INFO - Epoch 62 Batch 20: Train Loss = 0.2196
2023-11-08 15:55:45,898 - __main__ - INFO - Epoch 62 Batch 40: Train Loss = 0.2228
2023-11-08 15:56:00,937 - __main__ - INFO - Epoch 62 Batch 60: Train Loss = 0.2429
2023-11-08 15:56:15,452 - __main__ - INFO - Epoch 62 Batch 80: Train Loss = 0.1538
2023-11-08 15:56:29,783 - __main__ - INFO - Epoch 62 Batch 100: Train Loss = 0.1242
2023-11-08 15:56:43,829 - __main__ - INFO - Epoch 62 Batch 120: Train Loss = 0.1591
2023-11-08 15:56:53,846 - __main__ - INFO - Epoch 63 Batch 0: Train Loss = 0.2300
2023-11-08 15:57:09,517 - __main__ - INFO - Epoch 63 Batch 20: Train Loss = 0.2513
2023-11-08 15:57:23,175 - __main__ - INFO - Epoch 63 Batch 40: Train Loss = 0.2372
2023-11-08 15:57:37,266 - __main__ - INFO - Epoch 63 Batch 60: Train Loss = 0.2443
2023-11-08 15:57:50,368 - __main__ - INFO - Epoch 63 Batch 80: Train Loss = 0.1490
2023-11-08 15:58:04,757 - __main__ - INFO - Epoch 63 Batch 100: Train Loss = 0.1445
2023-11-08 15:58:18,689 - __main__ - INFO - Epoch 63 Batch 120: Train Loss = 0.1728
2023-11-08 15:58:29,158 - __main__ - INFO - Epoch 64 Batch 0: Train Loss = 0.2164
2023-11-08 15:58:44,091 - __main__ - INFO - Epoch 64 Batch 20: Train Loss = 0.2305
2023-11-08 15:58:58,749 - __main__ - INFO - Epoch 64 Batch 40: Train Loss = 0.2332
2023-11-08 15:59:13,630 - __main__ - INFO - Epoch 64 Batch 60: Train Loss = 0.2346
2023-11-08 15:59:27,687 - __main__ - INFO - Epoch 64 Batch 80: Train Loss = 0.1609
2023-11-08 15:59:41,652 - __main__ - INFO - Epoch 64 Batch 100: Train Loss = 0.1190
2023-11-08 15:59:55,901 - __main__ - INFO - Epoch 64 Batch 120: Train Loss = 0.1870
2023-11-08 16:00:06,508 - __main__ - INFO - Epoch 65 Batch 0: Train Loss = 0.2182
2023-11-08 16:00:21,137 - __main__ - INFO - Epoch 65 Batch 20: Train Loss = 0.2409
2023-11-08 16:00:35,235 - __main__ - INFO - Epoch 65 Batch 40: Train Loss = 0.2157
2023-11-08 16:00:49,605 - __main__ - INFO - Epoch 65 Batch 60: Train Loss = 0.2238
2023-11-08 16:01:04,741 - __main__ - INFO - Epoch 65 Batch 80: Train Loss = 0.1533
2023-11-08 16:01:19,230 - __main__ - INFO - Epoch 65 Batch 100: Train Loss = 0.1323
2023-11-08 16:01:32,452 - __main__ - INFO - Epoch 65 Batch 120: Train Loss = 0.1694
2023-11-08 16:01:43,448 - __main__ - INFO - Epoch 66 Batch 0: Train Loss = 0.2221
2023-11-08 16:01:57,585 - __main__ - INFO - Epoch 66 Batch 20: Train Loss = 0.2282
2023-11-08 16:02:11,354 - __main__ - INFO - Epoch 66 Batch 40: Train Loss = 0.2471
2023-11-08 16:02:25,608 - __main__ - INFO - Epoch 66 Batch 60: Train Loss = 0.2183
2023-11-08 16:02:39,537 - __main__ - INFO - Epoch 66 Batch 80: Train Loss = 0.1547
2023-11-08 16:02:52,521 - __main__ - INFO - Epoch 66 Batch 100: Train Loss = 0.1352
2023-11-08 16:03:06,554 - __main__ - INFO - Epoch 66 Batch 120: Train Loss = 0.1646
2023-11-08 16:03:17,599 - __main__ - INFO - Epoch 67 Batch 0: Train Loss = 0.2248
2023-11-08 16:03:32,341 - __main__ - INFO - Epoch 67 Batch 20: Train Loss = 0.2244
2023-11-08 16:03:46,405 - __main__ - INFO - Epoch 67 Batch 40: Train Loss = 0.2346
2023-11-08 16:04:00,526 - __main__ - INFO - Epoch 67 Batch 60: Train Loss = 0.2255
2023-11-08 16:04:14,224 - __main__ - INFO - Epoch 67 Batch 80: Train Loss = 0.1463
2023-11-08 16:04:27,786 - __main__ - INFO - Epoch 67 Batch 100: Train Loss = 0.1282
2023-11-08 16:04:40,929 - __main__ - INFO - Epoch 67 Batch 120: Train Loss = 0.1648
2023-11-08 16:04:52,043 - __main__ - INFO - Epoch 68 Batch 0: Train Loss = 0.2174
2023-11-08 16:05:06,504 - __main__ - INFO - Epoch 68 Batch 20: Train Loss = 0.2399
2023-11-08 16:05:20,608 - __main__ - INFO - Epoch 68 Batch 40: Train Loss = 0.2071
2023-11-08 16:05:34,949 - __main__ - INFO - Epoch 68 Batch 60: Train Loss = 0.2281
2023-11-08 16:05:49,999 - __main__ - INFO - Epoch 68 Batch 80: Train Loss = 0.1671
2023-11-08 16:06:04,281 - __main__ - INFO - Epoch 68 Batch 100: Train Loss = 0.1195
2023-11-08 16:06:18,590 - __main__ - INFO - Epoch 68 Batch 120: Train Loss = 0.1775
2023-11-08 16:06:29,447 - __main__ - INFO - Epoch 69 Batch 0: Train Loss = 0.2061
2023-11-08 16:06:43,888 - __main__ - INFO - Epoch 69 Batch 20: Train Loss = 0.2145
2023-11-08 16:06:57,908 - __main__ - INFO - Epoch 69 Batch 40: Train Loss = 0.2152
2023-11-08 16:07:12,072 - __main__ - INFO - Epoch 69 Batch 60: Train Loss = 0.2523
2023-11-08 16:07:25,564 - __main__ - INFO - Epoch 69 Batch 80: Train Loss = 0.1742
2023-11-08 16:07:40,172 - __main__ - INFO - Epoch 69 Batch 100: Train Loss = 0.1357
2023-11-08 16:07:54,401 - __main__ - INFO - Epoch 69 Batch 120: Train Loss = 0.1806
2023-11-08 16:08:05,399 - __main__ - INFO - Epoch 70 Batch 0: Train Loss = 0.2054
2023-11-08 16:08:20,728 - __main__ - INFO - Epoch 70 Batch 20: Train Loss = 0.2350
2023-11-08 16:08:34,786 - __main__ - INFO - Epoch 70 Batch 40: Train Loss = 0.2359
2023-11-08 16:08:48,331 - __main__ - INFO - Epoch 70 Batch 60: Train Loss = 0.2359
2023-11-08 16:09:02,300 - __main__ - INFO - Epoch 70 Batch 80: Train Loss = 0.1781
2023-11-08 16:09:16,333 - __main__ - INFO - Epoch 70 Batch 100: Train Loss = 0.1242
2023-11-08 16:09:29,931 - __main__ - INFO - Epoch 70 Batch 120: Train Loss = 0.1572
2023-11-08 16:09:40,273 - __main__ - INFO - Epoch 71 Batch 0: Train Loss = 0.2253
2023-11-08 16:09:54,721 - __main__ - INFO - Epoch 71 Batch 20: Train Loss = 0.2102
2023-11-08 16:10:08,450 - __main__ - INFO - Epoch 71 Batch 40: Train Loss = 0.1968
2023-11-08 16:10:22,565 - __main__ - INFO - Epoch 71 Batch 60: Train Loss = 0.2165
2023-11-08 16:10:36,842 - __main__ - INFO - Epoch 71 Batch 80: Train Loss = 0.1812
2023-11-08 16:10:51,244 - __main__ - INFO - Epoch 71 Batch 100: Train Loss = 0.1144
2023-11-08 16:11:04,892 - __main__ - INFO - Epoch 71 Batch 120: Train Loss = 0.1660
2023-11-08 16:11:15,942 - __main__ - INFO - Epoch 72 Batch 0: Train Loss = 0.1974
2023-11-08 16:11:30,710 - __main__ - INFO - Epoch 72 Batch 20: Train Loss = 0.2139
2023-11-08 16:11:43,879 - __main__ - INFO - Epoch 72 Batch 40: Train Loss = 0.2234
2023-11-08 16:11:57,527 - __main__ - INFO - Epoch 72 Batch 60: Train Loss = 0.2355
2023-11-08 16:12:11,581 - __main__ - INFO - Epoch 72 Batch 80: Train Loss = 0.1740
2023-11-08 16:12:26,354 - __main__ - INFO - Epoch 72 Batch 100: Train Loss = 0.1362
2023-11-08 16:12:41,083 - __main__ - INFO - Epoch 72 Batch 120: Train Loss = 0.1825
2023-11-08 16:12:51,810 - __main__ - INFO - Epoch 73 Batch 0: Train Loss = 0.2093
2023-11-08 16:13:06,812 - __main__ - INFO - Epoch 73 Batch 20: Train Loss = 0.2190
2023-11-08 16:13:21,029 - __main__ - INFO - Epoch 73 Batch 40: Train Loss = 0.2370
2023-11-08 16:13:34,516 - __main__ - INFO - Epoch 73 Batch 60: Train Loss = 0.2401
2023-11-08 16:13:48,875 - __main__ - INFO - Epoch 73 Batch 80: Train Loss = 0.1587
2023-11-08 16:14:02,877 - __main__ - INFO - Epoch 73 Batch 100: Train Loss = 0.1200
2023-11-08 16:14:16,672 - __main__ - INFO - Epoch 73 Batch 120: Train Loss = 0.1779
2023-11-08 16:14:26,761 - __main__ - INFO - Epoch 74 Batch 0: Train Loss = 0.2046
2023-11-08 16:14:41,014 - __main__ - INFO - Epoch 74 Batch 20: Train Loss = 0.2245
2023-11-08 16:14:54,255 - __main__ - INFO - Epoch 74 Batch 40: Train Loss = 0.2681
2023-11-08 16:15:07,702 - __main__ - INFO - Epoch 74 Batch 60: Train Loss = 0.2019
2023-11-08 16:15:22,128 - __main__ - INFO - Epoch 74 Batch 80: Train Loss = 0.1905
2023-11-08 16:15:36,356 - __main__ - INFO - Epoch 74 Batch 100: Train Loss = 0.1109
2023-11-08 16:15:51,316 - __main__ - INFO - Epoch 74 Batch 120: Train Loss = 0.1544
2023-11-08 16:16:01,809 - __main__ - INFO - Epoch 75 Batch 0: Train Loss = 0.2212
2023-11-08 16:16:15,466 - __main__ - INFO - Epoch 75 Batch 20: Train Loss = 0.2310
2023-11-08 16:16:28,422 - __main__ - INFO - Epoch 75 Batch 40: Train Loss = 0.2275
2023-11-08 16:16:41,442 - __main__ - INFO - Epoch 75 Batch 60: Train Loss = 0.1906
2023-11-08 16:16:54,985 - __main__ - INFO - Epoch 75 Batch 80: Train Loss = 0.1794
2023-11-08 16:17:09,837 - __main__ - INFO - Epoch 75 Batch 100: Train Loss = 0.1347
2023-11-08 16:17:23,575 - __main__ - INFO - Epoch 75 Batch 120: Train Loss = 0.1546
2023-11-08 16:17:33,729 - __main__ - INFO - Epoch 76 Batch 0: Train Loss = 0.2153
2023-11-08 16:17:47,683 - __main__ - INFO - Epoch 76 Batch 20: Train Loss = 0.2344
2023-11-08 16:18:01,660 - __main__ - INFO - Epoch 76 Batch 40: Train Loss = 0.2335
2023-11-08 16:18:15,353 - __main__ - INFO - Epoch 76 Batch 60: Train Loss = 0.2789
2023-11-08 16:18:29,957 - __main__ - INFO - Epoch 76 Batch 80: Train Loss = 0.1551
2023-11-08 16:18:43,655 - __main__ - INFO - Epoch 76 Batch 100: Train Loss = 0.1039
2023-11-08 16:18:56,434 - __main__ - INFO - Epoch 76 Batch 120: Train Loss = 0.1846
2023-11-08 16:19:06,473 - __main__ - INFO - Epoch 77 Batch 0: Train Loss = 0.2017
2023-11-08 16:19:19,892 - __main__ - INFO - Epoch 77 Batch 20: Train Loss = 0.2209
2023-11-08 16:19:32,991 - __main__ - INFO - Epoch 77 Batch 40: Train Loss = 0.2534
2023-11-08 16:19:46,020 - __main__ - INFO - Epoch 77 Batch 60: Train Loss = 0.2040
2023-11-08 16:19:59,375 - __main__ - INFO - Epoch 77 Batch 80: Train Loss = 0.1634
2023-11-08 16:20:12,778 - __main__ - INFO - Epoch 77 Batch 100: Train Loss = 0.1291
2023-11-08 16:20:25,764 - __main__ - INFO - Epoch 77 Batch 120: Train Loss = 0.1891
2023-11-08 16:20:36,086 - __main__ - INFO - Epoch 78 Batch 0: Train Loss = 0.2008
2023-11-08 16:20:49,973 - __main__ - INFO - Epoch 78 Batch 20: Train Loss = 0.2391
2023-11-08 16:21:03,131 - __main__ - INFO - Epoch 78 Batch 40: Train Loss = 0.2441
2023-11-08 16:21:17,303 - __main__ - INFO - Epoch 78 Batch 60: Train Loss = 0.2077
2023-11-08 16:21:32,215 - __main__ - INFO - Epoch 78 Batch 80: Train Loss = 0.1494
2023-11-08 16:21:46,630 - __main__ - INFO - Epoch 78 Batch 100: Train Loss = 0.1636
2023-11-08 16:22:00,440 - __main__ - INFO - Epoch 78 Batch 120: Train Loss = 0.1736
2023-11-08 16:22:10,467 - __main__ - INFO - Epoch 79 Batch 0: Train Loss = 0.2321
2023-11-08 16:22:23,846 - __main__ - INFO - Epoch 79 Batch 20: Train Loss = 0.2391
2023-11-08 16:22:36,726 - __main__ - INFO - Epoch 79 Batch 40: Train Loss = 0.2205
2023-11-08 16:22:49,953 - __main__ - INFO - Epoch 79 Batch 60: Train Loss = 0.1999
2023-11-08 16:23:03,547 - __main__ - INFO - Epoch 79 Batch 80: Train Loss = 0.1754
2023-11-08 16:23:16,752 - __main__ - INFO - Epoch 79 Batch 100: Train Loss = 0.1322
2023-11-08 16:23:30,032 - __main__ - INFO - Epoch 79 Batch 120: Train Loss = 0.1646
2023-11-08 16:23:39,906 - __main__ - INFO - Epoch 80 Batch 0: Train Loss = 0.2096
2023-11-08 16:23:51,912 - __main__ - INFO - Epoch 80 Batch 20: Train Loss = 0.2364
2023-11-08 16:24:03,683 - __main__ - INFO - Epoch 80 Batch 40: Train Loss = 0.2326
2023-11-08 16:24:17,259 - __main__ - INFO - Epoch 80 Batch 60: Train Loss = 0.2002
2023-11-08 16:24:30,459 - __main__ - INFO - Epoch 80 Batch 80: Train Loss = 0.1540
2023-11-08 16:24:42,375 - __main__ - INFO - Epoch 80 Batch 100: Train Loss = 0.1317
2023-11-08 16:24:55,974 - __main__ - INFO - Epoch 80 Batch 120: Train Loss = 0.1627
2023-11-08 16:25:06,011 - __main__ - INFO - Epoch 81 Batch 0: Train Loss = 0.2221
2023-11-08 16:25:20,116 - __main__ - INFO - Epoch 81 Batch 20: Train Loss = 0.2203
2023-11-08 16:25:33,201 - __main__ - INFO - Epoch 81 Batch 40: Train Loss = 0.2340
2023-11-08 16:25:47,074 - __main__ - INFO - Epoch 81 Batch 60: Train Loss = 0.2525
2023-11-08 16:26:01,014 - __main__ - INFO - Epoch 81 Batch 80: Train Loss = 0.1561
2023-11-08 16:26:14,633 - __main__ - INFO - Epoch 81 Batch 100: Train Loss = 0.1404
2023-11-08 16:26:27,341 - __main__ - INFO - Epoch 81 Batch 120: Train Loss = 0.1588
2023-11-08 16:26:37,386 - __main__ - INFO - Epoch 82 Batch 0: Train Loss = 0.2296
2023-11-08 16:26:49,892 - __main__ - INFO - Epoch 82 Batch 20: Train Loss = 0.2537
2023-11-08 16:27:02,804 - __main__ - INFO - Epoch 82 Batch 40: Train Loss = 0.2199
2023-11-08 16:27:16,044 - __main__ - INFO - Epoch 82 Batch 60: Train Loss = 0.2173
2023-11-08 16:27:29,412 - __main__ - INFO - Epoch 82 Batch 80: Train Loss = 0.1949
2023-11-08 16:27:42,434 - __main__ - INFO - Epoch 82 Batch 100: Train Loss = 0.1281
2023-11-08 16:27:54,780 - __main__ - INFO - Epoch 82 Batch 120: Train Loss = 0.1725
2023-11-08 16:28:04,561 - __main__ - INFO - Epoch 83 Batch 0: Train Loss = 0.2289
2023-11-08 16:28:17,540 - __main__ - INFO - Epoch 83 Batch 20: Train Loss = 0.2538
2023-11-08 16:28:30,098 - __main__ - INFO - Epoch 83 Batch 40: Train Loss = 0.2299
2023-11-08 16:28:43,228 - __main__ - INFO - Epoch 83 Batch 60: Train Loss = 0.2281
2023-11-08 16:28:56,557 - __main__ - INFO - Epoch 83 Batch 80: Train Loss = 0.1708
2023-11-08 16:29:09,479 - __main__ - INFO - Epoch 83 Batch 100: Train Loss = 0.1291
2023-11-08 16:29:22,423 - __main__ - INFO - Epoch 83 Batch 120: Train Loss = 0.1677
2023-11-08 16:29:32,198 - __main__ - INFO - Epoch 84 Batch 0: Train Loss = 0.2221
2023-11-08 16:29:46,055 - __main__ - INFO - Epoch 84 Batch 20: Train Loss = 0.2241
2023-11-08 16:29:59,618 - __main__ - INFO - Epoch 84 Batch 40: Train Loss = 0.2252
2023-11-08 16:30:13,252 - __main__ - INFO - Epoch 84 Batch 60: Train Loss = 0.2573
2023-11-08 16:30:26,924 - __main__ - INFO - Epoch 84 Batch 80: Train Loss = 0.1479
2023-11-08 16:30:40,157 - __main__ - INFO - Epoch 84 Batch 100: Train Loss = 0.1377
2023-11-08 16:30:52,891 - __main__ - INFO - Epoch 84 Batch 120: Train Loss = 0.1725
2023-11-08 16:31:02,751 - __main__ - INFO - Epoch 85 Batch 0: Train Loss = 0.2297
2023-11-08 16:31:16,912 - __main__ - INFO - Epoch 85 Batch 20: Train Loss = 0.2526
2023-11-08 16:31:30,408 - __main__ - INFO - Epoch 85 Batch 40: Train Loss = 0.2329
2023-11-08 16:31:44,987 - __main__ - INFO - Epoch 85 Batch 60: Train Loss = 0.2254
2023-11-08 16:31:58,205 - __main__ - INFO - Epoch 85 Batch 80: Train Loss = 0.1539
2023-11-08 16:32:11,253 - __main__ - INFO - Epoch 85 Batch 100: Train Loss = 0.1404
2023-11-08 16:32:23,818 - __main__ - INFO - Epoch 85 Batch 120: Train Loss = 0.1829
2023-11-08 16:32:33,694 - __main__ - INFO - Epoch 86 Batch 0: Train Loss = 0.1968
2023-11-08 16:32:47,250 - __main__ - INFO - Epoch 86 Batch 20: Train Loss = 0.2535
2023-11-08 16:33:00,302 - __main__ - INFO - Epoch 86 Batch 40: Train Loss = 0.2276
2023-11-08 16:33:13,705 - __main__ - INFO - Epoch 86 Batch 60: Train Loss = 0.2498
2023-11-08 16:33:26,994 - __main__ - INFO - Epoch 86 Batch 80: Train Loss = 0.1625
2023-11-08 16:33:40,914 - __main__ - INFO - Epoch 86 Batch 100: Train Loss = 0.1742
2023-11-08 16:33:54,576 - __main__ - INFO - Epoch 86 Batch 120: Train Loss = 0.1661
2023-11-08 16:34:04,883 - __main__ - INFO - Epoch 87 Batch 0: Train Loss = 0.2136
2023-11-08 16:34:19,348 - __main__ - INFO - Epoch 87 Batch 20: Train Loss = 0.2389
2023-11-08 16:34:32,684 - __main__ - INFO - Epoch 87 Batch 40: Train Loss = 0.2055
2023-11-08 16:34:46,336 - __main__ - INFO - Epoch 87 Batch 60: Train Loss = 0.2292
2023-11-08 16:35:00,132 - __main__ - INFO - Epoch 87 Batch 80: Train Loss = 0.1599
2023-11-08 16:35:13,658 - __main__ - INFO - Epoch 87 Batch 100: Train Loss = 0.1418
2023-11-08 16:35:25,679 - __main__ - INFO - Epoch 87 Batch 120: Train Loss = 0.1756
2023-11-08 16:35:35,639 - __main__ - INFO - Epoch 88 Batch 0: Train Loss = 0.2047
2023-11-08 16:35:49,588 - __main__ - INFO - Epoch 88 Batch 20: Train Loss = 0.2765
2023-11-08 16:36:02,555 - __main__ - INFO - Epoch 88 Batch 40: Train Loss = 0.2206
2023-11-08 16:36:16,176 - __main__ - INFO - Epoch 88 Batch 60: Train Loss = 0.2422
2023-11-08 16:36:30,079 - __main__ - INFO - Epoch 88 Batch 80: Train Loss = 0.1674
2023-11-08 16:36:43,912 - __main__ - INFO - Epoch 88 Batch 100: Train Loss = 0.1412
2023-11-08 16:36:57,874 - __main__ - INFO - Epoch 88 Batch 120: Train Loss = 0.1655
2023-11-08 16:37:07,980 - __main__ - INFO - Epoch 89 Batch 0: Train Loss = 0.2201
2023-11-08 16:37:21,172 - __main__ - INFO - Epoch 89 Batch 20: Train Loss = 0.2379
2023-11-08 16:37:34,903 - __main__ - INFO - Epoch 89 Batch 40: Train Loss = 0.2295
2023-11-08 16:37:48,516 - __main__ - INFO - Epoch 89 Batch 60: Train Loss = 0.2114
2023-11-08 16:38:02,136 - __main__ - INFO - Epoch 89 Batch 80: Train Loss = 0.1642
2023-11-08 16:38:16,286 - __main__ - INFO - Epoch 89 Batch 100: Train Loss = 0.1380
2023-11-08 16:38:30,625 - __main__ - INFO - Epoch 89 Batch 120: Train Loss = 0.1840
2023-11-08 16:38:41,026 - __main__ - INFO - Epoch 90 Batch 0: Train Loss = 0.2033
2023-11-08 16:38:54,826 - __main__ - INFO - Epoch 90 Batch 20: Train Loss = 0.2266
2023-11-08 16:39:08,114 - __main__ - INFO - Epoch 90 Batch 40: Train Loss = 0.2173
2023-11-08 16:39:21,919 - __main__ - INFO - Epoch 90 Batch 60: Train Loss = 0.2287
2023-11-08 16:39:34,865 - __main__ - INFO - Epoch 90 Batch 80: Train Loss = 0.1570
2023-11-08 16:39:48,741 - __main__ - INFO - Epoch 90 Batch 100: Train Loss = 0.1326
2023-11-08 16:40:02,116 - __main__ - INFO - Epoch 90 Batch 120: Train Loss = 0.1552
2023-11-08 16:40:12,220 - __main__ - INFO - Epoch 91 Batch 0: Train Loss = 0.2351
2023-11-08 16:40:27,122 - __main__ - INFO - Epoch 91 Batch 20: Train Loss = 0.2232
2023-11-08 16:40:40,439 - __main__ - INFO - Epoch 91 Batch 40: Train Loss = 0.2398
2023-11-08 16:40:54,177 - __main__ - INFO - Epoch 91 Batch 60: Train Loss = 0.2077
2023-11-08 16:41:08,594 - __main__ - INFO - Epoch 91 Batch 80: Train Loss = 0.1595
2023-11-08 16:41:23,182 - __main__ - INFO - Epoch 91 Batch 100: Train Loss = 0.1194
2023-11-08 16:41:36,339 - __main__ - INFO - Epoch 91 Batch 120: Train Loss = 0.1678
2023-11-08 16:41:46,387 - __main__ - INFO - Epoch 92 Batch 0: Train Loss = 0.2140
2023-11-08 16:42:00,365 - __main__ - INFO - Epoch 92 Batch 20: Train Loss = 0.2081
2023-11-08 16:42:13,693 - __main__ - INFO - Epoch 92 Batch 40: Train Loss = 0.2174
2023-11-08 16:42:27,163 - __main__ - INFO - Epoch 92 Batch 60: Train Loss = 0.2070
2023-11-08 16:42:41,169 - __main__ - INFO - Epoch 92 Batch 80: Train Loss = 0.1696
2023-11-08 16:42:56,459 - __main__ - INFO - Epoch 92 Batch 100: Train Loss = 0.1252
2023-11-08 16:43:09,588 - __main__ - INFO - Epoch 92 Batch 120: Train Loss = 0.1542
2023-11-08 16:43:20,164 - __main__ - INFO - Epoch 93 Batch 0: Train Loss = 0.2185
2023-11-08 16:43:33,918 - __main__ - INFO - Epoch 93 Batch 20: Train Loss = 0.2652
2023-11-08 16:43:46,557 - __main__ - INFO - Epoch 93 Batch 40: Train Loss = 0.2288
2023-11-08 16:43:58,477 - __main__ - INFO - Epoch 93 Batch 60: Train Loss = 0.2374
2023-11-08 16:44:11,882 - __main__ - INFO - Epoch 93 Batch 80: Train Loss = 0.1662
2023-11-08 16:44:25,615 - __main__ - INFO - Epoch 93 Batch 100: Train Loss = 0.1367
2023-11-08 16:44:39,448 - __main__ - INFO - Epoch 93 Batch 120: Train Loss = 0.1779
2023-11-08 16:44:49,482 - __main__ - INFO - Epoch 94 Batch 0: Train Loss = 0.2129
2023-11-08 16:45:02,373 - __main__ - INFO - Epoch 94 Batch 20: Train Loss = 0.2658
2023-11-08 16:45:15,796 - __main__ - INFO - Epoch 94 Batch 40: Train Loss = 0.2408
2023-11-08 16:45:28,761 - __main__ - INFO - Epoch 94 Batch 60: Train Loss = 0.2056
2023-11-08 16:45:42,972 - __main__ - INFO - Epoch 94 Batch 80: Train Loss = 0.1593
2023-11-08 16:45:56,788 - __main__ - INFO - Epoch 94 Batch 100: Train Loss = 0.1311
2023-11-08 16:46:10,446 - __main__ - INFO - Epoch 94 Batch 120: Train Loss = 0.1603
2023-11-08 16:46:20,715 - __main__ - INFO - Epoch 95 Batch 0: Train Loss = 0.2566
2023-11-08 16:46:34,855 - __main__ - INFO - Epoch 95 Batch 20: Train Loss = 0.2226
2023-11-08 16:46:49,052 - __main__ - INFO - Epoch 95 Batch 40: Train Loss = 0.2115
2023-11-08 16:47:03,347 - __main__ - INFO - Epoch 95 Batch 60: Train Loss = 0.1996
2023-11-08 16:47:17,685 - __main__ - INFO - Epoch 95 Batch 80: Train Loss = 0.1473
2023-11-08 16:47:31,385 - __main__ - INFO - Epoch 95 Batch 100: Train Loss = 0.1344
2023-11-08 16:47:44,560 - __main__ - INFO - Epoch 95 Batch 120: Train Loss = 0.1957
2023-11-08 16:47:54,682 - __main__ - INFO - Epoch 96 Batch 0: Train Loss = 0.2295
2023-11-08 16:48:08,376 - __main__ - INFO - Epoch 96 Batch 20: Train Loss = 0.2377
2023-11-08 16:48:21,138 - __main__ - INFO - Epoch 96 Batch 40: Train Loss = 0.2121
2023-11-08 16:48:33,899 - __main__ - INFO - Epoch 96 Batch 60: Train Loss = 0.1821
2023-11-08 16:48:48,128 - __main__ - INFO - Epoch 96 Batch 80: Train Loss = 0.1504
2023-11-08 16:49:01,536 - __main__ - INFO - Epoch 96 Batch 100: Train Loss = 0.1454
2023-11-08 16:49:15,041 - __main__ - INFO - Epoch 96 Batch 120: Train Loss = 0.1665
2023-11-08 16:49:25,816 - __main__ - INFO - Epoch 97 Batch 0: Train Loss = 0.1974
2023-11-08 16:49:40,207 - __main__ - INFO - Epoch 97 Batch 20: Train Loss = 0.2351
2023-11-08 16:49:53,669 - __main__ - INFO - Epoch 97 Batch 40: Train Loss = 0.2300
2023-11-08 16:50:06,934 - __main__ - INFO - Epoch 97 Batch 60: Train Loss = 0.2273
2023-11-08 16:50:20,349 - __main__ - INFO - Epoch 97 Batch 80: Train Loss = 0.1336
2023-11-08 16:50:33,288 - __main__ - INFO - Epoch 97 Batch 100: Train Loss = 0.1354
2023-11-08 16:50:46,230 - __main__ - INFO - Epoch 97 Batch 120: Train Loss = 0.1728
2023-11-08 16:50:56,452 - __main__ - INFO - Epoch 98 Batch 0: Train Loss = 0.2422
2023-11-08 16:51:11,443 - __main__ - INFO - Epoch 98 Batch 20: Train Loss = 0.2473
2023-11-08 16:51:26,578 - __main__ - INFO - Epoch 98 Batch 40: Train Loss = 0.2246
2023-11-08 16:51:39,446 - __main__ - INFO - Epoch 98 Batch 60: Train Loss = 0.2191
2023-11-08 16:51:53,191 - __main__ - INFO - Epoch 98 Batch 80: Train Loss = 0.1420
2023-11-08 16:52:06,281 - __main__ - INFO - Epoch 98 Batch 100: Train Loss = 0.1323
2023-11-08 16:52:19,516 - __main__ - INFO - Epoch 98 Batch 120: Train Loss = 0.1935
2023-11-08 16:52:29,746 - __main__ - INFO - Epoch 99 Batch 0: Train Loss = 0.2497
2023-11-08 16:52:43,702 - __main__ - INFO - Epoch 99 Batch 20: Train Loss = 0.2323
2023-11-08 16:52:56,743 - __main__ - INFO - Epoch 99 Batch 40: Train Loss = 0.2162
2023-11-08 16:53:09,400 - __main__ - INFO - Epoch 99 Batch 60: Train Loss = 0.2058
2023-11-08 16:53:22,166 - __main__ - INFO - Epoch 99 Batch 80: Train Loss = 0.1648
2023-11-08 16:53:34,945 - __main__ - INFO - Epoch 99 Batch 100: Train Loss = 0.1276
2023-11-08 16:53:47,933 - __main__ - INFO - Epoch 99 Batch 120: Train Loss = 0.1789
2023-11-08 16:53:56,978 - __main__ - INFO - auroc 0.8922
2023-11-08 16:53:56,979 - __main__ - INFO - auprc 0.6316
2023-11-08 16:53:56,981 - __main__ - INFO - minpse 0.5892
2023-11-08 16:53:57,064 - __main__ - INFO - last saved model is in epoch 58
2023-11-08 16:53:57,528 - __main__ - INFO - Batch 0: Test Loss = 0.2240
2023-11-08 16:54:02,991 - __main__ - INFO - 
==>Predicting on test
2023-11-08 16:54:02,994 - __main__ - INFO - Test Loss = 0.1453
2023-11-08 16:54:03,037 - __main__ - INFO - Transfer Target Dataset & Model
2023-11-08 16:54:03,113 - __main__ - INFO - [[-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8285543981909917, 2.770245567717861, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8374745525934485, 2.8093811444689463, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5338330207864587, 0.431156978499758, 0.8964660630930379, -1.342421748214616, -0.6946051308187712, -0.2120300841305121, 0.8952797203562032, 0.0387041303378994, -0.8612693268611251, 0.0726334784031089, -0.5051102137388988, -0.2865727201409924, 0.3988826585206329, -1.3794377690564883, -0.8237089728907875, -0.9221119476695248, -1.6036614531597553, -0.2379932071009605, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -1.1501193786706505, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, 1.0826051686869729, -0.609009148503521, 0.9684393533852332, -0.3880554131475662, 0.8439788318452395, 2.837917502516613, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 0.278477698357045, 1.2505677424119097, -0.6305581644917595, -0.9062745442328174, -0.7536814885080627, -0.4374103947888615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, -0.7710128163592748, -0.6738408269117502, -2.438483340619628, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8608899578998963, 2.912112033440545, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3396023676711391, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8763143915541441, 2.979783968239297, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5125273793649132, 0.601421850901989, 0.5034187451388209, -1.455529079732558, -0.5254345623915906, -0.4163803392884947, 0.7885904984786939, 0.2816638966057675, -0.5890172833864098, 0.5902661209826593, -0.3430637446868887, -0.7482466726318323, -0.1864257310498265, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.2249939001501796, -0.0341172340589738, 0.3466058957088718, 0.3482841380580905, -0.2890718868318484, 0.1923438589112976, -0.2031244129114749, -1.100153472115602, -0.7598199909551685, -0.6868858002659636, 0.3602180776283341, -0.700061185363224, -0.501359972189453, -0.2332733777972843, -1.36588558517574, 0.3571411263970316, -0.7447833078367583, 0.8765002281041955, 2.9805992927549454, 0.0776143028335215, -0.2769313825285214, 0.0755136678180621, -0.1474826847594624, -0.3999358135056357, -0.0169688050451549, -0.0937841116273902, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, -0.0122781725754626, -0.3292808750219816, -0.2769313825285214, -0.107625012968948, -0.1474826847594624, -0.5354516373428909, -0.0002229391773086, -0.0862088042532133, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, 0.0766269712424727, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.3951350096437179, -0.5239726809327129, -0.8378690925056301, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.8988006141103361, 3.0784382346326584, 0.0776143028335215, -0.3487779453829513, 0.070457637721598, -0.1474826847594624, -0.3999358135056357, 0.0045338857876385, -0.0729488956980065, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.1275947723114954, -0.4255010572823463, -0.3487779453829513, -0.1777070069371955, -0.1474826847594624, -0.5354516373428909, 0.029608437271828, -0.0566564538358881, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.0232601678654147, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.9106941533136118, 3.1306190036341057, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9108799898636633, 3.1314343281497536, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 1.0542620417323658, -0.6738408269117502, -0.0796833960305785, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9222160194167848, 3.1811691236042576, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.5107528363891513, 1.4769583166408096, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-1.038066534429697, 0.260892106097527, 0.1645848503507034, -1.455529079732558, -1.6391408045371951, 0.196670426185453, 1.2420196914581063, 0.4706326037029981, -0.6616178283130005, -0.5004598044528213, -0.4609157221792596, 0.021209914852902, -1.1769476210921423, 0.0909225332951111, -1.348247483817418, -1.2590833683251033, -0.9252694826312332, 0.781386658108973, -0.0798138719333467, -0.2032969502372001, -0.5728175028694968, 0.103242147293012, -0.2031244129114749, -0.6504603131201653, -0.5868915885967425, -0.6218024077192129, -0.0284729839577739, -0.3925684926321655, -0.4454382040900255, -0.2791761177909213, -1.5149673075505703, 0.3413397023846657, -0.8352815289623093, 0.9329945393197532, 3.228457945511819, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8995798651896526, 0.4879119359671683, 0.1510314945591795, -1.4131138304133295, -0.6523124887119757, 0.196670426185453, 0.148455167213642, 0.4706326037029981, -0.752368509471239, -0.9441449266638648, -0.9507380036319262, 0.1751012323498492, -1.0418764542681906, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -1.3629417216818924, 0.1697587389830128, -0.364093717028159, -0.2645837378255657, -0.2890718868318484, 0.0364158635792983, -0.2031244129114749, -1.8246591171638051, -0.4061028043129334, -0.5350245509902118, 0.0492652283594477, 0.0357249008146661, -0.4929416415078188, -0.5851943844151671, -1.170932563608653, 0.051337096981241, -0.8763418268751098, 0.9447022419729768, 3.279823389997619, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.962914223877992, 3.359725192531085, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.973321070680858, 3.405383365407351, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9735069072309092, 3.4061986899229986, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9980373318376644, 3.5138215259884835, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0340896225475933, 3.671994482024121, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.2639920224449301, -0.6738408269117502, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0396647190491284, 3.696454217493548, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7418059691584676, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, -0.6693833821778409, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.057505027854041, 3.774725370995719, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5792130238763558, -0.9062745442328174, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, 0.0888023142972107, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0760886828591592, 3.856257822560481, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]
2023-11-08 16:54:03,117 - __main__ - INFO - 69
2023-11-08 16:54:03,117 - __main__ - INFO - 325
2023-11-08 16:54:03,257 - __main__ - INFO - {'los_mean': 1055.0307777880782, 'los_std': 799.0879849276147}
2023-11-08 16:54:05,632 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 1.0946
2023-11-08 16:54:23,577 - __main__ - INFO - Fold 1, epoch 0: Loss = 0.9812 Valid loss = 1.0060 MSE = 704.8107
2023-11-08 16:54:23,579 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 704.8107 ------------
2023-11-08 16:54:23,843 - __main__ - INFO - ------------ Save best model - MSE: 704.8107 ------------
2023-11-08 16:54:23,845 - __main__ - INFO - Fold 1, mse = 704.8107, mad = 21.5331
2023-11-08 16:54:24,503 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 0.9723
2023-11-08 16:54:38,817 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 698.8112 ------------
2023-11-08 16:54:39,059 - __main__ - INFO - ------------ Save best model - MSE: 698.8112 ------------
2023-11-08 16:54:39,061 - __main__ - INFO - Fold 1, mse = 698.8112, mad = 21.4083
2023-11-08 16:54:39,679 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 0.8150
2023-11-08 16:54:54,264 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 694.4694 ------------
2023-11-08 16:54:54,637 - __main__ - INFO - ------------ Save best model - MSE: 694.4694 ------------
2023-11-08 16:54:54,639 - __main__ - INFO - Fold 1, mse = 694.4694, mad = 21.4219
2023-11-08 16:54:55,293 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 0.7394
2023-11-08 16:55:09,697 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 693.8707 ------------
2023-11-08 16:55:09,945 - __main__ - INFO - ------------ Save best model - MSE: 693.8707 ------------
2023-11-08 16:55:09,947 - __main__ - INFO - Fold 1, mse = 693.8707, mad = 21.3691
2023-11-08 16:55:10,538 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 0.9821
2023-11-08 16:55:25,633 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 688.9120 ------------
2023-11-08 16:55:25,907 - __main__ - INFO - ------------ Save best model - MSE: 688.9120 ------------
2023-11-08 16:55:25,909 - __main__ - INFO - Fold 1, mse = 688.9120, mad = 21.4042
2023-11-08 16:55:26,489 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 0.7501
2023-11-08 16:55:41,474 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 686.7549 ------------
2023-11-08 16:55:41,836 - __main__ - INFO - ------------ Save best model - MSE: 686.7549 ------------
2023-11-08 16:55:41,837 - __main__ - INFO - Fold 1, mse = 686.7549, mad = 21.2415
2023-11-08 16:55:42,437 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 0.9014
2023-11-08 16:55:58,360 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 681.2491 ------------
2023-11-08 16:55:58,695 - __main__ - INFO - ------------ Save best model - MSE: 681.2491 ------------
2023-11-08 16:55:58,696 - __main__ - INFO - Fold 1, mse = 681.2491, mad = 21.2183
2023-11-08 16:55:59,330 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 0.8142
2023-11-08 16:56:13,854 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 680.4464 ------------
2023-11-08 16:56:14,210 - __main__ - INFO - ------------ Save best model - MSE: 680.4464 ------------
2023-11-08 16:56:14,212 - __main__ - INFO - Fold 1, mse = 680.4464, mad = 21.1035
2023-11-08 16:56:14,841 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 0.8044
2023-11-08 16:56:29,611 - __main__ - INFO - Fold 1, mse = 692.2064, mad = 21.1819
2023-11-08 16:56:30,184 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 0.7455
2023-11-08 16:56:44,360 - __main__ - INFO - Fold 1, mse = 696.7222, mad = 21.2337
2023-11-08 16:56:44,929 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 0.6610
2023-11-08 16:57:00,065 - __main__ - INFO - Fold 1, epoch 10: Loss = 0.6212 Valid loss = 1.0296 MSE = 722.3647
2023-11-08 16:57:00,066 - __main__ - INFO - Fold 1, mse = 722.3647, mad = 21.5854
2023-11-08 16:57:00,671 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 0.6417
2023-11-08 16:57:15,475 - __main__ - INFO - Fold 1, mse = 726.6819, mad = 21.6186
2023-11-08 16:57:16,052 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 0.5312
2023-11-08 16:57:30,491 - __main__ - INFO - Fold 1, mse = 729.1454, mad = 21.7776
2023-11-08 16:57:31,146 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 0.5780
2023-11-08 16:57:46,784 - __main__ - INFO - Fold 1, mse = 752.5556, mad = 21.9898
2023-11-08 16:57:47,425 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 0.6196
2023-11-08 16:58:03,176 - __main__ - INFO - Fold 1, mse = 725.4078, mad = 21.7946
2023-11-08 16:58:03,892 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 0.5492
2023-11-08 16:58:19,728 - __main__ - INFO - Fold 1, mse = 734.2600, mad = 21.8165
2023-11-08 16:58:20,269 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 0.5306
2023-11-08 16:58:34,832 - __main__ - INFO - Fold 1, mse = 735.8933, mad = 22.0685
2023-11-08 16:58:35,446 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 0.5306
2023-11-08 16:58:50,686 - __main__ - INFO - Fold 1, mse = 770.6000, mad = 22.4466
2023-11-08 16:58:51,300 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 0.4197
2023-11-08 16:59:06,108 - __main__ - INFO - Fold 1, mse = 759.4420, mad = 22.3067
2023-11-08 16:59:06,729 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 0.4472
2023-11-08 16:59:21,602 - __main__ - INFO - Fold 1, mse = 772.7237, mad = 22.3257
2023-11-08 16:59:22,178 - __main__ - INFO - Fold 1 Epoch 20 Batch 0: Train Loss = 0.5161
2023-11-08 16:59:38,004 - __main__ - INFO - Fold 1, epoch 20: Loss = 0.4676 Valid loss = 1.0604 MSE = 745.2377
2023-11-08 16:59:38,006 - __main__ - INFO - Fold 1, mse = 745.2377, mad = 22.0778
2023-11-08 16:59:38,665 - __main__ - INFO - Fold 1 Epoch 21 Batch 0: Train Loss = 0.5074
2023-11-08 16:59:54,169 - __main__ - INFO - Fold 1, mse = 759.2287, mad = 22.1093
2023-11-08 16:59:54,735 - __main__ - INFO - Fold 1 Epoch 22 Batch 0: Train Loss = 0.4769
2023-11-08 17:00:09,564 - __main__ - INFO - Fold 1, mse = 741.7155, mad = 21.9817
2023-11-08 17:00:10,086 - __main__ - INFO - Fold 1 Epoch 23 Batch 0: Train Loss = 0.4611
2023-11-08 17:00:24,651 - __main__ - INFO - Fold 1, mse = 743.1299, mad = 22.0320
2023-11-08 17:00:25,216 - __main__ - INFO - Fold 1 Epoch 24 Batch 0: Train Loss = 0.5287
2023-11-08 17:00:39,880 - __main__ - INFO - Fold 1, mse = 710.7774, mad = 21.6455
2023-11-08 17:00:40,532 - __main__ - INFO - Fold 1 Epoch 25 Batch 0: Train Loss = 0.5268
2023-11-08 17:00:55,976 - __main__ - INFO - Fold 1, mse = 744.9190, mad = 21.9749
2023-11-08 17:00:56,615 - __main__ - INFO - Fold 1 Epoch 26 Batch 0: Train Loss = 0.4234
2023-11-08 17:01:11,371 - __main__ - INFO - Fold 1, mse = 738.8849, mad = 21.9806
2023-11-08 17:01:11,935 - __main__ - INFO - Fold 1 Epoch 27 Batch 0: Train Loss = 0.3917
2023-11-08 17:01:26,842 - __main__ - INFO - Fold 1, mse = 733.8549, mad = 21.7451
2023-11-08 17:01:27,491 - __main__ - INFO - Fold 1 Epoch 28 Batch 0: Train Loss = 0.4197
2023-11-08 17:01:42,002 - __main__ - INFO - Fold 1, mse = 717.7623, mad = 21.5185
2023-11-08 17:01:42,687 - __main__ - INFO - Fold 1 Epoch 29 Batch 0: Train Loss = 0.3863
2023-11-08 17:01:57,380 - __main__ - INFO - Fold 1, mse = 730.1588, mad = 21.6445
2023-11-08 17:01:58,296 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 0.9304
2023-11-08 17:02:12,984 - __main__ - INFO - Fold 2, epoch 0: Loss = 0.8577 Valid loss = 1.0300 MSE = 730.8051
2023-11-08 17:02:12,986 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 730.8051 ------------
2023-11-08 17:02:13,150 - __main__ - INFO - Fold 2, mse = 730.8051, mad = 21.6602
2023-11-08 17:02:13,800 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 0.8428
2023-11-08 17:02:28,443 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 726.0824 ------------
2023-11-08 17:02:28,706 - __main__ - INFO - Fold 2, mse = 726.0824, mad = 21.6353
2023-11-08 17:02:29,455 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 0.8018
2023-11-08 17:02:44,538 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 719.9605 ------------
2023-11-08 17:02:44,689 - __main__ - INFO - Fold 2, mse = 719.9605, mad = 21.6581
2023-11-08 17:02:45,191 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 0.8697
2023-11-08 17:03:00,774 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 716.9758 ------------
2023-11-08 17:03:00,922 - __main__ - INFO - Fold 2, mse = 716.9758, mad = 21.5927
2023-11-08 17:03:01,575 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 0.6721
2023-11-08 17:03:16,219 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 715.0046 ------------
2023-11-08 17:03:16,440 - __main__ - INFO - Fold 2, mse = 715.0046, mad = 21.4988
2023-11-08 17:03:17,080 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 0.6629
2023-11-08 17:03:32,927 - __main__ - INFO - Fold 2, mse = 718.3357, mad = 21.5022
2023-11-08 17:03:33,673 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 0.5917
2023-11-08 17:03:49,196 - __main__ - INFO - Fold 2, mse = 719.8253, mad = 21.5522
2023-11-08 17:03:49,928 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 0.6032
2023-11-08 17:04:06,065 - __main__ - INFO - Fold 2, mse = 731.0781, mad = 21.5386
2023-11-08 17:04:06,648 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 0.6054
2023-11-08 17:04:22,778 - __main__ - INFO - Fold 2, mse = 725.1255, mad = 21.4208
2023-11-08 17:04:23,535 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 0.5558
2023-11-08 17:04:38,314 - __main__ - INFO - Fold 2, mse = 726.9028, mad = 21.5060
2023-11-08 17:04:38,826 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 0.6412
2023-11-08 17:04:53,505 - __main__ - INFO - Fold 2, epoch 10: Loss = 0.5135 Valid loss = 1.0159 MSE = 720.5514
2023-11-08 17:04:53,508 - __main__ - INFO - Fold 2, mse = 720.5514, mad = 21.4116
2023-11-08 17:04:54,002 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 0.4961
2023-11-08 17:05:07,564 - __main__ - INFO - Fold 2, mse = 723.5347, mad = 21.4795
2023-11-08 17:05:08,113 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 0.4897
2023-11-08 17:05:22,788 - __main__ - INFO - Fold 2, mse = 750.6783, mad = 21.5419
2023-11-08 17:05:23,301 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 0.5196
2023-11-08 17:05:37,996 - __main__ - INFO - Fold 2, mse = 738.2691, mad = 21.4313
2023-11-08 17:05:38,612 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 0.5567
2023-11-08 17:05:53,459 - __main__ - INFO - Fold 2, mse = 728.5671, mad = 21.3846
2023-11-08 17:05:54,044 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 0.4836
2023-11-08 17:06:08,970 - __main__ - INFO - Fold 2, mse = 726.2787, mad = 21.2235
2023-11-08 17:06:09,524 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 0.5108
2023-11-08 17:06:24,720 - __main__ - INFO - Fold 2, mse = 720.3223, mad = 21.1993
2023-11-08 17:06:25,255 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 0.3243
2023-11-08 17:06:40,416 - __main__ - INFO - Fold 2, mse = 734.4516, mad = 21.4447
2023-11-08 17:06:40,983 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 0.3953
2023-11-08 17:06:55,952 - __main__ - INFO - Fold 2, mse = 737.1839, mad = 21.5742
2023-11-08 17:06:56,499 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 0.4300
2023-11-08 17:07:10,920 - __main__ - INFO - Fold 2, mse = 723.9039, mad = 21.2055
2023-11-08 17:07:11,506 - __main__ - INFO - Fold 2 Epoch 20 Batch 0: Train Loss = 0.4478
2023-11-08 17:07:25,939 - __main__ - INFO - Fold 2, epoch 20: Loss = 0.4278 Valid loss = 0.9953 MSE = 705.5965
2023-11-08 17:07:25,942 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 705.5965 ------------
2023-11-08 17:07:26,192 - __main__ - INFO - Fold 2, mse = 705.5965, mad = 20.9592
2023-11-08 17:07:26,836 - __main__ - INFO - Fold 2 Epoch 21 Batch 0: Train Loss = 0.4304
2023-11-08 17:07:41,055 - __main__ - INFO - Fold 2, mse = 717.6937, mad = 21.1882
2023-11-08 17:07:41,560 - __main__ - INFO - Fold 2 Epoch 22 Batch 0: Train Loss = 0.4971
2023-11-08 17:07:56,158 - __main__ - INFO - Fold 2, mse = 719.0783, mad = 21.3049
2023-11-08 17:07:56,704 - __main__ - INFO - Fold 2 Epoch 23 Batch 0: Train Loss = 0.3789
2023-11-08 17:08:11,851 - __main__ - INFO - Fold 2, mse = 738.4520, mad = 21.5651
2023-11-08 17:08:12,497 - __main__ - INFO - Fold 2 Epoch 24 Batch 0: Train Loss = 0.3248
2023-11-08 17:08:28,784 - __main__ - INFO - Fold 2, mse = 737.7259, mad = 21.4065
2023-11-08 17:08:29,436 - __main__ - INFO - Fold 2 Epoch 25 Batch 0: Train Loss = 0.4086
2023-11-08 17:08:44,030 - __main__ - INFO - Fold 2, mse = 751.2327, mad = 21.5173
2023-11-08 17:08:44,623 - __main__ - INFO - Fold 2 Epoch 26 Batch 0: Train Loss = 0.3854
2023-11-08 17:09:00,496 - __main__ - INFO - Fold 2, mse = 742.0209, mad = 21.5399
2023-11-08 17:09:01,150 - __main__ - INFO - Fold 2 Epoch 27 Batch 0: Train Loss = 0.3767
2023-11-08 17:09:15,899 - __main__ - INFO - Fold 2, mse = 737.5883, mad = 21.4365
2023-11-08 17:09:16,513 - __main__ - INFO - Fold 2 Epoch 28 Batch 0: Train Loss = 0.4467
2023-11-08 17:09:31,593 - __main__ - INFO - Fold 2, mse = 732.4642, mad = 21.3917
2023-11-08 17:09:32,151 - __main__ - INFO - Fold 2 Epoch 29 Batch 0: Train Loss = 0.3926
2023-11-08 17:09:46,823 - __main__ - INFO - Fold 2, mse = 740.0006, mad = 21.4781
2023-11-08 17:09:47,556 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 0.9962
2023-11-08 17:10:03,051 - __main__ - INFO - Fold 3, epoch 0: Loss = 0.9679 Valid loss = 1.0142 MSE = 714.4655
2023-11-08 17:10:03,054 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 714.4655 ------------
2023-11-08 17:10:03,254 - __main__ - INFO - Fold 3, mse = 714.4655, mad = 22.0161
2023-11-08 17:10:03,840 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 1.0149
2023-11-08 17:10:19,544 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 707.4648 ------------
2023-11-08 17:10:19,704 - __main__ - INFO - Fold 3, mse = 707.4648, mad = 21.8971
2023-11-08 17:10:20,502 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 0.8430
2023-11-08 17:10:36,598 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 703.0256 ------------
2023-11-08 17:10:36,791 - __main__ - INFO - Fold 3, mse = 703.0256, mad = 21.7955
2023-11-08 17:10:37,387 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 0.9823
2023-11-08 17:10:51,954 - __main__ - INFO - Fold 3, mse = 704.6070, mad = 21.7428
2023-11-08 17:10:52,533 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 0.8317
2023-11-08 17:11:08,014 - __main__ - INFO - Fold 3, mse = 712.5207, mad = 21.7307
2023-11-08 17:11:08,601 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 0.7380
2023-11-08 17:11:23,900 - __main__ - INFO - Fold 3, mse = 728.8591, mad = 21.8598
2023-11-08 17:11:24,489 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 0.6986
2023-11-08 17:11:39,216 - __main__ - INFO - Fold 3, mse = 753.6590, mad = 22.1289
2023-11-08 17:11:39,923 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 0.6702
2023-11-08 17:11:54,849 - __main__ - INFO - Fold 3, mse = 784.5359, mad = 22.4821
2023-11-08 17:11:55,423 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 0.6100
2023-11-08 17:12:13,092 - __main__ - INFO - Fold 3, mse = 789.1584, mad = 22.4501
2023-11-08 17:12:13,754 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 0.4976
2023-11-08 17:12:28,717 - __main__ - INFO - Fold 3, mse = 792.9618, mad = 22.4377
2023-11-08 17:12:29,364 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 0.5445
2023-11-08 17:12:46,203 - __main__ - INFO - Fold 3, epoch 10: Loss = 0.4918 Valid loss = 1.1875 MSE = 828.7936
2023-11-08 17:12:46,205 - __main__ - INFO - Fold 3, mse = 828.7936, mad = 22.9245
2023-11-08 17:12:46,870 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 0.3767
2023-11-08 17:13:02,206 - __main__ - INFO - Fold 3, mse = 821.6332, mad = 22.8909
2023-11-08 17:13:02,907 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 0.4134
2023-11-08 17:13:18,403 - __main__ - INFO - Fold 3, mse = 805.9200, mad = 22.7440
2023-11-08 17:13:19,186 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 0.4634
2023-11-08 17:13:34,567 - __main__ - INFO - Fold 3, mse = 844.6015, mad = 23.0696
2023-11-08 17:13:35,161 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 0.4456
2023-11-08 17:13:50,960 - __main__ - INFO - Fold 3, mse = 848.8634, mad = 23.1726
2023-11-08 17:13:51,534 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 0.4897
2023-11-08 17:14:06,631 - __main__ - INFO - Fold 3, mse = 827.4917, mad = 22.8330
2023-11-08 17:14:07,198 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 0.4349
2023-11-08 17:14:22,784 - __main__ - INFO - Fold 3, mse = 826.1342, mad = 22.8588
2023-11-08 17:14:23,480 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 0.4370
2023-11-08 17:14:39,089 - __main__ - INFO - Fold 3, mse = 835.6353, mad = 23.0228
2023-11-08 17:14:39,673 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 0.4188
2023-11-08 17:14:55,094 - __main__ - INFO - Fold 3, mse = 834.2996, mad = 22.9950
2023-11-08 17:14:55,687 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 0.4055
2023-11-08 17:15:11,138 - __main__ - INFO - Fold 3, mse = 842.7365, mad = 23.0895
2023-11-08 17:15:11,759 - __main__ - INFO - Fold 3 Epoch 20 Batch 0: Train Loss = 0.4209
2023-11-08 17:15:27,342 - __main__ - INFO - Fold 3, epoch 20: Loss = 0.3874 Valid loss = 1.2610 MSE = 854.8079
2023-11-08 17:15:27,345 - __main__ - INFO - Fold 3, mse = 854.8079, mad = 23.2901
2023-11-08 17:15:28,031 - __main__ - INFO - Fold 3 Epoch 21 Batch 0: Train Loss = 0.4552
2023-11-08 17:15:43,863 - __main__ - INFO - Fold 3, mse = 820.5413, mad = 22.8390
2023-11-08 17:15:44,533 - __main__ - INFO - Fold 3 Epoch 22 Batch 0: Train Loss = 0.3202
2023-11-08 17:15:59,860 - __main__ - INFO - Fold 3, mse = 856.6522, mad = 23.1634
2023-11-08 17:16:00,642 - __main__ - INFO - Fold 3 Epoch 23 Batch 0: Train Loss = 0.4221
2023-11-08 17:16:16,348 - __main__ - INFO - Fold 3, mse = 840.0885, mad = 23.0286
2023-11-08 17:16:17,040 - __main__ - INFO - Fold 3 Epoch 24 Batch 0: Train Loss = 0.4065
2023-11-08 17:16:32,579 - __main__ - INFO - Fold 3, mse = 831.4609, mad = 22.9385
2023-11-08 17:16:33,146 - __main__ - INFO - Fold 3 Epoch 25 Batch 0: Train Loss = 0.3479
2023-11-08 17:16:49,677 - __main__ - INFO - Fold 3, mse = 853.2305, mad = 23.1674
2023-11-08 17:16:50,396 - __main__ - INFO - Fold 3 Epoch 26 Batch 0: Train Loss = 0.3991
2023-11-08 17:17:07,408 - __main__ - INFO - Fold 3, mse = 865.9003, mad = 23.2949
2023-11-08 17:17:08,153 - __main__ - INFO - Fold 3 Epoch 27 Batch 0: Train Loss = 0.4437
2023-11-08 17:17:23,230 - __main__ - INFO - Fold 3, mse = 864.0394, mad = 23.3112
2023-11-08 17:17:23,826 - __main__ - INFO - Fold 3 Epoch 28 Batch 0: Train Loss = 0.3722
2023-11-08 17:17:37,837 - __main__ - INFO - Fold 3, mse = 860.0411, mad = 23.0625
2023-11-08 17:17:38,532 - __main__ - INFO - Fold 3 Epoch 29 Batch 0: Train Loss = 0.3913
2023-11-08 17:17:53,417 - __main__ - INFO - Fold 3, mse = 842.9297, mad = 23.0403
2023-11-08 17:17:54,227 - __main__ - INFO - Fold 4 Epoch 0 Batch 0: Train Loss = 1.2248
2023-11-08 17:18:10,199 - __main__ - INFO - Fold 4, epoch 0: Loss = 1.1806 Valid loss = 0.9651 MSE = 684.5031
2023-11-08 17:18:10,202 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 684.5031 ------------
2023-11-08 17:18:10,483 - __main__ - INFO - Fold 4, mse = 684.5031, mad = 21.8595
2023-11-08 17:18:11,177 - __main__ - INFO - Fold 4 Epoch 1 Batch 0: Train Loss = 1.0944
2023-11-08 17:18:26,306 - __main__ - INFO - Fold 4, mse = 686.8931, mad = 21.8846
2023-11-08 17:18:26,903 - __main__ - INFO - Fold 4 Epoch 2 Batch 0: Train Loss = 1.1937
2023-11-08 17:18:42,155 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 670.7075 ------------
2023-11-08 17:18:42,412 - __main__ - INFO - ------------ Save best model - MSE: 670.7075 ------------
2023-11-08 17:18:42,414 - __main__ - INFO - Fold 4, mse = 670.7075, mad = 21.5478
2023-11-08 17:18:42,978 - __main__ - INFO - Fold 4 Epoch 3 Batch 0: Train Loss = 1.1828
2023-11-08 17:18:58,454 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 654.2449 ------------
2023-11-08 17:18:58,821 - __main__ - INFO - ------------ Save best model - MSE: 654.2449 ------------
2023-11-08 17:18:58,823 - __main__ - INFO - Fold 4, mse = 654.2449, mad = 21.2002
2023-11-08 17:18:59,468 - __main__ - INFO - Fold 4 Epoch 4 Batch 0: Train Loss = 0.8803
2023-11-08 17:19:14,313 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 645.3749 ------------
2023-11-08 17:19:14,682 - __main__ - INFO - ------------ Save best model - MSE: 645.3749 ------------
2023-11-08 17:19:14,683 - __main__ - INFO - Fold 4, mse = 645.3749, mad = 20.9918
2023-11-08 17:19:15,411 - __main__ - INFO - Fold 4 Epoch 5 Batch 0: Train Loss = 0.9021
2023-11-08 17:19:30,534 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 629.6928 ------------
2023-11-08 17:19:30,901 - __main__ - INFO - ------------ Save best model - MSE: 629.6928 ------------
2023-11-08 17:19:30,903 - __main__ - INFO - Fold 4, mse = 629.6928, mad = 20.4915
2023-11-08 17:19:31,593 - __main__ - INFO - Fold 4 Epoch 6 Batch 0: Train Loss = 0.9272
2023-11-08 17:19:48,438 - __main__ - INFO - Fold 4, mse = 631.1834, mad = 20.3796
2023-11-08 17:19:49,158 - __main__ - INFO - Fold 4 Epoch 7 Batch 0: Train Loss = 0.7558
2023-11-08 17:20:03,995 - __main__ - INFO - Fold 4, mse = 632.9747, mad = 20.2914
2023-11-08 17:20:04,662 - __main__ - INFO - Fold 4 Epoch 8 Batch 0: Train Loss = 0.6259
2023-11-08 17:20:19,035 - __main__ - INFO - Fold 4, mse = 638.1502, mad = 20.2635
2023-11-08 17:20:19,623 - __main__ - INFO - Fold 4 Epoch 9 Batch 0: Train Loss = 0.6085
2023-11-08 17:20:34,882 - __main__ - INFO - Fold 4, mse = 641.9306, mad = 20.3029
2023-11-08 17:20:35,451 - __main__ - INFO - Fold 4 Epoch 10 Batch 0: Train Loss = 0.7041
2023-11-08 17:20:51,416 - __main__ - INFO - Fold 4, epoch 10: Loss = 0.6358 Valid loss = 0.8942 MSE = 634.4354
2023-11-08 17:20:51,418 - __main__ - INFO - Fold 4, mse = 634.4354, mad = 20.2599
2023-11-08 17:20:52,058 - __main__ - INFO - Fold 4 Epoch 11 Batch 0: Train Loss = 0.6890
2023-11-08 17:21:07,627 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 627.9062 ------------
2023-11-08 17:21:07,881 - __main__ - INFO - ------------ Save best model - MSE: 627.9062 ------------
2023-11-08 17:21:07,883 - __main__ - INFO - Fold 4, mse = 627.9062, mad = 20.0771
2023-11-08 17:21:08,561 - __main__ - INFO - Fold 4 Epoch 12 Batch 0: Train Loss = 0.6895
2023-11-08 17:21:25,140 - __main__ - INFO - Fold 4, mse = 634.2557, mad = 20.2452
2023-11-08 17:21:25,867 - __main__ - INFO - Fold 4 Epoch 13 Batch 0: Train Loss = 0.4611
2023-11-08 17:21:41,362 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 625.5510 ------------
2023-11-08 17:21:41,728 - __main__ - INFO - ------------ Save best model - MSE: 625.5510 ------------
2023-11-08 17:21:41,730 - __main__ - INFO - Fold 4, mse = 625.5510, mad = 20.0430
2023-11-08 17:21:42,423 - __main__ - INFO - Fold 4 Epoch 14 Batch 0: Train Loss = 0.5133
2023-11-08 17:21:57,923 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 620.9622 ------------
2023-11-08 17:21:58,292 - __main__ - INFO - ------------ Save best model - MSE: 620.9622 ------------
2023-11-08 17:21:58,293 - __main__ - INFO - Fold 4, mse = 620.9622, mad = 19.8715
2023-11-08 17:21:58,895 - __main__ - INFO - Fold 4 Epoch 15 Batch 0: Train Loss = 0.4940
2023-11-08 17:22:14,807 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 615.5759 ------------
2023-11-08 17:22:15,118 - __main__ - INFO - ------------ Save best model - MSE: 615.5759 ------------
2023-11-08 17:22:15,121 - __main__ - INFO - Fold 4, mse = 615.5759, mad = 19.8030
2023-11-08 17:22:15,738 - __main__ - INFO - Fold 4 Epoch 16 Batch 0: Train Loss = 0.6448
2023-11-08 17:22:31,074 - __main__ - INFO - Fold 4, mse = 625.2642, mad = 20.0349
2023-11-08 17:22:31,668 - __main__ - INFO - Fold 4 Epoch 17 Batch 0: Train Loss = 0.4848
2023-11-08 17:22:47,196 - __main__ - INFO - Fold 4, mse = 620.3527, mad = 20.0577
2023-11-08 17:22:47,918 - __main__ - INFO - Fold 4 Epoch 18 Batch 0: Train Loss = 0.5103
2023-11-08 17:23:03,736 - __main__ - INFO - Fold 4, mse = 623.6733, mad = 19.9119
2023-11-08 17:23:04,405 - __main__ - INFO - Fold 4 Epoch 19 Batch 0: Train Loss = 0.4586
2023-11-08 17:23:18,957 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 613.1711 ------------
2023-11-08 17:23:19,205 - __main__ - INFO - ------------ Save best model - MSE: 613.1711 ------------
2023-11-08 17:23:19,207 - __main__ - INFO - Fold 4, mse = 613.1711, mad = 19.9001
2023-11-08 17:23:19,763 - __main__ - INFO - Fold 4 Epoch 20 Batch 0: Train Loss = 0.5408
2023-11-08 17:23:35,255 - __main__ - INFO - Fold 4, epoch 20: Loss = 0.5026 Valid loss = 0.8688 MSE = 616.2459
2023-11-08 17:23:35,258 - __main__ - INFO - Fold 4, mse = 616.2459, mad = 19.9445
2023-11-08 17:23:35,988 - __main__ - INFO - Fold 4 Epoch 21 Batch 0: Train Loss = 0.4763
2023-11-08 17:23:51,212 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 610.9017 ------------
2023-11-08 17:23:51,505 - __main__ - INFO - ------------ Save best model - MSE: 610.9017 ------------
2023-11-08 17:23:51,507 - __main__ - INFO - Fold 4, mse = 610.9017, mad = 19.9040
2023-11-08 17:23:52,169 - __main__ - INFO - Fold 4 Epoch 22 Batch 0: Train Loss = 0.4939
2023-11-08 17:24:07,543 - __main__ - INFO - Fold 4, mse = 623.4927, mad = 20.0953
2023-11-08 17:24:08,219 - __main__ - INFO - Fold 4 Epoch 23 Batch 0: Train Loss = 0.4638
2023-11-08 17:24:22,761 - __main__ - INFO - Fold 4, mse = 614.4404, mad = 19.8768
2023-11-08 17:24:23,424 - __main__ - INFO - Fold 4 Epoch 24 Batch 0: Train Loss = 0.4524
2023-11-08 17:24:38,277 - __main__ - INFO - Fold 4, mse = 615.3014, mad = 20.0648
2023-11-08 17:24:38,906 - __main__ - INFO - Fold 4 Epoch 25 Batch 0: Train Loss = 0.4705
2023-11-08 17:24:54,332 - __main__ - INFO - Fold 4, mse = 631.7523, mad = 20.1081
2023-11-08 17:24:54,879 - __main__ - INFO - Fold 4 Epoch 26 Batch 0: Train Loss = 0.4242
2023-11-08 17:25:10,890 - __main__ - INFO - Fold 4, mse = 615.1992, mad = 20.0213
2023-11-08 17:25:11,523 - __main__ - INFO - Fold 4 Epoch 27 Batch 0: Train Loss = 0.4695
2023-11-08 17:25:26,880 - __main__ - INFO - Fold 4, mse = 635.7979, mad = 20.1140
2023-11-08 17:25:27,554 - __main__ - INFO - Fold 4 Epoch 28 Batch 0: Train Loss = 0.4350
2023-11-08 17:25:43,109 - __main__ - INFO - Fold 4, mse = 619.7320, mad = 19.9569
2023-11-08 17:25:43,789 - __main__ - INFO - Fold 4 Epoch 29 Batch 0: Train Loss = 0.5613
2023-11-08 17:25:59,025 - __main__ - INFO - Fold 4, mse = 620.8091, mad = 19.9124
2023-11-08 17:25:59,776 - __main__ - INFO - Fold 5 Epoch 0 Batch 0: Train Loss = 1.0700
2023-11-08 17:26:13,783 - __main__ - INFO - Fold 5, epoch 0: Loss = 1.0463 Valid loss = 0.9937 MSE = 696.6147
2023-11-08 17:26:13,785 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 696.6147 ------------
2023-11-08 17:26:13,995 - __main__ - INFO - Fold 5, mse = 696.6147, mad = 21.4679
2023-11-08 17:26:14,551 - __main__ - INFO - Fold 5 Epoch 1 Batch 0: Train Loss = 1.0346
2023-11-08 17:26:28,905 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 684.5840 ------------
2023-11-08 17:26:29,154 - __main__ - INFO - Fold 5, mse = 684.5840, mad = 21.2195
2023-11-08 17:26:29,793 - __main__ - INFO - Fold 5 Epoch 2 Batch 0: Train Loss = 0.8454
2023-11-08 17:26:43,527 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 675.0285 ------------
2023-11-08 17:26:43,674 - __main__ - INFO - Fold 5, mse = 675.0285, mad = 21.0062
2023-11-08 17:26:44,198 - __main__ - INFO - Fold 5 Epoch 3 Batch 0: Train Loss = 0.9028
2023-11-08 17:26:58,058 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 672.5578 ------------
2023-11-08 17:26:58,252 - __main__ - INFO - Fold 5, mse = 672.5578, mad = 20.9099
2023-11-08 17:26:58,809 - __main__ - INFO - Fold 5 Epoch 4 Batch 0: Train Loss = 0.8084
2023-11-08 17:27:12,115 - __main__ - INFO - Fold 5, mse = 675.8861, mad = 20.8368
2023-11-08 17:27:12,731 - __main__ - INFO - Fold 5 Epoch 5 Batch 0: Train Loss = 0.7674
2023-11-08 17:27:27,256 - __main__ - INFO - Fold 5, mse = 687.4293, mad = 20.9467
2023-11-08 17:27:27,795 - __main__ - INFO - Fold 5 Epoch 6 Batch 0: Train Loss = 0.7976
2023-11-08 17:27:42,430 - __main__ - INFO - Fold 5, mse = 700.9905, mad = 21.0927
2023-11-08 17:27:43,121 - __main__ - INFO - Fold 5 Epoch 7 Batch 0: Train Loss = 0.7533
2023-11-08 17:27:57,853 - __main__ - INFO - Fold 5, mse = 716.4093, mad = 21.1849
2023-11-08 17:27:58,483 - __main__ - INFO - Fold 5 Epoch 8 Batch 0: Train Loss = 0.6995
2023-11-08 17:28:12,851 - __main__ - INFO - Fold 5, mse = 721.4573, mad = 21.3307
2023-11-08 17:28:13,396 - __main__ - INFO - Fold 5 Epoch 9 Batch 0: Train Loss = 0.5736
2023-11-08 17:28:27,381 - __main__ - INFO - Fold 5, mse = 720.1019, mad = 21.4237
2023-11-08 17:28:27,915 - __main__ - INFO - Fold 5 Epoch 10 Batch 0: Train Loss = 0.6822
2023-11-08 17:28:43,596 - __main__ - INFO - Fold 5, epoch 10: Loss = 0.6310 Valid loss = 1.0274 MSE = 725.3873
2023-11-08 17:28:43,599 - __main__ - INFO - Fold 5, mse = 725.3873, mad = 21.5230
2023-11-08 17:28:44,095 - __main__ - INFO - Fold 5 Epoch 11 Batch 0: Train Loss = 0.6557
2023-11-08 17:28:58,115 - __main__ - INFO - Fold 5, mse = 706.1568, mad = 21.1255
2023-11-08 17:28:58,729 - __main__ - INFO - Fold 5 Epoch 12 Batch 0: Train Loss = 0.5533
2023-11-08 17:29:13,849 - __main__ - INFO - Fold 5, mse = 734.8889, mad = 21.5558
2023-11-08 17:29:14,466 - __main__ - INFO - Fold 5 Epoch 13 Batch 0: Train Loss = 0.5533
2023-11-08 17:29:29,556 - __main__ - INFO - Fold 5, mse = 724.0796, mad = 21.4757
2023-11-08 17:29:30,179 - __main__ - INFO - Fold 5 Epoch 14 Batch 0: Train Loss = 0.5251
2023-11-08 17:29:45,761 - __main__ - INFO - Fold 5, mse = 743.1757, mad = 21.6117
2023-11-08 17:29:46,309 - __main__ - INFO - Fold 5 Epoch 15 Batch 0: Train Loss = 0.5495
2023-11-08 17:30:00,952 - __main__ - INFO - Fold 5, mse = 727.2741, mad = 21.3918
2023-11-08 17:30:01,498 - __main__ - INFO - Fold 5 Epoch 16 Batch 0: Train Loss = 0.4606
2023-11-08 17:30:17,088 - __main__ - INFO - Fold 5, mse = 747.1841, mad = 21.7029
2023-11-08 17:30:17,755 - __main__ - INFO - Fold 5 Epoch 17 Batch 0: Train Loss = 0.4454
2023-11-08 17:30:33,000 - __main__ - INFO - Fold 5, mse = 749.9362, mad = 21.8028
2023-11-08 17:30:33,631 - __main__ - INFO - Fold 5 Epoch 18 Batch 0: Train Loss = 0.4894
2023-11-08 17:30:48,017 - __main__ - INFO - Fold 5, mse = 751.6348, mad = 21.7944
2023-11-08 17:30:48,576 - __main__ - INFO - Fold 5 Epoch 19 Batch 0: Train Loss = 0.4632
2023-11-08 17:31:03,557 - __main__ - INFO - Fold 5, mse = 757.2227, mad = 21.7807
2023-11-08 17:31:04,162 - __main__ - INFO - Fold 5 Epoch 20 Batch 0: Train Loss = 0.4394
2023-11-08 17:31:19,033 - __main__ - INFO - Fold 5, epoch 20: Loss = 0.4594 Valid loss = 1.0668 MSE = 747.4766
2023-11-08 17:31:19,035 - __main__ - INFO - Fold 5, mse = 747.4766, mad = 21.7709
2023-11-08 17:31:19,622 - __main__ - INFO - Fold 5 Epoch 21 Batch 0: Train Loss = 0.3709
2023-11-08 17:31:34,122 - __main__ - INFO - Fold 5, mse = 745.6022, mad = 21.7383
2023-11-08 17:31:34,734 - __main__ - INFO - Fold 5 Epoch 22 Batch 0: Train Loss = 0.4218
2023-11-08 17:31:48,721 - __main__ - INFO - Fold 5, mse = 755.6427, mad = 21.7055
2023-11-08 17:31:49,232 - __main__ - INFO - Fold 5 Epoch 23 Batch 0: Train Loss = 0.4082
2023-11-08 17:32:03,421 - __main__ - INFO - Fold 5, mse = 764.4572, mad = 21.8696
2023-11-08 17:32:04,021 - __main__ - INFO - Fold 5 Epoch 24 Batch 0: Train Loss = 0.3996
2023-11-08 17:32:19,085 - __main__ - INFO - Fold 5, mse = 758.9320, mad = 21.8800
2023-11-08 17:32:19,590 - __main__ - INFO - Fold 5 Epoch 25 Batch 0: Train Loss = 0.4658
2023-11-08 17:32:33,973 - __main__ - INFO - Fold 5, mse = 780.5985, mad = 22.2040
2023-11-08 17:32:34,467 - __main__ - INFO - Fold 5 Epoch 26 Batch 0: Train Loss = 0.4541
2023-11-08 17:32:47,736 - __main__ - INFO - Fold 5, mse = 747.3853, mad = 21.6879
2023-11-08 17:32:48,268 - __main__ - INFO - Fold 5 Epoch 27 Batch 0: Train Loss = 0.3252
2023-11-08 17:33:02,301 - __main__ - INFO - Fold 5, mse = 760.7726, mad = 21.8528
2023-11-08 17:33:02,919 - __main__ - INFO - Fold 5 Epoch 28 Batch 0: Train Loss = 0.4105
2023-11-08 17:33:17,132 - __main__ - INFO - Fold 5, mse = 746.3192, mad = 21.8269
2023-11-08 17:33:17,753 - __main__ - INFO - Fold 5 Epoch 29 Batch 0: Train Loss = 0.3930
2023-11-08 17:33:32,079 - __main__ - INFO - Fold 5, mse = 760.4849, mad = 21.7859
2023-11-08 17:33:32,082 - __main__ - INFO - mse 674.5056(34.2479)
2023-11-08 17:33:32,084 - __main__ - INFO - mad 20.9344(0.6055)
2023-11-08 17:33:32,097 - __main__ - INFO - mse 674.5056(34.2479)
2023-11-08 17:33:32,099 - __main__ - INFO - mad 20.9344(0.6055)
