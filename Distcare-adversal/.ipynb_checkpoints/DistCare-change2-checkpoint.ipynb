{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:55:29.996921Z",
     "start_time": "2021-02-10T14:55:28.704721Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "import pickle5 as pickle\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import logging\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from utils import utils\n",
    "from utils.readers import InHospitalMortalityReader\n",
    "from utils.preprocessing import Discretizer, Normalizer\n",
    "from utils import metrics\n",
    "from utils import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:55:52.924543Z",
     "start_time": "2021-02-10T14:55:52.918220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available device: cuda:6\n"
     ]
    }
   ],
   "source": [
    "# Select the target dataset: COVID-19 Dataset from TJ Hospital or HM Hospital\n",
    "target_dataset = 'PD' \n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() == True else 'cpu')\n",
    "print(\"available device: {}\".format(device))\n",
    "reverse = True\n",
    "model_name = 'distcare_change2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:06:11,904 - INFO - 这是希望输出的info内容\n",
      "2023-08-11 16:06:11,905 - WARNING - 这是希望输出的warning内容\n"
     ]
    }
   ],
   "source": [
    "if reverse:\n",
    "    file_name = 'log_file' + '_' + model_name + '_' + target_dataset + '_' + 'reverse' + '.log'\n",
    "else:\n",
    "    file_name = 'log_file' + '_' + model_name + '_' + target_dataset + '.log'\n",
    "def get_logger(name, file_name):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # 以下两行是为了在jupyter notebook 中不重复输出日志\n",
    "    if logger.root.handlers:\n",
    "        logger.root.handlers[0].setLevel(logging.WARNING)\n",
    " \n",
    "    handler_stdout = logging.StreamHandler()\n",
    "    handler_stdout.setLevel(logging.INFO)\n",
    "    handler_stdout.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler_stdout)\n",
    " \n",
    "    handler_file = logging.FileHandler(filename=file_name, mode='w', encoding='utf-8')\n",
    "    handler_file.setLevel(logging.DEBUG)\n",
    "    handler_file.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler_file)\n",
    " \n",
    "    return logger\n",
    "\n",
    "logger = get_logger(__name__,file_name)\n",
    "\n",
    "logger.debug('这是希望输出的debug内容')\n",
    "logger.info('这是希望输出的info内容')\n",
    "logger.warning('这是希望输出的warning内容')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Source Data & Model: \n",
    "#### Teacher Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:55:32.579746Z",
     "start_time": "2021-02-10T14:55:32.571939Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = './data/Challenge/'\n",
    "small_part = False\n",
    "arg_timestep = 1.0\n",
    "batch_size = 256\n",
    "epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T04:27:29.073353Z",
     "start_time": "2021-01-30T04:27:12.159717Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:06:27,547 - INFO - [[-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 1.2605692444279462, 0.585371321489137, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.9546043520029015, 0.2987563180527675, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 1.0769903089729194, 0.4420638197709522, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.566534136852991, 1.803485086093707, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, 0.46506052412282983, -0.05951243624269434, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, 0.281481588667803, -0.2744736888199714, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.8934113735178925, 0.9436400757845987, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:06:27,553 - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "2023-08-11 16:06:27,555 - INFO - 110609\n",
      "2023-08-11 16:06:27,556 - INFO - [[-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8962118618028821, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8617355345389544, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8272592072750267, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7927828800110989, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7583065527471711, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7238302254832434, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6893538982193156, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6548775709553878, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.62040124369146, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5859249164275323, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5514485891636045, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5169722618996767, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.48249593463574897, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.4480196073718212, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.41354328010789343, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3790669528439657, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3445906255800379, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.31011429831611015, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.27563797105218235, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.2411616437882546, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.20668531652432684, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.17220898926039907, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.1377326619964713, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.10325633473254354, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.06878000746861578, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.034303680204688006, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.0001726470592397616, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.034648974323167527, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.06912530158709529, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.10360162885102306, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.13807795611495083, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.1725542833788786, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.20703061064280637, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.24150693790673414, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.2759832651706619, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.31045959243458965, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.34493591969851745, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.3794122469624452, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.413888574226373, '0']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 1.2605692444279462, 0.585371321489137, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.9546043520029015, 0.2987563180527675, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 1.0769903089729194, 0.4420638197709522, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.566534136852991, 1.803485086093707, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, 0.46506052412282983, -0.05951243624269434, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, 0.281481588667803, -0.2744736888199714, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.8934113735178925, 0.9436400757845987, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "110609\n",
      "[[-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8962118618028821, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8617355345389544, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8272592072750267, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7927828800110989, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7583065527471711, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7238302254832434, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6893538982193156, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6548775709553878, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.62040124369146, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5859249164275323, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5514485891636045, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5169722618996767, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.48249593463574897, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.4480196073718212, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.41354328010789343, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3790669528439657, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3445906255800379, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.31011429831611015, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.27563797105218235, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.2411616437882546, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.20668531652432684, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.17220898926039907, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.1377326619964713, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.10325633473254354, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.06878000746861578, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.034303680204688006, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.0001726470592397616, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.034648974323167527, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.06912530158709529, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.10360162885102306, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.13807795611495083, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.1725542833788786, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.20703061064280637, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.24150693790673414, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.2759832651706619, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.31045959243458965, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.34493591969851745, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.3794122469624452, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.413888574226373, '0']]\n"
     ]
    }
   ],
   "source": [
    "all_x = pickle.load(open(data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "all_y = pickle.load(open(data_path + 'new_y_front_fill.dat', 'rb'))\n",
    "all_names = pickle.load(open(data_path + 'new_name.dat', 'rb'))\n",
    "static = pickle.load(open(data_path + 'new_demo_front_fill.dat', 'rb'))\n",
    "mask_x = pickle.load(open(data_path + 'new_mask_x.dat', 'rb'))\n",
    "mask_demo = pickle.load(open(data_path + 'new_mask_demo.dat', 'rb'))\n",
    "all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "print(all_x[0])\n",
    "print(mask_x[0])\n",
    "print(all_names[0])\n",
    "print(static[0])\n",
    "logger.info(all_x[0])\n",
    "logger.info(mask_x[0])\n",
    "logger.info(all_names[0])\n",
    "logger.info(static[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_y[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T04:27:41.051036Z",
     "start_time": "2021-01-30T04:27:29.075798Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:06:40,919 - INFO - [[-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.38817852813712, 0.585371321489137, 0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.2605692444279462, 0.6420908338073934, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.043819195154697, 0.2987563180527675, 0.6590707727034506, -0.4065324680479176, -1.009369603802189, 0.9546043520029015, 0.6420908338073934, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.38817852813712, 0.4420638197709522, 0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.0769903089729194, 0.8382388140070928, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.4742683613827257, 1.803485086093707, 1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.566534136852991, 0.8382388140070928, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.44119036243545645, -0.05951243624269434, 0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.46506052412282983, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.6564149455494709, -0.2744736888199714, 0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.281481588667803, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.09683102945303342, 0.9436400757845987, 0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.8934113735178925, 1.4266827546061909, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.3981454458126536, -0.05951243624269434, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.3981454458126536, -0.05951243624269434, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.37666305339779826, -0.5610886922563408, 0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.26925521769727756, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.37666305339779826, -0.5610886922563408, 0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.26925521769727756, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.1299090284003026, 0.585371321489137, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.0157973304879104, 0.44594285360769403, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.1299090284003026, 0.585371321489137, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.0157973304879104, 0.44594285360769403, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.613370028926668, 1.6601775843755224, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 1.1993762659429372, -0.9270930077902015, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.613370028926668, 1.6601775843755224, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 1.1993762659429372, -0.9270930077902015, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:06:40,923 - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "2023-08-11 16:06:40,924 - INFO - 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.38817852813712, 0.585371321489137, 0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.2605692444279462, 0.6420908338073934, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.043819195154697, 0.2987563180527675, 0.6590707727034506, -0.4065324680479176, -1.009369603802189, 0.9546043520029015, 0.6420908338073934, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.38817852813712, 0.4420638197709522, 0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.0769903089729194, 0.8382388140070928, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.4742683613827257, 1.803485086093707, 1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.566534136852991, 0.8382388140070928, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.44119036243545645, -0.05951243624269434, 0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.46506052412282983, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.6564149455494709, -0.2744736888199714, 0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.281481588667803, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.09683102945303342, 0.9436400757845987, 0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.8934113735178925, 1.4266827546061909, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.3981454458126536, -0.05951243624269434, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.3981454458126536, -0.05951243624269434, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.37666305339779826, -0.5610886922563408, 0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.26925521769727756, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.37666305339779826, -0.5610886922563408, 0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.26925521769727756, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.1299090284003026, 0.585371321489137, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.0157973304879104, 0.44594285360769403, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.1299090284003026, 0.585371321489137, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.0157973304879104, 0.44594285360769403, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.613370028926668, 1.6601775843755224, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 1.1993762659429372, -0.9270930077902015, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.613370028926668, 1.6601775843755224, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 1.1993762659429372, -0.9270930077902015, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "if target_dataset == 'PD':\n",
    "    subset_idx = [31, 29, 28, 33, 25, 18, 7, 21, 16, 15, 19, 17, 24, 3, 5, 0]\n",
    "\n",
    "subset_cnt = len(subset_idx)\n",
    "other_idx = []\n",
    "for i in range(len(all_x[0][0])):\n",
    "    if i not in subset_idx:\n",
    "        other_idx.append(i)\n",
    "\n",
    "for i in range(len(all_x)):\n",
    "    cur = np.array(all_x[i], dtype=float)\n",
    "    cur_mask = np.array(mask_x[i])\n",
    "    cur_subset = cur[:, subset_idx]\n",
    "    cur_other = cur[:, other_idx]\n",
    "    cur_mask_subset = cur_mask[:, subset_idx]\n",
    "    cur_mask_other = cur_mask[:, other_idx]\n",
    "    all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    mask_x[i] = np.concatenate((cur_mask_subset, cur_mask_other), axis=1).tolist()\n",
    "print(all_x[0])\n",
    "print(mask_x[0])\n",
    "print(len(all_x[0][0]))\n",
    "logger.info(all_x[0])\n",
    "logger.info(mask_x[0])\n",
    "logger.info(len(all_x[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T04:28:59.378928Z",
     "start_time": "2021-01-30T04:28:57.102580Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:06:42,187 - INFO - 32269\n",
      "2023-08-11 16:06:42,189 - INFO - 4034\n",
      "2023-08-11 16:06:42,194 - INFO - 4033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32269\n",
      "4034\n",
      "4033\n"
     ]
    }
   ],
   "source": [
    "train_num =int( len(all_x) * 0.8) + 1\n",
    "print(train_num)\n",
    "logger.info(train_num)\n",
    "dev_num =int( len(all_x) * 0.1) + 1\n",
    "print(dev_num)\n",
    "logger.info(dev_num)\n",
    "test_num =int( len(all_x) * 0.1)\n",
    "print(test_num)\n",
    "logger.info(test_num)\n",
    "assert(train_num+dev_num+test_num == len(all_x))\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "train_names = []\n",
    "train_static = []\n",
    "train_x_len = []\n",
    "train_mask_x = []\n",
    "for idx in range(train_num):\n",
    "    train_x.append(all_x[idx])\n",
    "    train_y.append(int(all_y[idx][-1]))\n",
    "    train_names.append(all_names[idx])\n",
    "    train_static.append(static[idx])\n",
    "    train_x_len.append(all_x_len[idx])\n",
    "    train_mask_x.append(mask_x[idx])\n",
    "\n",
    "dev_x = []\n",
    "dev_y = []\n",
    "dev_names = []\n",
    "dev_static = []\n",
    "dev_x_len = []\n",
    "dev_mask_x = []\n",
    "for idx in range(train_num, train_num + dev_num):\n",
    "    dev_x.append(all_x[idx])\n",
    "    dev_y.append(int(all_y[idx][-1]))\n",
    "    dev_names.append(all_names[idx])\n",
    "    dev_static.append(static[idx])\n",
    "    dev_x_len.append(all_x_len[idx])\n",
    "    dev_mask_x.append(mask_x[idx])\n",
    "\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "test_names = []\n",
    "test_static = []\n",
    "test_x_len = []\n",
    "test_mask_x = []\n",
    "for idx in range(train_num + dev_num, train_num + dev_num + test_num):\n",
    "    test_x.append(all_x[idx])\n",
    "    test_y.append(int(all_y[idx][-1]))\n",
    "    test_names.append(all_names[idx])\n",
    "    test_static.append(static[idx])\n",
    "    test_x_len.append(all_x_len[idx])\n",
    "    test_mask_x.append(mask_x[idx])\n",
    "\n",
    "\n",
    "assert(len(train_x) == train_num)\n",
    "assert(len(dev_x) == dev_num)\n",
    "assert(len(test_x) == test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T04:29:02.456915Z",
     "start_time": "2021-01-30T04:29:02.443296Z"
    }
   },
   "outputs": [],
   "source": [
    "long_x = all_x\n",
    "long_y = [y[-1] for y in all_y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:38.783950Z",
     "start_time": "2021-02-10T15:05:38.778517Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_loss(y_pred, y_true):\n",
    "    loss = torch.nn.BCELoss()\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:39.004588Z",
     "start_time": "2021-02-10T15:05:38.999638Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_re_loss(y_pred, y_true):\n",
    "    loss = torch.nn.MSELoss()\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:39.151905Z",
     "start_time": "2021-02-10T15:05:39.147577Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_kl_loss(x_pred, x_target):\n",
    "    loss = torch.nn.KLDivLoss(reduce=True, size_average=True)\n",
    "    return loss(x_pred, x_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:39.322887Z",
     "start_time": "2021-02-10T15:05:39.313036Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wass_dist(x_pred, x_target):\n",
    "    m1 = torch.mean(x_pred, dim=0)\n",
    "    m2 = torch.mean(x_target, dim=0)\n",
    "    v1 = torch.var(x_pred, dim=0)\n",
    "    v2 = torch.var(x_target, dim=0)\n",
    "    p1 = torch.sum(torch.pow((m1 - m2), 2))\n",
    "    p2 = torch.sum(torch.pow(torch.pow(v1, 1/2) - torch.pow(v2, 1/2), 2))\n",
    "    return torch.pow(p1+p2, 1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:41.142115Z",
     "start_time": "2021-02-10T15:05:41.134517Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_sents(sents, pad_token):\n",
    "\n",
    "    sents_padded = []\n",
    "\n",
    "    max_length = max([len(_) for _ in sents])\n",
    "    for i in sents:\n",
    "        padded = list(i) + [pad_token]*(max_length-len(i))\n",
    "        sents_padded.append(np.array(padded))\n",
    "\n",
    "\n",
    "    return np.array(sents_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:41.301945Z",
     "start_time": "2021-02-10T15:05:41.287538Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_iter(x, y, mask, lens, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(x) / batch_size) # 向下取整\n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size] #  fetch out all the induces\n",
    "        \n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx], mask[idx], lens[idx]))\n",
    "       \n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    \n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "        batch_mask_x = [e[2] for e in examples]\n",
    "#         batch_name = [e[2] for e in examples]\n",
    "        batch_lens = [e[3] for e in examples]\n",
    "\n",
    "        yield batch_x, batch_y, batch_mask_x, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:41.394432Z",
     "start_time": "2021-02-10T15:05:41.386828Z"
    }
   },
   "outputs": [],
   "source": [
    "def length_to_mask(length, max_len=None, dtype=None):\n",
    "    \"\"\"length: B.\n",
    "    return B x max_len.\n",
    "    If max_len is None, then max of length will be used.\n",
    "    \"\"\"\n",
    "    assert len(length.shape) == 1, 'Length shape should be 1 dimensional.'\n",
    "    max_len = max_len or length.max().item()\n",
    "    mask = torch.arange(max_len, device=length.device,\n",
    "                        dtype=length.dtype).expand(len(length), max_len) < length.unsqueeze(1)\n",
    "    if dtype is not None:\n",
    "        mask = torch.as_tensor(mask, dtype=dtype, device=length.device)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:41.786588Z",
     "start_time": "2021-02-10T15:05:41.673200Z"
    }
   },
   "outputs": [],
   "source": [
    "class SingleAttention(nn.Module):\n",
    "    def __init__(self, attention_input_dim, attention_hidden_dim, attention_type='add', demographic_dim=12, time_aware=False, use_demographic=False):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        \n",
    "        self.attention_type = attention_type\n",
    "        self.attention_hidden_dim = attention_hidden_dim\n",
    "        self.attention_input_dim = attention_input_dim\n",
    "        self.use_demographic = use_demographic\n",
    "        self.demographic_dim = demographic_dim\n",
    "        self.time_aware = time_aware\n",
    "\n",
    "        # batch_time = torch.arange(0, batch_mask.size()[1], dtype=torch.float32).reshape(1, batch_mask.size()[1], 1)\n",
    "        # batch_time = batch_time.repeat(batch_mask.size()[0], 1, 1)\n",
    "        \n",
    "        if attention_type == 'add':\n",
    "            if self.time_aware == True:\n",
    "                # self.Wx = nn.Parameter(torch.randn(attention_input_dim+1, attention_hidden_dim))\n",
    "                self.Wx = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "                self.Wtime_aware = nn.Parameter(torch.randn(1, attention_hidden_dim))\n",
    "                nn.init.kaiming_uniform_(self.Wtime_aware, a=math.sqrt(5))\n",
    "            else:\n",
    "                self.Wx = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "            self.Wt = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "            self.Wd = nn.Parameter(torch.randn(demographic_dim, attention_hidden_dim))\n",
    "            self.bh = nn.Parameter(torch.zeros(attention_hidden_dim,))\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wd, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wx, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wt, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        elif attention_type == 'mul':\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_input_dim, attention_input_dim))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        elif attention_type == 'concat':\n",
    "            if self.time_aware == True:\n",
    "                self.Wh = nn.Parameter(torch.randn(2*attention_input_dim+1, attention_hidden_dim))\n",
    "            else:\n",
    "                self.Wh = nn.Parameter(torch.randn(2*attention_input_dim, attention_hidden_dim))\n",
    "\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wh, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        else:\n",
    "            raise RuntimeError('Wrong attention type.')\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, input, demo=None):\n",
    " \n",
    "        batch_size, time_step, input_dim = input.size() # batch_size * time_step * hidden_dim(i)\n",
    "        time_decays = torch.tensor(range(time_step-1,-1,-1), dtype=torch.float32).unsqueeze(-1).unsqueeze(0).to(device)# 1*t*1\n",
    "        b_time_decays = time_decays.repeat(batch_size,1,1)# b t 1\n",
    "        \n",
    "        if self.attention_type == 'add': #B*T*I  @ H*I\n",
    "            q = torch.matmul(input[:,-1,:], self.Wt)# b h\n",
    "            q = torch.reshape(q, (batch_size, 1, self.attention_hidden_dim)) #B*1*H\n",
    "            if self.time_aware == True:\n",
    "                # k_input = torch.cat((input, time), dim=-1)\n",
    "                k = torch.matmul(input, self.Wx)#b t h\n",
    "                # k = torch.reshape(k, (batch_size, 1, time_step, self.attention_hidden_dim)) #B*1*T*H\n",
    "                time_hidden = torch.matmul(b_time_decays, self.Wtime_aware)#  b t h\n",
    "            else:\n",
    "                k = torch.matmul(input, self.Wx)# b t h\n",
    "                # k = torch.reshape(k, (batch_size, 1, time_step, self.attention_hidden_dim)) #B*1*T*H\n",
    "            if self.use_demographic == True:\n",
    "                d = torch.matmul(demo, self.Wd) #B*H\n",
    "                d = torch.reshape(d, (batch_size, 1, self.attention_hidden_dim)) # b 1 h\n",
    "            h = q + k + self.bh # b t h\n",
    "            if self.time_aware == True:\n",
    "                h += time_hidden\n",
    "            h = self.tanh(h) #B*T*H\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step))# b t\n",
    "        elif self.attention_type == 'mul':\n",
    "            e = torch.matmul(input[:,-1,:], self.Wa)#b i\n",
    "            e = torch.matmul(e.unsqueeze(1), input.permute(0,2,1)).squeeze() + self.ba #b t\n",
    "        elif self.attention_type == 'concat':\n",
    "            q = input[:,-1,:].unsqueeze(1).repeat(1,time_step,1)# b t i\n",
    "            k = input\n",
    "            c = torch.cat((q, k), dim=-1) #B*T*2I\n",
    "            if self.time_aware == True:\n",
    "                c = torch.cat((c, b_time_decays), dim=-1) #B*T*2I+1\n",
    "            h = torch.matmul(c, self.Wh)\n",
    "            h = self.tanh(h)\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step)) # b t \n",
    "        \n",
    "        a = self.softmax(e) #B*T\n",
    "        v = torch.matmul(a.unsqueeze(1), input).squeeze() #B*I\n",
    "\n",
    "        return v, a\n",
    "\n",
    "class FinalAttentionQKV(nn.Module):\n",
    "    def __init__(self, attention_input_dim, attention_hidden_dim, attention_type='add', dropout=None):\n",
    "        super(FinalAttentionQKV, self).__init__()\n",
    "        \n",
    "        self.attention_type = attention_type\n",
    "        self.attention_hidden_dim = attention_hidden_dim\n",
    "        self.attention_input_dim = attention_input_dim\n",
    "\n",
    "\n",
    "        self.W_q = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "        self.W_k = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "        self.W_v = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "\n",
    "        self.W_out = nn.Linear(attention_hidden_dim, 1)\n",
    "\n",
    "        self.b_in = nn.Parameter(torch.zeros(1,))\n",
    "        self.b_out = nn.Parameter(torch.zeros(1,))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.W_q.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_k.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_v.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_out.weight, a=math.sqrt(5))\n",
    "\n",
    "        self.Wh = nn.Parameter(torch.randn(2*attention_input_dim, attention_hidden_dim))\n",
    "        self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "        self.ba = nn.Parameter(torch.zeros(1,))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.Wh, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input):\n",
    " \n",
    "        batch_size, time_step, input_dim = input.size() # batch_size * input_dim + 1 * hidden_dim(i)\n",
    "        input_q = self.W_q(torch.mean(input, dim=1)) # b h\n",
    "        input_k = self.W_k(input)# b t h\n",
    "        input_v = self.W_v(input)# b t h\n",
    "\n",
    "        if self.attention_type == 'add': #B*T*I  @ H*I\n",
    "\n",
    "            q = torch.reshape(input_q, (batch_size, 1, self.attention_hidden_dim)) #B*1*H\n",
    "            h = q + input_k + self.b_in # b t h\n",
    "            h = self.tanh(h) #B*T*H\n",
    "            e = self.W_out(h) # b t 1\n",
    "            e = torch.reshape(e, (batch_size, time_step))# b t\n",
    "\n",
    "        elif self.attention_type == 'mul':\n",
    "            q = torch.reshape(input_q, (batch_size, self.attention_hidden_dim, 1)) #B*h 1\n",
    "            e = torch.matmul(input_k, q).squeeze()#b t\n",
    "            \n",
    "        elif self.attention_type == 'concat':\n",
    "            q = input_q.unsqueeze(1).repeat(1,time_step,1)# b t h\n",
    "            k = input_k\n",
    "            c = torch.cat((q, k), dim=-1) #B*T*2I\n",
    "            h = torch.matmul(c, self.Wh)\n",
    "            h = self.tanh(h)\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step)) # b t \n",
    "        \n",
    "        a = self.softmax(e) #B*T\n",
    "        if self.dropout is not None:\n",
    "            a = self.dropout(a)\n",
    "        v = torch.matmul(a.unsqueeze(1), input_v).squeeze() #B*I\n",
    "\n",
    "        return v, a\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def tile(a, dim, n_tile):\n",
    "    init_dim = a.size(dim)\n",
    "    repeat_idx = [1] * a.dim()\n",
    "    repeat_idx[dim] = n_tile\n",
    "    a = a.repeat(*(repeat_idx))\n",
    "    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).to(device)\n",
    "    return torch.index_select(a, dim, order_index).to(device)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module): # new added\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x)))), None\n",
    "\n",
    "    \n",
    "class PositionalEncoding(nn.Module): # new added / not use anymore\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=400):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0 \n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)# b h t d_k\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k) # b h t t\n",
    "    if mask is not None:# 1 1 t t\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)# b h t t \n",
    "    p_attn = F.softmax(scores, dim = -1)# b h t t\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn # b h t v (d_k) \n",
    "    \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, self.d_k * self.h), 3)\n",
    "        self.final_linear = nn.Linear(d_model, d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1) # 1 1 t t\n",
    "\n",
    "        nbatches = query.size(0)# b\n",
    "        input_dim = query.size(1)# i+1\n",
    "        feature_dim = query.size(-1)# i+1\n",
    "\n",
    "        #input size -> # batch_size * d_input * hidden_dim\n",
    "        \n",
    "        # d_model => h * d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))] # b num_head d_input d_k\n",
    "        \n",
    "       \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)# b num_head d_input d_v (d_k) \n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)# batch_size * d_input * hidden_dim\n",
    "\n",
    "        #DeCov \n",
    "        DeCov_contexts = x.transpose(0, 1).transpose(1, 2) # I+1 H B\n",
    "#         print(DeCov_contexts.shape)\n",
    "        Covs = cov(DeCov_contexts[0,:,:])\n",
    "        DeCov_loss = 0.5 * (torch.norm(Covs, p = 'fro')**2 - torch.norm(torch.diag(Covs))**2 ) \n",
    "        for i in range(11 -1):\n",
    "            Covs = cov(DeCov_contexts[i+1,:,:])\n",
    "            DeCov_loss += 0.5 * (torch.norm(Covs, p = 'fro')**2 - torch.norm(torch.diag(Covs))**2 ) \n",
    "\n",
    "\n",
    "        return self.final_linear(x), DeCov_loss\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-7):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "def cov(m, y=None):\n",
    "    if y is not None:\n",
    "        m = torch.cat((m, y), dim=0)\n",
    "    m_exp = torch.mean(m, dim=1)\n",
    "    x = m - m_exp[:, None]\n",
    "    cov = 1 / (x.size(1) - 1) * x.mm(x.t())\n",
    "    return cov\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        returned_value = sublayer(self.norm(x))\n",
    "        return x + self.dropout(returned_value[0]) , returned_value[1]\n",
    "\n",
    "class distcare_teacher(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.6):\n",
    "        super(distcare_teacher, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 400)\n",
    "\n",
    "        self.GRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        self.LastStepAttentions = clones(SingleAttention(self.hidden_dim, 8, attention_type='concat', demographic_dim=12, time_aware=True, use_demographic=False),self.input_dim)\n",
    "        \n",
    "        self.FinalAttentionQKV = FinalAttentionQKV(self.hidden_dim, self.hidden_dim, attention_type='mul',dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model,dropout = 1 - self.keep_prob)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.output = nn.Linear(self.input_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, input, lens):\n",
    "        lens = lens.to('cpu')\n",
    "        # input shape [batch_size, timestep, feature_dim]\n",
    "#         demo_main = self.tanh(self.demo_proj_main(demo_input)).unsqueeze(1)# b hidden_dim\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "\n",
    "        \n",
    "        GRU_embeded_input = self.GRUs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "#         print(GRU_embeded_input.shape)\n",
    "        for i in range(feature_dim-1):\n",
    "            embeded_input = self.GRUs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "            GRU_embeded_input = torch.cat((GRU_embeded_input, embeded_input), 1)\n",
    "\n",
    "#         GRU_embeded_input = torch.cat((GRU_embeded_input, demo_main), 1)# b i+1 h\n",
    "        posi_input = self.dropout(GRU_embeded_input) # batch_size * d_input * hidden_dim\n",
    "\n",
    "#         #mask = subsequent_mask(time_step).to(device) # 1 t t \n",
    "#         contexts = self.SublayerConnection(posi_input, lambda x: self.MultiHeadedAttention(posi_input, posi_input, posi_input, None))# # batch_size * d_input * hidden_dim\n",
    "    \n",
    "#         DeCov_loss = contexts[1]\n",
    "#         contexts = contexts[0]\n",
    "\n",
    "#         contexts = self.SublayerConnection(contexts, lambda x: self.PositionwiseFeedForward(contexts))[0]# # batch_size * d_input * hidden_dim\n",
    "#         #contexts = contexts.view(batch_size, feature_dim * self.hidden_dim)#\n",
    "#         # contexts = torch.matmul(self.Wproj, contexts) + self.bproj\n",
    "#         # contexts = contexts.squeeze()\n",
    "#         # demo_key = self.demo_proj(demo_input)# b hidden_dim\n",
    "#         # demo_key = self.relu(demo_key)\n",
    "#         # input_dim_scores = torch.matmul(contexts, demo_key.unsqueeze(-1)).squeeze() # b i\n",
    "#         # input_dim_scores = self.dropout(self.sigmoid(input_dim_scores)).unsqueeze(1)# b i\n",
    "        \n",
    "#         # weighted_contexts = torch.matmul(input_dim_scores, contexts).squeeze()\n",
    "# #         print(contexts.shape)\n",
    "\n",
    "#         weighted_contexts = self.FinalAttentionQKV(contexts)[0]\n",
    "        contexts = self.Linear(posi_input).squeeze()# b i\n",
    "        output = self.output(self.dropout(contexts))# b 1\n",
    "        output = self.sigmoid(output)\n",
    "#         print(weighted_contexts.shape)\n",
    "          \n",
    "        return output, None, contexts\n",
    "    #, self.MultiHeadedAttention.attn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:51.991644Z",
     "start_time": "2021-02-10T15:05:42.504740Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED) #numpy\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED) # cpu\n",
    "torch.cuda.manual_seed(RANDOM_SEED) #gpu\n",
    "torch.backends.cudnn.deterministic=True # cudnn\n",
    "    \n",
    "epochs = 150\n",
    "batch_size = 256\n",
    "input_dim = 34\n",
    "hidden_dim = 32\n",
    "d_model = 32\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1\n",
    "\n",
    "model = distcare_teacher(input_dim = input_dim, hidden_dim = hidden_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, output_dim = output_dim).to(device)\n",
    "# input_dim, d_model, d_k, d_v, MHD_num_head, d_ff, output_dim\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T09:16:04.789735Z",
     "start_time": "2021-01-30T04:30:23.650979Z"
    },
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Teacher\n",
    "# If you don't want to train Teacher Model:\n",
    "# - The pretrained taecher model is in direcrtory './model/', and can be directly loaded, \n",
    "# - Simply skip this cell and load the model to validate on Dev Dataset.\n",
    "# logger.info('Training Teacher')\n",
    "\n",
    "# total_train_loss = []\n",
    "# total_valid_loss = []\n",
    "# global_best = 0\n",
    "# auroc = []\n",
    "# auprc = []\n",
    "# minpse = []\n",
    "# history = []\n",
    "\n",
    "# pad_token = np.zeros(input_dim)\n",
    "# # begin_time = time.time()\n",
    "# best_auroc = 0\n",
    "# best_auprc = 0\n",
    "# best_minpse = 0\n",
    "    \n",
    "# if target_dataset == 'TJ':    \n",
    "#     file_name = './model/pretrained-challenge-front-fill-teacher-2covid'\n",
    "# elif target_dataset == 'HM':\n",
    "#     file_name = './model/pretrained-challenge-front-fill-teacher-2spain'\n",
    "\n",
    "# for each_epoch in range(epochs):\n",
    "\n",
    "#     epoch_loss = []\n",
    "#     counter_batch = 0\n",
    "#     model.train()  \n",
    "\n",
    "#     for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(train_x, train_y, train_mask_x, train_x_len, batch_size, shuffle=True)):  \n",
    "#         optimizer.zero_grad()\n",
    "#         batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "#         batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "#         batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#         batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "# #        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "#         opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "\n",
    "#         BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1)) # b t 1\n",
    "# #             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "#         loss = BCE_Loss #+ 1000 * decov_loss\n",
    "\n",
    "#         epoch_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if step % 20 == 0:\n",
    "#             print('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "#             logger.info('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "\n",
    "#     epoch_loss = np.mean(epoch_loss)\n",
    "#     total_train_loss.append(epoch_loss)\n",
    "\n",
    "#     #Validation\n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         valid_loss = []\n",
    "#         valid_true = []\n",
    "#         valid_pred = []\n",
    "#         for batch_x, batch_y, batch_mask_x, batch_lens in batch_iter(dev_x, dev_y, dev_mask_x, dev_x_len, batch_size):\n",
    "#             batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "#             batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "#             batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#             batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "# #            masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "\n",
    "#             opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "\n",
    "#             BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "# #                 REC_Loss = F.mse_loss(recon, batch_x, reduction='mean').to(device)\n",
    "\n",
    "#             valid_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "\n",
    "#             y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "#             y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "#         valid_loss = np.mean(valid_loss)\n",
    "#         total_valid_loss.append(valid_loss)\n",
    "#         ret = metrics.print_metrics_binary(y_true, y_pred,verbose = 0)\n",
    "#         history.append(ret)\n",
    "#         #print()\n",
    "\n",
    "#         print('Epoch %d: Loss = %.4f Valid loss = %.4f roc = %.4f'%(each_epoch, total_train_loss[-1], total_valid_loss[-1], ret['auroc']))\n",
    "#         logger.info('Epoch %d: Loss = %.4f Valid loss = %.4f roc = %.4f'%(each_epoch, total_train_loss[-1], total_valid_loss[-1], ret['auroc']))\n",
    "#         metrics.print_metrics_binary(y_true, y_pred)\n",
    "\n",
    "#         cur_auroc = ret['auroc']\n",
    "#         if cur_auroc > best_auroc:\n",
    "#             best_auroc = cur_auroc\n",
    "#             best_auprc = ret['auprc']\n",
    "#             best_minpse = ret['minpse']\n",
    "#             state = {\n",
    "#                 'net': model.state_dict(),\n",
    "#                 'optimizer': optimizer.state_dict(),\n",
    "#                 'epoch': each_epoch\n",
    "#             }\n",
    "#             torch.save(state, file_name)\n",
    "#             print('------------ Save best model - AUROC: %.4f ------------'%cur_auroc)       \n",
    "\n",
    "# print('auroc %.4f'%(best_auroc))\n",
    "# print('auprc %.4f'%(best_auprc))\n",
    "# print('minpse %.4f'%(best_minpse))  \n",
    "# logger.info('auroc %.4f'%(best_auroc))\n",
    "# logger.info('auprc %.4f'%(best_auprc))\n",
    "# logger.info('minpse %.4f'%(best_minpse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T10:41:54.613732Z",
     "start_time": "2021-01-30T10:41:54.090698Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:06:52,310 - INFO - last saved model is in epoch 115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last saved model is in epoch 115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "distcare_teacher(\n",
       "  (PositionalEncoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (GRUs): ModuleList(\n",
       "    (0): GRU(1, 32, batch_first=True)\n",
       "    (1): GRU(1, 32, batch_first=True)\n",
       "    (2): GRU(1, 32, batch_first=True)\n",
       "    (3): GRU(1, 32, batch_first=True)\n",
       "    (4): GRU(1, 32, batch_first=True)\n",
       "    (5): GRU(1, 32, batch_first=True)\n",
       "    (6): GRU(1, 32, batch_first=True)\n",
       "    (7): GRU(1, 32, batch_first=True)\n",
       "    (8): GRU(1, 32, batch_first=True)\n",
       "    (9): GRU(1, 32, batch_first=True)\n",
       "    (10): GRU(1, 32, batch_first=True)\n",
       "    (11): GRU(1, 32, batch_first=True)\n",
       "    (12): GRU(1, 32, batch_first=True)\n",
       "    (13): GRU(1, 32, batch_first=True)\n",
       "    (14): GRU(1, 32, batch_first=True)\n",
       "    (15): GRU(1, 32, batch_first=True)\n",
       "    (16): GRU(1, 32, batch_first=True)\n",
       "    (17): GRU(1, 32, batch_first=True)\n",
       "    (18): GRU(1, 32, batch_first=True)\n",
       "    (19): GRU(1, 32, batch_first=True)\n",
       "    (20): GRU(1, 32, batch_first=True)\n",
       "    (21): GRU(1, 32, batch_first=True)\n",
       "    (22): GRU(1, 32, batch_first=True)\n",
       "    (23): GRU(1, 32, batch_first=True)\n",
       "    (24): GRU(1, 32, batch_first=True)\n",
       "    (25): GRU(1, 32, batch_first=True)\n",
       "    (26): GRU(1, 32, batch_first=True)\n",
       "    (27): GRU(1, 32, batch_first=True)\n",
       "    (28): GRU(1, 32, batch_first=True)\n",
       "    (29): GRU(1, 32, batch_first=True)\n",
       "    (30): GRU(1, 32, batch_first=True)\n",
       "    (31): GRU(1, 32, batch_first=True)\n",
       "    (32): GRU(1, 32, batch_first=True)\n",
       "    (33): GRU(1, 32, batch_first=True)\n",
       "  )\n",
       "  (LastStepAttentions): ModuleList(\n",
       "    (0): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (1): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (2): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (3): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (4): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (5): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (6): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (7): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (8): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (9): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (10): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (11): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (12): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (13): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (14): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (15): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (16): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (17): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (18): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (19): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (20): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (21): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (22): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (23): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (24): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (25): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (26): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (27): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (28): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (29): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (30): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (31): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (32): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (33): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "  )\n",
       "  (FinalAttentionQKV): FinalAttentionQKV(\n",
       "    (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_k): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_v): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_out): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "    (tanh): Tanh()\n",
       "    (softmax): Softmax(dim=None)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (MultiHeadedAttention): MultiHeadedAttention(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (final_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (SublayerConnection): SublayerConnection(\n",
       "    (norm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (PositionwiseFeedForward): PositionwiseFeedForward(\n",
       "    (w_1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (w_2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (demo_proj_main): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (demo_proj): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (Linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (output): Linear(in_features=34, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (tanh): Tanh()\n",
       "  (softmax): Softmax(dim=None)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if target_dataset == 'PD':    \n",
    "    file_name = './model/pretrained-challenge-front-fill-teacher-2pd'\n",
    "    \n",
    "checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu') )\n",
    "save_epoch = checkpoint['epoch']\n",
    "print(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "logger.info(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T10:42:04.714142Z",
     "start_time": "2021-01-30T10:41:56.611967Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:06:53,365 - INFO - Batch 0: Test Loss = 0.0931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Test Loss = 0.0931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:06:59,134 - INFO - \n",
      "==>Predicting on test\n",
      "2023-08-11 16:06:59,137 - INFO - Test Loss = 0.1227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>Predicting on test\n",
      "Test Loss = 0.1227\n",
      "confusion matrix:\n",
      "[[3712   24]\n",
      " [ 133  164]]\n",
      "accuracy = 0.9610711336135864\n",
      "precision class 0 = 0.9654096364974976\n",
      "precision class 1 = 0.8723404407501221\n",
      "recall class 0 = 0.9935759902000427\n",
      "recall class 1 = 0.5521885752677917\n",
      "AUC of ROC = 0.9417155134499888\n",
      "AUC of PRC = 0.7497692675220317\n",
      "min(+P, Se) = 0.7104377104377104\n",
      "f1_score = 0.6762886533793953\n"
     ]
    }
   ],
   "source": [
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "pad_token = np.zeros(34)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(test_x, test_y, test_mask_x, test_x_len, batch_size, shuffle=True)):  \n",
    "        optimizer.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "        opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "\n",
    "        loss = model_loss\n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "            logger.info('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "        y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "        y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "logger.info(\"\\n==>Predicting on test\")\n",
    "logger.info('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "test_res = metrics.print_metrics_binary(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:02.685007Z",
     "start_time": "2021-02-10T15:06:02.655919Z"
    }
   },
   "outputs": [],
   "source": [
    "class distcare_student(nn.Module):\n",
    "    def __init__(self, input_dim, input_diff_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(distcare_student, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        self.input_diff_dim = input_diff_dim\n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 400)\n",
    "\n",
    "        self.GRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        self.generalGRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_diff_dim)\n",
    "        self.LastStepAttentions = clones(SingleAttention(self.hidden_dim, 8, attention_type='concat', demographic_dim=12, time_aware=True, use_demographic=False),self.input_dim)\n",
    "        \n",
    "        self.FinalAttentionQKV = FinalAttentionQKV(self.hidden_dim, self.hidden_dim, attention_type='mul',dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model,dropout = 1 - self.keep_prob)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.output = nn.Linear(self.input_dim + self.input_diff_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.FC_embed = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.to_MMD = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    def forward(self, input, input_diff, lens, tar, train):\n",
    "        lens = lens.to('cpu')\n",
    "        tar_x, tar_lens = tar\n",
    "        # input shape [batch_size, timestep, feature_dim]\n",
    "#         demo_main = self.tanh(self.demo_proj_main(demo_input)).unsqueeze(1)# b hidden_dim\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        feature_dim_diff = input_diff.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "\n",
    "        # Initialization\n",
    "        #cur_hs = Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0))\n",
    "\n",
    "        # forward\n",
    "        # GRU_embeded_input = self.GRUs[0](input[:,:,0].unsqueeze(-1), Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device))[0] # b t h\n",
    "        # Attention_embeded_input = self.LastStepAttentions[0](GRU_embeded_input)[0].unsqueeze(1)# b 1 h\n",
    "        # for i in range(feature_dim-1):\n",
    "        #     embeded_input = self.GRUs[i+1](input[:,:,i+1].unsqueeze(-1), Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device))[0] # b 1 h\n",
    "        #     embeded_input = self.LastStepAttentions[i+1](embeded_input)[0].unsqueeze(1)# b 1 h\n",
    "        #     Attention_embeded_input = torch.cat((Attention_embeded_input, embeded_input), 1)# b i h\n",
    "\n",
    "        # Attention_embeded_input = torch.cat((Attention_embeded_input, demo_main), 1)# b i+1 h\n",
    "        # posi_input = self.dropout(Attention_embeded_input) # batch_size * d_input+1 * hidden_dim\n",
    "\n",
    "#         input = pack_padded_sequence(input, lens, batch_first=True)\n",
    "        \n",
    "        GRU_embeded_input = self.GRUs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "#         print(GRU_embeded_input.shape)\n",
    "        for i in range(feature_dim-1):\n",
    "            embeded_input = self.GRUs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "            GRU_embeded_input = torch.cat((GRU_embeded_input, embeded_input), 1)\n",
    "\n",
    "#         GRU_embeded_input = torch.cat((GRU_embeded_input, demo_main), 1)# b i+1 h\n",
    "        General_GRU_embeded_input = self.generalGRUs[0](pack_padded_sequence(input_diff[:,:,0].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "        for i in range(feature_dim_diff - 1):\n",
    "            general_embeded_input = self.generalGRUs[i + 1](pack_padded_sequence(input_diff[:,:,i].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "            General_GRU_embeded_input = torch.cat((General_GRU_embeded_input,general_embeded_input), 1)\n",
    "        \n",
    "#         posi_input = self.dropout(GRU_embeded_input) # batch_size * d_input * hidden_dim\n",
    "        posi_input = self.dropout(torch.cat((GRU_embeded_input, General_GRU_embeded_input), 1)) # batch_size * d_input * hidden_dim\n",
    "    \n",
    "        # cul tar loss\n",
    "        if train:\n",
    "            tar_lens = tar_lens.to('cpu')\n",
    "            GRU_tar_input = self.GRUs[0](pack_padded_sequence(tar_x[:,:,0].unsqueeze(-1), tar_lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "            for i in range(feature_dim-1):\n",
    "                tar_input = self.GRUs[i+1](pack_padded_sequence(tar_x[:,:,i+1].unsqueeze(-1), tar_lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "                GRU_tar_input = torch.cat((GRU_tar_input, tar_input), 1)\n",
    "            GRU_embeded_output = self.to_MMD(GRU_embeded_input)\n",
    "            GRU_tar_output = self.to_MMD(GRU_tar_input)\n",
    "        else:\n",
    "            GRU_embeded_output = []\n",
    "            GRU_tar_output = []\n",
    "        \n",
    "        #mask = subsequent_mask(time_step).to(device) # 1 t t 下三角 N to 1任务不用mask\n",
    "        # contexts = self.SublayerConnection(posi_input, lambda x: self.MultiHeadedAttention(posi_input, posi_input, posi_input, None))# # batch_size * d_input * hidden_dim\n",
    "    \n",
    "        # DeCov_loss = contexts[1]\n",
    "        # contexts = contexts[0]\n",
    "\n",
    "        # contexts = self.SublayerConnection(contexts, lambda x: self.PositionwiseFeedForward(contexts))[0]# # batch_size * d_input * hidden_dim\n",
    "        #contexts = contexts.view(batch_size, feature_dim * self.hidden_dim)#\n",
    "        # contexts = torch.matmul(self.Wproj, contexts) + self.bproj\n",
    "        # contexts = contexts.squeeze()\n",
    "        # demo_key = self.demo_proj(demo_input)# b hidden_dim\n",
    "        # demo_key = self.relu(demo_key)\n",
    "        # input_dim_scores = torch.matmul(contexts, demo_key.unsqueeze(-1)).squeeze() # b i\n",
    "        # input_dim_scores = self.dropout(self.sigmoid(input_dim_scores)).unsqueeze(1)# b i\n",
    "        \n",
    "        # weighted_contexts = torch.matmul(input_dim_scores, contexts).squeeze()\n",
    "#         print(contexts.shape)\n",
    "\n",
    "        # weighted_contexts = self.FinalAttentionQKV(contexts)[0]\n",
    "        # output_embed = self.FC_embed(weighted_contexts)\n",
    "        # output = self.output(self.dropout(weighted_contexts))# b 1\n",
    "        # output = self.sigmoid(output)\n",
    "#         print(weighted_contexts.shape)\n",
    "        contexts = self.Linear(posi_input).squeeze()# b i\n",
    "        output = self.output(self.dropout(contexts))# b 1\n",
    "        output = self.sigmoid(output)\n",
    "          \n",
    "        return output, None, contexts, [GRU_embeded_output, GRU_tar_output]\n",
    "    #, self.MultiHeadedAttention.attn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:06:59,197 - INFO - load target data\n",
      "2023-08-11 16:06:59,214 - INFO - [[-0.84276482  0.37440202  0.67961237 -1.39897541 -0.48314192 -0.21203008\n",
      "   1.58875966  0.79457896 -0.86126933 -0.48197292 -0.67452243  0.71372084\n",
      "  -1.44708995 -0.77101282 -1.42318156 -0.58514053]\n",
      " [-0.84276482  0.37440202  0.67961237 -1.39897541 -0.48314192 -0.21203008\n",
      "   1.58875966  0.79457896 -0.86126933 -0.48197292 -0.67452243  0.71372084\n",
      "  -1.44708995 -0.77101282 -1.42318156 -0.58514053]\n",
      " [-0.53383302  0.43115698  0.89646606 -1.34242175 -0.69460513 -0.21203008\n",
      "   0.89527972  0.03870413 -0.86126933  0.07263348 -0.50511021 -0.28657272\n",
      "   0.39888266 -1.37943777 -0.82370897 -0.92211195]\n",
      " [-0.71138003  0.09062723  0.55763217 -1.39897541 -0.8637757  -0.21203008\n",
      "   0.46852283  0.33565496 -0.86126933 -0.57440732 -0.50142734 -0.17115423\n",
      "  -0.25396131 -0.77101282 -0.67384083 -2.43848334]\n",
      " [-0.71138003  0.09062723  0.55763217 -1.39897541 -0.8637757  -0.21203008\n",
      "   0.46852283  0.33565496 -0.86126933 -0.57440732 -0.50142734 -0.17115423\n",
      "  -0.25396131  0.64864541  0.0754999  -0.58514053]\n",
      " [-0.51252738  0.60142185  0.50341875 -1.45552908 -0.52543456 -0.41638034\n",
      "   0.7885905   0.2816639  -0.58901728  0.59026612 -0.34306374 -0.74824667\n",
      "  -0.18642573  0.64864541  0.0754999  -0.58514053]\n",
      " [-0.46281422 -0.24990251 -0.37754938 -1.11620709 -1.17392174  0.19667043\n",
      "   0.68190128  0.4706326  -0.60716742 -0.14920908 -0.335698   -0.44046404\n",
      "  -1.2444832   0.39513501 -0.52397268 -0.83786909]\n",
      " [-0.46281422 -0.24990251 -0.37754938 -1.11620709 -1.17392174  0.19667043\n",
      "   0.68190128  0.4706326  -0.60716742 -0.14920908 -0.335698   -0.44046404\n",
      "  -1.2444832   0.69934749 -0.67384083 -0.92211195]\n",
      " [-0.64391217  0.03387228 -0.17424904 -1.37069858 -0.55362966  0.40102068\n",
      "   1.24201969  0.4706326  -0.60716742 -0.87019741 -0.4351356   0.02120991\n",
      "  -1.06438832  0.69934749 -0.67384083 -0.92211195]\n",
      " [-0.64391217  0.03387228 -0.17424904 -1.37069858 -0.55362966  0.40102068\n",
      "   1.24201969  0.4706326  -0.60716742 -0.87019741 -0.4351356   0.02120991\n",
      "  -1.06438832  1.05426204 -0.67384083 -0.0796834 ]\n",
      " [-1.03806653  0.26089211  0.16458485 -1.45552908 -1.6391408   0.19667043\n",
      "   1.24201969  0.4706326  -0.66161783 -0.5004598  -0.46091572  0.02120991\n",
      "  -1.17694762  0.09092253 -1.34824748 -1.25908337]\n",
      " [-0.89957987  0.48791194  0.15103149 -1.41311383 -0.65231249  0.19667043\n",
      "   0.14845517  0.4706326  -0.75236851 -0.94414493 -0.950738    0.17510123\n",
      "  -1.04187645  0.54724125 -0.67384083 -1.0063548 ]\n",
      " [-0.5764443   0.31764706 -0.12003562 -1.3000065  -0.63821494  0.19667043\n",
      "   0.86860741  0.4706326  -0.80681892 -0.24164348 -1.00229824 -4.7109481\n",
      "   6.99485797  0.54724125 -0.67384083 -1.0063548 ]\n",
      " [-0.5764443   0.31764706 -0.12003562 -1.3000065  -0.63821494  0.19667043\n",
      "   0.86860741  0.4706326  -0.80681892 -0.24164348 -1.00229824 -4.7109481\n",
      "   6.99485797  0.24302877  0.82484063 -1.42756908]\n",
      " [-0.2320031  -0.64718721 -0.41820945 -1.15862233 -1.32899476 -0.21203008\n",
      "   0.25514439  1.1995119  -0.75236851 -0.33407788 -0.93600651 -0.09420857\n",
      "  -0.09637829  0.24302877  0.82484063 -1.42756908]\n",
      " [-0.2320031  -0.64718721 -0.41820945 -1.15862233 -1.32899476 -0.21203008\n",
      "   0.25514439  1.1995119  -0.75236851 -0.33407788 -0.93600651 -0.09420857\n",
      "  -0.09637829 -1.07522529 -1.04851119  2.27911655]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -1.07522529 -1.04851119  2.27911655]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -0.26399202 -0.67384083  2.27911655]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -0.77101282 -0.67384083 -0.66938338]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -0.77101282 -0.67384083  0.08880231]]\n",
      "2023-08-11 16:06:59,220 - INFO - 16\n",
      "2023-08-11 16:06:59,220 - INFO - 325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.84276482  0.37440202  0.67961237 -1.39897541 -0.48314192 -0.21203008\n",
      "   1.58875966  0.79457896 -0.86126933 -0.48197292 -0.67452243  0.71372084\n",
      "  -1.44708995 -0.77101282 -1.42318156 -0.58514053]\n",
      " [-0.84276482  0.37440202  0.67961237 -1.39897541 -0.48314192 -0.21203008\n",
      "   1.58875966  0.79457896 -0.86126933 -0.48197292 -0.67452243  0.71372084\n",
      "  -1.44708995 -0.77101282 -1.42318156 -0.58514053]\n",
      " [-0.53383302  0.43115698  0.89646606 -1.34242175 -0.69460513 -0.21203008\n",
      "   0.89527972  0.03870413 -0.86126933  0.07263348 -0.50511021 -0.28657272\n",
      "   0.39888266 -1.37943777 -0.82370897 -0.92211195]\n",
      " [-0.71138003  0.09062723  0.55763217 -1.39897541 -0.8637757  -0.21203008\n",
      "   0.46852283  0.33565496 -0.86126933 -0.57440732 -0.50142734 -0.17115423\n",
      "  -0.25396131 -0.77101282 -0.67384083 -2.43848334]\n",
      " [-0.71138003  0.09062723  0.55763217 -1.39897541 -0.8637757  -0.21203008\n",
      "   0.46852283  0.33565496 -0.86126933 -0.57440732 -0.50142734 -0.17115423\n",
      "  -0.25396131  0.64864541  0.0754999  -0.58514053]\n",
      " [-0.51252738  0.60142185  0.50341875 -1.45552908 -0.52543456 -0.41638034\n",
      "   0.7885905   0.2816639  -0.58901728  0.59026612 -0.34306374 -0.74824667\n",
      "  -0.18642573  0.64864541  0.0754999  -0.58514053]\n",
      " [-0.46281422 -0.24990251 -0.37754938 -1.11620709 -1.17392174  0.19667043\n",
      "   0.68190128  0.4706326  -0.60716742 -0.14920908 -0.335698   -0.44046404\n",
      "  -1.2444832   0.39513501 -0.52397268 -0.83786909]\n",
      " [-0.46281422 -0.24990251 -0.37754938 -1.11620709 -1.17392174  0.19667043\n",
      "   0.68190128  0.4706326  -0.60716742 -0.14920908 -0.335698   -0.44046404\n",
      "  -1.2444832   0.69934749 -0.67384083 -0.92211195]\n",
      " [-0.64391217  0.03387228 -0.17424904 -1.37069858 -0.55362966  0.40102068\n",
      "   1.24201969  0.4706326  -0.60716742 -0.87019741 -0.4351356   0.02120991\n",
      "  -1.06438832  0.69934749 -0.67384083 -0.92211195]\n",
      " [-0.64391217  0.03387228 -0.17424904 -1.37069858 -0.55362966  0.40102068\n",
      "   1.24201969  0.4706326  -0.60716742 -0.87019741 -0.4351356   0.02120991\n",
      "  -1.06438832  1.05426204 -0.67384083 -0.0796834 ]\n",
      " [-1.03806653  0.26089211  0.16458485 -1.45552908 -1.6391408   0.19667043\n",
      "   1.24201969  0.4706326  -0.66161783 -0.5004598  -0.46091572  0.02120991\n",
      "  -1.17694762  0.09092253 -1.34824748 -1.25908337]\n",
      " [-0.89957987  0.48791194  0.15103149 -1.41311383 -0.65231249  0.19667043\n",
      "   0.14845517  0.4706326  -0.75236851 -0.94414493 -0.950738    0.17510123\n",
      "  -1.04187645  0.54724125 -0.67384083 -1.0063548 ]\n",
      " [-0.5764443   0.31764706 -0.12003562 -1.3000065  -0.63821494  0.19667043\n",
      "   0.86860741  0.4706326  -0.80681892 -0.24164348 -1.00229824 -4.7109481\n",
      "   6.99485797  0.54724125 -0.67384083 -1.0063548 ]\n",
      " [-0.5764443   0.31764706 -0.12003562 -1.3000065  -0.63821494  0.19667043\n",
      "   0.86860741  0.4706326  -0.80681892 -0.24164348 -1.00229824 -4.7109481\n",
      "   6.99485797  0.24302877  0.82484063 -1.42756908]\n",
      " [-0.2320031  -0.64718721 -0.41820945 -1.15862233 -1.32899476 -0.21203008\n",
      "   0.25514439  1.1995119  -0.75236851 -0.33407788 -0.93600651 -0.09420857\n",
      "  -0.09637829  0.24302877  0.82484063 -1.42756908]\n",
      " [-0.2320031  -0.64718721 -0.41820945 -1.15862233 -1.32899476 -0.21203008\n",
      "   0.25514439  1.1995119  -0.75236851 -0.33407788 -0.93600651 -0.09420857\n",
      "  -0.09637829 -1.07522529 -1.04851119  2.27911655]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -1.07522529 -1.04851119  2.27911655]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -0.26399202 -0.67384083  2.27911655]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -0.77101282 -0.67384083 -0.66938338]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -0.77101282 -0.67384083  0.08880231]]\n",
      "16\n",
      "325\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"load target data\")\n",
    "if target_dataset == 'PD':\n",
    "    data_path = './data/PD/'\n",
    "    tar_all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    tar_all_time = pickle.load(open(data_path + 'y_z.pkl', 'rb'))\n",
    "    tar_all_x_len = [len(i) for i in tar_all_x]\n",
    "\n",
    "    tar_subset_idx = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    tar_other_idx = list(range(69))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(tar_all_x)):\n",
    "        cur = np.array(tar_all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        tar_all_x[i] = cur_subset\n",
    "    \n",
    "print(tar_all_x[0])\n",
    "print(len(tar_all_x[0][0]))\n",
    "print(len(tar_all_x))\n",
    "logger.info(tar_all_x[0])\n",
    "logger.info(len(tar_all_x[0][0]))\n",
    "logger.info(len(tar_all_x))\n",
    "\n",
    "assert(subset_cnt == len(tar_subset_idx))\n",
    "\n",
    "examples = []\n",
    "for idx in range(len(tar_all_x)):\n",
    "    examples.append((tar_all_x[idx], tar_all_time[idx], tar_all_x_len[idx]))\n",
    "examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "tar_all_x = [e[0] for e in examples]\n",
    "tar_all_time = [e[1] for e in examples]\n",
    "tar_all_x_len = [e[2] for e in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:06.077135Z",
     "start_time": "2021-02-10T15:06:05.101451Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 256\n",
    "# input_dim = subset_cnt\n",
    "common_dim = subset_cnt \n",
    "diff_dim = input_dim - subset_cnt\n",
    "hidden_dim = 32\n",
    "d_model = 32\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1\n",
    "\n",
    "model_student = distcare_student(input_dim = common_dim, input_diff_dim = diff_dim, hidden_dim = hidden_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, output_dim = output_dim).to(device)\n",
    "\n",
    "# input_dim, d_model, d_k, d_v, MHD_num_head, d_ff, output_dim\n",
    "optimizer_student = torch.optim.Adam(model_student.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T10:43:10.804571Z",
     "start_time": "2021-01-30T10:42:10.264393Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:07:00,216 - INFO - Batch 0: Test Loss = 0.1131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Test Loss = 0.1131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:07:07,682 - INFO - Batch 20: Test Loss = 0.1478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20: Test Loss = 0.1478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:07:15,247 - INFO - Batch 40: Test Loss = 0.1526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40: Test Loss = 0.1526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:07:22,620 - INFO - Batch 60: Test Loss = 0.1207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 60: Test Loss = 0.1207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:07:29,916 - INFO - Batch 80: Test Loss = 0.0851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 80: Test Loss = 0.0851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:07:37,011 - INFO - Batch 100: Test Loss = 0.0724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100: Test Loss = 0.0724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:07:44,293 - INFO - Batch 120: Test Loss = 0.1097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 120: Test Loss = 0.1097\n"
     ]
    }
   ],
   "source": [
    "#Generate Teacher model embedding\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.eval()\n",
    "\n",
    "train_teacher_emb = []\n",
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "pad_token = np.zeros(34)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(train_x, train_y, train_mask_x, train_x_len, batch_size, shuffle=False)):  \n",
    "        optimizer.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "        opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "        train_teacher_emb.append(emb.cpu().detach().numpy())\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, model_loss.cpu().detach().numpy()))\n",
    "            logger.info('Batch %d: Test Loss = %.4f'%(step, model_loss.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_all_x = torch.tensor(pad_sents(tar_all_x, np.zeros(len(tar_subset_idx))), dtype=torch.float32).to(device)\n",
    "tar_all_x_len = torch.tensor(tar_all_x_len, dtype=torch.float32).to(device).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5, fix_sigma=None, **kwargs):\n",
    "        super(MMDLoss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = None\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul, kernel_num, fix_sigma):\n",
    "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2)\n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
    "                          for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
    "                      for bandwidth_temp in bandwidth_list]\n",
    "        return sum(kernel_val)\n",
    "\n",
    "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
    "        loss = 0.0\n",
    "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
    "        loss = delta.dot(delta.T)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        if self.kernel_type == 'linear':\n",
    "            return self.linear_mmd2(source, target)\n",
    "        elif self.kernel_type == 'rbf':\n",
    "            batch_size = int(source.size()[0])\n",
    "            kernels = self.guassian_kernel(\n",
    "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
    "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
    "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
    "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
    "            loss = torch.mean(XX + YY - XY - YX)\n",
    "            return loss\n",
    "\n",
    "class MultitaskLoss(nn.Module):\n",
    "    def __init__(self, task_num=3):\n",
    "        super(MultitaskLoss, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.alpha = nn.Parameter(torch.ones((task_num)), requires_grad=True)\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.kl = nn.KLDivLoss(reduce=True, size_average=True)\n",
    "        self.tar = MMDLoss()\n",
    "\n",
    "    def forward(self, opt_student, batch_y, emb_student, emb_teacher, tar_source, tar_tar):\n",
    "        BCE_Loss = self.bce(opt_student, batch_y)\n",
    "        emb_Loss = self.kl(emb_student, emb_teacher)\n",
    "        tar_Loss = self.tar(source=tar_source, target=tar_tar)\n",
    "        return BCE_Loss * self.alpha[0] + emb_Loss * self.alpha[1] + tar_Loss * self.alpha[2]\n",
    "\n",
    "def get_multitask_loss(opt_student, batch_y, emb_student, emb_teacher, tar_source, tar_tar):\n",
    "    mtl = MultitaskLoss(task_num=3)\n",
    "    return mtl(opt_student, batch_y, emb_student, emb_teacher, tar_source, tar_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T15:39:24.489286Z",
     "start_time": "2021-01-30T10:43:58.619303Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Training Student\n",
    "# # If you don't want to train Student Model:\n",
    "# # - The pretrained student model is in direcrtory './model/', and can be directly loaded, \n",
    "# # - Simply skip this cell and load the model to validate on Dev Dataset.\n",
    "\n",
    "# logger.info('Training Student')\n",
    "# teacher_flag = True\n",
    "# epochs = 30\n",
    "# total_train_loss = []\n",
    "# total_valid_loss = []\n",
    "# global_best = 0\n",
    "# auroc = []\n",
    "# auprc = []\n",
    "# minpse = []\n",
    "# history = []\n",
    "\n",
    "# pad_token = np.zeros(34)\n",
    "# # begin_time = time.time()\n",
    "# best_auroc = 0\n",
    "# best_auprc = 0\n",
    "# best_minpse = 0\n",
    "\n",
    "# if target_dataset == 'PD':    \n",
    "#     data_str = 'pd'\n",
    "\n",
    "\n",
    "# if teacher_flag:\n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str\n",
    "# else: \n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher'\n",
    "\n",
    "# for each_epoch in range(epochs):\n",
    "\n",
    "#     epoch_loss = []\n",
    "#     counter_batch = 0\n",
    "#     model_student.train()  \n",
    "#     model.eval()\n",
    "#     for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(train_x, train_y, train_mask_x, train_x_len, batch_size, shuffle=False)):  \n",
    "#         optimizer_student.zero_grad()\n",
    "#         batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "#         batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "#         batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#         batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "# #        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "#         opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "#         emb_teacher = torch.tensor(train_teacher_emb[step], dtype=torch.float32).to(device)\n",
    "#         # opt_teacher, decov_loss_teacher, emb_teacher = model(batch_x, batch_lens)\n",
    "# #         BCE_Loss = get_loss(opt_student, batch_y.unsqueeze(-1)) # b t 1\n",
    "#         #emb_Loss = 0.1 * get_re_loss(emb_student, emb_teacher.detach())\n",
    "#         if teacher_flag:\n",
    "#             emb_student = F.log_softmax(emb_student, dim=1)\n",
    "#             emb_teacher = F.softmax(emb_teacher.detach(), dim=1)\n",
    "# #             emb_Loss = get_kl_loss(emb_student, emb_teacher)\n",
    "            \n",
    "#             tar_source = F.softmax(tar_result[0].squeeze(), dim=-1)\n",
    "#             tar_tar = F.softmax(tar_result[1].squeeze(), dim=-1)\n",
    "# #             shrink_indices = torch.tensor(np.random.choice(range(len(tar_tar)), len(tar_source))).to(device)\n",
    "# #             tar_tar = torch.index_select(tar_tar, 0, shrink_indices)\n",
    "# #             tar_Loss = get_kl_loss(tar_source, tar_tar)\n",
    "# #             logger.info(tar_tar)\n",
    "# #             logger.info(tar_source)\n",
    "# #             logger.info(tar_Loss)\n",
    "#             loss = get_multitask_loss(opt_student, batch_y.unsqueeze(-1), emb_student, emb_teacher, tar_source, tar_tar)\n",
    "#         # emb_Loss = 0.1 * get_wass_dist(emb_student, emb_teacher.detach())\n",
    "# #             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "#         else:\n",
    "#             loss = BCE_Loss #+ decov_loss_student\n",
    "\n",
    "#         epoch_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model_student.parameters(), 20)\n",
    "#         optimizer_student.step()\n",
    "\n",
    "#         if step % 20 == 0:\n",
    "#             print('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "#             logger.info('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "\n",
    "#     epoch_loss = np.mean(epoch_loss)\n",
    "#     total_train_loss.append(epoch_loss)\n",
    "\n",
    "#     #Validation\n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         model_student.eval()\n",
    "#         valid_loss = []\n",
    "#         valid_true = []\n",
    "#         valid_pred = []\n",
    "#         for batch_x, batch_y, batch_mask_x, batch_lens in batch_iter(dev_x, dev_y, dev_mask_x, dev_x_len, batch_size):\n",
    "#             batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "#             batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "#             batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#             batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "# #            masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "\n",
    "#             opt_student, decov_loss_student, emb, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [[], []], False)\n",
    "\n",
    "#             BCE_Loss = get_loss(opt_student, batch_y.unsqueeze(-1))\n",
    "# #                 REC_Loss = F.mse_loss(recon, batch_x, reduction='mean').to(device)\n",
    "\n",
    "#             valid_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "\n",
    "#             y_pred += list(opt_student.cpu().detach().numpy().flatten())\n",
    "#             y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "#         valid_loss = np.mean(valid_loss)\n",
    "#         total_valid_loss.append(valid_loss)\n",
    "#         ret = metrics.print_metrics_binary(y_true, y_pred,verbose = 0)\n",
    "#         history.append(ret)\n",
    "#         #print()\n",
    "\n",
    "#         print('Epoch %d: Loss = %.4f Valid loss = %.4f roc = %.4f'%(each_epoch, total_train_loss[-1], total_valid_loss[-1], ret['auroc']))\n",
    "#         metrics.print_metrics_binary(y_true, y_pred)\n",
    "\n",
    "#         cur_auroc = ret['auroc']\n",
    "#         if cur_auroc > best_auroc:\n",
    "#             best_auroc = cur_auroc\n",
    "#             best_auprc = ret['auprc']\n",
    "#             best_minpse = ret['minpse']\n",
    "#             state = {\n",
    "#                 'net': model_student.state_dict(),\n",
    "#                 'optimizer': optimizer_student.state_dict(),\n",
    "#                 'epoch': each_epoch\n",
    "#             }\n",
    "#             torch.save(state, file_name)\n",
    "#             print('------------ Save best model - AUROC: %.4f ------------'%cur_auroc)\n",
    "#             logger.info('------------ Save best model - AUROC: %.4f ------------'%cur_auroc)\n",
    "\n",
    "# print('auroc %.4f'%(best_auroc))\n",
    "# print('auprc %.4f'%(best_auprc))\n",
    "# print('minpse %.4f'%(best_minpse)) \n",
    "# logger.info('auroc %.4f'%(best_auroc))\n",
    "# logger.info('auprc %.4f'%(best_auprc))\n",
    "# logger.info('minpse %.4f'%(best_minpse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:17.118640Z",
     "start_time": "2021-02-10T15:06:15.660556Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:07:46,460 - INFO - last saved model is in epoch 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last saved model is in epoch 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "distcare_student(\n",
       "  (PositionalEncoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (GRUs): ModuleList(\n",
       "    (0): GRU(1, 32, batch_first=True)\n",
       "    (1): GRU(1, 32, batch_first=True)\n",
       "    (2): GRU(1, 32, batch_first=True)\n",
       "    (3): GRU(1, 32, batch_first=True)\n",
       "    (4): GRU(1, 32, batch_first=True)\n",
       "    (5): GRU(1, 32, batch_first=True)\n",
       "    (6): GRU(1, 32, batch_first=True)\n",
       "    (7): GRU(1, 32, batch_first=True)\n",
       "    (8): GRU(1, 32, batch_first=True)\n",
       "    (9): GRU(1, 32, batch_first=True)\n",
       "    (10): GRU(1, 32, batch_first=True)\n",
       "    (11): GRU(1, 32, batch_first=True)\n",
       "    (12): GRU(1, 32, batch_first=True)\n",
       "    (13): GRU(1, 32, batch_first=True)\n",
       "    (14): GRU(1, 32, batch_first=True)\n",
       "    (15): GRU(1, 32, batch_first=True)\n",
       "  )\n",
       "  (generalGRUs): ModuleList(\n",
       "    (0): GRU(1, 32, batch_first=True)\n",
       "    (1): GRU(1, 32, batch_first=True)\n",
       "    (2): GRU(1, 32, batch_first=True)\n",
       "    (3): GRU(1, 32, batch_first=True)\n",
       "    (4): GRU(1, 32, batch_first=True)\n",
       "    (5): GRU(1, 32, batch_first=True)\n",
       "    (6): GRU(1, 32, batch_first=True)\n",
       "    (7): GRU(1, 32, batch_first=True)\n",
       "    (8): GRU(1, 32, batch_first=True)\n",
       "    (9): GRU(1, 32, batch_first=True)\n",
       "    (10): GRU(1, 32, batch_first=True)\n",
       "    (11): GRU(1, 32, batch_first=True)\n",
       "    (12): GRU(1, 32, batch_first=True)\n",
       "    (13): GRU(1, 32, batch_first=True)\n",
       "    (14): GRU(1, 32, batch_first=True)\n",
       "    (15): GRU(1, 32, batch_first=True)\n",
       "    (16): GRU(1, 32, batch_first=True)\n",
       "    (17): GRU(1, 32, batch_first=True)\n",
       "  )\n",
       "  (LastStepAttentions): ModuleList(\n",
       "    (0): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (1): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (2): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (3): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (4): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (5): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (6): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (7): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (8): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (9): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (10): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (11): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (12): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (13): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (14): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (15): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "  )\n",
       "  (FinalAttentionQKV): FinalAttentionQKV(\n",
       "    (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_k): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_v): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_out): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (tanh): Tanh()\n",
       "    (softmax): Softmax(dim=None)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (MultiHeadedAttention): MultiHeadedAttention(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (final_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (SublayerConnection): SublayerConnection(\n",
       "    (norm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (PositionwiseFeedForward): PositionwiseFeedForward(\n",
       "    (w_1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (w_2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (demo_proj_main): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (demo_proj): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (Linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (output): Linear(in_features=34, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (FC_embed): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (softmax): Softmax(dim=None)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       "  (to_MMD): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_flag = True\n",
    "\n",
    "if target_dataset == 'PD':    \n",
    "    data_str = 'pd'\n",
    "\n",
    "if teacher_flag:\n",
    "    file_name = './model/pretrained-challenge-front-fill-2'+ data_str\n",
    "else: \n",
    "    file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher'\n",
    "\n",
    "checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu') )\n",
    "save_epoch = checkpoint['epoch']\n",
    "print(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "logger.info(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "model_student.load_state_dict(checkpoint['net'])\n",
    "optimizer_student.load_state_dict(checkpoint['optimizer'])\n",
    "model_student.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T15:40:05.462185Z",
     "start_time": "2021-01-30T15:40:00.917653Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:07:46,970 - INFO - Batch 0: Test Loss = 0.2126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Test Loss = 0.2126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:07:52,751 - INFO - \n",
      "==>Predicting on test\n",
      "2023-08-11 16:07:52,753 - INFO - Test Loss = 0.1377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>Predicting on test\n",
      "Test Loss = 0.1377\n",
      "confusion matrix:\n",
      "[[3713   23]\n",
      " [ 148  149]]\n",
      "accuracy = 0.9575998187065125\n",
      "precision class 0 = 0.9616679549217224\n",
      "precision class 1 = 0.8662790656089783\n",
      "recall class 0 = 0.993843674659729\n",
      "recall class 1 = 0.5016834735870361\n",
      "AUC of ROC = 0.9284574870763308\n",
      "AUC of PRC = 0.7098101411898149\n",
      "min(+P, Se) = 0.6734006734006734\n",
      "f1_score = 0.6353944049515263\n"
     ]
    }
   ],
   "source": [
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    model_student.eval()\n",
    "    for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(test_x, test_y, test_mask_x, test_x_len, batch_size, shuffle=True)):  \n",
    "        optimizer_student.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "        opt, decov_loss, emb, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [[], []], False)\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "\n",
    "        loss = model_loss\n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "            logger.info('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "        y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "        y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "logger.info(\"\\n==>Predicting on test\")\n",
    "logger.info('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "test_res = metrics.print_metrics_binary(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Target Dataset & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(source_x) is 40336\n",
      "len(source_x_diff) is 40336\n",
      "len(target_x_diff) is 325\n"
     ]
    }
   ],
   "source": [
    "# if target_dataset == 'TJ':\n",
    "#     source_common_idx = [27, 29, 18, 16, 26, 33, 28, 31, 32, 15, 11, 25, 21, 20, 9, 17, 30, 19]\n",
    "#     target_common_idx = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "#     source_data_path = './data/Challenge/'\n",
    "#     source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "#     target_data_path = './data/Tongji/'\n",
    "#     target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "\n",
    "# elif target_dataset == 'HM':\n",
    "#     source_common_idx = [0, 1, 2, 3, 5, 9, 11, 12, 13, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
    "#     target_common_idx = [5, 6, 4, 2, 3, 48, 79, 76, 87, 25, 30, 31, 18, 43, 58, 66, 40, 57, 23, 92, 50, 54, 91, 60, 39, 81]\n",
    "#     source_data_path = './data/Challenge/'\n",
    "#     source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "#     target_data_path = './data/CDSL/'\n",
    "#     target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    source_common_idx = [31, 29, 28, 33, 25, 18, 7, 21, 16, 15, 19, 17, 24, 3, 5, 0]\n",
    "    target_common_idx = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    source_data_path = './data/Challenge/'\n",
    "    source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "    target_data_path = './data/PD/'\n",
    "    target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "\n",
    "assert(len(source_common_idx) == len(target_common_idx))\n",
    "common_len = len(source_common_idx)\n",
    "source_x_diff = []\n",
    "target_x_diff = []\n",
    "\n",
    "source_total_len = 34\n",
    "source_other_idx = list(range(source_total_len))\n",
    "for i in source_common_idx:\n",
    "    source_other_idx.remove(i)\n",
    "\n",
    "# if target_dataset == 'TJ':\n",
    "#     target_other_idx = list(range(75))\n",
    "#     target_total_len = 75\n",
    "# else:\n",
    "#     target_other_idx = list(range(99))\n",
    "#     target_total_len = 99\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    target_total_len = 69\n",
    "    target_other_idx = list(range(target_total_len))\n",
    "    for i in target_common_idx:\n",
    "        target_other_idx.remove(i)\n",
    "\n",
    "print(f'len(source_x) is {len(source_x)}')\n",
    "for i in range(len(source_x)):\n",
    "    cur = np.array(source_x[i], dtype=float)\n",
    "    cur_subset = cur[:, source_common_idx]\n",
    "    cur_other = cur[:, source_other_idx]\n",
    "    source_x_diff.append(cur_other.tolist())\n",
    "    source_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "print(f'len(source_x_diff) is {len(source_x_diff)}')\n",
    "for i in range(len(target_x)):\n",
    "    cur = np.array(target_x[i], dtype=float)\n",
    "    cur_subset = cur[:, target_common_idx]\n",
    "    cur_other = cur[:, target_other_idx]\n",
    "    target_x_diff.append(cur_other.tolist())\n",
    "    target_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "print(f'len(target_x_diff) is {len(target_x_diff)}')\n",
    "\n",
    "source_max = 0\n",
    "for i in range(len(source_x_diff)):\n",
    "    if source_max < len(source_x_diff[i]):\n",
    "        source_max = len(source_x_diff[i])\n",
    "\n",
    "source_x_diff_longest = max(list(len(_) for _ in source_x_diff))\n",
    "source_x_longest = max(list(len(_) for _ in source_x))\n",
    "source_batch = len(source_x_diff)\n",
    "source_diff_features = source_total_len - common_len\n",
    "source_x_diff_ex = torch.zeros((source_batch, source_x_diff_longest, source_diff_features))\n",
    "source_x_ex = torch.zeros((source_batch, source_x_longest, source_total_len))\n",
    "\n",
    "for i in range(len(source_x_diff)):\n",
    "    for j in range(source_x_diff_longest):\n",
    "        cur_len = len(source_x_diff[i])\n",
    "        if j < cur_len:\n",
    "            source_x_diff_ex[i,j,:] = torch.Tensor(source_x_diff[i])[j,:]\n",
    "        else:\n",
    "            source_x_diff_ex[i,j,:] = torch.Tensor(source_x_diff[i])[cur_len - 1,:]\n",
    "\n",
    "for i in range(len(source_x)):\n",
    "    for j in range(source_x_longest):\n",
    "        cur_len = len(source_x[i])\n",
    "        if j < cur_len:\n",
    "            source_x_ex[i,j,:] = torch.Tensor(source_x[i])[j,:]\n",
    "        else:\n",
    "            source_x_ex[i,j,:] = torch.Tensor(source_x[i])[cur_len - 1,:]\n",
    "\n",
    "target_x_diff_longest = max(list(len(_) for _ in target_x_diff))\n",
    "target_batch = len(target_x_diff)\n",
    "target_features = target_total_len - common_len\n",
    "target_x_diff_ex = torch.zeros((target_batch, target_x_diff_longest, target_features))\n",
    "\n",
    "for i in range(len(target_x_diff)):\n",
    "    for j in range(target_x_diff_longest):\n",
    "        cur_len = len(target_x_diff[i])\n",
    "        if j < cur_len:\n",
    "            target_x_diff_ex[i,j,:] = torch.Tensor(target_x_diff[i])[j,:]\n",
    "        else:\n",
    "            target_x_diff_ex[i,j,:] = torch.Tensor(target_x_diff[i])[cur_len - 1,:]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_x_diff_ex.shape is (40336, 336, 18), max_len is 336\n",
      "source_x_ex.shape is [40336   336    34]\n",
      "target_x_diff_ex.shape is (325, 143, 53)\n"
     ]
    }
   ],
   "source": [
    "print(f'source_x_diff_ex.shape is {np.array(source_x_diff_ex).shape}, max_len is {source_max}')\n",
    "print(f'source_x_ex.shape is {np.array(source_x_ex.shape)}')\n",
    "print(f'target_x_diff_ex.shape is {np.array(target_x_diff_ex).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((336, 18), (143, 53))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_x_diff_mean = np.mean(np.array(source_x_diff_ex), 0)\n",
    "source_x_mean = np.mean(np.array(source_x_ex), 0 )\n",
    "target_x_diff_mean = np.mean(np.array(target_x_diff_ex), 0)\n",
    "source_x_diff_mean.shape, target_x_diff_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 32,\n",
       " 13,\n",
       " 2,\n",
       " 28,\n",
       " 27,\n",
       " 28,\n",
       " 32,\n",
       " 5,\n",
       " 15,\n",
       " 4,\n",
       " 20,\n",
       " 11,\n",
       " 7,\n",
       " 27,\n",
       " 25,\n",
       " 32,\n",
       " 25,\n",
       " 2,\n",
       " 24,\n",
       " 8,\n",
       " 17,\n",
       " 24,\n",
       " 16,\n",
       " 15,\n",
       " 24,\n",
       " 24,\n",
       " 5,\n",
       " 15,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 26,\n",
       " 24,\n",
       " 16,\n",
       " 15,\n",
       " 24,\n",
       " 24,\n",
       " 5,\n",
       " 15,\n",
       " 4,\n",
       " 10,\n",
       " 19,\n",
       " 26,\n",
       " 13,\n",
       " 27,\n",
       " 1,\n",
       " 2,\n",
       " 20,\n",
       " 1,\n",
       " 18,\n",
       " 20,\n",
       " 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dtw import *\n",
    "latest_idx = []\n",
    "for i in range(target_x_diff_mean.shape[1]):\n",
    "    min_idx = 0\n",
    "    min_distance = float('inf')\n",
    "    for j in range(source_x_mean.shape[1]):\n",
    "        source_feature = source_x_mean[:, j]\n",
    "        target_feature = target_x_diff_mean[:, i]\n",
    "        alignment = dtw(source_feature, target_feature)\n",
    "        distance = alignment.distance\n",
    "        if min_distance > distance:\n",
    "            min_distance = distance\n",
    "            min_idx = j\n",
    "    latest_idx.append(min_idx)\n",
    "latest_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:28.889438Z",
     "start_time": "2021-02-10T15:06:28.597765Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:24:45,132 - INFO - Transfer Target Dataset & Model\n",
      "2023-08-11 16:24:45,208 - INFO - [[-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8285543981909917, 2.770245567717861, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8374745525934485, 2.8093811444689463, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5338330207864587, 0.431156978499758, 0.8964660630930379, -1.342421748214616, -0.6946051308187712, -0.2120300841305121, 0.8952797203562032, 0.0387041303378994, -0.8612693268611251, 0.0726334784031089, -0.5051102137388988, -0.2865727201409924, 0.3988826585206329, -1.3794377690564883, -0.8237089728907875, -0.9221119476695248, -1.6036614531597553, -0.2379932071009605, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -1.1501193786706505, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, 1.0826051686869729, -0.609009148503521, 0.9684393533852332, -0.3880554131475662, 0.8439788318452395, 2.837917502516613, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 0.278477698357045, 1.2505677424119097, -0.6305581644917595, -0.9062745442328174, -0.7536814885080627, -0.4374103947888615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, -0.7710128163592748, -0.6738408269117502, -2.438483340619628, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8608899578998963, 2.912112033440545, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3396023676711391, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8763143915541441, 2.979783968239297, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5125273793649132, 0.601421850901989, 0.5034187451388209, -1.455529079732558, -0.5254345623915906, -0.4163803392884947, 0.7885904984786939, 0.2816638966057675, -0.5890172833864098, 0.5902661209826593, -0.3430637446868887, -0.7482466726318323, -0.1864257310498265, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.2249939001501796, -0.0341172340589738, 0.3466058957088718, 0.3482841380580905, -0.2890718868318484, 0.1923438589112976, -0.2031244129114749, -1.100153472115602, -0.7598199909551685, -0.6868858002659636, 0.3602180776283341, -0.700061185363224, -0.501359972189453, -0.2332733777972843, -1.36588558517574, 0.3571411263970316, -0.7447833078367583, 0.8765002281041955, 2.9805992927549454, 0.0776143028335215, -0.2769313825285214, 0.0755136678180621, -0.1474826847594624, -0.3999358135056357, -0.0169688050451549, -0.0937841116273902, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, -0.0122781725754626, -0.3292808750219816, -0.2769313825285214, -0.107625012968948, -0.1474826847594624, -0.5354516373428909, -0.0002229391773086, -0.0862088042532133, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, 0.0766269712424727, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.3951350096437179, -0.5239726809327129, -0.8378690925056301, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.8988006141103361, 3.0784382346326584, 0.0776143028335215, -0.3487779453829513, 0.070457637721598, -0.1474826847594624, -0.3999358135056357, 0.0045338857876385, -0.0729488956980065, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.1275947723114954, -0.4255010572823463, -0.3487779453829513, -0.1777070069371955, -0.1474826847594624, -0.5354516373428909, 0.029608437271828, -0.0566564538358881, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.0232601678654147, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.9106941533136118, 3.1306190036341057, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9108799898636633, 3.1314343281497536, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 1.0542620417323658, -0.6738408269117502, -0.0796833960305785, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9222160194167848, 3.1811691236042576, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.5107528363891513, 1.4769583166408096, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-1.038066534429697, 0.260892106097527, 0.1645848503507034, -1.455529079732558, -1.6391408045371951, 0.196670426185453, 1.2420196914581063, 0.4706326037029981, -0.6616178283130005, -0.5004598044528213, -0.4609157221792596, 0.021209914852902, -1.1769476210921423, 0.0909225332951111, -1.348247483817418, -1.2590833683251033, -0.9252694826312332, 0.781386658108973, -0.0798138719333467, -0.2032969502372001, -0.5728175028694968, 0.103242147293012, -0.2031244129114749, -0.6504603131201653, -0.5868915885967425, -0.6218024077192129, -0.0284729839577739, -0.3925684926321655, -0.4454382040900255, -0.2791761177909213, -1.5149673075505703, 0.3413397023846657, -0.8352815289623093, 0.9329945393197532, 3.228457945511819, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8995798651896526, 0.4879119359671683, 0.1510314945591795, -1.4131138304133295, -0.6523124887119757, 0.196670426185453, 0.148455167213642, 0.4706326037029981, -0.752368509471239, -0.9441449266638648, -0.9507380036319262, 0.1751012323498492, -1.0418764542681906, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -1.3629417216818924, 0.1697587389830128, -0.364093717028159, -0.2645837378255657, -0.2890718868318484, 0.0364158635792983, -0.2031244129114749, -1.8246591171638051, -0.4061028043129334, -0.5350245509902118, 0.0492652283594477, 0.0357249008146661, -0.4929416415078188, -0.5851943844151671, -1.170932563608653, 0.051337096981241, -0.8763418268751098, 0.9447022419729768, 3.279823389997619, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.962914223877992, 3.359725192531085, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.973321070680858, 3.405383365407351, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9735069072309092, 3.4061986899229986, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9980373318376644, 3.5138215259884835, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0340896225475933, 3.671994482024121, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.2639920224449301, -0.6738408269117502, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0396647190491284, 3.696454217493548, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7418059691584676, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, -0.6693833821778409, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.057505027854041, 3.774725370995719, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5792130238763558, -0.9062745442328174, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, 0.0888023142972107, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0760886828591592, 3.856257822560481, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:24:45,212 - INFO - 69\n",
      "2023-08-11 16:24:45,213 - INFO - 325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8285543981909917, 2.770245567717861, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8374745525934485, 2.8093811444689463, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5338330207864587, 0.431156978499758, 0.8964660630930379, -1.342421748214616, -0.6946051308187712, -0.2120300841305121, 0.8952797203562032, 0.0387041303378994, -0.8612693268611251, 0.0726334784031089, -0.5051102137388988, -0.2865727201409924, 0.3988826585206329, -1.3794377690564883, -0.8237089728907875, -0.9221119476695248, -1.6036614531597553, -0.2379932071009605, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -1.1501193786706505, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, 1.0826051686869729, -0.609009148503521, 0.9684393533852332, -0.3880554131475662, 0.8439788318452395, 2.837917502516613, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 0.278477698357045, 1.2505677424119097, -0.6305581644917595, -0.9062745442328174, -0.7536814885080627, -0.4374103947888615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, -0.7710128163592748, -0.6738408269117502, -2.438483340619628, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8608899578998963, 2.912112033440545, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3396023676711391, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8763143915541441, 2.979783968239297, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5125273793649132, 0.601421850901989, 0.5034187451388209, -1.455529079732558, -0.5254345623915906, -0.4163803392884947, 0.7885904984786939, 0.2816638966057675, -0.5890172833864098, 0.5902661209826593, -0.3430637446868887, -0.7482466726318323, -0.1864257310498265, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.2249939001501796, -0.0341172340589738, 0.3466058957088718, 0.3482841380580905, -0.2890718868318484, 0.1923438589112976, -0.2031244129114749, -1.100153472115602, -0.7598199909551685, -0.6868858002659636, 0.3602180776283341, -0.700061185363224, -0.501359972189453, -0.2332733777972843, -1.36588558517574, 0.3571411263970316, -0.7447833078367583, 0.8765002281041955, 2.9805992927549454, 0.0776143028335215, -0.2769313825285214, 0.0755136678180621, -0.1474826847594624, -0.3999358135056357, -0.0169688050451549, -0.0937841116273902, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, -0.0122781725754626, -0.3292808750219816, -0.2769313825285214, -0.107625012968948, -0.1474826847594624, -0.5354516373428909, -0.0002229391773086, -0.0862088042532133, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, 0.0766269712424727, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.3951350096437179, -0.5239726809327129, -0.8378690925056301, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.8988006141103361, 3.0784382346326584, 0.0776143028335215, -0.3487779453829513, 0.070457637721598, -0.1474826847594624, -0.3999358135056357, 0.0045338857876385, -0.0729488956980065, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.1275947723114954, -0.4255010572823463, -0.3487779453829513, -0.1777070069371955, -0.1474826847594624, -0.5354516373428909, 0.029608437271828, -0.0566564538358881, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.0232601678654147, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.9106941533136118, 3.1306190036341057, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9108799898636633, 3.1314343281497536, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 1.0542620417323658, -0.6738408269117502, -0.0796833960305785, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9222160194167848, 3.1811691236042576, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.5107528363891513, 1.4769583166408096, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-1.038066534429697, 0.260892106097527, 0.1645848503507034, -1.455529079732558, -1.6391408045371951, 0.196670426185453, 1.2420196914581063, 0.4706326037029981, -0.6616178283130005, -0.5004598044528213, -0.4609157221792596, 0.021209914852902, -1.1769476210921423, 0.0909225332951111, -1.348247483817418, -1.2590833683251033, -0.9252694826312332, 0.781386658108973, -0.0798138719333467, -0.2032969502372001, -0.5728175028694968, 0.103242147293012, -0.2031244129114749, -0.6504603131201653, -0.5868915885967425, -0.6218024077192129, -0.0284729839577739, -0.3925684926321655, -0.4454382040900255, -0.2791761177909213, -1.5149673075505703, 0.3413397023846657, -0.8352815289623093, 0.9329945393197532, 3.228457945511819, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8995798651896526, 0.4879119359671683, 0.1510314945591795, -1.4131138304133295, -0.6523124887119757, 0.196670426185453, 0.148455167213642, 0.4706326037029981, -0.752368509471239, -0.9441449266638648, -0.9507380036319262, 0.1751012323498492, -1.0418764542681906, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -1.3629417216818924, 0.1697587389830128, -0.364093717028159, -0.2645837378255657, -0.2890718868318484, 0.0364158635792983, -0.2031244129114749, -1.8246591171638051, -0.4061028043129334, -0.5350245509902118, 0.0492652283594477, 0.0357249008146661, -0.4929416415078188, -0.5851943844151671, -1.170932563608653, 0.051337096981241, -0.8763418268751098, 0.9447022419729768, 3.279823389997619, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.962914223877992, 3.359725192531085, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.973321070680858, 3.405383365407351, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9735069072309092, 3.4061986899229986, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9980373318376644, 3.5138215259884835, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0340896225475933, 3.671994482024121, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.2639920224449301, -0.6738408269117502, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0396647190491284, 3.696454217493548, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7418059691584676, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, -0.6693833821778409, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.057505027854041, 3.774725370995719, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5792130238763558, -0.9062745442328174, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, 0.0888023142972107, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0760886828591592, 3.856257822560481, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "69\n",
      "325\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Transfer Target Dataset & Model\")\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    data_path = './data/PD/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y_z.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    tar_subset_idx = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    tar_other_idx = list(range(69))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    \n",
    "print(all_x[0])\n",
    "print(len(all_x[0][0]))\n",
    "print(len(all_x))\n",
    "logger.info(all_x[0])\n",
    "logger.info(len(all_x[0][0]))\n",
    "logger.info(len(all_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:34.621807Z",
     "start_time": "2021-02-10T15:06:34.616350Z"
    }
   },
   "outputs": [],
   "source": [
    "long_x = all_x\n",
    "long_y = all_y\n",
    "long_y_kfold = [each[-1] for each in all_y]\n",
    "long_time = all_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:34.845288Z",
     "start_time": "2021-02-10T15:06:34.837259Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_n2n_data(x, y, x_len):\n",
    "#     length = len(x)\n",
    "#     assert length == len(y)\n",
    "#     assert length == len(x_len)\n",
    "#     new_x = []\n",
    "#     new_y = []\n",
    "#     new_x_len = []\n",
    "#     for i in range(length):\n",
    "#         for j in range(len(x[i])):\n",
    "#             new_x.append(x[i][:j+1])\n",
    "#             new_y.append(y[i][j])\n",
    "#             new_x_len.append(j+1)\n",
    "#     return new_x, new_y, new_x_len\n",
    "def get_n2n_data(x, y, x_len):\n",
    "    length = len(x)\n",
    "    assert length == len(y)\n",
    "    assert length == len(x_len)\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    new_x_len = []\n",
    "    for i in range(length):\n",
    "        for j in range(len(x[i])):\n",
    "            new_x.append(x[i][:j+1])\n",
    "            new_y.append(y[i][j])\n",
    "            new_x_len.append(j+1)\n",
    "    return new_x, new_y, new_x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:35.791565Z",
     "start_time": "2021-02-10T15:06:35.745700Z"
    }
   },
   "outputs": [],
   "source": [
    "class distcare_target(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(distcare_target, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 400)\n",
    "\n",
    "        self.GRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        \n",
    "        self.LastStepAttentions = clones(SingleAttention(self.hidden_dim, 16, attention_type='concat', demographic_dim=12, time_aware=True, use_demographic=False),self.input_dim)\n",
    "        \n",
    "        self.FinalAttentionQKV = FinalAttentionQKV(self.hidden_dim, self.hidden_dim, attention_type='mul',dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model,dropout = 1 - self.keep_prob)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.output = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.FC_embed = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.Linear_los = nn.Linear(self.input_dim, self.output_dim)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, input, lens):\n",
    "        lens = lens.to('cpu')\n",
    "        # input shape [batch_size, timestep, feature_dim]\n",
    "#         demo_main = self.tanh(self.demo_proj_main(demo_input)).unsqueeze(1)# b hidden_dim\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "\n",
    "        # Initialization\n",
    "        #cur_hs = Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0))\n",
    "\n",
    "        # forward\n",
    "        # GRU_embeded_input = self.GRUs[0](input[:,:,0].unsqueeze(-1), Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device))[0] # b t h\n",
    "        # Attention_embeded_input = self.LastStepAttentions[0](GRU_embeded_input)[0].unsqueeze(1)# b 1 h\n",
    "        # for i in range(feature_dim-1):\n",
    "        #     embeded_input = self.GRUs[i+1](input[:,:,i+1].unsqueeze(-1), Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device))[0] # b 1 h\n",
    "        #     embeded_input = self.LastStepAttentions[i+1](embeded_input)[0].unsqueeze(1)# b 1 h\n",
    "        #     Attention_embeded_input = torch.cat((Attention_embeded_input, embeded_input), 1)# b i h\n",
    "\n",
    "        # Attention_embeded_input = torch.cat((Attention_embeded_input, demo_main), 1)# b i+1 h\n",
    "        # posi_input = self.dropout(Attention_embeded_input) # batch_size * d_input+1 * hidden_dim\n",
    "\n",
    "#         input = pack_padded_sequence(input, lens, batch_first=True)\n",
    "        \n",
    "        GRU_embeded_input = self.GRUs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "#         print(GRU_embeded_input.shape)\n",
    "        for i in range(feature_dim-1):\n",
    "            embeded_input = self.GRUs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "            GRU_embeded_input = torch.cat((GRU_embeded_input, embeded_input), 1)\n",
    "        \n",
    "\n",
    "#         GRU_embeded_input = torch.cat((GRU_embeded_input, demo_main), 1)# b i+1 h\n",
    "        posi_input = self.dropout(GRU_embeded_input) # batch_size * d_input * hidden_dim\n",
    "        contexts = self.Linear(posi_input).squeeze()# b i\n",
    "        output = self.Linear_los(self.dropout(contexts))# b 1\n",
    "        #mask = subsequent_mask(time_step).to(device) # 1 t t 下三角 N to 1任务不用mask\n",
    "        return output, None, None\n",
    "    #, self.MultiHeadedAttention.attn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:37.622869Z",
     "start_time": "2021-02-10T15:06:37.615260Z"
    }
   },
   "outputs": [],
   "source": [
    "# def transfer_gru_dict(pretrain_dict, model_dict):\n",
    "#     state_dict = {}\n",
    "#     for k, v in pretrain_dict.items():\n",
    "#         if 'GRUs' in k:\n",
    "#             state_dict[k] = v\n",
    "#             print(\"transfered weight: {}\".format(k))\n",
    "#             logger.info(\"transfered weight: {}\".format(k))\n",
    "#         else:\n",
    "#             print(\"Other weight in model_dict: {}\".format(k))\n",
    "#             logger.info(\"Other weight in model_dict: {}\".format(k))\n",
    "#     return state_dict\n",
    "# def transfer_gru_dict(pretrain_dict, model_dict):\n",
    "#     state_dict = {}\n",
    "#     for k, v in model_dict.items():\n",
    "#         if \"GRUs\" in k:\n",
    "#             if k in pretrain_dict:\n",
    "#                 state_dict[k] = pretrain_dict[k]\n",
    "#             else:\n",
    "#                 target_k = \"generalGRU.\"\n",
    "#                 point_position1 = k.find('.')\n",
    "#                 point_position2 = k.find('.', point_position1+1)\n",
    "#                 target_k += k[point_position2+1:]\n",
    "#                 state_dict[k] = pretrain_dict[target_k]\n",
    "#     return state_dict\n",
    "def transfer_gru_dict(pretrain_dict, model_dict, latest_idx, common_len):\n",
    "    state_dict = {}\n",
    "    \n",
    "    for k, v in model_dict.items():\n",
    "        model_point_position1 = k.find('.')\n",
    "        model_module_name = k[:model_point_position1]\n",
    "        if \"GRUs\" == model_module_name:\n",
    "            model_point_position2 = k.find('.', model_point_position1+1)\n",
    "            model_module_idx = int(k[model_point_position1 + 1: model_point_position2])\n",
    "            print(f'model_module_idx is {model_module_idx}')\n",
    "            if model_module_idx < common_len:\n",
    "                state_dict[k] = pretrain_dict[k]\n",
    "            else:\n",
    "                diff_idx = model_module_idx - common_len\n",
    "                target_module_idx = int(str(latest_idx[diff_idx]))\n",
    "                if target_module_idx < common_len:\n",
    "                    target_module_name = \"GRUs\"\n",
    "                    target_k = target_module_name +'.' + str(target_module_idx) + '.' + k[model_point_position2+1:]\n",
    "                    state_dict[k] = pretrain_dict[target_k]\n",
    "                else:\n",
    "                    target_module_name = \"generalGRUs\"\n",
    "                    target_k = target_module_name +'.' + str(target_module_idx - common_len) + '.' + k[model_point_position2+1:]\n",
    "                    state_dict[k] = pretrain_dict[target_k]\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:37.864449Z",
     "start_time": "2021-02-10T15:06:37.857471Z"
    }
   },
   "outputs": [],
   "source": [
    "if target_dataset == 'PD':\n",
    "    input_dim = 69\n",
    "    \n",
    "cell = 'GRU'\n",
    "hidden_dim = 32\n",
    "d_model = 32\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:39.629505Z",
     "start_time": "2021-02-10T15:06:39.615897Z"
    }
   },
   "outputs": [],
   "source": [
    "def ckd_batch_iter(x, y, lens, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(x) / batch_size) # 向下取整\n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size] #  fetch out all the induces\n",
    "        \n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx],  lens[idx]))\n",
    "       \n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    \n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "#         batch_name = [e[2] for e in examples]\n",
    "        batch_lens = [e[2] for e in examples]\n",
    "       \n",
    "\n",
    "        yield batch_x, batch_y, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetMultitaskLoss(nn.Module):\n",
    "    def __init__(self, task_num=2):\n",
    "        super(TargetMultitaskLoss, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.alpha = nn.Parameter(torch.ones((task_num)), requires_grad=True)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, opt_student, los, outcome, outcome_y):\n",
    "        MSE_Loss = self.mse(opt_student, los)\n",
    "        BCE_Loss = self.bce(outcome, outcome_y)\n",
    "        return MSE_Loss * self.alpha[0] + BCE_Loss * self.alpha[1]\n",
    "\n",
    "def get_target_multitask_loss(opt_student, los, outcome, outcome_y):\n",
    "    mtl = TargetMultitaskLoss(task_num=2)\n",
    "    return mtl(opt_student, los, outcome, outcome_y)\n",
    "\n",
    "def reverse_los(y, los_info):\n",
    "    return y * los_info[\"los_std\"] + los_info[\"los_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 16:24:45,370 - INFO - {'los_mean': 1055.0307777880782, 'los_std': 799.0879849276147}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'los_mean': 1055.0307777880782, 'los_std': 799.0879849276147}\n"
     ]
    }
   ],
   "source": [
    "los_info = pickle.load(open(data_path + 'los_info.pkl', 'rb'))\n",
    "print(los_info)\n",
    "logger.info(los_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-10T15:08:58.832Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:13:27,275 - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 1.0642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 0 Batch 0: Train Loss = 1.0642\n",
      "Fold 1, epoch 0: Loss = 0.9974 Valid loss = 0.9939 MSE = 671.7752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:13:53,557 - INFO - Fold 1, epoch 0: Loss = 0.9974 Valid loss = 0.9939 MSE = 671.7752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save FOLD-BEST model - MSE: 671.7752 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:13:53,559 - INFO - ------------ Save FOLD-BEST model - MSE: 671.7752 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0  74 192   0]\n",
      " [  0 285 674   0]\n",
      " [  0 130 378   0]\n",
      " [  0  21 321   0]]\n",
      "Mean absolute deviation (MAD) = 21.643434853978384\n",
      "Mean squared error (MSE) = 671.7751554135303\n",
      "Mean absolute percentage error (MAPE) = 277.0944615688826\n",
      "Cohen kappa score = 0.06653793714400402\n",
      "------------ Save best model - MSE: 671.7752 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:13:53,890 - INFO - ------------ Save best model - MSE: 671.7752 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 671.7752, mad = 21.6434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:13:53,892 - INFO - Fold 1, mse = 671.7752, mad = 21.6434\n",
      "2023-08-11 17:13:54,564 - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 0.9883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1 Batch 0: Train Loss = 0.9883\n",
      "------------ Save FOLD-BEST model - MSE: 637.7109 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:14:20,374 - INFO - ------------ Save FOLD-BEST model - MSE: 637.7109 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 169  97   0]\n",
      " [  0 489 470   0]\n",
      " [  0 206 302   0]\n",
      " [  0  67 275   0]]\n",
      "Mean absolute deviation (MAD) = 20.904969147554823\n",
      "Mean squared error (MSE) = 637.7109299111895\n",
      "Mean absolute percentage error (MAPE) = 256.06099578616346\n",
      "Cohen kappa score = 0.13019815594829698\n",
      "------------ Save best model - MSE: 637.7109 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:14:20,744 - INFO - ------------ Save best model - MSE: 637.7109 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 637.7109, mad = 20.9050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:14:20,745 - INFO - Fold 1, mse = 637.7109, mad = 20.9050\n",
      "2023-08-11 17:14:21,471 - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 0.8813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 2 Batch 0: Train Loss = 0.8813\n",
      "------------ Save FOLD-BEST model - MSE: 620.1938 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:14:47,871 - INFO - ------------ Save FOLD-BEST model - MSE: 620.1938 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 201  65   0]\n",
      " [  0 586 373   0]\n",
      " [  0 259 249   0]\n",
      " [  0  93 249   0]]\n",
      "Mean absolute deviation (MAD) = 20.396152700492298\n",
      "Mean squared error (MSE) = 620.1938278047478\n",
      "Mean absolute percentage error (MAPE) = 234.50327480825158\n",
      "Cohen kappa score = 0.1408325558665423\n",
      "------------ Save best model - MSE: 620.1938 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:14:48,198 - INFO - ------------ Save best model - MSE: 620.1938 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 620.1938, mad = 20.3962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:14:48,200 - INFO - Fold 1, mse = 620.1938, mad = 20.3962\n",
      "2023-08-11 17:14:49,011 - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 0.8633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 3 Batch 0: Train Loss = 0.8633\n",
      "------------ Save FOLD-BEST model - MSE: 595.5034 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:15:15,620 - INFO - ------------ Save FOLD-BEST model - MSE: 595.5034 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  6 183  77   0]\n",
      " [  3 531 425   0]\n",
      " [  0 190 318   0]\n",
      " [  0  57 285   0]]\n",
      "Mean absolute deviation (MAD) = 20.199157612463218\n",
      "Mean squared error (MSE) = 595.5033655802874\n",
      "Mean absolute percentage error (MAPE) = 242.33764119164678\n",
      "Cohen kappa score = 0.1864893913966016\n",
      "------------ Save best model - MSE: 595.5034 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:15:15,860 - INFO - ------------ Save best model - MSE: 595.5034 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 595.5034, mad = 20.1992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:15:15,862 - INFO - Fold 1, mse = 595.5034, mad = 20.1992\n",
      "2023-08-11 17:15:16,435 - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 0.7744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 4 Batch 0: Train Loss = 0.7744\n",
      "------------ Save FOLD-BEST model - MSE: 591.6711 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:15:42,656 - INFO - ------------ Save FOLD-BEST model - MSE: 591.6711 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  8 183  75   0]\n",
      " [ 13 545 401   0]\n",
      " [  0 221 287   0]\n",
      " [  0  73 269   0]]\n",
      "Mean absolute deviation (MAD) = 20.087831448246135\n",
      "Mean squared error (MSE) = 591.6711465040925\n",
      "Mean absolute percentage error (MAPE) = 232.08022036962484\n",
      "Cohen kappa score = 0.16692865932622747\n",
      "------------ Save best model - MSE: 591.6711 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:15:42,917 - INFO - ------------ Save best model - MSE: 591.6711 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 591.6711, mad = 20.0878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:15:42,919 - INFO - Fold 1, mse = 591.6711, mad = 20.0878\n",
      "2023-08-11 17:15:43,511 - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 0.8716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 5 Batch 0: Train Loss = 0.8716\n",
      "Fold 1, mse = 600.1860, mad = 19.8993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:16:09,774 - INFO - Fold 1, mse = 600.1860, mad = 19.8993\n",
      "2023-08-11 17:16:10,460 - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 0.8050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 6 Batch 0: Train Loss = 0.8050\n",
      "Fold 1, mse = 601.6845, mad = 20.2845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:16:36,800 - INFO - Fold 1, mse = 601.6845, mad = 20.2845\n",
      "2023-08-11 17:16:37,673 - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 0.8244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 7 Batch 0: Train Loss = 0.8244\n",
      "Fold 1, mse = 604.3359, mad = 20.1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:17:03,915 - INFO - Fold 1, mse = 604.3359, mad = 20.1600\n",
      "2023-08-11 17:17:04,642 - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 0.8078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 8 Batch 0: Train Loss = 0.8078\n",
      "Fold 1, mse = 605.0623, mad = 20.3857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:17:30,618 - INFO - Fold 1, mse = 605.0623, mad = 20.3857\n",
      "2023-08-11 17:17:31,234 - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 0.6275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 9 Batch 0: Train Loss = 0.6275\n",
      "Fold 1, mse = 625.6888, mad = 20.4052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:17:57,854 - INFO - Fold 1, mse = 625.6888, mad = 20.4052\n",
      "2023-08-11 17:17:58,505 - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 0.6995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 10 Batch 0: Train Loss = 0.6995\n",
      "Fold 1, epoch 10: Loss = 0.7544 Valid loss = 0.8784 MSE = 627.3524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:18:25,136 - INFO - Fold 1, epoch 10: Loss = 0.7544 Valid loss = 0.8784 MSE = 627.3524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 627.3524, mad = 20.6740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:18:25,140 - INFO - Fold 1, mse = 627.3524, mad = 20.6740\n",
      "2023-08-11 17:18:25,832 - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 0.7205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 11 Batch 0: Train Loss = 0.7205\n",
      "Fold 1, mse = 623.4025, mad = 20.4441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:18:51,965 - INFO - Fold 1, mse = 623.4025, mad = 20.4441\n",
      "2023-08-11 17:18:52,749 - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 0.6645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 12 Batch 0: Train Loss = 0.6645\n",
      "Fold 1, mse = 625.6674, mad = 20.4809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:19:18,906 - INFO - Fold 1, mse = 625.6674, mad = 20.4809\n",
      "2023-08-11 17:19:19,707 - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 0.7590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 13 Batch 0: Train Loss = 0.7590\n",
      "Fold 1, mse = 648.8112, mad = 20.7002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:19:45,936 - INFO - Fold 1, mse = 648.8112, mad = 20.7002\n",
      "2023-08-11 17:19:46,649 - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 0.7572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 14 Batch 0: Train Loss = 0.7572\n",
      "Fold 1, mse = 630.6136, mad = 20.5317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:20:13,228 - INFO - Fold 1, mse = 630.6136, mad = 20.5317\n",
      "2023-08-11 17:20:13,896 - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 0.6740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 15 Batch 0: Train Loss = 0.6740\n",
      "Fold 1, mse = 610.0710, mad = 20.1531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:20:40,078 - INFO - Fold 1, mse = 610.0710, mad = 20.1531\n",
      "2023-08-11 17:20:40,839 - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 0.7815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 16 Batch 0: Train Loss = 0.7815\n",
      "Fold 1, mse = 618.4023, mad = 20.5803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:21:07,007 - INFO - Fold 1, mse = 618.4023, mad = 20.5803\n",
      "2023-08-11 17:21:07,730 - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 0.7588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 17 Batch 0: Train Loss = 0.7588\n",
      "Fold 1, mse = 617.1149, mad = 20.2512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:21:34,074 - INFO - Fold 1, mse = 617.1149, mad = 20.2512\n",
      "2023-08-11 17:21:34,794 - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 0.7220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 18 Batch 0: Train Loss = 0.7220\n",
      "Fold 1, mse = 606.6052, mad = 20.3813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:22:01,964 - INFO - Fold 1, mse = 606.6052, mad = 20.3813\n",
      "2023-08-11 17:22:02,660 - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 0.6965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 19 Batch 0: Train Loss = 0.6965\n",
      "Fold 1, mse = 645.5347, mad = 20.8364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:22:28,901 - INFO - Fold 1, mse = 645.5347, mad = 20.8364\n",
      "2023-08-11 17:22:29,597 - INFO - Fold 1 Epoch 20 Batch 0: Train Loss = 0.5799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 20 Batch 0: Train Loss = 0.5799\n",
      "Fold 1, epoch 20: Loss = 0.6676 Valid loss = 0.8708 MSE = 633.6583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:22:55,505 - INFO - Fold 1, epoch 20: Loss = 0.6676 Valid loss = 0.8708 MSE = 633.6583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 633.6583, mad = 20.6658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:22:55,506 - INFO - Fold 1, mse = 633.6583, mad = 20.6658\n",
      "2023-08-11 17:22:56,215 - INFO - Fold 1 Epoch 21 Batch 0: Train Loss = 0.6538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 21 Batch 0: Train Loss = 0.6538\n",
      "Fold 1, mse = 632.0003, mad = 20.7015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:23:22,867 - INFO - Fold 1, mse = 632.0003, mad = 20.7015\n",
      "2023-08-11 17:23:23,623 - INFO - Fold 1 Epoch 22 Batch 0: Train Loss = 0.6262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 22 Batch 0: Train Loss = 0.6262\n",
      "Fold 1, mse = 630.5673, mad = 20.6696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:23:49,594 - INFO - Fold 1, mse = 630.5673, mad = 20.6696\n",
      "2023-08-11 17:23:50,427 - INFO - Fold 1 Epoch 23 Batch 0: Train Loss = 0.6124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 23 Batch 0: Train Loss = 0.6124\n",
      "Fold 1, mse = 618.4685, mad = 20.3482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:24:16,433 - INFO - Fold 1, mse = 618.4685, mad = 20.3482\n",
      "2023-08-11 17:24:17,192 - INFO - Fold 1 Epoch 24 Batch 0: Train Loss = 0.7110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 24 Batch 0: Train Loss = 0.7110\n",
      "Fold 1, mse = 623.9511, mad = 20.5778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:24:44,344 - INFO - Fold 1, mse = 623.9511, mad = 20.5778\n",
      "2023-08-11 17:24:45,170 - INFO - Fold 1 Epoch 25 Batch 0: Train Loss = 0.6872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 25 Batch 0: Train Loss = 0.6872\n",
      "Fold 1, mse = 649.8909, mad = 20.7824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:25:12,955 - INFO - Fold 1, mse = 649.8909, mad = 20.7824\n",
      "2023-08-11 17:25:13,605 - INFO - Fold 1 Epoch 26 Batch 0: Train Loss = 0.8091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 26 Batch 0: Train Loss = 0.8091\n",
      "Fold 1, mse = 611.9157, mad = 20.1922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:25:39,937 - INFO - Fold 1, mse = 611.9157, mad = 20.1922\n",
      "2023-08-11 17:25:40,688 - INFO - Fold 1 Epoch 27 Batch 0: Train Loss = 0.6246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 27 Batch 0: Train Loss = 0.6246\n",
      "Fold 1, mse = 623.3490, mad = 20.2276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:26:06,400 - INFO - Fold 1, mse = 623.3490, mad = 20.2276\n",
      "2023-08-11 17:26:07,124 - INFO - Fold 1 Epoch 28 Batch 0: Train Loss = 0.5733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 28 Batch 0: Train Loss = 0.5733\n",
      "Fold 1, mse = 607.7832, mad = 20.1639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:26:33,621 - INFO - Fold 1, mse = 607.7832, mad = 20.1639\n",
      "2023-08-11 17:26:34,333 - INFO - Fold 1 Epoch 29 Batch 0: Train Loss = 0.6855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 29 Batch 0: Train Loss = 0.6855\n",
      "Fold 1, mse = 642.8498, mad = 20.5237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:27:00,687 - INFO - Fold 1, mse = 642.8498, mad = 20.5237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:27:01,730 - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 1.0312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 0 Batch 0: Train Loss = 1.0312\n",
      "Fold 2, epoch 0: Loss = 1.0295 Valid loss = 0.8197 MSE = 583.9526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:27:27,526 - INFO - Fold 2, epoch 0: Loss = 1.0295 Valid loss = 0.8197 MSE = 583.9526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save FOLD-BEST model - MSE: 583.9526 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:27:27,528 - INFO - ------------ Save FOLD-BEST model - MSE: 583.9526 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 135 136   0]\n",
      " [  0 443 530   0]\n",
      " [  0 188 383   0]\n",
      " [  0  23 247   0]]\n",
      "Mean absolute deviation (MAD) = 19.74064582953329\n",
      "Mean squared error (MSE) = 583.9526301146196\n",
      "Mean absolute percentage error (MAPE) = 270.44103305901916\n",
      "Cohen kappa score = 0.13139241979012606\n",
      "------------ Save best model - MSE: 583.9526 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:27:27,777 - INFO - ------------ Save best model - MSE: 583.9526 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 583.9526, mad = 19.7406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:27:27,779 - INFO - Fold 2, mse = 583.9526, mad = 19.7406\n",
      "2023-08-11 17:27:28,527 - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 0.9258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 1 Batch 0: Train Loss = 0.9258\n",
      "------------ Save FOLD-BEST model - MSE: 551.0304 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:27:55,148 - INFO - ------------ Save FOLD-BEST model - MSE: 551.0304 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 173  98   0]\n",
      " [  0 508 465   0]\n",
      " [  0 240 331   0]\n",
      " [  0  23 247   0]]\n",
      "Mean absolute deviation (MAD) = 19.083799712777598\n",
      "Mean squared error (MSE) = 551.0303675703005\n",
      "Mean absolute percentage error (MAPE) = 253.96198894698406\n",
      "Cohen kappa score = 0.1469784302065098\n",
      "------------ Save best model - MSE: 551.0304 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:27:55,403 - INFO - ------------ Save best model - MSE: 551.0304 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 551.0304, mad = 19.0838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:27:55,405 - INFO - Fold 2, mse = 551.0304, mad = 19.0838\n",
      "2023-08-11 17:27:56,059 - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 1.0204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 2 Batch 0: Train Loss = 1.0204\n",
      "------------ Save FOLD-BEST model - MSE: 525.9013 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:28:22,375 - INFO - ------------ Save FOLD-BEST model - MSE: 525.9013 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 156 115   0]\n",
      " [  0 490 483   0]\n",
      " [  0 222 349   0]\n",
      " [  0   2 268   0]]\n",
      "Mean absolute deviation (MAD) = 18.634853504720045\n",
      "Mean squared error (MSE) = 525.9012771246644\n",
      "Mean absolute percentage error (MAPE) = 252.6515262677433\n",
      "Cohen kappa score = 0.15699850049612663\n",
      "------------ Save best model - MSE: 525.9013 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:28:22,620 - INFO - ------------ Save best model - MSE: 525.9013 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 525.9013, mad = 18.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:28:22,621 - INFO - Fold 2, mse = 525.9013, mad = 18.6349\n",
      "2023-08-11 17:28:23,208 - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 0.8201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 3 Batch 0: Train Loss = 0.8201\n",
      "------------ Save FOLD-BEST model - MSE: 504.4525 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:28:48,951 - INFO - ------------ Save FOLD-BEST model - MSE: 504.4525 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  3 181  87   0]\n",
      " [  0 587 386   0]\n",
      " [  0 242 329   0]\n",
      " [  0   6 264   0]]\n",
      "Mean absolute deviation (MAD) = 18.136789364091932\n",
      "Mean squared error (MSE) = 504.45254561470875\n",
      "Mean absolute percentage error (MAPE) = 232.84356250355575\n",
      "Cohen kappa score = 0.20830673007817058\n",
      "------------ Save best model - MSE: 504.4525 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:28:49,209 - INFO - ------------ Save best model - MSE: 504.4525 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 504.4525, mad = 18.1368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:28:49,211 - INFO - Fold 2, mse = 504.4525, mad = 18.1368\n",
      "2023-08-11 17:28:50,062 - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 0.8419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 4 Batch 0: Train Loss = 0.8419\n",
      "------------ Save FOLD-BEST model - MSE: 497.1077 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:29:16,589 - INFO - ------------ Save FOLD-BEST model - MSE: 497.1077 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  3 183  85   0]\n",
      " [  0 531 442   0]\n",
      " [  0 231 340   0]\n",
      " [  0   4 266   0]]\n",
      "Mean absolute deviation (MAD) = 18.069267015038225\n",
      "Mean squared error (MSE) = 497.1076810685377\n",
      "Mean absolute percentage error (MAPE) = 235.81572329702743\n",
      "Cohen kappa score = 0.18912806361807144\n",
      "------------ Save best model - MSE: 497.1077 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:29:16,937 - INFO - ------------ Save best model - MSE: 497.1077 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 497.1077, mad = 18.0693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:29:16,939 - INFO - Fold 2, mse = 497.1077, mad = 18.0693\n",
      "2023-08-11 17:29:17,629 - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 0.7914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 5 Batch 0: Train Loss = 0.7914\n",
      "Fold 2, mse = 500.6532, mad = 18.2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:29:43,852 - INFO - Fold 2, mse = 500.6532, mad = 18.2016\n",
      "2023-08-11 17:29:44,573 - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 0.7213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 6 Batch 0: Train Loss = 0.7213\n",
      "------------ Save FOLD-BEST model - MSE: 496.7453 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:30:07,306 - INFO - ------------ Save FOLD-BEST model - MSE: 496.7453 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[ 18 183  70   0]\n",
      " [  6 576 391   0]\n",
      " [  0 265 306   0]\n",
      " [  0  13 257   0]]\n",
      "Mean absolute deviation (MAD) = 18.02496496168297\n",
      "Mean squared error (MSE) = 496.74527759948285\n",
      "Mean absolute percentage error (MAPE) = 224.318367366884\n",
      "Cohen kappa score = 0.20635187474445948\n",
      "------------ Save best model - MSE: 496.7453 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:30:07,581 - INFO - ------------ Save best model - MSE: 496.7453 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 496.7453, mad = 18.0250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:30:07,582 - INFO - Fold 2, mse = 496.7453, mad = 18.0250\n",
      "2023-08-11 17:30:08,224 - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 0.7991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 7 Batch 0: Train Loss = 0.7991\n",
      "Fold 2, mse = 499.2559, mad = 18.1741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:30:30,345 - INFO - Fold 2, mse = 499.2559, mad = 18.1741\n",
      "2023-08-11 17:30:30,933 - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 0.6471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 8 Batch 0: Train Loss = 0.6471\n",
      "Fold 2, mse = 504.4985, mad = 18.2213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:30:52,751 - INFO - Fold 2, mse = 504.4985, mad = 18.2213\n",
      "2023-08-11 17:30:53,417 - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 0.7750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 9 Batch 0: Train Loss = 0.7750\n",
      "Fold 2, mse = 500.3849, mad = 18.1627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:31:16,088 - INFO - Fold 2, mse = 500.3849, mad = 18.1627\n",
      "2023-08-11 17:31:16,625 - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 0.7589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 10 Batch 0: Train Loss = 0.7589\n",
      "Fold 2, epoch 10: Loss = 0.7436 Valid loss = 0.6774 MSE = 503.3218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:31:39,093 - INFO - Fold 2, epoch 10: Loss = 0.7436 Valid loss = 0.6774 MSE = 503.3218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 503.3218, mad = 18.3733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:31:39,095 - INFO - Fold 2, mse = 503.3218, mad = 18.3733\n",
      "2023-08-11 17:31:39,700 - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 0.7404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 11 Batch 0: Train Loss = 0.7404\n",
      "Fold 2, mse = 498.1639, mad = 18.0881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:32:02,448 - INFO - Fold 2, mse = 498.1639, mad = 18.0881\n",
      "2023-08-11 17:32:03,074 - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 0.5884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 12 Batch 0: Train Loss = 0.5884\n",
      "Fold 2, mse = 500.4764, mad = 18.1188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:32:26,410 - INFO - Fold 2, mse = 500.4764, mad = 18.1188\n",
      "2023-08-11 17:32:26,953 - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 0.6422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 13 Batch 0: Train Loss = 0.6422\n",
      "------------ Save FOLD-BEST model - MSE: 491.8153 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:32:48,726 - INFO - ------------ Save FOLD-BEST model - MSE: 491.8153 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[ 28 170  73   0]\n",
      " [ 48 580 345   0]\n",
      " [  5 280 286   0]\n",
      " [  0  13 257   0]]\n",
      "Mean absolute deviation (MAD) = 17.959488703727757\n",
      "Mean squared error (MSE) = 491.81526985228027\n",
      "Mean absolute percentage error (MAPE) = 215.02642016072153\n",
      "Cohen kappa score = 0.21238128127821365\n",
      "------------ Save best model - MSE: 491.8153 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:32:48,985 - INFO - ------------ Save best model - MSE: 491.8153 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 491.8153, mad = 17.9595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:32:48,986 - INFO - Fold 2, mse = 491.8153, mad = 17.9595\n",
      "2023-08-11 17:32:49,601 - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 0.7759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 14 Batch 0: Train Loss = 0.7759\n",
      "Fold 2, mse = 513.4994, mad = 18.5494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:33:12,664 - INFO - Fold 2, mse = 513.4994, mad = 18.5494\n",
      "2023-08-11 17:33:13,529 - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 0.7879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 15 Batch 0: Train Loss = 0.7879\n",
      "Fold 2, mse = 499.8415, mad = 18.2849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:33:41,515 - INFO - Fold 2, mse = 499.8415, mad = 18.2849\n",
      "2023-08-11 17:33:42,267 - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 0.5851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 16 Batch 0: Train Loss = 0.5851\n",
      "Fold 2, mse = 504.0020, mad = 18.3887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:34:09,888 - INFO - Fold 2, mse = 504.0020, mad = 18.3887\n",
      "2023-08-11 17:34:10,679 - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 0.6754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 17 Batch 0: Train Loss = 0.6754\n",
      "Fold 2, mse = 520.3689, mad = 18.5755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:34:38,141 - INFO - Fold 2, mse = 520.3689, mad = 18.5755\n",
      "2023-08-11 17:34:38,955 - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 0.7624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 18 Batch 0: Train Loss = 0.7624\n",
      "Fold 2, mse = 498.5210, mad = 18.2508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:35:07,212 - INFO - Fold 2, mse = 498.5210, mad = 18.2508\n",
      "2023-08-11 17:35:08,003 - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 0.7208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 19 Batch 0: Train Loss = 0.7208\n",
      "Fold 2, mse = 499.7317, mad = 18.1535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:35:36,661 - INFO - Fold 2, mse = 499.7317, mad = 18.1535\n",
      "2023-08-11 17:35:37,632 - INFO - Fold 2 Epoch 20 Batch 0: Train Loss = 0.6147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 20 Batch 0: Train Loss = 0.6147\n",
      "Fold 2, epoch 20: Loss = 0.6659 Valid loss = 0.6752 MSE = 500.8613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:36:05,788 - INFO - Fold 2, epoch 20: Loss = 0.6659 Valid loss = 0.6752 MSE = 500.8613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 500.8613, mad = 18.1814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:36:05,790 - INFO - Fold 2, mse = 500.8613, mad = 18.1814\n",
      "2023-08-11 17:36:06,663 - INFO - Fold 2 Epoch 21 Batch 0: Train Loss = 0.6252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 21 Batch 0: Train Loss = 0.6252\n",
      "Fold 2, mse = 516.2093, mad = 18.3788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:36:35,012 - INFO - Fold 2, mse = 516.2093, mad = 18.3788\n",
      "2023-08-11 17:36:35,761 - INFO - Fold 2 Epoch 22 Batch 0: Train Loss = 0.6885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 22 Batch 0: Train Loss = 0.6885\n",
      "Fold 2, mse = 516.6323, mad = 18.5576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:37:04,065 - INFO - Fold 2, mse = 516.6323, mad = 18.5576\n",
      "2023-08-11 17:37:05,061 - INFO - Fold 2 Epoch 23 Batch 0: Train Loss = 0.5818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 23 Batch 0: Train Loss = 0.5818\n",
      "Fold 2, mse = 511.9445, mad = 18.3703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:37:34,981 - INFO - Fold 2, mse = 511.9445, mad = 18.3703\n",
      "2023-08-11 17:37:35,906 - INFO - Fold 2 Epoch 24 Batch 0: Train Loss = 0.6340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 24 Batch 0: Train Loss = 0.6340\n",
      "Fold 2, mse = 532.4532, mad = 18.8661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:38:03,679 - INFO - Fold 2, mse = 532.4532, mad = 18.8661\n",
      "2023-08-11 17:38:04,577 - INFO - Fold 2 Epoch 25 Batch 0: Train Loss = 0.6440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 25 Batch 0: Train Loss = 0.6440\n",
      "Fold 2, mse = 505.3745, mad = 18.3447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:38:32,641 - INFO - Fold 2, mse = 505.3745, mad = 18.3447\n",
      "2023-08-11 17:38:33,500 - INFO - Fold 2 Epoch 26 Batch 0: Train Loss = 0.6769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 26 Batch 0: Train Loss = 0.6769\n",
      "Fold 2, mse = 517.1368, mad = 18.5369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:39:01,518 - INFO - Fold 2, mse = 517.1368, mad = 18.5369\n",
      "2023-08-11 17:39:02,237 - INFO - Fold 2 Epoch 27 Batch 0: Train Loss = 0.6290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 27 Batch 0: Train Loss = 0.6290\n",
      "Fold 2, mse = 513.0514, mad = 18.4438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:39:31,130 - INFO - Fold 2, mse = 513.0514, mad = 18.4438\n",
      "2023-08-11 17:39:31,979 - INFO - Fold 2 Epoch 28 Batch 0: Train Loss = 0.6555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 28 Batch 0: Train Loss = 0.6555\n",
      "Fold 2, mse = 529.3110, mad = 18.6256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:39:59,995 - INFO - Fold 2, mse = 529.3110, mad = 18.6256\n",
      "2023-08-11 17:40:00,814 - INFO - Fold 2 Epoch 29 Batch 0: Train Loss = 0.6387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 29 Batch 0: Train Loss = 0.6387\n",
      "Fold 2, mse = 510.0240, mad = 18.2422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:40:30,715 - INFO - Fold 2, mse = 510.0240, mad = 18.2422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:40:31,695 - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 1.0800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 0 Batch 0: Train Loss = 1.0800\n",
      "Fold 3, epoch 0: Loss = 0.9901 Valid loss = 0.9086 MSE = 647.4876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:41:00,344 - INFO - Fold 3, epoch 0: Loss = 0.9901 Valid loss = 0.9086 MSE = 647.4876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save FOLD-BEST model - MSE: 647.4876 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:41:00,347 - INFO - ------------ Save FOLD-BEST model - MSE: 647.4876 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 126 155   0]\n",
      " [  0 450 669   0]\n",
      " [  0 278 415   0]\n",
      " [  0  76 321   0]]\n",
      "Mean absolute deviation (MAD) = 20.862903711666636\n",
      "Mean squared error (MSE) = 647.487590368748\n",
      "Mean absolute percentage error (MAPE) = 205.48551838249423\n",
      "Cohen kappa score = 0.054131054131054124\n",
      "Fold 3, mse = 647.4876, mad = 20.8629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:41:00,473 - INFO - Fold 3, mse = 647.4876, mad = 20.8629\n",
      "2023-08-11 17:41:01,237 - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 1.0919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 1 Batch 0: Train Loss = 1.0919\n",
      "------------ Save FOLD-BEST model - MSE: 634.2531 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:41:29,829 - INFO - ------------ Save FOLD-BEST model - MSE: 634.2531 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 158 123   0]\n",
      " [  0 488 631   0]\n",
      " [  0 257 436   0]\n",
      " [  0  63 334   0]]\n",
      "Mean absolute deviation (MAD) = 20.725265185348572\n",
      "Mean squared error (MSE) = 634.2530912400663\n",
      "Mean absolute percentage error (MAPE) = 199.6418938083749\n",
      "Cohen kappa score = 0.10508825049848602\n",
      "Fold 3, mse = 634.2531, mad = 20.7253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:41:29,952 - INFO - Fold 3, mse = 634.2531, mad = 20.7253\n",
      "2023-08-11 17:41:30,644 - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 0.9026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 2 Batch 0: Train Loss = 0.9026\n",
      "------------ Save FOLD-BEST model - MSE: 619.1989 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:41:57,434 - INFO - ------------ Save FOLD-BEST model - MSE: 619.1989 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  2 154 125   0]\n",
      " [  0 443 676   0]\n",
      " [  0 204 489   0]\n",
      " [  0  43 354   0]]\n",
      "Mean absolute deviation (MAD) = 20.577689559457383\n",
      "Mean squared error (MSE) = 619.1988672370186\n",
      "Mean absolute percentage error (MAPE) = 199.93154501761109\n",
      "Cohen kappa score = 0.12674518330222284\n",
      "Fold 3, mse = 619.1989, mad = 20.5777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:41:57,562 - INFO - Fold 3, mse = 619.1989, mad = 20.5777\n",
      "2023-08-11 17:41:58,335 - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 0.9001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 3 Batch 0: Train Loss = 0.9001\n",
      "Fold 3, mse = 621.1483, mad = 20.5173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:42:26,234 - INFO - Fold 3, mse = 621.1483, mad = 20.5173\n",
      "2023-08-11 17:42:26,807 - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 0.7946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 4 Batch 0: Train Loss = 0.7946\n",
      "Fold 3, mse = 625.7727, mad = 20.6154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:42:55,219 - INFO - Fold 3, mse = 625.7727, mad = 20.6154\n",
      "2023-08-11 17:42:56,061 - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 0.8313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 5 Batch 0: Train Loss = 0.8313\n",
      "Fold 3, mse = 622.2098, mad = 20.5623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:43:24,956 - INFO - Fold 3, mse = 622.2098, mad = 20.5623\n",
      "2023-08-11 17:43:25,741 - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 0.8276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 6 Batch 0: Train Loss = 0.8276\n",
      "Fold 3, mse = 640.2260, mad = 20.9690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:43:53,531 - INFO - Fold 3, mse = 640.2260, mad = 20.9690\n",
      "2023-08-11 17:43:54,513 - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 0.6757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 7 Batch 0: Train Loss = 0.6757\n",
      "Fold 3, mse = 639.7786, mad = 20.8758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:44:21,736 - INFO - Fold 3, mse = 639.7786, mad = 20.8758\n",
      "2023-08-11 17:44:22,586 - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 0.7074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 8 Batch 0: Train Loss = 0.7074\n",
      "Fold 3, mse = 648.0068, mad = 20.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:44:49,857 - INFO - Fold 3, mse = 648.0068, mad = 20.9375\n",
      "2023-08-11 17:44:50,769 - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 0.8073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 9 Batch 0: Train Loss = 0.8073\n",
      "Fold 3, mse = 662.0636, mad = 21.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:45:18,662 - INFO - Fold 3, mse = 662.0636, mad = 21.0073\n",
      "2023-08-11 17:45:19,421 - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 0.7053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 10 Batch 0: Train Loss = 0.7053\n",
      "Fold 3, epoch 10: Loss = 0.7183 Valid loss = 0.9110 MSE = 649.7853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:45:45,720 - INFO - Fold 3, epoch 10: Loss = 0.7183 Valid loss = 0.9110 MSE = 649.7853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, mse = 649.7853, mad = 20.8967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:45:45,722 - INFO - Fold 3, mse = 649.7853, mad = 20.8967\n",
      "2023-08-11 17:45:46,496 - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 0.6794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 11 Batch 0: Train Loss = 0.6794\n",
      "Fold 3, mse = 640.6808, mad = 20.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:46:13,365 - INFO - Fold 3, mse = 640.6808, mad = 20.6922\n",
      "2023-08-11 17:46:14,238 - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 0.6068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 12 Batch 0: Train Loss = 0.6068\n",
      "Fold 3, mse = 649.5832, mad = 20.8406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:46:42,506 - INFO - Fold 3, mse = 649.5832, mad = 20.8406\n",
      "2023-08-11 17:46:43,370 - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 0.7519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 13 Batch 0: Train Loss = 0.7519\n",
      "Fold 3, mse = 654.4444, mad = 20.8686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:47:10,958 - INFO - Fold 3, mse = 654.4444, mad = 20.8686\n",
      "2023-08-11 17:47:11,732 - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 0.6020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 14 Batch 0: Train Loss = 0.6020\n",
      "Fold 3, mse = 660.5208, mad = 21.0736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:47:38,270 - INFO - Fold 3, mse = 660.5208, mad = 21.0736\n",
      "2023-08-11 17:47:39,013 - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 0.6605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 15 Batch 0: Train Loss = 0.6605\n",
      "Fold 3, mse = 671.7528, mad = 21.2723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:48:07,941 - INFO - Fold 3, mse = 671.7528, mad = 21.2723\n",
      "2023-08-11 17:48:08,815 - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 0.5993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 16 Batch 0: Train Loss = 0.5993\n",
      "Fold 3, mse = 670.7166, mad = 21.1828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:48:35,962 - INFO - Fold 3, mse = 670.7166, mad = 21.1828\n",
      "2023-08-11 17:48:36,789 - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 0.6946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 17 Batch 0: Train Loss = 0.6946\n",
      "Fold 3, mse = 680.8279, mad = 21.4080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:49:04,387 - INFO - Fold 3, mse = 680.8279, mad = 21.4080\n",
      "2023-08-11 17:49:05,293 - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 0.6626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 18 Batch 0: Train Loss = 0.6626\n",
      "Fold 3, mse = 677.8752, mad = 21.3907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:49:32,359 - INFO - Fold 3, mse = 677.8752, mad = 21.3907\n",
      "2023-08-11 17:49:33,096 - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 0.7422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 19 Batch 0: Train Loss = 0.7422\n",
      "Fold 3, mse = 685.8209, mad = 21.5109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:50:00,296 - INFO - Fold 3, mse = 685.8209, mad = 21.5109\n",
      "2023-08-11 17:50:01,063 - INFO - Fold 3 Epoch 20 Batch 0: Train Loss = 0.6523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 20 Batch 0: Train Loss = 0.6523\n",
      "Fold 3, epoch 20: Loss = 0.6460 Valid loss = 0.9774 MSE = 694.4552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:50:28,463 - INFO - Fold 3, epoch 20: Loss = 0.6460 Valid loss = 0.9774 MSE = 694.4552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, mse = 694.4552, mad = 21.8018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:50:28,470 - INFO - Fold 3, mse = 694.4552, mad = 21.8018\n",
      "2023-08-11 17:50:29,196 - INFO - Fold 3 Epoch 21 Batch 0: Train Loss = 0.7472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 21 Batch 0: Train Loss = 0.7472\n",
      "Fold 3, mse = 694.2317, mad = 21.8512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:50:56,613 - INFO - Fold 3, mse = 694.2317, mad = 21.8512\n",
      "2023-08-11 17:50:57,405 - INFO - Fold 3 Epoch 22 Batch 0: Train Loss = 0.6557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 22 Batch 0: Train Loss = 0.6557\n",
      "Fold 3, mse = 694.7521, mad = 21.7763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:51:25,659 - INFO - Fold 3, mse = 694.7521, mad = 21.7763\n",
      "2023-08-11 17:51:26,500 - INFO - Fold 3 Epoch 23 Batch 0: Train Loss = 0.6620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 23 Batch 0: Train Loss = 0.6620\n",
      "Fold 3, mse = 695.2508, mad = 21.8106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:51:54,370 - INFO - Fold 3, mse = 695.2508, mad = 21.8106\n",
      "2023-08-11 17:51:55,089 - INFO - Fold 3 Epoch 24 Batch 0: Train Loss = 0.6069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 24 Batch 0: Train Loss = 0.6069\n",
      "Fold 3, mse = 695.3345, mad = 21.7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:52:22,643 - INFO - Fold 3, mse = 695.3345, mad = 21.7199\n",
      "2023-08-11 17:52:23,589 - INFO - Fold 3 Epoch 25 Batch 0: Train Loss = 0.6563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 25 Batch 0: Train Loss = 0.6563\n",
      "Fold 3, mse = 720.9229, mad = 22.1912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:52:51,509 - INFO - Fold 3, mse = 720.9229, mad = 22.1912\n",
      "2023-08-11 17:52:52,335 - INFO - Fold 3 Epoch 26 Batch 0: Train Loss = 0.6482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 26 Batch 0: Train Loss = 0.6482\n",
      "Fold 3, mse = 708.8175, mad = 22.2003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:53:20,535 - INFO - Fold 3, mse = 708.8175, mad = 22.2003\n",
      "2023-08-11 17:53:21,200 - INFO - Fold 3 Epoch 27 Batch 0: Train Loss = 0.5359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 27 Batch 0: Train Loss = 0.5359\n",
      "Fold 3, mse = 703.0705, mad = 21.8563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:53:48,608 - INFO - Fold 3, mse = 703.0705, mad = 21.8563\n",
      "2023-08-11 17:53:49,464 - INFO - Fold 3 Epoch 28 Batch 0: Train Loss = 0.6361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 28 Batch 0: Train Loss = 0.6361\n",
      "Fold 3, mse = 727.8524, mad = 22.1412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:54:16,993 - INFO - Fold 3, mse = 727.8524, mad = 22.1412\n",
      "2023-08-11 17:54:17,902 - INFO - Fold 3 Epoch 29 Batch 0: Train Loss = 0.5303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 29 Batch 0: Train Loss = 0.5303\n",
      "Fold 3, mse = 747.4800, mad = 22.4177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:54:45,839 - INFO - Fold 3, mse = 747.4800, mad = 22.4177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:54:47,094 - INFO - Fold 4 Epoch 0 Batch 0: Train Loss = 0.9567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 0 Batch 0: Train Loss = 0.9567\n",
      "Fold 4, epoch 0: Loss = 0.9476 Valid loss = 1.1810 MSE = 828.8110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:55:14,681 - INFO - Fold 4, epoch 0: Loss = 0.9476 Valid loss = 1.1810 MSE = 828.8110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save FOLD-BEST model - MSE: 828.8110 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:55:14,684 - INFO - ------------ Save FOLD-BEST model - MSE: 828.8110 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 120 130   0]\n",
      " [  0 437 440   0]\n",
      " [  0 333 222   0]\n",
      " [  0 168 252   0]]\n",
      "Mean absolute deviation (MAD) = 23.171475592588703\n",
      "Mean squared error (MSE) = 828.8109518716545\n",
      "Mean absolute percentage error (MAPE) = 222.93586025665326\n",
      "Cohen kappa score = -0.011918778435815236\n",
      "Fold 4, mse = 828.8110, mad = 23.1715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:55:14,889 - INFO - Fold 4, mse = 828.8110, mad = 23.1715\n",
      "2023-08-11 17:55:15,701 - INFO - Fold 4 Epoch 1 Batch 0: Train Loss = 0.8996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 1 Batch 0: Train Loss = 0.8996\n",
      "------------ Save FOLD-BEST model - MSE: 783.4028 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:55:42,689 - INFO - ------------ Save FOLD-BEST model - MSE: 783.4028 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 118 132   0]\n",
      " [  0 456 421   0]\n",
      " [  0 291 264   0]\n",
      " [  0 122 298   0]]\n",
      "Mean absolute deviation (MAD) = 22.61563911543928\n",
      "Mean squared error (MSE) = 783.4028159133397\n",
      "Mean absolute percentage error (MAPE) = 220.15021013132977\n",
      "Cohen kappa score = 0.05193933729774736\n",
      "Fold 4, mse = 783.4028, mad = 22.6156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:55:42,836 - INFO - Fold 4, mse = 783.4028, mad = 22.6156\n",
      "2023-08-11 17:55:43,621 - INFO - Fold 4 Epoch 2 Batch 0: Train Loss = 0.7282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 2 Batch 0: Train Loss = 0.7282\n",
      "------------ Save FOLD-BEST model - MSE: 755.0372 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:56:10,751 - INFO - ------------ Save FOLD-BEST model - MSE: 755.0372 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 175  75   0]\n",
      " [  0 588 289   0]\n",
      " [  0 343 212   0]\n",
      " [  0 147 273   0]]\n",
      "Mean absolute deviation (MAD) = 21.865630930981233\n",
      "Mean squared error (MSE) = 755.0371622359451\n",
      "Mean absolute percentage error (MAPE) = 192.58353968703338\n",
      "Cohen kappa score = 0.10688801668767867\n",
      "Fold 4, mse = 755.0372, mad = 21.8656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:56:10,895 - INFO - Fold 4, mse = 755.0372, mad = 21.8656\n",
      "2023-08-11 17:56:11,728 - INFO - Fold 4 Epoch 3 Batch 0: Train Loss = 0.8157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 3 Batch 0: Train Loss = 0.8157\n",
      "------------ Save FOLD-BEST model - MSE: 724.9889 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:56:39,909 - INFO - ------------ Save FOLD-BEST model - MSE: 724.9889 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 171  79   0]\n",
      " [  0 554 323   0]\n",
      " [  0 293 262   0]\n",
      " [  0 107 313   0]]\n",
      "Mean absolute deviation (MAD) = 21.531604125034907\n",
      "Mean squared error (MSE) = 724.9889067830627\n",
      "Mean absolute percentage error (MAPE) = 191.36142420181417\n",
      "Cohen kappa score = 0.1420155982845548\n",
      "Fold 4, mse = 724.9889, mad = 21.5316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:56:40,122 - INFO - Fold 4, mse = 724.9889, mad = 21.5316\n",
      "2023-08-11 17:56:40,842 - INFO - Fold 4 Epoch 4 Batch 0: Train Loss = 0.9039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 4 Batch 0: Train Loss = 0.9039\n",
      "------------ Save FOLD-BEST model - MSE: 711.2745 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:57:08,067 - INFO - ------------ Save FOLD-BEST model - MSE: 711.2745 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  7 159  84   0]\n",
      " [  0 555 322   0]\n",
      " [  0 256 299   0]\n",
      " [  0  98 322   0]]\n",
      "Mean absolute deviation (MAD) = 21.247234427469234\n",
      "Mean squared error (MSE) = 711.2745200777407\n",
      "Mean absolute percentage error (MAPE) = 187.81263583913892\n",
      "Cohen kappa score = 0.1748808587220022\n",
      "Fold 4, mse = 711.2745, mad = 21.2472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:57:08,217 - INFO - Fold 4, mse = 711.2745, mad = 21.2472\n",
      "2023-08-11 17:57:09,029 - INFO - Fold 4 Epoch 5 Batch 0: Train Loss = 0.7540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 5 Batch 0: Train Loss = 0.7540\n",
      "------------ Save FOLD-BEST model - MSE: 708.7541 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:57:36,675 - INFO - ------------ Save FOLD-BEST model - MSE: 708.7541 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  5 162  83   0]\n",
      " [  0 547 330   0]\n",
      " [  0 235 320   0]\n",
      " [  0 120 300   0]]\n",
      "Mean absolute deviation (MAD) = 21.128119093590794\n",
      "Mean squared error (MSE) = 708.7540645577675\n",
      "Mean absolute percentage error (MAPE) = 186.66863816117575\n",
      "Cohen kappa score = 0.16855672493441554\n",
      "Fold 4, mse = 708.7541, mad = 21.1281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:57:36,827 - INFO - Fold 4, mse = 708.7541, mad = 21.1281\n",
      "2023-08-11 17:57:37,668 - INFO - Fold 4 Epoch 6 Batch 0: Train Loss = 0.8269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 6 Batch 0: Train Loss = 0.8269\n",
      "Fold 4, mse = 719.5737, mad = 21.2332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:58:04,835 - INFO - Fold 4, mse = 719.5737, mad = 21.2332\n",
      "2023-08-11 17:58:05,481 - INFO - Fold 4 Epoch 7 Batch 0: Train Loss = 0.7356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 7 Batch 0: Train Loss = 0.7356\n",
      "Fold 4, mse = 720.1874, mad = 21.2424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:58:32,355 - INFO - Fold 4, mse = 720.1874, mad = 21.2424\n",
      "2023-08-11 17:58:33,091 - INFO - Fold 4 Epoch 8 Batch 0: Train Loss = 0.8661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 8 Batch 0: Train Loss = 0.8661\n",
      "Fold 4, mse = 724.7689, mad = 21.2286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:59:00,882 - INFO - Fold 4, mse = 724.7689, mad = 21.2286\n",
      "2023-08-11 17:59:01,715 - INFO - Fold 4 Epoch 9 Batch 0: Train Loss = 0.6730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 9 Batch 0: Train Loss = 0.6730\n",
      "------------ Save FOLD-BEST model - MSE: 706.7843 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:59:28,141 - INFO - ------------ Save FOLD-BEST model - MSE: 706.7843 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[ 16 172  62   0]\n",
      " [  7 592 278   0]\n",
      " [  0 325 230   0]\n",
      " [  0 127 293   0]]\n",
      "Mean absolute deviation (MAD) = 21.05935831143548\n",
      "Mean squared error (MSE) = 706.7843253747055\n",
      "Mean absolute percentage error (MAPE) = 168.3528447529467\n",
      "Cohen kappa score = 0.1576491003946151\n",
      "Fold 4, mse = 706.7843, mad = 21.0594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:59:28,322 - INFO - Fold 4, mse = 706.7843, mad = 21.0594\n",
      "2023-08-11 17:59:29,031 - INFO - Fold 4 Epoch 10 Batch 0: Train Loss = 0.7982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 10 Batch 0: Train Loss = 0.7982\n",
      "Fold 4, epoch 10: Loss = 0.7040 Valid loss = 0.9605 MSE = 695.7336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:59:57,058 - INFO - Fold 4, epoch 10: Loss = 0.7040 Valid loss = 0.9605 MSE = 695.7336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save FOLD-BEST model - MSE: 695.7336 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:59:57,060 - INFO - ------------ Save FOLD-BEST model - MSE: 695.7336 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[ 14 178  58   0]\n",
      " [  0 630 247   0]\n",
      " [  0 345 210   0]\n",
      " [  0 139 281   0]]\n",
      "Mean absolute deviation (MAD) = 20.722102136927116\n",
      "Mean squared error (MSE) = 695.7335801168339\n",
      "Mean absolute percentage error (MAPE) = 162.82867752002528\n",
      "Cohen kappa score = 0.1565640437519612\n",
      "Fold 4, mse = 695.7336, mad = 20.7221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 17:59:57,185 - INFO - Fold 4, mse = 695.7336, mad = 20.7221\n",
      "2023-08-11 17:59:57,866 - INFO - Fold 4 Epoch 11 Batch 0: Train Loss = 0.7046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 11 Batch 0: Train Loss = 0.7046\n",
      "------------ Save FOLD-BEST model - MSE: 685.5982 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:00:25,345 - INFO - ------------ Save FOLD-BEST model - MSE: 685.5982 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[ 16 174  60   0]\n",
      " [  1 603 273   0]\n",
      " [  0 310 245   0]\n",
      " [  0 110 310   0]]\n",
      "Mean absolute deviation (MAD) = 20.49640894539761\n",
      "Mean squared error (MSE) = 685.598212013758\n",
      "Mean absolute percentage error (MAPE) = 161.70850739916733\n",
      "Cohen kappa score = 0.1824265193370166\n",
      "Fold 4, mse = 685.5982, mad = 20.4964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:00:25,501 - INFO - Fold 4, mse = 685.5982, mad = 20.4964\n",
      "2023-08-11 18:00:26,352 - INFO - Fold 4 Epoch 12 Batch 0: Train Loss = 0.6897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 12 Batch 0: Train Loss = 0.6897\n",
      "Fold 4, mse = 698.4566, mad = 20.5623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:00:54,013 - INFO - Fold 4, mse = 698.4566, mad = 20.5623\n",
      "2023-08-11 18:00:54,873 - INFO - Fold 4 Epoch 13 Batch 0: Train Loss = 0.6708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 13 Batch 0: Train Loss = 0.6708\n",
      "Fold 4, mse = 711.7531, mad = 20.7363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:01:21,752 - INFO - Fold 4, mse = 711.7531, mad = 20.7363\n",
      "2023-08-11 18:01:22,477 - INFO - Fold 4 Epoch 14 Batch 0: Train Loss = 0.7009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 14 Batch 0: Train Loss = 0.7009\n",
      "------------ Save FOLD-BEST model - MSE: 667.5137 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:01:51,111 - INFO - ------------ Save FOLD-BEST model - MSE: 667.5137 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[ 15 168  67   0]\n",
      " [  0 542 335   0]\n",
      " [  0 263 292   0]\n",
      " [  0 108 312   0]]\n",
      "Mean absolute deviation (MAD) = 20.307341140444308\n",
      "Mean squared error (MSE) = 667.5137395255372\n",
      "Mean absolute percentage error (MAPE) = 167.13818940514363\n",
      "Cohen kappa score = 0.17417405189708624\n",
      "Fold 4, mse = 667.5137, mad = 20.3073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:01:51,266 - INFO - Fold 4, mse = 667.5137, mad = 20.3073\n",
      "2023-08-11 18:01:51,979 - INFO - Fold 4 Epoch 15 Batch 0: Train Loss = 0.6778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 15 Batch 0: Train Loss = 0.6778\n",
      "Fold 4, mse = 688.0448, mad = 20.5415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:02:20,024 - INFO - Fold 4, mse = 688.0448, mad = 20.5415\n",
      "2023-08-11 18:02:20,847 - INFO - Fold 4 Epoch 16 Batch 0: Train Loss = 0.6451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 16 Batch 0: Train Loss = 0.6451\n",
      "Fold 4, mse = 696.2148, mad = 20.8643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:02:47,656 - INFO - Fold 4, mse = 696.2148, mad = 20.8643\n",
      "2023-08-11 18:02:48,319 - INFO - Fold 4 Epoch 17 Batch 0: Train Loss = 0.6701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 17 Batch 0: Train Loss = 0.6701\n",
      "Fold 4, mse = 709.4482, mad = 20.9161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:03:14,862 - INFO - Fold 4, mse = 709.4482, mad = 20.9161\n",
      "2023-08-11 18:03:15,653 - INFO - Fold 4 Epoch 18 Batch 0: Train Loss = 0.6349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 18 Batch 0: Train Loss = 0.6349\n",
      "Fold 4, mse = 675.4053, mad = 20.4346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:03:43,388 - INFO - Fold 4, mse = 675.4053, mad = 20.4346\n",
      "2023-08-11 18:03:44,194 - INFO - Fold 4 Epoch 19 Batch 0: Train Loss = 0.6634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 19 Batch 0: Train Loss = 0.6634\n",
      "Fold 4, mse = 700.9921, mad = 20.7560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:04:10,758 - INFO - Fold 4, mse = 700.9921, mad = 20.7560\n",
      "2023-08-11 18:04:11,504 - INFO - Fold 4 Epoch 20 Batch 0: Train Loss = 0.5735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 20 Batch 0: Train Loss = 0.5735\n",
      "Fold 4, epoch 20: Loss = 0.6484 Valid loss = 0.9873 MSE = 717.2393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:04:38,439 - INFO - Fold 4, epoch 20: Loss = 0.6484 Valid loss = 0.9873 MSE = 717.2393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, mse = 717.2393, mad = 20.9360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:04:38,442 - INFO - Fold 4, mse = 717.2393, mad = 20.9360\n",
      "2023-08-11 18:04:39,244 - INFO - Fold 4 Epoch 21 Batch 0: Train Loss = 0.6499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 21 Batch 0: Train Loss = 0.6499\n",
      "Fold 4, mse = 710.1417, mad = 21.2777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:05:06,017 - INFO - Fold 4, mse = 710.1417, mad = 21.2777\n",
      "2023-08-11 18:05:06,765 - INFO - Fold 4 Epoch 22 Batch 0: Train Loss = 0.7185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 22 Batch 0: Train Loss = 0.7185\n",
      "Fold 4, mse = 684.8062, mad = 20.6607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:05:36,414 - INFO - Fold 4, mse = 684.8062, mad = 20.6607\n",
      "2023-08-11 18:05:37,289 - INFO - Fold 4 Epoch 23 Batch 0: Train Loss = 0.6077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 23 Batch 0: Train Loss = 0.6077\n",
      "Fold 4, mse = 711.9572, mad = 20.9546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:06:06,044 - INFO - Fold 4, mse = 711.9572, mad = 20.9546\n",
      "2023-08-11 18:06:06,831 - INFO - Fold 4 Epoch 24 Batch 0: Train Loss = 0.5696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 24 Batch 0: Train Loss = 0.5696\n",
      "Fold 4, mse = 712.8504, mad = 20.8656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:06:35,047 - INFO - Fold 4, mse = 712.8504, mad = 20.8656\n",
      "2023-08-11 18:06:35,796 - INFO - Fold 4 Epoch 25 Batch 0: Train Loss = 0.6355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 25 Batch 0: Train Loss = 0.6355\n",
      "Fold 4, mse = 709.5671, mad = 20.8864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:07:03,630 - INFO - Fold 4, mse = 709.5671, mad = 20.8864\n",
      "2023-08-11 18:07:04,379 - INFO - Fold 4 Epoch 26 Batch 0: Train Loss = 0.6209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 26 Batch 0: Train Loss = 0.6209\n",
      "Fold 4, mse = 726.7003, mad = 21.2231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:07:31,481 - INFO - Fold 4, mse = 726.7003, mad = 21.2231\n",
      "2023-08-11 18:07:32,210 - INFO - Fold 4 Epoch 27 Batch 0: Train Loss = 0.5537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 27 Batch 0: Train Loss = 0.5537\n",
      "Fold 4, mse = 734.4575, mad = 21.3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:07:58,926 - INFO - Fold 4, mse = 734.4575, mad = 21.3500\n",
      "2023-08-11 18:07:59,662 - INFO - Fold 4 Epoch 28 Batch 0: Train Loss = 0.6776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 28 Batch 0: Train Loss = 0.6776\n",
      "Fold 4, mse = 729.1819, mad = 20.9654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:08:27,685 - INFO - Fold 4, mse = 729.1819, mad = 20.9654\n",
      "2023-08-11 18:08:28,464 - INFO - Fold 4 Epoch 29 Batch 0: Train Loss = 0.5668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 29 Batch 0: Train Loss = 0.5668\n",
      "Fold 4, mse = 729.8287, mad = 21.2246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:08:55,173 - INFO - Fold 4, mse = 729.8287, mad = 21.2246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 0\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 1\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 2\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 3\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 4\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 5\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 6\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 7\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 8\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 9\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 10\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 11\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 12\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 13\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 14\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 15\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 16\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 17\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 18\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 19\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 20\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 21\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 22\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 23\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 24\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 25\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 26\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 27\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 28\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 29\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 30\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 31\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 32\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 33\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 34\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 35\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 36\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 37\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 38\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 39\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 40\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 41\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 42\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 43\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 44\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 45\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 46\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 47\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 48\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 49\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 50\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 51\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 52\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 53\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 54\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 55\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 56\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 57\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 58\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 59\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 60\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 61\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 62\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 63\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 64\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 65\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 66\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 67\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n",
      "model_module_idx is 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:08:56,632 - INFO - Fold 5 Epoch 0 Batch 0: Train Loss = 1.0995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 0 Batch 0: Train Loss = 1.0995\n",
      "Fold 5, epoch 0: Loss = 0.9725 Valid loss = 0.9893 MSE = 702.9783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:09:24,885 - INFO - Fold 5, epoch 0: Loss = 0.9725 Valid loss = 0.9893 MSE = 702.9783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save FOLD-BEST model - MSE: 702.9783 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:09:24,887 - INFO - ------------ Save FOLD-BEST model - MSE: 702.9783 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 147 146   0]\n",
      " [  0 339 571   0]\n",
      " [  0 160 361   0]\n",
      " [  0  68 243   0]]\n",
      "Mean absolute deviation (MAD) = 21.807675595065902\n",
      "Mean squared error (MSE) = 702.9782741314889\n",
      "Mean absolute percentage error (MAPE) = 274.43246345652267\n",
      "Cohen kappa score = 0.0762336582608295\n",
      "Fold 5, mse = 702.9783, mad = 21.8077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:09:25,054 - INFO - Fold 5, mse = 702.9783, mad = 21.8077\n",
      "2023-08-11 18:09:25,887 - INFO - Fold 5 Epoch 1 Batch 0: Train Loss = 1.0234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 1 Batch 0: Train Loss = 1.0234\n",
      "------------ Save FOLD-BEST model - MSE: 684.0852 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:09:54,462 - INFO - ------------ Save FOLD-BEST model - MSE: 684.0852 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 205  88   0]\n",
      " [  0 425 485   0]\n",
      " [  0 210 311   0]\n",
      " [  0  84 227   0]]\n",
      "Mean absolute deviation (MAD) = 21.533839443812738\n",
      "Mean squared error (MSE) = 684.0852252525186\n",
      "Mean absolute percentage error (MAPE) = 263.34400474577967\n",
      "Cohen kappa score = 0.10225282138745295\n",
      "Fold 5, mse = 684.0852, mad = 21.5338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:09:54,588 - INFO - Fold 5, mse = 684.0852, mad = 21.5338\n",
      "2023-08-11 18:09:55,317 - INFO - Fold 5 Epoch 2 Batch 0: Train Loss = 0.9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 2 Batch 0: Train Loss = 0.9122\n",
      "Fold 5, mse = 699.9651, mad = 21.3751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:10:24,306 - INFO - Fold 5, mse = 699.9651, mad = 21.3751\n",
      "2023-08-11 18:10:25,177 - INFO - Fold 5 Epoch 3 Batch 0: Train Loss = 0.8027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 3 Batch 0: Train Loss = 0.8027\n",
      "Fold 5, mse = 713.5651, mad = 21.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:10:55,062 - INFO - Fold 5, mse = 713.5651, mad = 21.5500\n",
      "2023-08-11 18:10:56,002 - INFO - Fold 5 Epoch 4 Batch 0: Train Loss = 0.7873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 4 Batch 0: Train Loss = 0.7873\n",
      "------------ Save FOLD-BEST model - MSE: 680.9543 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:11:24,880 - INFO - ------------ Save FOLD-BEST model - MSE: 680.9543 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  1 215  77   0]\n",
      " [  1 545 364   0]\n",
      " [  0 288 233   0]\n",
      " [  0 138 173   0]]\n",
      "Mean absolute deviation (MAD) = 21.00000599072653\n",
      "Mean squared error (MSE) = 680.9542841999543\n",
      "Mean absolute percentage error (MAPE) = 227.77827280097344\n",
      "Cohen kappa score = 0.07591232313441243\n",
      "Fold 5, mse = 680.9543, mad = 21.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:11:25,012 - INFO - Fold 5, mse = 680.9543, mad = 21.0000\n",
      "2023-08-11 18:11:25,762 - INFO - Fold 5 Epoch 5 Batch 0: Train Loss = 0.8236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 5 Batch 0: Train Loss = 0.8236\n",
      "Fold 5, mse = 711.8631, mad = 21.5272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:11:54,082 - INFO - Fold 5, mse = 711.8631, mad = 21.5272\n",
      "2023-08-11 18:11:54,914 - INFO - Fold 5 Epoch 6 Batch 0: Train Loss = 0.8739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 6 Batch 0: Train Loss = 0.8739\n",
      "Fold 5, mse = 725.8438, mad = 21.4966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:12:24,057 - INFO - Fold 5, mse = 725.8438, mad = 21.4966\n",
      "2023-08-11 18:12:24,838 - INFO - Fold 5 Epoch 7 Batch 0: Train Loss = 0.8267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 7 Batch 0: Train Loss = 0.8267\n",
      "Fold 5, mse = 764.8982, mad = 21.9590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:12:53,749 - INFO - Fold 5, mse = 764.8982, mad = 21.9590\n",
      "2023-08-11 18:12:54,599 - INFO - Fold 5 Epoch 8 Batch 0: Train Loss = 0.7736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 8 Batch 0: Train Loss = 0.7736\n",
      "Fold 5, mse = 740.8958, mad = 21.7487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:13:22,487 - INFO - Fold 5, mse = 740.8958, mad = 21.7487\n",
      "2023-08-11 18:13:23,236 - INFO - Fold 5 Epoch 9 Batch 0: Train Loss = 0.6747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 9 Batch 0: Train Loss = 0.6747\n",
      "Fold 5, mse = 730.1785, mad = 21.3787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:13:51,993 - INFO - Fold 5, mse = 730.1785, mad = 21.3787\n",
      "2023-08-11 18:13:52,869 - INFO - Fold 5 Epoch 10 Batch 0: Train Loss = 0.7277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 10 Batch 0: Train Loss = 0.7277\n",
      "Fold 5, epoch 10: Loss = 0.7043 Valid loss = 1.0209 MSE = 727.1272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:14:20,191 - INFO - Fold 5, epoch 10: Loss = 0.7043 Valid loss = 1.0209 MSE = 727.1272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, mse = 727.1272, mad = 21.6276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:14:20,193 - INFO - Fold 5, mse = 727.1272, mad = 21.6276\n",
      "2023-08-11 18:14:20,956 - INFO - Fold 5 Epoch 11 Batch 0: Train Loss = 0.7385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 11 Batch 0: Train Loss = 0.7385\n",
      "Fold 5, mse = 749.8348, mad = 21.6497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:14:49,301 - INFO - Fold 5, mse = 749.8348, mad = 21.6497\n",
      "2023-08-11 18:14:50,017 - INFO - Fold 5 Epoch 12 Batch 0: Train Loss = 0.6740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 12 Batch 0: Train Loss = 0.6740\n",
      "Fold 5, mse = 737.8172, mad = 21.7125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:15:19,047 - INFO - Fold 5, mse = 737.8172, mad = 21.7125\n",
      "2023-08-11 18:15:19,827 - INFO - Fold 5 Epoch 13 Batch 0: Train Loss = 0.6821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 13 Batch 0: Train Loss = 0.6821\n",
      "Fold 5, mse = 740.3772, mad = 21.4476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:15:47,800 - INFO - Fold 5, mse = 740.3772, mad = 21.4476\n",
      "2023-08-11 18:15:48,662 - INFO - Fold 5 Epoch 14 Batch 0: Train Loss = 0.6995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 14 Batch 0: Train Loss = 0.6995\n",
      "Fold 5, mse = 752.9314, mad = 21.6726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:16:16,986 - INFO - Fold 5, mse = 752.9314, mad = 21.6726\n",
      "2023-08-11 18:16:17,809 - INFO - Fold 5 Epoch 15 Batch 0: Train Loss = 0.6955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 15 Batch 0: Train Loss = 0.6955\n",
      "Fold 5, mse = 746.2040, mad = 22.0855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:16:47,526 - INFO - Fold 5, mse = 746.2040, mad = 22.0855\n",
      "2023-08-11 18:16:48,289 - INFO - Fold 5 Epoch 16 Batch 0: Train Loss = 0.6300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 16 Batch 0: Train Loss = 0.6300\n",
      "Fold 5, mse = 745.7671, mad = 22.0448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:17:16,562 - INFO - Fold 5, mse = 745.7671, mad = 22.0448\n",
      "2023-08-11 18:17:17,270 - INFO - Fold 5 Epoch 17 Batch 0: Train Loss = 0.7730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 17 Batch 0: Train Loss = 0.7730\n",
      "Fold 5, mse = 760.7490, mad = 22.1412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:17:44,722 - INFO - Fold 5, mse = 760.7490, mad = 22.1412\n",
      "2023-08-11 18:17:45,540 - INFO - Fold 5 Epoch 18 Batch 0: Train Loss = 0.6407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 18 Batch 0: Train Loss = 0.6407\n",
      "Fold 5, mse = 760.7679, mad = 21.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:18:15,266 - INFO - Fold 5, mse = 760.7679, mad = 21.9960\n",
      "2023-08-11 18:18:16,119 - INFO - Fold 5 Epoch 19 Batch 0: Train Loss = 0.7308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 19 Batch 0: Train Loss = 0.7308\n",
      "Fold 5, mse = 782.7438, mad = 22.3268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:18:44,981 - INFO - Fold 5, mse = 782.7438, mad = 22.3268\n",
      "2023-08-11 18:18:45,792 - INFO - Fold 5 Epoch 20 Batch 0: Train Loss = 0.5613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 20 Batch 0: Train Loss = 0.5613\n",
      "Fold 5, epoch 20: Loss = 0.6511 Valid loss = 1.0763 MSE = 756.9349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:19:15,173 - INFO - Fold 5, epoch 20: Loss = 0.6511 Valid loss = 1.0763 MSE = 756.9349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, mse = 756.9349, mad = 22.3265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:19:15,175 - INFO - Fold 5, mse = 756.9349, mad = 22.3265\n",
      "2023-08-11 18:19:15,915 - INFO - Fold 5 Epoch 21 Batch 0: Train Loss = 0.7723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 21 Batch 0: Train Loss = 0.7723\n",
      "Fold 5, mse = 772.3761, mad = 22.1603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:19:46,013 - INFO - Fold 5, mse = 772.3761, mad = 22.1603\n",
      "2023-08-11 18:19:46,613 - INFO - Fold 5 Epoch 22 Batch 0: Train Loss = 0.5676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 22 Batch 0: Train Loss = 0.5676\n",
      "Fold 5, mse = 787.6836, mad = 22.4469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:20:15,042 - INFO - Fold 5, mse = 787.6836, mad = 22.4469\n",
      "2023-08-11 18:20:15,918 - INFO - Fold 5 Epoch 23 Batch 0: Train Loss = 0.6475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 23 Batch 0: Train Loss = 0.6475\n",
      "Fold 5, mse = 767.2284, mad = 22.2748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:20:44,919 - INFO - Fold 5, mse = 767.2284, mad = 22.2748\n",
      "2023-08-11 18:20:45,741 - INFO - Fold 5 Epoch 24 Batch 0: Train Loss = 0.6730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 24 Batch 0: Train Loss = 0.6730\n",
      "Fold 5, mse = 809.0442, mad = 22.5437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:21:14,139 - INFO - Fold 5, mse = 809.0442, mad = 22.5437\n",
      "2023-08-11 18:21:14,793 - INFO - Fold 5 Epoch 25 Batch 0: Train Loss = 0.5502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 25 Batch 0: Train Loss = 0.5502\n",
      "Fold 5, mse = 794.0545, mad = 22.6509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:21:44,585 - INFO - Fold 5, mse = 794.0545, mad = 22.6509\n",
      "2023-08-11 18:21:45,437 - INFO - Fold 5 Epoch 26 Batch 0: Train Loss = 0.6067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 26 Batch 0: Train Loss = 0.6067\n",
      "Fold 5, mse = 797.9099, mad = 22.6506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:22:14,909 - INFO - Fold 5, mse = 797.9099, mad = 22.6506\n",
      "2023-08-11 18:22:15,679 - INFO - Fold 5 Epoch 27 Batch 0: Train Loss = 0.6085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 27 Batch 0: Train Loss = 0.6085\n",
      "Fold 5, mse = 806.9133, mad = 22.9431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:22:40,384 - INFO - Fold 5, mse = 806.9133, mad = 22.9431\n",
      "2023-08-11 18:22:41,044 - INFO - Fold 5 Epoch 28 Batch 0: Train Loss = 0.5767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 28 Batch 0: Train Loss = 0.5767\n",
      "Fold 5, mse = 807.1086, mad = 22.7119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:23:03,363 - INFO - Fold 5, mse = 807.1086, mad = 22.7119\n",
      "2023-08-11 18:23:04,016 - INFO - Fold 5 Epoch 29 Batch 0: Train Loss = 0.6316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 29 Batch 0: Train Loss = 0.6316\n",
      "Fold 5, mse = 817.5987, mad = 22.8073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 18:23:26,172 - INFO - Fold 5, mse = 817.5987, mad = 22.8073\n",
      "2023-08-11 18:23:26,178 - INFO - mse 610.2307(67.4244)\n",
      "2023-08-11 18:23:26,179 - INFO - mad 19.9865(1.0582)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse 610.2307(67.4244)\n",
      "mad 19.9865(1.0582)\n"
     ]
    }
   ],
   "source": [
    "if target_dataset == 'PD':\n",
    "    n_splits = 5\n",
    "    epochs = 30\n",
    "\n",
    "teacher_flag = True\n",
    "transfer_flag = True\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "if target_dataset == 'PD':    \n",
    "    data_str = 'pd'\n",
    "\n",
    "if teacher_flag:\n",
    "    file_name = './model/pretrained-challenge-front-fill-2'+ data_str\n",
    "else: \n",
    "    file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher'\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "fold_count = 0\n",
    "total_train_loss = []\n",
    "total_valid_loss = []\n",
    "\n",
    "global_best = 10000\n",
    "mse = []\n",
    "mad = []\n",
    "mape = []\n",
    "kappa = []\n",
    "history = []\n",
    "\n",
    "pad_token = np.zeros(input_dim)\n",
    "# begin_time = time.time()\n",
    "\n",
    "for train, test in kfold.split(long_x):\n",
    "        \n",
    "    train_x = [long_x[i] for i in train]\n",
    "    train_y = [long_time[i] for i in train]\n",
    "    train_x_len = [all_x_len[i] for i in train]\n",
    "    #train_static = [long_static[i] for i in train]\n",
    "    \n",
    "    train_x, train_y, train_x_len = get_n2n_data(train_x, train_y, train_x_len)\n",
    "    if len(train_x) % 256 == 1:\n",
    "        print(len(train_x))\n",
    "        print('wrong squeeze!')\n",
    "\n",
    "# for train, test in kfold.split(long_x):\n",
    "for train, test in kfold.split(long_x):\n",
    "    if reverse:\n",
    "        temp = train\n",
    "        train = test\n",
    "        test = temp\n",
    "    \n",
    "    model = distcare_target(input_dim = input_dim,output_dim=output_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, hidden_dim=hidden_dim).to(device)\n",
    "    \n",
    "    if transfer_flag:\n",
    "        checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu'))\n",
    "        pretrain_dict = checkpoint['net']\n",
    "        model_dict = model.state_dict()\n",
    "        pretrain_dict = transfer_gru_dict(pretrain_dict, model_dict,latest_idx, common_len)\n",
    "        model_dict.update(pretrain_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    fold_count += 1\n",
    "#     print(train)\n",
    "\n",
    "    \n",
    "    train_x = [long_x[i] for i in train]\n",
    "    train_y = [long_time[i] for i in train]\n",
    "    train_x_len = [all_x_len[i] for i in train]\n",
    "    #train_static = [long_static[i] for i in train]\n",
    "    \n",
    "    train_x, train_y, train_x_len = get_n2n_data(train_x, train_y, train_x_len)\n",
    "    \n",
    "    test_x = [long_x[i] for i in test]\n",
    "    test_y = [long_time[i] for i in test]\n",
    "    test_x_len = [all_x_len[i] for i in test]\n",
    "    #test_static = [long_static[i] for i in test]\n",
    "    \n",
    "    test_x, test_y, test_x_len = get_n2n_data(test_x, test_y, test_x_len)\n",
    "    \n",
    "    if not os.path.exists('./model/'+data_str):\n",
    "        os.mkdir('./model/'+data_str)\n",
    "        \n",
    "    if transfer_flag:\n",
    "        target_file_name = './model/'+data_str+'/distcare-trans-'+str(n_splits)+'-fold-LOS-regression' + str(fold_count)#4114\n",
    "    else:\n",
    "        target_file_name = './model/'+data_str+'/distcare-no-trans-'+str(n_splits)+'-fold-LOS-regression' + str(fold_count)#4114\n",
    "    \n",
    "    fold_train_loss = []\n",
    "    fold_valid_loss = []\n",
    "    best_mse = 10000\n",
    "    best_mad = 0\n",
    "    best_mape = 0\n",
    "    best_kappa = 0\n",
    "    \n",
    "    for each_epoch in range(epochs):\n",
    "       \n",
    "        \n",
    "        epoch_loss = []\n",
    "        counter_batch = 0\n",
    "        model.train()  \n",
    "        \n",
    "        for step, (batch_x, batch_y, batch_lens) in enumerate(ckd_batch_iter(train_x, train_y, train_x_len, batch_size, shuffle=True)):  \n",
    "            optimizer.zero_grad()\n",
    "            batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "\n",
    "            masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "            opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "\n",
    "            MSE_Loss = get_re_loss(opt, batch_y.unsqueeze(-1))\n",
    "\n",
    "#             model_loss = pred_loss + 1e7*decov_loss\n",
    "            model_loss = MSE_Loss\n",
    "\n",
    "            loss = model_loss\n",
    "\n",
    "            epoch_loss.append(MSE_Loss.cpu().detach().numpy())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % 50 == 0:\n",
    "                print('Fold %d Epoch %d Batch %d: Train Loss = %.4f'%(fold_count,each_epoch, step, loss.cpu().detach().numpy()))\n",
    "                logger.info('Fold %d Epoch %d Batch %d: Train Loss = %.4f'%(fold_count,each_epoch, step, loss.cpu().detach().numpy()))\n",
    "            \n",
    "        epoch_loss = np.mean(epoch_loss)\n",
    "        fold_train_loss.append(epoch_loss)\n",
    "\n",
    "        #Validation\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_pred_flatten = []\n",
    "        y_true_flatten = []\n",
    "        outcome_pred_flatten = []\n",
    "        outcome_true_flatten = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            valid_true = []\n",
    "            valid_pred = []\n",
    "            for batch_x, batch_y, batch_lens in ckd_batch_iter(test_x, test_y, test_x_len, batch_size):\n",
    "                batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "                batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "                batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "                masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "               \n",
    "                opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "                \n",
    "                MSE_Loss = get_re_loss(opt, batch_y.unsqueeze(-1))\n",
    "                \n",
    "                valid_loss.append(MSE_Loss.cpu().detach().numpy())\n",
    "\n",
    "                y_pred_flatten += [reverse_los(x, los_info) / 30 for x in list(opt.cpu().detach().numpy().flatten())]\n",
    "                y_true_flatten += [reverse_los(x, los_info) / 30 for x in list(batch_y.cpu().numpy().flatten())]\n",
    "            \n",
    "\n",
    "            valid_loss = np.mean(valid_loss)\n",
    "            fold_valid_loss.append(valid_loss)\n",
    "            ret = metrics.print_metrics_regression(y_true_flatten, y_pred_flatten, verbose=0)\n",
    "            history.append(ret)\n",
    "            #print()\n",
    "\n",
    "            if each_epoch % 10 == 0:\n",
    "                print('Fold %d, epoch %d: Loss = %.4f Valid loss = %.4f MSE = %.4f' % (\n",
    "                    fold_count, each_epoch, fold_train_loss[-1], fold_valid_loss[-1], ret['mse']), flush=True)\n",
    "                logger.info('Fold %d, epoch %d: Loss = %.4f Valid loss = %.4f MSE = %.4f' % (\n",
    "                    fold_count, each_epoch, fold_train_loss[-1], fold_valid_loss[-1], ret['mse']))\n",
    "                # metrics.print_metrics_regression(y_true_flatten, y_pred_flatten)\n",
    "                \n",
    "            cur_mse = ret['mse']\n",
    "            if cur_mse < best_mse:\n",
    "                print('------------ Save FOLD-BEST model - MSE: %.4f ------------' % cur_mse, flush=True)\n",
    "                logger.info('------------ Save FOLD-BEST model - MSE: %.4f ------------' % cur_mse)\n",
    "                metrics.print_metrics_regression(y_true_flatten, y_pred_flatten)\n",
    "                best_mse = cur_mse\n",
    "                best_mad = ret['mad']\n",
    "                state = {\n",
    "                    'net': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': each_epoch\n",
    "                }\n",
    "                torch.save(state, target_file_name + '_' + str(fold_count))\n",
    "\n",
    "                if cur_mse < global_best:\n",
    "                    global_best = cur_mse\n",
    "                    state = {\n",
    "                        'net': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'epoch': each_epoch\n",
    "                    }\n",
    "                    torch.save(state, target_file_name)\n",
    "                    print('------------ Save best model - MSE: %.4f ------------' % cur_mse, flush=True)\n",
    "                    logger.info('------------ Save best model - MSE: %.4f ------------' % cur_mse)\n",
    "\n",
    "        print('Fold %d, mse = %.4f, mad = %.4f' % (fold_count, ret['mse'], ret['mad']), flush=True)\n",
    "        logger.info('Fold %d, mse = %.4f, mad = %.4f' % (fold_count, ret['mse'], ret['mad']))\n",
    "\n",
    "    mse.append(best_mse)\n",
    "    mad.append(best_mad)\n",
    "    total_train_loss.append(fold_train_loss)\n",
    "    total_valid_loss.append(fold_valid_loss)\n",
    "\n",
    "\n",
    "print('mse %.4f(%.4f)' % (np.mean(mse), np.std(mse)))\n",
    "print('mad %.4f(%.4f)' % (np.mean(mad), np.std(mad)))\n",
    "logger.info('mse %.4f(%.4f)' % (np.mean(mse), np.std(mse)))\n",
    "logger.info('mad %.4f(%.4f)' % (np.mean(mad), np.std(mad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
