{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "import pickle5 as pickle\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import logging\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from utils import utils\n",
    "from utils.readers import InHospitalMortalityReader\n",
    "from utils.preprocessing import Discretizer, Normalizer\n",
    "from utils import metrics\n",
    "from utils import common_utils\n",
    "from torch.autograd import Function\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = 'PD' \n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED) #numpy\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED) # cpu\n",
    "torch.cuda.manual_seed(RANDOM_SEED) #gpu\n",
    "torch.backends.cudnn.deterministic=True # cudnn\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() == True else 'cpu')\n",
    "# print(\"available device: {}\".format(device))\n",
    "reverse = False\n",
    "model_name = 'codats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reverse:\n",
    "    file_name = 'log_file' + '_' + model_name + '_' + target_dataset + '_' + 'reverse' + '.log'\n",
    "else:\n",
    "    file_name = 'log_file' + '_' + model_name + '_' + target_dataset + '.log'\n",
    "def get_logger(name, file_name):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # 以下两行是为了在jupyter notebook 中不重复输出日志\n",
    "    if logger.root.handlers:\n",
    "        logger.root.handlers[0].setLevel(logging.WARNING)\n",
    " \n",
    "    handler_stdout = logging.StreamHandler()\n",
    "    handler_stdout.setLevel(logging.INFO)\n",
    "    handler_stdout.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    # logger.addHandler(handler_stdout)\n",
    " \n",
    "    handler_file = logging.FileHandler(filename=file_name, mode='w', encoding='utf-8')\n",
    "    handler_file.setLevel(logging.DEBUG)\n",
    "    handler_file.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler_file)\n",
    " \n",
    "    return logger\n",
    "\n",
    "logger = get_logger(__name__,file_name)\n",
    "\n",
    "logger.debug('这是希望输出的debug内容')\n",
    "logger.info('这是希望输出的info内容')\n",
    "logger.warning('这是希望输出的warning内容')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get source data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n2n_data(x, y, x_len):\n",
    "    length = len(x)\n",
    "    assert length == len(y)\n",
    "    assert length == len(x_len)\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    new_x_len = []\n",
    "    for i in range(length):\n",
    "        for j in range(len(x[i])):\n",
    "            new_x.append(x[i][:j+1])\n",
    "            new_y.append(y[i][j])\n",
    "            new_x_len.append(j+1)\n",
    "    return new_x, new_y, new_x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data_path = './data/Challenge/'\n",
    "small_part = False\n",
    "arg_timestep = 1.0\n",
    "batch_size = 256\n",
    "epochs = 100\n",
    "all_x_source = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "all_y_source = pickle.load(open(source_data_path + 'new_y_front_fill.dat', 'rb'))\n",
    "all_names_source = pickle.load(open(source_data_path + 'new_name.dat', 'rb'))\n",
    "static_source = pickle.load(open(source_data_path + 'new_demo_front_fill.dat', 'rb'))\n",
    "mask_x_source = pickle.load(open(source_data_path + 'new_mask_x.dat', 'rb'))\n",
    "mask_demo_source = pickle.load(open(source_data_path + 'new_mask_demo.dat', 'rb'))\n",
    "all_x_len_source = [len(i) for i in all_x_source]\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    subset_idx_source = [31, 29, 28, 33, 25, 18, 7, 21, 16, 15, 19, 17, 24, 3, 5, 0]\n",
    "elif target_dataset == 'TJ':\n",
    "    subset_idx_source = [27, 29, 18, 16, 26, 33, 28, 31, 32, 15, 11, 25, 21, 20, 9, 17, 30, 19]\n",
    "elif target_dataset == 'HM':\n",
    "    subset_idx_source = [0, 1, 2, 3, 5, 9, 11, 12, 13, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
    "\n",
    "subset_cnt = len(subset_idx_source)\n",
    "other_idx = []\n",
    "for i in range(len(all_x_source[0][0])):\n",
    "    if i not in subset_idx_source:\n",
    "        other_idx.append(i)\n",
    "\n",
    "for i in range(len(all_x_source)): #将共同特征移动到最开始，非共同特征移动到末尾\n",
    "    cur = np.array(all_x_source[i], dtype=float)\n",
    "    cur_mask = np.array(mask_x_source[i])\n",
    "    cur_subset = cur[:, subset_idx_source]\n",
    "    cur_other = cur[:, other_idx]\n",
    "    cur_mask_subset = cur_mask[:, subset_idx_source]\n",
    "    cur_mask_other = cur_mask[:, other_idx]\n",
    "    all_x_source[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    mask_x_source[i] = np.concatenate((cur_mask_subset, cur_mask_other), axis=1).tolist()\n",
    "\n",
    "\n",
    "train_num_source =int( len(all_x_source) * 0.8) + 1\n",
    "logger.info(train_num_source)\n",
    "dev_num_source =int( len(all_x_source) * 0.1) + 1\n",
    "logger.info(dev_num_source)\n",
    "test_num_source =int( len(all_x_source) * 0.1)\n",
    "logger.info(test_num_source)\n",
    "assert(train_num_source+dev_num_source+test_num_source == len(all_x_source))\n",
    "\n",
    "train_x_source = []\n",
    "train_y_source = []\n",
    "train_names_source = []\n",
    "train_static_source = []\n",
    "train_x_len_source = []\n",
    "train_mask_x_source = []\n",
    "for idx in range(train_num_source):\n",
    "    train_x_source.append(all_x_source[idx])\n",
    "    train_y_source.append(int(all_y_source[idx][-1]))\n",
    "    train_names_source.append(all_names_source[idx])\n",
    "    train_static_source.append(static_source[idx])\n",
    "    train_x_len_source.append(all_x_len_source[idx])\n",
    "    train_mask_x_source.append(mask_x_source[idx])\n",
    "\n",
    "dev_x_source = []\n",
    "dev_y_source = []\n",
    "dev_names_source = []\n",
    "dev_static_source = []\n",
    "dev_x_len_source = []\n",
    "dev_mask_x_source = []\n",
    "for idx in range(train_num_source, train_num_source + dev_num_source):\n",
    "    dev_x_source.append(all_x_source[idx])\n",
    "    dev_y_source.append(int(all_y_source[idx][-1]))\n",
    "    dev_names_source.append(all_names_source[idx])\n",
    "    dev_static_source.append(static_source[idx])\n",
    "    dev_x_len_source.append(all_x_len_source[idx])\n",
    "    dev_mask_x_source.append(mask_x_source[idx])\n",
    "\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "test_names = []\n",
    "test_static = []\n",
    "test_x_len = []\n",
    "test_mask_x = []\n",
    "for idx in range(train_num_source + dev_num_source, train_num_source + dev_num_source + test_num_source):\n",
    "    test_x.append(all_x_source[idx])\n",
    "    test_y.append(int(all_y_source[idx][-1]))\n",
    "    test_names.append(all_names_source[idx])\n",
    "    test_static.append(static_source[idx])\n",
    "    test_x_len.append(all_x_len_source[idx])\n",
    "    test_mask_x.append(mask_x_source[idx])\n",
    "\n",
    "\n",
    "assert(len(train_x_source) == train_num_source)\n",
    "assert(len(dev_x_source) == dev_num_source)\n",
    "assert(len(test_x) == test_num_source)\n",
    "\n",
    "long_x_source = all_x_source\n",
    "long_y_source = [y[-1] for y in all_y_source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(y_pred, y_true):\n",
    "    loss = torch.nn.BCELoss()\n",
    "    return loss(y_pred, y_true)\n",
    "def get_re_loss(y_pred, y_true):\n",
    "    loss = torch.nn.MSELoss()\n",
    "    return loss(y_pred, y_true)\n",
    "def get_kl_loss(x_pred, x_target):\n",
    "    loss = torch.nn.KLDivLoss(reduce=True, size_average=True)\n",
    "    return loss(x_pred, x_target)\n",
    "def get_wass_dist(x_pred, x_target):\n",
    "    m1 = torch.mean(x_pred, dim=0)\n",
    "    m2 = torch.mean(x_target, dim=0)\n",
    "    v1 = torch.var(x_pred, dim=0)\n",
    "    v2 = torch.var(x_target, dim=0)\n",
    "    p1 = torch.sum(torch.pow((m1 - m2), 2))\n",
    "    p2 = torch.sum(torch.pow(torch.pow(v1, 1/2) - torch.pow(v2, 1/2), 2))\n",
    "    return torch.pow(p1+p2, 1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sents(sents, pad_token):\n",
    "#     print(f'len(pad_token) is {len(pad_token)}')\n",
    "#     print(f'sents is {sents}')\n",
    "\n",
    "    sents_padded = []\n",
    "\n",
    "    max_length = max([len(_) for _ in sents])\n",
    "    for i in sents:\n",
    "        padded = list(i) + [pad_token]*(max_length-len(i))\n",
    "#         print(f'padded is {padded}')\n",
    "        sents_padded.append(np.array(padded))\n",
    "\n",
    "\n",
    "    return np.array(sents_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(x, y, lens, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    # batch_num = math.ceil(len(x) / batch_size) # 向下取整\n",
    "    batch_num = len(x) // batch_size if len(x) % batch_size == 0 else len(x) // batch_size + 1\n",
    "    # print(f\"len(x) is {len(x)}, len(y) is {len(y)}, len(lens) is {len(lens)}, batch_size is {batch_size}, batch_num is {batch_num}\")\n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        if (i + 1) * batch_size  < len(x):\n",
    "            indices = index_array[i * batch_size: (i + 1) * batch_size] #  fetch out all the induces\n",
    "        else:\n",
    "            indices = index_array[i * batch_size: ]\n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx],lens[idx]))\n",
    "       \n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    \n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "        batch_lens = [e[2] for e in examples]\n",
    "\n",
    "        yield batch_x, batch_y, batch_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_to_mask(length, max_len=None, dtype=None):\n",
    "    \"\"\"length: B.\n",
    "    return B x max_len.\n",
    "    If max_len is None, then max of length will be used.\n",
    "    \"\"\"\n",
    "    assert len(length.shape) == 1, 'Length shape should be 1 dimensional.'\n",
    "    max_len = max_len or length.max().item()\n",
    "    mask = torch.arange(max_len, device=length.device,\n",
    "                        dtype=length.dtype).expand(len(length), max_len) < length.unsqueeze(1)\n",
    "    if dtype is not None:\n",
    "        mask = torch.as_tensor(mask, dtype=dtype, device=length.device)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class codats(nn.Module):\n",
    "    def __init__(self, common_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(codats, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = common_dim\n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.layers = nn.Sequential(\n",
    "                nn.Conv1d(1, hidden_dim, kernel_size=8, padding=4, bias=False),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Conv1d(hidden_dim, 2*hidden_dim, kernel_size=5, padding=2, bias=False),\n",
    "                nn.BatchNorm1d(2*hidden_dim),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Conv1d(2*hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.AdaptiveAvgPool1d(1),\n",
    "            )\n",
    "        \n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.output = nn.Linear(self.input_dim, self.output_dim)\n",
    "\n",
    "        # adversal方法中的域分类器  \n",
    "        self.domain_classifier = nn.Sequential()\n",
    "        self.domain_classifier.add_module('d_fc1', nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(self.hidden_dim))\n",
    "        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))\n",
    "        self.domain_classifier.add_module('d_fc2', nn.Linear(hidden_dim, 2))\n",
    "        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.FC_embed = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.to_MMD = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    def forward(self, input, lens, alpha, is_teacher):\n",
    "        lens = lens.to('cpu')\n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "        \n",
    "        codats_embeded_input = self.layers(input[:,:,0].squeeze().unsqueeze(1)).unsqueeze(1)\n",
    "        # print(f'input[:,:,i].shape is {input[:,:,0].shape}')\n",
    "        for i in range(1, feature_dim):\n",
    "            embeded_input = self.layers(input[:,:,i].squeeze().unsqueeze(1)).unsqueeze(1)\n",
    "            codats_embeded_input = torch.cat((codats_embeded_input, embeded_input), 1)\n",
    "            # print(f'docats_embeded_input.shape is {codats_embeded_input.shape}')\n",
    "        codats_embeded_input = codats_embeded_input.squeeze()\n",
    "        # print(f'codats_embeded_input.shape is {codats_embeded_input.shape}')\n",
    "        # GRU_embeded_input = self.GRUs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "        # for i in range(feature_dim-1):\n",
    "        #     embeded_input = self.GRUs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "        #     GRU_embeded_input = torch.cat((GRU_embeded_input, embeded_input), 1)\n",
    "        # print(f\"GRU_embeded_input.shape is {GRU_embeded_input.shape}\")\n",
    "        \n",
    "\n",
    "\n",
    "        if is_teacher: # 来自源数据集\n",
    "            common_input = codats_embeded_input[:, 0, :]\n",
    "            for i in range(1, feature_dim):\n",
    "                common_input = common_input + codats_embeded_input[:, i, :]  \n",
    "            # print(f\"common_input1.shape is {common_input.shape}\")\n",
    "            common_input = torch.squeeze(common_input, 1) # batch * hidden\n",
    "            reverse_input = ReverseLayerF.apply(common_input, alpha)\n",
    "            # print(f\"common_input2.shape is {common_input.shape}\")\n",
    "            domain_output = self.domain_classifier(reverse_input)\n",
    "\n",
    "            posi_input = self.dropout(codats_embeded_input) # batch_size * d_input + d_input_diff * hidden_dim\n",
    "            \n",
    "            contexts = self.Linear(posi_input).squeeze()# b i\n",
    "            output = self.output(self.dropout(contexts))# b 1\n",
    "            output = self.sigmoid(output)\n",
    "            return output, domain_output, contexts\n",
    "        else: # 来自目标数据集，主要是为了混淆domain classifier\n",
    "            common_input = codats_embeded_input[:, 0, :]\n",
    "            for i in range(1, feature_dim):\n",
    "                common_input = common_input + codats_embeded_input[:, i, :]  \n",
    "            common_input = torch.squeeze(common_input, 1) # batch * hidden\n",
    "            reverse_input = ReverseLayerF.apply(common_input, alpha)\n",
    "            domain_output = self.domain_classifier(reverse_input)\n",
    "            return domain_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "def getSplitData(x, lens, y):\n",
    "    train_num =int( len(x) * 0.8) + 1\n",
    "    dev_num =int( len(x) * 0.1) + 1\n",
    "    test_num = len(x) - train_num - dev_num\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    train_len = []\n",
    "    for idx in range(train_num):\n",
    "        train_x.append(x[idx])\n",
    "        train_y.append(int(y[idx][-1]))\n",
    "        train_len.append(lens[idx])\n",
    "\n",
    "    dev_x = []\n",
    "    dev_y = []\n",
    "    dev_len = []\n",
    "    for idx in range(train_num, train_num + dev_num):\n",
    "        dev_x.append(x[idx])\n",
    "        dev_y.append(int(y[idx][-1]))\n",
    "        dev_len.append(lens[idx])\n",
    "\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    test_len = []\n",
    "\n",
    "    for idx in range(train_num + dev_num, train_num + dev_num + test_num):\n",
    "        test_x.append(x[idx])\n",
    "        test_y.append(int(y[idx][-1]))\n",
    "        test_len.append(lens[idx])\n",
    "    return train_x, train_y, train_len, dev_x, dev_y, dev_len, test_x, test_y, test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"load target data\")\n",
    "if target_dataset == 'PD':\n",
    "    data_path = './data/PD/'\n",
    "    all_x_target = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_time_target = pickle.load(open(data_path + 'y_z.pkl', 'rb'))\n",
    "    all_x_len_target = [len(i) for i in all_x_target]\n",
    "\n",
    "    subset_idx_target = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    other_idx_target = list(range(69))\n",
    "    for i in subset_idx_target:\n",
    "        other_idx_target.remove(i)\n",
    "    for i in range(len(all_x_target)):\n",
    "        cur = np.array(all_x_target[i], dtype=float)\n",
    "        cur_subset = cur[:, subset_idx_target]\n",
    "        cur_other = cur[:, other_idx_target]\n",
    "        all_x_target[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "elif target_dataset == 'TJ':\n",
    "    data_path = './data/Tongji/'\n",
    "    all_x_target = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y_target = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time_target = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len_target = [len(i) for i in all_x_target]\n",
    "\n",
    "    for i in range(len(all_time_target)):\n",
    "        for j in range(len(all_time_target[i])):\n",
    "            all_time_target[i][j] = all_time_target[i][j][-1]\n",
    "            all_y_target[i][j] = all_y_target[i][j][0]\n",
    "\n",
    "    subset_idx_target = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "    other_idx_target = list(range(75))\n",
    "    for i in subset_idx_target:\n",
    "        other_idx_target.remove(i)\n",
    "    for i in range(len(all_x_target)):\n",
    "        cur = np.array(all_x_target[i], dtype=float)\n",
    "        cur_subset = cur[:, subset_idx_target]\n",
    "        cur_other = cur[:, other_idx_target]\n",
    "        all_x_target[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "elif target_dataset == 'HM':\n",
    "    data_path = './data/CDSL/'\n",
    "    all_x_target = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y_target = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time_target = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len_target = [len(i) for i in all_x_target]\n",
    "\n",
    "    for i in range(len(all_time_target)):\n",
    "        for j in range(len(all_time_target[i])):\n",
    "            all_time_target[i][j] = all_time_target[i][j][-1]\n",
    "            all_y_target[i][j] = all_y_target[i][j][0]\n",
    "\n",
    "    subset_idx_target = [5, 6, 4, 2, 3, 48, 79, 76, 87, 25, 30, 31, 18, 43, 58, 66, 40, 57, 23, 92, 50, 54, 91, 60, 39, 81]\n",
    "    other_idx_target= list(range(99))\n",
    "    for i in subset_idx_target:\n",
    "        other_idx_target.remove(i)\n",
    "    for i in range(len(all_x_target)):\n",
    "        cur = np.array(all_x_target[i], dtype=float)\n",
    "        cur_subset = cur[:, subset_idx_target]\n",
    "        cur_other = cur[:, other_idx_target]\n",
    "    #     tar_all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "        all_x_target[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    \n",
    "if target_dataset == 'PD':\n",
    "    all_x_target = all_x_target\n",
    "    all_y_target = all_time_target\n",
    "elif  target_dataset == 'HM' or target_dataset == 'TJ':\n",
    "    examples = []\n",
    "    for idx in range(len(all_x_target)):\n",
    "        examples.append((all_x_target[idx], all_y_target[idx], all_time_target[idx], all_x_len_target[idx]))\n",
    "    examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    all_x_target = [e[0] for e in examples]\n",
    "    all_y_target = [e[1] for e in examples]\n",
    "    all_time_target = [e[2] for e in examples]\n",
    "    all_x_len_target = [e[3] for e in examples]\n",
    "\n",
    "num_source = len(all_x_source)\n",
    "num_target = len(all_x_target)\n",
    "# print(target_dataset,len(all_x_target), len(all_x_target[0]),len(all_x_target[0][0]))\n",
    "all_x_target_confuse = []\n",
    "all_x_len_target_confuse = []\n",
    "all_y_target_confuse = []\n",
    "all_x_source_confuse = []\n",
    "all_x_len_source_confuse = []\n",
    "all_y_source_confuse = []\n",
    "repeat_times = 0\n",
    "\n",
    "if num_source < num_target:\n",
    "    all_x_target_confuse = all_x_target\n",
    "    all_y_target_confuse = all_y_target\n",
    "    all_x_len_target_confuse = all_x_len_target\n",
    "    while repeat_times * num_source < num_target:\n",
    "        all_x_source_confuse = all_x_source_confuse + all_x_source\n",
    "        all_x_len_source_confuse = all_x_len_source_confuse + all_x_len_source\n",
    "        all_y_source_confuse =  all_y_source_confuse + all_y_source\n",
    "        repeat_times = repeat_times + 1\n",
    "    all_x_source_confuse = all_x_source_confuse[:num_target]\n",
    "    all_x_len_source_confuse = all_x_len_source_confuse[:num_target]\n",
    "    all_y_source_confuse = all_y_source_confuse[:num_target]\n",
    "elif num_target < num_source:\n",
    "    all_x_source_confuse = all_x_source\n",
    "    all_x_len_source_confuse = all_x_len_source\n",
    "    all_y_source_confuse = all_y_source\n",
    "    while repeat_times * num_target < num_source:\n",
    "        all_x_target_confuse = all_x_target_confuse + all_x_target\n",
    "        all_x_len_target_confuse = all_x_len_target_confuse + all_x_len_target\n",
    "        all_y_target_confuse = all_y_target_confuse + all_y_target\n",
    "        repeat_times = repeat_times + 1\n",
    "    all_x_target_confuse = all_x_target_confuse[:num_source]\n",
    "    all_x_len_target_confuse = all_x_len_target_confuse[:num_source]\n",
    "    all_y_target_confuse = all_y_target_confuse[:num_source]\n",
    "\n",
    "# print(f\"len(all_x_source_confuse) is {len(all_x_source_confuse)}, len(all_x_target_confuse) is {len(all_x_target_confuse)}\")\n",
    "\n",
    "#todo 划分train、dev、test \n",
    "# all_x_source_confuse = pad_sents(all_x_source_confuse, pad_token_source)\n",
    "# all_x_target_confuse = pad_sents(all_x_target_confuse, pad_token_target)\n",
    "train_x_source_confuse, train_y_source_confuse, train_len_source_confuse, dev_x_source_confuse, dev_y_source_confuse, dev_len_source_confuse, test_x_source_confuse,\\\n",
    "test_y_source_confuse, test_len_source_confuse = getSplitData(all_x_source_confuse, all_x_len_source_confuse, all_y_source_confuse)\n",
    "\n",
    "train_x_target_confuse, train_y_target_confuse, train_len_target_confuse, dev_x_target_confuse, dev_y_target_confuse, dev_len_target_confuse, test_x_target_confuse,\\\n",
    "test_y_target_confuse, test_len_target_confuse = getSplitData(all_x_target_confuse, all_x_len_target_confuse, all_y_target_confuse)\n",
    "\n",
    "# long_x_source = all_x_source\n",
    "# long_y_source = [y[-1] for y in all_y_source]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_source = np.zeros(34)\n",
    "if target_dataset == 'PD':\n",
    "    pad_token_target = np.zeros(69)\n",
    "elif target_dataset == 'TJ':\n",
    "    pad_token_target = np.zeros(75)\n",
    "elif target_dataset == 'HM':\n",
    "    pad_token_target = np.zeros(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 256\n",
    "common_dim = subset_cnt \n",
    "hidden_dim = 64\n",
    "d_model = 64\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1\n",
    "model_student = codats(common_dim = common_dim, hidden_dim = hidden_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, output_dim = output_dim).to(device)\n",
    "optimizer_student = torch.optim.Adam(model_student.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskLoss(nn.Module):\n",
    "    def __init__(self, task_num=2):\n",
    "        super(MultitaskLoss, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.alpha = nn.Parameter(torch.ones((task_num)), requires_grad=True)\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.kl = nn.KLDivLoss(reduce=True, size_average=True)\n",
    "\n",
    "    def forward(self, opt_student, batch_y, emb_student, emb_teacher, tar_source, tar_tar):\n",
    "        BCE_Loss = self.bce(opt_student, batch_y)\n",
    "        emb_Loss = self.kl(emb_student, emb_teacher)\n",
    "        return BCE_Loss * self.alpha[0] + emb_Loss * self.alpha[1]\n",
    "\n",
    "def get_multitask_loss(opt_student, batch_y, emb_student, emb_teacher):\n",
    "    mtl = MultitaskLoss(task_num=3)\n",
    "    return mtl(opt_student, batch_y, emb_student, emb_teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_dataset == 'PD':\n",
    "    data_str = 'pd'\n",
    "elif target_dataset == 'TJ':\n",
    "    data_str = 'covid'\n",
    "elif target_dataset == 'HM':\n",
    "    data_str = 'spain'\n",
    "\n",
    "\n",
    "# if teacher_flag:\n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str\n",
    "# else: \n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher'\n",
    "\n",
    "file_name = './model/pretrained-dann'+ data_str;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_source_iter) is 32269, len(train_target_iter) is 32269, steps is 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch 0: Train Loss = 1.7484\n",
      "Epoch 0 Batch 20: Train Loss = 1.5566\n",
      "Epoch 0 Batch 40: Train Loss = 1.5362\n",
      "Epoch 0 Batch 60: Train Loss = 1.5054\n",
      "Epoch 0 Batch 80: Train Loss = 1.4599\n",
      "Epoch 0 Batch 100: Train Loss = 1.4356\n",
      "Epoch 0 Batch 120: Train Loss = 1.4167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 5 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45742/2519713225.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mdomain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mopt_student\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_domain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_student\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_student\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msubset_cnt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubset_cnt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;31m# emb_teacher = torch.tensor(dev_teacher_emb[step], dtype=torch.float32).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0memb_student\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_student\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 5 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "# Training dann model\n",
    "# If you don't want to train Student Model:\n",
    "# - The pretrained student model is in direcrtory './model/', and can be directly loaded, \n",
    "# - Simply skip this cell and load the model to validate on Dev Dataset.\n",
    "\n",
    "logger.info('Training Student')\n",
    "teacher_flag = False\n",
    "epochs = 30\n",
    "total_train_loss = []\n",
    "total_valid_loss = []\n",
    "global_best = 0\n",
    "auroc = []\n",
    "auprc = []\n",
    "minpse = []\n",
    "history = []\n",
    "# begin_time = time.time()\n",
    "best_auroc = 0\n",
    "best_auprc = 0\n",
    "best_minpse = 0\n",
    "best_total_loss = 0x3f3f3f3f\n",
    "loss_domain = torch.nn.NLLLoss()\n",
    "loss_predict = torch.nn.MSELoss()\n",
    "loss_embed = nn.KLDivLoss(reduce=True, size_average=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'len(train_source_iter) is {len(train_x_source_confuse)}, len(train_target_iter) is {len(train_x_target_confuse)}, steps is {len(train_x_source_confuse) // batch_size + 1}')\n",
    "\n",
    "\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    train_source_iter = batch_iter(train_x_source_confuse, train_y_source_confuse, train_len_source_confuse, batch_size=batch_size)\n",
    "    dev_source_iter = batch_iter(dev_x_source_confuse, dev_y_source_confuse, dev_len_source_confuse, batch_size=batch_size)\n",
    "    test_source_iter = batch_iter(test_x_source_confuse, test_y_source_confuse, test_len_source_confuse, batch_size=batch_size)\n",
    "    train_target_iter = batch_iter(train_x_target_confuse, train_y_target_confuse, train_len_target_confuse, batch_size=batch_size)\n",
    "    dev_target_iter = batch_iter(dev_x_target_confuse, dev_y_target_confuse, dev_len_target_confuse, batch_size=batch_size)\n",
    "    test_target_iter = batch_iter(test_x_target_confuse, test_y_target_confuse, test_len_target_confuse, batch_size=batch_size)\n",
    "    epoch_loss = []\n",
    "    counter_batch = 0\n",
    "    model_student.train()  \n",
    "    steps = len(train_x_source_confuse) // batch_size + 1 if len(train_x_source_confuse) % batch_size != 0 else len(train_x_source_confuse) // batch_size\n",
    "    for step in range(steps):\n",
    "        # -----source_domain--------\n",
    "        batch_x, batch_y, batch_lens= next(train_source_iter)\n",
    "        p = float(step + each_epoch * steps) / epochs / steps\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        optimizer_student.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token_source), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        # batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "        # opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "        domain_label = torch.zeros(min(batch_size, batch_x.shape[0])).long().to(device)\n",
    "        opt_student, opt_domain, emb_student = model_student(batch_x[:,:,:subset_cnt], batch_lens, alpha, True)\n",
    "        emb_student = F.log_softmax(emb_student, dim=1)\n",
    "        err_predict = loss_predict(opt_student, batch_y)\n",
    "        err_domain1 = loss_domain(opt_domain, domain_label)\n",
    "            # loss = get_multitask_loss(opt_student, batch_y.unsqueeze(-1), emb_student, emb_teacher)\n",
    "\n",
    "        # -----target_domain--------\n",
    "        batch_x, batch_y, batch_lens = next(train_target_iter)\n",
    "        p = float(step + each_epoch * len(train_x_source)) / epochs / len(train_x_len_source)\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        optimizer_student.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token_target), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        # batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "        # opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "        domain_label = torch.ones(min(batch_size, batch_x.shape[0])).long().to(device)\n",
    "        opt_domain = model_student(batch_x[:,:,:subset_cnt], batch_lens, alpha, False)\n",
    "        err_domain2 = loss_domain(opt_domain, domain_label)\n",
    "\n",
    "        # -----common--------\n",
    "        loss = err_predict + err_domain1 + err_domain2\n",
    "        epoch_loss.append(loss.cpu().detach().numpy())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_student.parameters(), 20)\n",
    "        optimizer_student.step()\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "            logger.info('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    total_train_loss.append(epoch_loss)\n",
    "\n",
    "\n",
    "    # dev_source_dataset = MyDataset(dev_x_source_confuse, dev_len_source_confuse, dev_y_source_confuse)\n",
    "    # dev_target_dataset = MyDataset(dev_x_target_confuse, dev_len_target_confuse, dev_y_target_confuse)\n",
    "    # dev_source_dataloader = DataLoader(dev_source_dataset, batch_size= batch_size)\n",
    "    # dev_target_dataloader = DataLoader(dev_target_dataset, batch_size=batch_size)\n",
    "    #Validation\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        steps = len(dev_x_source_confuse) // batch_size + 1 if len(dev_x_source_confuse) % batch_size != 0 else len(dev_x_source_confuse) // batch_size\n",
    "        for step in range(steps):\n",
    "            # -----source_domain--------\n",
    "            batch_x, batch_y, batch_lens= next(dev_source_iter)\n",
    "            p = float(step + each_epoch * steps) / epochs / steps\n",
    "            alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "            optimizer_student.zero_grad()\n",
    "            batch_x = torch.tensor(pad_sents(batch_x, pad_token_source), dtype=torch.float32).to(device)\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "            # batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "            # opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "            domain_label = torch.zeros(min(batch_size, batch_x.shape[0])).long().to(device)\n",
    "            opt_student, opt_domain, emb_student = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, alpha, True)\n",
    "            # emb_teacher = torch.tensor(dev_teacher_emb[step], dtype=torch.float32).to(device)\n",
    "            emb_student = F.log_softmax(emb_student, dim=1)\n",
    "            # err_emb = loss_embed(emb_student, emb_teacher) #todo 是否考虑它\n",
    "            err_predict = loss_predict(opt_student, batch_y)\n",
    "            err_domain1 = loss_domain(opt_domain, domain_label)\n",
    "                # loss = get_multitask_loss(opt_student, batch_y.unsqueeze(-1), emb_student, emb_teacher)\n",
    "\n",
    "            # -----target_domain--------\n",
    "            batch_x, batch_y, batch_lens = next(dev_target_iter)\n",
    "            optimizer_student.zero_grad()\n",
    "            batch_x = torch.tensor(pad_sents(batch_x, pad_token_target), dtype=torch.float32).to(device)\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "            # batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "            # opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "            domain_label = torch.ones(min(batch_size, batch_x.shape[0])).long().to(device)\n",
    "            opt_domain = model_student(batch_x[:,:,:subset_cnt], batch_lens, alpha, False)\n",
    "            err_domain2 = loss_domain(opt_domain, domain_label)\n",
    "\n",
    "            # -----common--------\n",
    "            loss = err_domain1 + err_domain2\n",
    "            if loss < best_total_loss:\n",
    "                best_total_loss = loss\n",
    "                state = {\n",
    "                    'net': model_student.state_dict(),\n",
    "                    'optimizer': optimizer_student.state_dict(),\n",
    "                    'epoch': each_epoch\n",
    "                }\n",
    "                torch.save(state, file_name)\n",
    "                print('------------ Save best model - TOTAL_LOSS: %.4f ------------'%best_total_loss)\n",
    "                logger.info('------------ Save best model - TOTAL_LOSS: %.4f ------------'%best_total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:6\" if torch.cuda.is_available() == True else 'cpu') )\n",
    "save_epoch = checkpoint['epoch']\n",
    "print(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "logger.info(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "model_student.load_state_dict(checkpoint['net'])\n",
    "optimizer_student.load_state_dict(checkpoint['optimizer'])\n",
    "model_student.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anchor\n",
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    model_student.eval()\n",
    "    test_target_iter = batch_iter(test_x_target_confuse, test_y_target_confuse, test_len_target_confuse, batch_size=batch_size, shuffle=True)\n",
    "    steps = len(test_x_target_confuse) // batch_size + 1 if len(test_x_target_confuse) % batch_size != 0 else len(test_x_target_confuse) // batch_size\n",
    "    for step in range(steps):\n",
    "        # -----target_domain--------\n",
    "        batch_x, batch_y, batch_lens= next(test_target_iter) \n",
    "        optimizer_student.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token_target), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "        opt, _, _  = model_student(batch_x[:,:,:subset_cnt], batch_lens, 1, True)\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "\n",
    "        loss = model_loss\n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "            logger.info('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "        y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "        y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "logger.info(\"\\n==>Predicting on test\")\n",
    "logger.info('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "# test_res = metrics.print_metrics_binary(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_dataset == 'PD':\n",
    "    source_common_idx = [31, 29, 28, 33, 25, 18, 7, 21, 16, 15, 19, 17, 24, 3, 5, 0]\n",
    "    target_common_idx = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    source_data_path = './data/Challenge/'\n",
    "    source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "    target_data_path = './data/PD/'\n",
    "    target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "elif target_dataset == 'TJ':\n",
    "    source_common_idx = [27, 29, 18, 16, 26, 33, 28, 31, 32, 15, 11, 25, 21, 20, 9, 17, 30, 19]\n",
    "    target_common_idx = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "    source_data_path = './data/Challenge/'\n",
    "    source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "    target_data_path = './data/Tongji/'\n",
    "    target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "\n",
    "elif target_dataset == 'HM':\n",
    "    source_common_idx = [0, 1, 2, 3, 5, 9, 11, 12, 13, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
    "    target_common_idx = [5, 6, 4, 2, 3, 48, 79, 76, 87, 25, 30, 31, 18, 43, 58, 66, 40, 57, 23, 92, 50, 54, 91, 60, 39, 81]\n",
    "    source_data_path = './data/Challenge/'\n",
    "    source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "    target_data_path = './data/CDSL/'\n",
    "    target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "\n",
    "assert(len(source_common_idx) == len(target_common_idx))\n",
    "common_len = len(source_common_idx)\n",
    "source_x_diff = []\n",
    "target_x_diff = []\n",
    "\n",
    "source_total_len = 34\n",
    "source_other_idx = list(range(source_total_len))\n",
    "for i in source_common_idx:\n",
    "    source_other_idx.remove(i)\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    target_total_len = 69\n",
    "    target_other_idx = list(range(target_total_len))\n",
    "    for i in target_common_idx:\n",
    "        target_other_idx.remove(i)\n",
    "elif target_dataset == 'TJ':\n",
    "    target_other_idx = list(range(75))\n",
    "    target_total_len = 75\n",
    "    for i in target_common_idx:\n",
    "        target_other_idx.remove(i)\n",
    "elif target_dataset == 'HM':\n",
    "    target_other_idx = list(range(99))\n",
    "    target_total_len = 99\n",
    "    for i in target_common_idx:\n",
    "        target_other_idx.remove(i)\n",
    "\n",
    "for i in range(len(source_x)):\n",
    "    cur = np.array(source_x[i], dtype=float)\n",
    "    cur_subset = cur[:, source_common_idx]\n",
    "    cur_other = cur[:, source_other_idx]\n",
    "    source_x_diff.append(cur_other.tolist())\n",
    "    source_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "\n",
    "for i in range(len(target_x)):\n",
    "    cur = np.array(target_x[i], dtype=float)\n",
    "    cur_subset = cur[:, target_common_idx]\n",
    "    cur_other = cur[:, target_other_idx]\n",
    "    target_x_diff.append(cur_other.tolist())\n",
    "    target_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "\n",
    "\n",
    "source_max = 0\n",
    "for i in range(len(source_x_diff)):\n",
    "    if source_max < len(source_x_diff[i]):\n",
    "        source_max = len(source_x_diff[i])\n",
    "\n",
    "source_x_diff_longest = max(list(len(_) for _ in source_x_diff))\n",
    "source_x_longest = max(list(len(_) for _ in source_x))\n",
    "source_batch = len(source_x_diff)\n",
    "source_diff_features = source_total_len - common_len\n",
    "source_x_diff_ex = torch.zeros((source_batch, source_x_diff_longest, source_diff_features))\n",
    "source_x_ex = torch.zeros((source_batch, source_x_longest, source_total_len))\n",
    "\n",
    "for i in range(len(source_x_diff)):\n",
    "    for j in range(source_x_diff_longest):\n",
    "        cur_len = len(source_x_diff[i])\n",
    "        if j < cur_len:\n",
    "            source_x_diff_ex[i,j,:] = torch.Tensor(source_x_diff[i])[j,:]\n",
    "        else:\n",
    "            source_x_diff_ex[i,j,:] = torch.Tensor(source_x_diff[i])[cur_len - 1,:]\n",
    "\n",
    "for i in range(len(source_x)):\n",
    "    for j in range(source_x_longest):\n",
    "        cur_len = len(source_x[i])\n",
    "        if j < cur_len:\n",
    "            source_x_ex[i,j,:] = torch.Tensor(source_x[i])[j,:]\n",
    "        else:\n",
    "            source_x_ex[i,j,:] = torch.Tensor(source_x[i])[cur_len - 1,:]\n",
    "\n",
    "target_x_diff_longest = max(list(len(_) for _ in target_x_diff))\n",
    "target_batch = len(target_x_diff)\n",
    "target_features = target_total_len - common_len\n",
    "target_x_diff_ex = torch.zeros((target_batch, target_x_diff_longest, target_features))\n",
    "\n",
    "for i in range(len(target_x_diff)):\n",
    "    for j in range(target_x_diff_longest):\n",
    "        cur_len = len(target_x_diff[i])\n",
    "        if j < cur_len:\n",
    "            target_x_diff_ex[i,j,:] = torch.Tensor(target_x_diff[i])[j,:]\n",
    "        else:\n",
    "            target_x_diff_ex[i,j,:] = torch.Tensor(target_x_diff[i])[cur_len - 1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Transfer Target Dataset & Model\")\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    data_path = './data/PD/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y_z.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    tar_subset_idx = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    tar_other_idx = list(range(69))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "elif target_dataset == 'TJ':\n",
    "    data_path = './data/Tongji/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    for i in range(len(all_time)):\n",
    "        for j in range(len(all_time[i])):\n",
    "            all_time[i][j] = all_time[i][j][-1]\n",
    "            all_y[i][j] = all_y[i][j][0]\n",
    "\n",
    "    tar_subset_idx = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "    tar_other_idx = list(range(75))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "elif target_dataset == 'HM':\n",
    "    data_path = './data/CDSL/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    for i in range(len(all_time)):\n",
    "        for j in range(len(all_time[i])):\n",
    "            all_time[i][j] = all_time[i][j][-1]\n",
    "            all_y[i][j] = all_y[i][j][0]\n",
    "\n",
    "    tar_subset_idx = [5, 6, 4, 2, 3, 48, 79, 76, 87, 25, 30, 31, 18, 43, 58, 66, 40, 57, 23, 92, 50, 54, 91, 60, 39, 81]\n",
    "    tar_other_idx = list(range(99))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    \n",
    "print(all_x[0])\n",
    "print(len(all_x[0][0]))\n",
    "print(len(all_x))\n",
    "logger.info(all_x[0])\n",
    "logger.info(len(all_x[0][0]))\n",
    "logger.info(len(all_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_x = all_x\n",
    "# long_y = all_y\n",
    "# long_y_kfold = [each[-1] for each in all_y]\n",
    "long_time = all_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_n2n_data(x, y, x_len):\n",
    "    length = len(x)\n",
    "    assert length == len(y)\n",
    "    assert length == len(x_len)\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    new_x_len = []\n",
    "    for i in range(length):\n",
    "        for j in range(len(x[i])):\n",
    "            new_x.append(x[i][:j+1])\n",
    "            new_y.append(y[i][j])\n",
    "            new_x_len.append(j+1)\n",
    "    return new_x, new_y, new_x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class target_model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(target_model, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.GRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        \n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.output = nn.Linear(self.input_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.FC_embed = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden_dim, kernel_size=8, padding=4, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(hidden_dim, 2*hidden_dim, kernel_size=5, padding=2, bias=False),\n",
    "            nn.BatchNorm1d(2*hidden_dim),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(2*hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        \n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, self.output_dim)\n",
    "        )\n",
    "        self.MLP_outcome = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, self.output_dim)\n",
    "        )\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, input, lens):\n",
    "        lens = lens.to('cpu')\n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "        codats_embeded_input = self.layers(input[:,:,0].squeeze().unsqueeze(1)).unsqueeze(1)\n",
    "        # print(f'input[:,:,i].shape is {input[:,:,0].shape}')\n",
    "        for i in range(1, feature_dim):\n",
    "            embeded_input = self.layers(input[:,:,i].squeeze().unsqueeze(1)).unsqueeze(1)\n",
    "            codats_embeded_input = torch.cat((codats_embeded_input, embeded_input), 1)\n",
    "            # print(f'docats_embeded_input.shape is {codats_embeded_input.shape}')\n",
    "        codats_embeded_input = codats_embeded_input.squeeze()\n",
    "\n",
    "        codats_input = codats_embeded_input[:, 0, :]\n",
    "        for i in range(1, feature_dim):\n",
    "             codats_input =  codats_input + codats_embeded_input[:, i, :]  \n",
    "        # print(f\"common_input1.shape is {gru_input.shape}\")\n",
    "        codats_input = torch.squeeze(codats_input, 1) # batch * hidden\n",
    "        # print(f\"common_input2.shape is {gru_input.shape}\")\n",
    "\n",
    "        posi_input = self.dropout(codats_input) # batch_size * d_input + d_input_diff * hidden_dim\n",
    "        output = self.MLP(posi_input)\n",
    "        outcome = self.MLP_outcome(posi_input)\n",
    "        outcome = F.sigmoid(outcome)\n",
    "        if self.output_dim != 1:\n",
    "            output = F.softmax(output, dim=1)\n",
    "        return output, outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_gru_dict(pretrain_dict, model_dict):\n",
    "    state_dict = {}\n",
    "    \n",
    "    for k, v in pretrain_dict.items():\n",
    "        model_point_position1 = k.find('.')\n",
    "        model_module_name = k[:model_point_position1]\n",
    "        if \"layers\" == model_module_name:\n",
    "            model_point_position2 = k.find('.', model_point_position1+1)\n",
    "            model_module_idx = int(k[model_point_position1 + 1: model_point_position2])\n",
    "            print(f'model_module_idx is {model_module_idx}')\n",
    "            state_dict[k] = pretrain_dict[k]\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_dataset == 'PD':\n",
    "    input_dim = 69\n",
    "elif target_dataset == 'TJ':\n",
    "    input_dim = 75\n",
    "elif target_dataset == 'HM':\n",
    "    input_dim = 99\n",
    "    \n",
    "cell = 'GRU'\n",
    "hidden_dim = 64\n",
    "d_model = 64\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ckd_batch_iter(x, y, lens, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(x) / batch_size) # 向下取整\n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size] #  fetch out all the induces\n",
    "        \n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx],  lens[idx]))\n",
    "       \n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    \n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "#         batch_name = [e[2] for e in examples]\n",
    "        batch_lens = [e[2] for e in examples]\n",
    "       \n",
    "\n",
    "        yield batch_x, batch_y, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetMultitaskLoss(nn.Module):\n",
    "    def __init__(self, task_num=2):\n",
    "        super(TargetMultitaskLoss, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.alpha = nn.Parameter(torch.ones((task_num)), requires_grad=True)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, opt_student, los, outcome, outcome_y):\n",
    "        MSE_Loss = self.mse(opt_student, los)\n",
    "        BCE_Loss = self.bce(outcome, outcome_y)\n",
    "        return MSE_Loss * self.alpha[0] + BCE_Loss * self.alpha[1]\n",
    "\n",
    "def get_target_multitask_loss(opt_student, los, outcome, outcome_y):\n",
    "    mtl = TargetMultitaskLoss(task_num=2)\n",
    "    return mtl(opt_student, los, outcome, outcome_y)\n",
    "\n",
    "def reverse_los(y, los_info):\n",
    "    return y * los_info[\"los_std\"] + los_info[\"los_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "los_info = pickle.load(open(data_path + 'los_info.pkl', 'rb'))\n",
    "print(los_info)\n",
    "logger.info(los_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_dataset == 'PD':\n",
    "    n_splits = 5\n",
    "    epochs = 30\n",
    "elif target_dataset == 'TJ':\n",
    "    n_splits = 10\n",
    "    epochs = 150\n",
    "elif target_dataset == 'HM':\n",
    "    n_splits = 3\n",
    "    epochs = 20\n",
    "\n",
    "teacher_flag = True\n",
    "transfer_flag = True\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "if target_dataset == 'PD':    \n",
    "    data_str = 'pd'\n",
    "elif target_dataset == 'TJ':    \n",
    "    data_str = 'covid'\n",
    "elif target_dataset == 'HM':\n",
    "    data_str = 'spain'\n",
    "\n",
    "# if teacher_flag:\n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str\n",
    "# else: \n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher'\n",
    "    \n",
    "file_name = './model/pretrained-dann'+ data_str;\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "fold_count = 0\n",
    "total_train_loss = []\n",
    "total_valid_loss = []\n",
    "\n",
    "global_best = 10000\n",
    "mse = []\n",
    "mad = []\n",
    "mape = []\n",
    "kappa = []\n",
    "history = []\n",
    "\n",
    "pad_token = np.zeros(input_dim)\n",
    "# begin_time = time.time()\n",
    "\n",
    "for train, test in kfold.split(long_x):\n",
    "        \n",
    "    train_x = [long_x[i] for i in train]\n",
    "    train_y = [long_time[i] for i in train]\n",
    "    train_x_len = [all_x_len[i] for i in train]\n",
    "    #train_static = [long_static[i] for i in train]\n",
    "    \n",
    "    train_x, train_y, train_x_len = get_n2n_data(train_x, train_y, train_x_len)\n",
    "    if len(train_x) % 256 == 1:\n",
    "        print(len(train_x))\n",
    "        print('wrong squeeze!')\n",
    "\n",
    "# for train, test in kfold.split(long_x):\n",
    "for train, test in kfold.split(long_x):\n",
    "    if reverse:\n",
    "        temp = train\n",
    "        train = test\n",
    "        test = temp\n",
    "    \n",
    "    model = target_model(input_dim = input_dim,output_dim=output_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, hidden_dim=hidden_dim).to(device)\n",
    "    \n",
    "    if transfer_flag:\n",
    "        checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:6\" if torch.cuda.is_available() == True else 'cpu'))\n",
    "        pretrain_dict = checkpoint['net']\n",
    "        model_dict = model.state_dict()\n",
    "        pretrain_dict = transfer_gru_dict(pretrain_dict, model_dict)\n",
    "        model_dict.update(pretrain_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    fold_count += 1\n",
    "#     print(train)\n",
    "\n",
    "    \n",
    "    train_x = [long_x[i] for i in train]\n",
    "    train_y = [long_time[i] for i in train]\n",
    "    train_x_len = [all_x_len[i] for i in train]\n",
    "    #train_static = [long_static[i] for i in train]\n",
    "    \n",
    "    train_x, train_y, train_x_len = get_n2n_data(train_x, train_y, train_x_len)\n",
    "    \n",
    "    test_x = [long_x[i] for i in test]\n",
    "    test_y = [long_time[i] for i in test]\n",
    "    test_x_len = [all_x_len[i] for i in test]\n",
    "    #test_static = [long_static[i] for i in test]\n",
    "    \n",
    "    test_x, test_y, test_x_len = get_n2n_data(test_x, test_y, test_x_len)\n",
    "    \n",
    "    if not os.path.exists('./model/'+data_str):\n",
    "        os.mkdir('./model/'+data_str)\n",
    "        \n",
    "    if transfer_flag:\n",
    "        target_file_name = './model/'+data_str+'/distcare-trans-'+str(n_splits)+'-fold-LOS-regression' + str(fold_count)#4114\n",
    "    else:\n",
    "        target_file_name = './model/'+data_str+'/distcare-no-trans-'+str(n_splits)+'-fold-LOS-regression' + str(fold_count)#4114\n",
    "    \n",
    "    fold_train_loss = []\n",
    "    fold_valid_loss = []\n",
    "    best_mse = 10000\n",
    "    best_mad = 0\n",
    "    best_mape = 0\n",
    "    best_kappa = 0\n",
    "    \n",
    "    for each_epoch in range(epochs):\n",
    "       \n",
    "        \n",
    "        epoch_loss = []\n",
    "        counter_batch = 0\n",
    "        model.train()  \n",
    "        \n",
    "        for step, (batch_x, batch_y, batch_lens) in enumerate(ckd_batch_iter(train_x, train_y, train_x_len, batch_size, shuffle=True)):  \n",
    "            optimizer.zero_grad()\n",
    "            batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "\n",
    "            masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "            opt, emb = model(batch_x, batch_lens)\n",
    "\n",
    "            MSE_Loss = get_re_loss(opt, batch_y.unsqueeze(-1))\n",
    "\n",
    "#             model_loss = pred_loss + 1e7*decov_loss\n",
    "            model_loss = MSE_Loss\n",
    "\n",
    "            loss = model_loss\n",
    "\n",
    "            epoch_loss.append(MSE_Loss.cpu().detach().numpy())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % 50 == 0:\n",
    "                print('Fold %d Epoch %d Batch %d: Train Loss = %.4f'%(fold_count,each_epoch, step, loss.cpu().detach().numpy()))\n",
    "                logger.info('Fold %d Epoch %d Batch %d: Train Loss = %.4f'%(fold_count,each_epoch, step, loss.cpu().detach().numpy()))\n",
    "            \n",
    "        epoch_loss = np.mean(epoch_loss)\n",
    "        fold_train_loss.append(epoch_loss)\n",
    "\n",
    "        #Validation\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_pred_flatten = []\n",
    "        y_true_flatten = []\n",
    "        outcome_pred_flatten = []\n",
    "        outcome_true_flatten = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            valid_true = []\n",
    "            valid_pred = []\n",
    "            for batch_x, batch_y, batch_lens in ckd_batch_iter(test_x, test_y, test_x_len, batch_size):\n",
    "                batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "                batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "                batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "                masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "               \n",
    "                opt, emb = model(batch_x, batch_lens)\n",
    "                \n",
    "                MSE_Loss = get_re_loss(opt, batch_y.unsqueeze(-1))\n",
    "                \n",
    "                valid_loss.append(MSE_Loss.cpu().detach().numpy())\n",
    "\n",
    "                y_pred_flatten += [reverse_los(x, los_info) / 30 for x in list(opt.cpu().detach().numpy().flatten())]\n",
    "                y_true_flatten += [reverse_los(x, los_info) / 30 for x in list(batch_y.cpu().numpy().flatten())]\n",
    "            \n",
    "\n",
    "            valid_loss = np.mean(valid_loss)\n",
    "            fold_valid_loss.append(valid_loss)\n",
    "            ret = metrics.print_metrics_regression(y_true_flatten, y_pred_flatten, verbose=0)\n",
    "            history.append(ret)\n",
    "            #print()\n",
    "\n",
    "            if each_epoch % 10 == 0:\n",
    "                print('Fold %d, epoch %d: Loss = %.4f Valid loss = %.4f MSE = %.4f' % (\n",
    "                    fold_count, each_epoch, fold_train_loss[-1], fold_valid_loss[-1], ret['mse']), flush=True)\n",
    "                logger.info('Fold %d, epoch %d: Loss = %.4f Valid loss = %.4f MSE = %.4f' % (\n",
    "                    fold_count, each_epoch, fold_train_loss[-1], fold_valid_loss[-1], ret['mse']))\n",
    "                # metrics.print_metrics_regression(y_true_flatten, y_pred_flatten)\n",
    "                \n",
    "            cur_mse = ret['mse']\n",
    "            if cur_mse < best_mse:\n",
    "                print('------------ Save FOLD-BEST model - MSE: %.4f ------------' % cur_mse, flush=True)\n",
    "                logger.info('------------ Save FOLD-BEST model - MSE: %.4f ------------' % cur_mse)\n",
    "                metrics.print_metrics_regression(y_true_flatten, y_pred_flatten)\n",
    "                best_mse = cur_mse\n",
    "                best_mad = ret['mad']\n",
    "                state = {\n",
    "                    'net': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': each_epoch\n",
    "                }\n",
    "                torch.save(state, target_file_name + '_' + str(fold_count))\n",
    "\n",
    "                if cur_mse < global_best:\n",
    "                    global_best = cur_mse\n",
    "                    state = {\n",
    "                        'net': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'epoch': each_epoch\n",
    "                    }\n",
    "                    torch.save(state, target_file_name)\n",
    "                    print('------------ Save best model - MSE: %.4f ------------' % cur_mse, flush=True)\n",
    "                    logger.info('------------ Save best model - MSE: %.4f ------------' % cur_mse)\n",
    "\n",
    "        print('Fold %d, mse = %.4f, mad = %.4f' % (fold_count, ret['mse'], ret['mad']), flush=True)\n",
    "        logger.info('Fold %d, mse = %.4f, mad = %.4f' % (fold_count, ret['mse'], ret['mad']))\n",
    "\n",
    "    mse.append(best_mse)\n",
    "    mad.append(best_mad)\n",
    "    total_train_loss.append(fold_train_loss)\n",
    "    total_valid_loss.append(fold_valid_loss)\n",
    "\n",
    "\n",
    "print('mse %.4f(%.4f)' % (np.mean(mse), np.std(mse)))\n",
    "print('mad %.4f(%.4f)' % (np.mean(mad), np.std(mad)))\n",
    "logger.info('mse %.4f(%.4f)' % (np.mean(mse), np.std(mse)))\n",
    "logger.info('mad %.4f(%.4f)' % (np.mean(mad), np.std(mad)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
