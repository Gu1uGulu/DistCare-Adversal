{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:55:29.996921Z",
     "start_time": "2021-02-10T14:55:28.704721Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "import pickle5 as pickle\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import logging\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from utils import utils\n",
    "from utils.readers import InHospitalMortalityReader\n",
    "from utils.preprocessing import Discretizer, Normalizer\n",
    "from utils import metrics\n",
    "from utils import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:55:52.924543Z",
     "start_time": "2021-02-10T14:55:52.918220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available device: cuda:6\n"
     ]
    }
   ],
   "source": [
    "# Select the target dataset: COVID-19 Dataset from TJ Hospital or HM Hospital\n",
    "target_dataset = 'PD' \n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() == True else 'cpu')\n",
    "print(\"available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:03:32,438 - INFO - 这是希望输出的info内容\n",
      "2023-08-11 12:03:32,439 - WARNING - 这是希望输出的warning内容\n"
     ]
    }
   ],
   "source": [
    "def get_logger(name):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # 以下两行是为了在jupyter notebook 中不重复输出日志\n",
    "    if logger.root.handlers:\n",
    "        logger.root.handlers[0].setLevel(logging.WARNING)\n",
    " \n",
    "    handler_stdout = logging.StreamHandler()\n",
    "    handler_stdout.setLevel(logging.INFO)\n",
    "    handler_stdout.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler_stdout)\n",
    " \n",
    "    handler_file = logging.FileHandler('log_file_pd_change.log', mode=\"w\", encoding='utf-8')\n",
    "    handler_file.setLevel(logging.DEBUG)\n",
    "    handler_file.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler_file)\n",
    " \n",
    "    return logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "logger.debug('这是希望输出的debug内容')\n",
    "logger.info('这是希望输出的info内容')\n",
    "logger.warning('这是希望输出的warning内容')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Source Data & Model: \n",
    "#### Teacher Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:55:32.579746Z",
     "start_time": "2021-02-10T14:55:32.571939Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = './data/Challenge/'\n",
    "small_part = False\n",
    "arg_timestep = 1.0\n",
    "batch_size = 256\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T04:27:29.073353Z",
     "start_time": "2021-01-30T04:27:12.159717Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:03:47,231 - INFO - [[-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 1.2605692444279462, 0.585371321489137, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.9546043520029015, 0.2987563180527675, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 1.0769903089729194, 0.4420638197709522, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.566534136852991, 1.803485086093707, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, 0.46506052412282983, -0.05951243624269434, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, 0.281481588667803, -0.2744736888199714, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.8934113735178925, 0.9436400757845987, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:03:47,236 - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "2023-08-11 12:03:47,237 - INFO - 110609\n",
      "2023-08-11 12:03:47,238 - INFO - [[-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8962118618028821, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8617355345389544, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8272592072750267, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7927828800110989, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7583065527471711, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7238302254832434, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6893538982193156, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6548775709553878, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.62040124369146, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5859249164275323, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5514485891636045, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5169722618996767, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.48249593463574897, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.4480196073718212, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.41354328010789343, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3790669528439657, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3445906255800379, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.31011429831611015, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.27563797105218235, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.2411616437882546, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.20668531652432684, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.17220898926039907, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.1377326619964713, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.10325633473254354, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.06878000746861578, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.034303680204688006, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.0001726470592397616, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.034648974323167527, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.06912530158709529, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.10360162885102306, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.13807795611495083, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.1725542833788786, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.20703061064280637, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.24150693790673414, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.2759832651706619, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.31045959243458965, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.34493591969851745, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.3794122469624452, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.413888574226373, '0']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.34587251014597875, -0.2371260510881844, 0.3051487380880145, 0.029264930048844468, -0.3160730875843661, -0.3766591890459441, -0.1935716453287135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.05532679134287014, -0.2815944741293343, -0.32211134163582006, -0.0899704549996704, -0.06645805008034258, -0.33684497792590695, -0.14828727498338712, -0.24435830491350327, -0.14487324959363315], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [-0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.11839355366098099, -0.14686926072725967, -0.1311661871017867, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.38817852813712, 1.2605692444279462, 0.585371321489137, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -0.4065324680479176, -1.009369603802189, 1.043819195154697, 0.9546043520029015, 0.2987563180527675, 0.6420908338073934, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.38817852813712, 1.0769903089729194, 0.4420638197709522, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.4742683613827257, 1.566534136852991, 1.803485086093707, 0.8382388140070928, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.44119036243545645, 0.46506052412282983, -0.05951243624269434, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.6564149455494709, 0.281481588667803, -0.2744736888199714, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.09683102945303342, 0.8934113735178925, 0.9436400757845987, 1.4266827546061909, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.6223326938143058, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.613370028926668, 0.46506052412282983, -0.05951243624269434, 0.24979487340799464, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.5053978624091442, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.3981454458126536, 0.46506052412282983, -0.05951243624269434, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, 0.6149447909519286, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.6590707727034506, -1.4280097270477636, -0.48989820488893626, -0.07534863703817811, 0.22028861018279403, -0.2744736888199714, -0.5347970473908028, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.37666305339779826, -0.26925521769727756, -0.5610886922563408, -0.1425010869914041, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.26404955735950336, 0.029264930048844468, -0.2606896206457801, -0.3766591890459441, -0.31050647673387505, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, -0.36678162470457243, -0.2815944741293343, -0.3333992081010445, -0.1992256677835492, -0.7268083069317782, -0.33684497792590695, -0.3293770132508767, -0.24435830491350327, 0.2603960082428797], [0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.1299090284003026, 1.0157973304879104, 0.585371321489137, 0.44594285360769403, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.9577293619090911, 0.6486394595778567, 0.08379506547549039, 1.0343867942067921, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.043819195154697, 1.627727115338, 1.373562580939153, 1.6228307348058901, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.7855496954178796, 0.8934113735178925, 0.2271025671936751, 1.2305347744064914, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592], [0.3127551420282933, 0.9554372106185439, 0.419176743209254, 0.613370028926668, 1.1993762659429372, 1.6601775843755224, -0.9270930077902015, 0.005325137533142886, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.49591584998532545, -0.2371260510881844, 0.4695454610020563, 0.029264930048844468, -0.34930316774751763, -0.3766591890459441, -0.7002892480844135, -0.3351561343174943, -0.12930577851172065, -0.17160263114220592, 0.10040062533798204, -0.2815944741293343, -0.3333992081010445, 0.2742135876132575, -0.16805039728825624, -0.33684497792590695, -0.17415723759302862, -0.24435830491350327, 0.2893438123740592]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "110609\n",
      "[[-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8962118618028821, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8617355345389544, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.8272592072750267, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7927828800110989, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7583065527471711, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.7238302254832434, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6893538982193156, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.6548775709553878, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.62040124369146, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5859249164275323, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5514485891636045, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.5169722618996767, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.48249593463574897, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.4480196073718212, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.41354328010789343, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3790669528439657, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.3445906255800379, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.31011429831611015, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.27563797105218235, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.2411616437882546, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.20668531652432684, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.17220898926039907, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.1377326619964713, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.10325633473254354, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.06878000746861578, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, -0.034303680204688006, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.0001726470592397616, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.034648974323167527, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.06912530158709529, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.10360162885102306, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.13807795611495083, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.1725542833788786, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.20703061064280637, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.24150693790673414, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.2759832651706619, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.31045959243458965, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.34493591969851745, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.3794122469624452, '0'], [-1.1264803265802585, -0.9931656033414253, 0.9931656033414252, 0.11854736004896246, 0.413888574226373, '0']]\n"
     ]
    }
   ],
   "source": [
    "all_x = pickle.load(open(data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "all_y = pickle.load(open(data_path + 'new_y_front_fill.dat', 'rb'))\n",
    "all_names = pickle.load(open(data_path + 'new_name.dat', 'rb'))\n",
    "static = pickle.load(open(data_path + 'new_demo_front_fill.dat', 'rb'))\n",
    "mask_x = pickle.load(open(data_path + 'new_mask_x.dat', 'rb'))\n",
    "mask_demo = pickle.load(open(data_path + 'new_mask_demo.dat', 'rb'))\n",
    "all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "print(all_x[0])\n",
    "print(mask_x[0])\n",
    "print(all_names[0])\n",
    "print(static[0])\n",
    "logger.info(all_x[0])\n",
    "logger.info(mask_x[0])\n",
    "logger.info(all_names[0])\n",
    "logger.info(static[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_y[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T04:27:41.051036Z",
     "start_time": "2021-01-30T04:27:29.075798Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:03:59,100 - INFO - [[-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.38817852813712, 0.585371321489137, 0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.2605692444279462, 0.6420908338073934, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.043819195154697, 0.2987563180527675, 0.6590707727034506, -0.4065324680479176, -1.009369603802189, 0.9546043520029015, 0.6420908338073934, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.38817852813712, 0.4420638197709522, 0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.0769903089729194, 0.8382388140070928, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.4742683613827257, 1.803485086093707, 1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.566534136852991, 0.8382388140070928, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.44119036243545645, -0.05951243624269434, 0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.46506052412282983, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.6564149455494709, -0.2744736888199714, 0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.281481588667803, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.09683102945303342, 0.9436400757845987, 0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.8934113735178925, 1.4266827546061909, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.3981454458126536, -0.05951243624269434, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.3981454458126536, -0.05951243624269434, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.37666305339779826, -0.5610886922563408, 0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.26925521769727756, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.37666305339779826, -0.5610886922563408, 0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.26925521769727756, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.1299090284003026, 0.585371321489137, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.0157973304879104, 0.44594285360769403, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.1299090284003026, 0.585371321489137, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.0157973304879104, 0.44594285360769403, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.613370028926668, 1.6601775843755224, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 1.1993762659429372, -0.9270930077902015, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.613370028926668, 1.6601775843755224, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 1.1993762659429372, -0.9270930077902015, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:03:59,104 - INFO - [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "2023-08-11 12:03:59,105 - INFO - 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.14828727498338712, -0.06645805008034258, -0.0899704549996704, -0.14487324959363315, -0.05532679134287014, 0.029264930048844468, 0.005325137533142886, -0.1935716453287135, -0.2371260510881844, -0.34587251014597875, -0.3160730875843661, 0.3051487380880145, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.32211134163582006, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.11839355366098099, -0.1311661871017867, -0.06242012453646045, 0.2744523712853132, 0.02957319402431667, -0.14686926072725967, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.38817852813712, 0.585371321489137, 0.6013515009242577, 0.6149447909519286, -1.009369603802189, 1.2605692444279462, 0.6420908338073934, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.043819195154697, 0.2987563180527675, 0.6590707727034506, -0.4065324680479176, -1.009369603802189, 0.9546043520029015, 0.6420908338073934, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.38817852813712, 0.4420638197709522, 0.8899478598202222, -0.747024887714533, -1.009369603802189, 1.0769903089729194, 0.8382388140070928, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.4742683613827257, 1.803485086093707, 1.0631056751578007, -0.747024887714533, -1.009369603802189, 1.566534136852991, 0.8382388140070928, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.44119036243545645, -0.05951243624269434, 0.7745093162618364, 0.6149447909519286, -1.2691053032588202, 0.46506052412282983, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.6564149455494709, -0.2744736888199714, 0.3704744138074862, 0.2744523712853132, -1.2691053032588202, 0.281481588667803, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.6223326938143058, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.09683102945303342, 0.9436400757845987, 0.8899478598202222, 0.9554372106185439, -1.2691053032588202, 0.8934113735178925, 1.4266827546061909, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.3127551420282933, -0.4065324680479176, -1.2691053032588202, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.613370028926668, -0.05951243624269434, 0.4281936855866791, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, 0.24979487340799464, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.5053978624091442, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.3981454458126536, -0.05951243624269434, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 0.3981454458126536, -0.05951243624269434, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.46506052412282983, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, 0.6149447909519286, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.07534863703817811, -0.2744736888199714, 0.6590707727034506, -1.4280097270477636, -0.48989820488893626, 0.22028861018279403, -0.5347970473908028, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.37666305339779826, -0.5610886922563408, 0.7745093162618364, 0.2744523712853132, -0.3600303551606207, -0.26925521769727756, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, -0.37666305339779826, -0.5610886922563408, 0.7167900444826435, 0.2744523712853132, -0.3600303551606207, -0.26925521769727756, -0.1425010869914041, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.3293770132508767, -0.7268083069317782, -0.1992256677835492, 0.2603960082428797, -0.36678162470457243, 0.029264930048844468, 0.005325137533142886, -0.31050647673387505, -0.2371260510881844, -0.49591584998532545, -0.2606896206457801, 0.26404955735950336, -0.17160263114220592, 1.1299090284003026, 0.585371321489137, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.0157973304879104, 0.44594285360769403, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.1299090284003026, 0.585371321489137, 0.7745093162618364, -0.0660400483813022, -0.3600303551606207, 1.0157973304879104, 0.44594285360769403, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.9577293619090911, 0.08379506547549039, 0.6590707727034506, 0.2744523712853132, -0.6197660546172518, 0.6486394595778567, 1.0343867942067921, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 1.043819195154697, 1.373562580939153, 0.4281936855866791, 0.9554372106185439, -0.48989820488893626, 1.627727115338, 1.6228307348058901, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.7855496954178796, 0.2271025671936751, 0.19731659846990754, 0.9554372106185439, -0.2301625054323144, 0.8934113735178925, 1.2305347744064914, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.613370028926668, 1.6601775843755224, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 1.1993762659429372, -0.9270930077902015, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327], [-0.17415723759302862, -0.16805039728825624, 0.2742135876132575, 0.2893438123740592, 0.10040062533798204, 0.029264930048844468, 0.005325137533142886, -0.7002892480844135, -0.2371260510881844, -0.49591584998532545, -0.34930316774751763, 0.4695454610020563, -0.17160263114220592, 0.613370028926668, 1.6601775843755224, 0.3127551420282933, 0.9554372106185439, 0.419176743209254, 1.1993762659429372, -0.9270930077902015, 0.16066034068876195, -0.01724690479013476, -0.004930129148398766, 0.014295447077977357, -0.11026740609348425, 0.39895882324044557, -0.2561829490343818, -0.3766591890459441, -0.3351561343174943, -0.12930577851172065, -0.2815944741293343, -0.3333992081010445, -0.33684497792590695, -0.24435830491350327]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "if target_dataset == 'PD':\n",
    "    subset_idx = [31, 29, 28, 33, 25, 18, 7, 21, 16, 15, 19, 17, 24, 3, 5, 0]\n",
    "\n",
    "subset_cnt = len(subset_idx)\n",
    "other_idx = []\n",
    "for i in range(len(all_x[0][0])):\n",
    "    if i not in subset_idx:\n",
    "        other_idx.append(i)\n",
    "\n",
    "for i in range(len(all_x)):\n",
    "    cur = np.array(all_x[i], dtype=float)\n",
    "    cur_mask = np.array(mask_x[i])\n",
    "    cur_subset = cur[:, subset_idx]\n",
    "    cur_other = cur[:, other_idx]\n",
    "    cur_mask_subset = cur_mask[:, subset_idx]\n",
    "    cur_mask_other = cur_mask[:, other_idx]\n",
    "    all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    mask_x[i] = np.concatenate((cur_mask_subset, cur_mask_other), axis=1).tolist()\n",
    "print(all_x[0])\n",
    "print(mask_x[0])\n",
    "print(len(all_x[0][0]))\n",
    "logger.info(all_x[0])\n",
    "logger.info(mask_x[0])\n",
    "logger.info(len(all_x[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T04:28:59.378928Z",
     "start_time": "2021-01-30T04:28:57.102580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:00,816 - INFO - 32269\n",
      "2023-08-11 12:04:00,818 - INFO - 4034\n",
      "2023-08-11 12:04:00,819 - INFO - 4033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4034\n",
      "4033\n"
     ]
    }
   ],
   "source": [
    "train_num =int( len(all_x) * 0.8) + 1\n",
    "print(train_num)\n",
    "logger.info(train_num)\n",
    "dev_num =int( len(all_x) * 0.1) + 1\n",
    "print(dev_num)\n",
    "logger.info(dev_num)\n",
    "test_num =int( len(all_x) * 0.1)\n",
    "print(test_num)\n",
    "logger.info(test_num)\n",
    "assert(train_num+dev_num+test_num == len(all_x))\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "train_names = []\n",
    "train_static = []\n",
    "train_x_len = []\n",
    "train_mask_x = []\n",
    "for idx in range(train_num):\n",
    "    train_x.append(all_x[idx])\n",
    "    train_y.append(int(all_y[idx][-1]))\n",
    "    train_names.append(all_names[idx])\n",
    "    train_static.append(static[idx])\n",
    "    train_x_len.append(all_x_len[idx])\n",
    "    train_mask_x.append(mask_x[idx])\n",
    "\n",
    "dev_x = []\n",
    "dev_y = []\n",
    "dev_names = []\n",
    "dev_static = []\n",
    "dev_x_len = []\n",
    "dev_mask_x = []\n",
    "for idx in range(train_num, train_num + dev_num):\n",
    "    dev_x.append(all_x[idx])\n",
    "    dev_y.append(int(all_y[idx][-1]))\n",
    "    dev_names.append(all_names[idx])\n",
    "    dev_static.append(static[idx])\n",
    "    dev_x_len.append(all_x_len[idx])\n",
    "    dev_mask_x.append(mask_x[idx])\n",
    "\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "test_names = []\n",
    "test_static = []\n",
    "test_x_len = []\n",
    "test_mask_x = []\n",
    "for idx in range(train_num + dev_num, train_num + dev_num + test_num):\n",
    "    test_x.append(all_x[idx])\n",
    "    test_y.append(int(all_y[idx][-1]))\n",
    "    test_names.append(all_names[idx])\n",
    "    test_static.append(static[idx])\n",
    "    test_x_len.append(all_x_len[idx])\n",
    "    test_mask_x.append(mask_x[idx])\n",
    "\n",
    "\n",
    "assert(len(train_x) == train_num)\n",
    "assert(len(dev_x) == dev_num)\n",
    "assert(len(test_x) == test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T04:29:02.456915Z",
     "start_time": "2021-01-30T04:29:02.443296Z"
    }
   },
   "outputs": [],
   "source": [
    "long_x = all_x\n",
    "long_y = [y[-1] for y in all_y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:38.783950Z",
     "start_time": "2021-02-10T15:05:38.778517Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_loss(y_pred, y_true):\n",
    "    loss = torch.nn.BCELoss()\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:39.004588Z",
     "start_time": "2021-02-10T15:05:38.999638Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_re_loss(y_pred, y_true):\n",
    "    loss = torch.nn.MSELoss()\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:39.151905Z",
     "start_time": "2021-02-10T15:05:39.147577Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_kl_loss(x_pred, x_target):\n",
    "    loss = torch.nn.KLDivLoss(reduce=True, size_average=True)\n",
    "    return loss(x_pred, x_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:39.322887Z",
     "start_time": "2021-02-10T15:05:39.313036Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wass_dist(x_pred, x_target):\n",
    "    m1 = torch.mean(x_pred, dim=0)\n",
    "    m2 = torch.mean(x_target, dim=0)\n",
    "    v1 = torch.var(x_pred, dim=0)\n",
    "    v2 = torch.var(x_target, dim=0)\n",
    "    p1 = torch.sum(torch.pow((m1 - m2), 2))\n",
    "    p2 = torch.sum(torch.pow(torch.pow(v1, 1/2) - torch.pow(v2, 1/2), 2))\n",
    "    return torch.pow(p1+p2, 1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:41.142115Z",
     "start_time": "2021-02-10T15:05:41.134517Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_sents(sents, pad_token):\n",
    "\n",
    "    sents_padded = []\n",
    "\n",
    "    max_length = max([len(_) for _ in sents])\n",
    "    for i in sents:\n",
    "        padded = list(i) + [pad_token]*(max_length-len(i))\n",
    "        sents_padded.append(np.array(padded))\n",
    "\n",
    "\n",
    "    return np.array(sents_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:41.301945Z",
     "start_time": "2021-02-10T15:05:41.287538Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_iter(x, y, mask, lens, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(x) / batch_size) # 向下取整\n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size] #  fetch out all the induces\n",
    "        \n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx], mask[idx], lens[idx]))\n",
    "       \n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    \n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "        batch_mask_x = [e[2] for e in examples]\n",
    "#         batch_name = [e[2] for e in examples]\n",
    "        batch_lens = [e[3] for e in examples]\n",
    "\n",
    "        yield batch_x, batch_y, batch_mask_x, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:41.394432Z",
     "start_time": "2021-02-10T15:05:41.386828Z"
    }
   },
   "outputs": [],
   "source": [
    "def length_to_mask(length, max_len=None, dtype=None):\n",
    "    \"\"\"length: B.\n",
    "    return B x max_len.\n",
    "    If max_len is None, then max of length will be used.\n",
    "    \"\"\"\n",
    "    assert len(length.shape) == 1, 'Length shape should be 1 dimensional.'\n",
    "    max_len = max_len or length.max().item()\n",
    "    mask = torch.arange(max_len, device=length.device,\n",
    "                        dtype=length.dtype).expand(len(length), max_len) < length.unsqueeze(1)\n",
    "    if dtype is not None:\n",
    "        mask = torch.as_tensor(mask, dtype=dtype, device=length.device)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:41.786588Z",
     "start_time": "2021-02-10T15:05:41.673200Z"
    }
   },
   "outputs": [],
   "source": [
    "class SingleAttention(nn.Module):\n",
    "    def __init__(self, attention_input_dim, attention_hidden_dim, attention_type='add', demographic_dim=12, time_aware=False, use_demographic=False):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        \n",
    "        self.attention_type = attention_type\n",
    "        self.attention_hidden_dim = attention_hidden_dim\n",
    "        self.attention_input_dim = attention_input_dim\n",
    "        self.use_demographic = use_demographic\n",
    "        self.demographic_dim = demographic_dim\n",
    "        self.time_aware = time_aware\n",
    "\n",
    "        # batch_time = torch.arange(0, batch_mask.size()[1], dtype=torch.float32).reshape(1, batch_mask.size()[1], 1)\n",
    "        # batch_time = batch_time.repeat(batch_mask.size()[0], 1, 1)\n",
    "        \n",
    "        if attention_type == 'add':\n",
    "            if self.time_aware == True:\n",
    "                # self.Wx = nn.Parameter(torch.randn(attention_input_dim+1, attention_hidden_dim))\n",
    "                self.Wx = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "                self.Wtime_aware = nn.Parameter(torch.randn(1, attention_hidden_dim))\n",
    "                nn.init.kaiming_uniform_(self.Wtime_aware, a=math.sqrt(5))\n",
    "            else:\n",
    "                self.Wx = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "            self.Wt = nn.Parameter(torch.randn(attention_input_dim, attention_hidden_dim))\n",
    "            self.Wd = nn.Parameter(torch.randn(demographic_dim, attention_hidden_dim))\n",
    "            self.bh = nn.Parameter(torch.zeros(attention_hidden_dim,))\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wd, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wx, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wt, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        elif attention_type == 'mul':\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_input_dim, attention_input_dim))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        elif attention_type == 'concat':\n",
    "            if self.time_aware == True:\n",
    "                self.Wh = nn.Parameter(torch.randn(2*attention_input_dim+1, attention_hidden_dim))\n",
    "            else:\n",
    "                self.Wh = nn.Parameter(torch.randn(2*attention_input_dim, attention_hidden_dim))\n",
    "\n",
    "            self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "            self.ba = nn.Parameter(torch.zeros(1,))\n",
    "            \n",
    "            nn.init.kaiming_uniform_(self.Wh, a=math.sqrt(5))\n",
    "            nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        else:\n",
    "            raise RuntimeError('Wrong attention type.')\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, input, demo=None):\n",
    " \n",
    "        batch_size, time_step, input_dim = input.size() # batch_size * time_step * hidden_dim(i)\n",
    "        time_decays = torch.tensor(range(time_step-1,-1,-1), dtype=torch.float32).unsqueeze(-1).unsqueeze(0).to(device)# 1*t*1\n",
    "        b_time_decays = time_decays.repeat(batch_size,1,1)# b t 1\n",
    "        \n",
    "        if self.attention_type == 'add': #B*T*I  @ H*I\n",
    "            q = torch.matmul(input[:,-1,:], self.Wt)# b h\n",
    "            q = torch.reshape(q, (batch_size, 1, self.attention_hidden_dim)) #B*1*H\n",
    "            if self.time_aware == True:\n",
    "                # k_input = torch.cat((input, time), dim=-1)\n",
    "                k = torch.matmul(input, self.Wx)#b t h\n",
    "                # k = torch.reshape(k, (batch_size, 1, time_step, self.attention_hidden_dim)) #B*1*T*H\n",
    "                time_hidden = torch.matmul(b_time_decays, self.Wtime_aware)#  b t h\n",
    "            else:\n",
    "                k = torch.matmul(input, self.Wx)# b t h\n",
    "                # k = torch.reshape(k, (batch_size, 1, time_step, self.attention_hidden_dim)) #B*1*T*H\n",
    "            if self.use_demographic == True:\n",
    "                d = torch.matmul(demo, self.Wd) #B*H\n",
    "                d = torch.reshape(d, (batch_size, 1, self.attention_hidden_dim)) # b 1 h\n",
    "            h = q + k + self.bh # b t h\n",
    "            if self.time_aware == True:\n",
    "                h += time_hidden\n",
    "            h = self.tanh(h) #B*T*H\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step))# b t\n",
    "        elif self.attention_type == 'mul':\n",
    "            e = torch.matmul(input[:,-1,:], self.Wa)#b i\n",
    "            e = torch.matmul(e.unsqueeze(1), input.permute(0,2,1)).squeeze() + self.ba #b t\n",
    "        elif self.attention_type == 'concat':\n",
    "            q = input[:,-1,:].unsqueeze(1).repeat(1,time_step,1)# b t i\n",
    "            k = input\n",
    "            c = torch.cat((q, k), dim=-1) #B*T*2I\n",
    "            if self.time_aware == True:\n",
    "                c = torch.cat((c, b_time_decays), dim=-1) #B*T*2I+1\n",
    "            h = torch.matmul(c, self.Wh)\n",
    "            h = self.tanh(h)\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step)) # b t \n",
    "        \n",
    "        a = self.softmax(e) #B*T\n",
    "        v = torch.matmul(a.unsqueeze(1), input).squeeze() #B*I\n",
    "\n",
    "        return v, a\n",
    "\n",
    "class FinalAttentionQKV(nn.Module):\n",
    "    def __init__(self, attention_input_dim, attention_hidden_dim, attention_type='add', dropout=None):\n",
    "        super(FinalAttentionQKV, self).__init__()\n",
    "        \n",
    "        self.attention_type = attention_type\n",
    "        self.attention_hidden_dim = attention_hidden_dim\n",
    "        self.attention_input_dim = attention_input_dim\n",
    "\n",
    "\n",
    "        self.W_q = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "        self.W_k = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "        self.W_v = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "\n",
    "        self.W_out = nn.Linear(attention_hidden_dim, 1)\n",
    "\n",
    "        self.b_in = nn.Parameter(torch.zeros(1,))\n",
    "        self.b_out = nn.Parameter(torch.zeros(1,))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.W_q.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_k.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_v.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_out.weight, a=math.sqrt(5))\n",
    "\n",
    "        self.Wh = nn.Parameter(torch.randn(2*attention_input_dim, attention_hidden_dim))\n",
    "        self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "        self.ba = nn.Parameter(torch.zeros(1,))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.Wh, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input):\n",
    " \n",
    "        batch_size, time_step, input_dim = input.size() # batch_size * input_dim + 1 * hidden_dim(i)\n",
    "        input_q = self.W_q(torch.mean(input, dim=1)) # b h\n",
    "        input_k = self.W_k(input)# b t h\n",
    "        input_v = self.W_v(input)# b t h\n",
    "\n",
    "        if self.attention_type == 'add': #B*T*I  @ H*I\n",
    "\n",
    "            q = torch.reshape(input_q, (batch_size, 1, self.attention_hidden_dim)) #B*1*H\n",
    "            h = q + input_k + self.b_in # b t h\n",
    "            h = self.tanh(h) #B*T*H\n",
    "            e = self.W_out(h) # b t 1\n",
    "            e = torch.reshape(e, (batch_size, time_step))# b t\n",
    "\n",
    "        elif self.attention_type == 'mul':\n",
    "            q = torch.reshape(input_q, (batch_size, self.attention_hidden_dim, 1)) #B*h 1\n",
    "            e = torch.matmul(input_k, q).squeeze()#b t\n",
    "            \n",
    "        elif self.attention_type == 'concat':\n",
    "            q = input_q.unsqueeze(1).repeat(1,time_step,1)# b t h\n",
    "            k = input_k\n",
    "            c = torch.cat((q, k), dim=-1) #B*T*2I\n",
    "            h = torch.matmul(c, self.Wh)\n",
    "            h = self.tanh(h)\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step)) # b t \n",
    "        \n",
    "        a = self.softmax(e) #B*T\n",
    "        if self.dropout is not None:\n",
    "            a = self.dropout(a)\n",
    "        v = torch.matmul(a.unsqueeze(1), input_v).squeeze() #B*I\n",
    "\n",
    "        return v, a\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "def tile(a, dim, n_tile):\n",
    "    init_dim = a.size(dim)\n",
    "    repeat_idx = [1] * a.dim()\n",
    "    repeat_idx[dim] = n_tile\n",
    "    a = a.repeat(*(repeat_idx))\n",
    "    order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).to(device)\n",
    "    return torch.index_select(a, dim, order_index).to(device)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module): # new added\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x)))), None\n",
    "\n",
    "    \n",
    "class PositionalEncoding(nn.Module): # new added / not use anymore\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=400):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0 \n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)# b h t d_k\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k) # b h t t\n",
    "    if mask is not None:# 1 1 t t\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)# b h t t \n",
    "    p_attn = F.softmax(scores, dim = -1)# b h t t\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn # b h t v (d_k) \n",
    "    \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, self.d_k * self.h), 3)\n",
    "        self.final_linear = nn.Linear(d_model, d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1) # 1 1 t t\n",
    "\n",
    "        nbatches = query.size(0)# b\n",
    "        input_dim = query.size(1)# i+1\n",
    "        feature_dim = query.size(-1)# i+1\n",
    "\n",
    "        #input size -> # batch_size * d_input * hidden_dim\n",
    "        \n",
    "        # d_model => h * d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))] # b num_head d_input d_k\n",
    "        \n",
    "       \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)# b num_head d_input d_v (d_k) \n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)# batch_size * d_input * hidden_dim\n",
    "\n",
    "        #DeCov \n",
    "        DeCov_contexts = x.transpose(0, 1).transpose(1, 2) # I+1 H B\n",
    "#         print(DeCov_contexts.shape)\n",
    "        Covs = cov(DeCov_contexts[0,:,:])\n",
    "        DeCov_loss = 0.5 * (torch.norm(Covs, p = 'fro')**2 - torch.norm(torch.diag(Covs))**2 ) \n",
    "        for i in range(11 -1):\n",
    "            Covs = cov(DeCov_contexts[i+1,:,:])\n",
    "            DeCov_loss += 0.5 * (torch.norm(Covs, p = 'fro')**2 - torch.norm(torch.diag(Covs))**2 ) \n",
    "\n",
    "\n",
    "        return self.final_linear(x), DeCov_loss\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-7):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "def cov(m, y=None):\n",
    "    if y is not None:\n",
    "        m = torch.cat((m, y), dim=0)\n",
    "    m_exp = torch.mean(m, dim=1)\n",
    "    x = m - m_exp[:, None]\n",
    "    cov = 1 / (x.size(1) - 1) * x.mm(x.t())\n",
    "    return cov\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        returned_value = sublayer(self.norm(x))\n",
    "        return x + self.dropout(returned_value[0]) , returned_value[1]\n",
    "\n",
    "class distcare_teacher(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.6):\n",
    "        super(distcare_teacher, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 400)\n",
    "\n",
    "        self.GRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        self.LastStepAttentions = clones(SingleAttention(self.hidden_dim, 8, attention_type='concat', demographic_dim=12, time_aware=True, use_demographic=False),self.input_dim)\n",
    "        \n",
    "        self.FinalAttentionQKV = FinalAttentionQKV(self.hidden_dim, self.hidden_dim, attention_type='mul',dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model,dropout = 1 - self.keep_prob)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.output = nn.Linear(self.input_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, input, lens):\n",
    "        lens = lens.to('cpu')\n",
    "        # input shape [batch_size, timestep, feature_dim]\n",
    "#         demo_main = self.tanh(self.demo_proj_main(demo_input)).unsqueeze(1)# b hidden_dim\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "\n",
    "        \n",
    "        GRU_embeded_input = self.GRUs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "#         print(GRU_embeded_input.shape)\n",
    "        for i in range(feature_dim-1):\n",
    "            embeded_input = self.GRUs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "            GRU_embeded_input = torch.cat((GRU_embeded_input, embeded_input), 1)\n",
    "\n",
    "#         GRU_embeded_input = torch.cat((GRU_embeded_input, demo_main), 1)# b i+1 h\n",
    "        posi_input = self.dropout(GRU_embeded_input) # batch_size * d_input * hidden_dim\n",
    "\n",
    "#         #mask = subsequent_mask(time_step).to(device) # 1 t t \n",
    "#         contexts = self.SublayerConnection(posi_input, lambda x: self.MultiHeadedAttention(posi_input, posi_input, posi_input, None))# # batch_size * d_input * hidden_dim\n",
    "    \n",
    "#         DeCov_loss = contexts[1]\n",
    "#         contexts = contexts[0]\n",
    "\n",
    "#         contexts = self.SublayerConnection(contexts, lambda x: self.PositionwiseFeedForward(contexts))[0]# # batch_size * d_input * hidden_dim\n",
    "#         #contexts = contexts.view(batch_size, feature_dim * self.hidden_dim)#\n",
    "#         # contexts = torch.matmul(self.Wproj, contexts) + self.bproj\n",
    "#         # contexts = contexts.squeeze()\n",
    "#         # demo_key = self.demo_proj(demo_input)# b hidden_dim\n",
    "#         # demo_key = self.relu(demo_key)\n",
    "#         # input_dim_scores = torch.matmul(contexts, demo_key.unsqueeze(-1)).squeeze() # b i\n",
    "#         # input_dim_scores = self.dropout(self.sigmoid(input_dim_scores)).unsqueeze(1)# b i\n",
    "        \n",
    "#         # weighted_contexts = torch.matmul(input_dim_scores, contexts).squeeze()\n",
    "# #         print(contexts.shape)\n",
    "\n",
    "#         weighted_contexts = self.FinalAttentionQKV(contexts)[0]\n",
    "        contexts = self.Linear(posi_input).squeeze()# b i\n",
    "        output = self.output(self.dropout(contexts))# b 1\n",
    "        output = self.sigmoid(output)\n",
    "#         print(weighted_contexts.shape)\n",
    "          \n",
    "        return output, None, contexts\n",
    "    #, self.MultiHeadedAttention.attn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:05:51.991644Z",
     "start_time": "2021-02-10T15:05:42.504740Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED) #numpy\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED) # cpu\n",
    "torch.cuda.manual_seed(RANDOM_SEED) #gpu\n",
    "torch.backends.cudnn.deterministic=True # cudnn\n",
    "    \n",
    "epochs = 150\n",
    "batch_size = 256\n",
    "input_dim = 34\n",
    "hidden_dim = 32\n",
    "d_model = 32\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1\n",
    "\n",
    "model = distcare_teacher(input_dim = input_dim, hidden_dim = hidden_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, output_dim = output_dim).to(device)\n",
    "# input_dim, d_model, d_k, d_v, MHD_num_head, d_ff, output_dim\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T09:16:04.789735Z",
     "start_time": "2021-01-30T04:30:23.650979Z"
    },
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Teacher\n",
    "# If you don't want to train Teacher Model:\n",
    "# - The pretrained taecher model is in direcrtory './model/', and can be directly loaded, \n",
    "# - Simply skip this cell and load the model to validate on Dev Dataset.\n",
    "# logger.info('Training Teacher')\n",
    "\n",
    "# total_train_loss = []\n",
    "# total_valid_loss = []\n",
    "# global_best = 0\n",
    "# auroc = []\n",
    "# auprc = []\n",
    "# minpse = []\n",
    "# history = []\n",
    "\n",
    "# pad_token = np.zeros(input_dim)\n",
    "# # begin_time = time.time()\n",
    "# best_auroc = 0\n",
    "# best_auprc = 0\n",
    "# best_minpse = 0\n",
    "    \n",
    "# if target_dataset == 'TJ':    \n",
    "#     file_name = './model/pretrained-challenge-front-fill-teacher-2covid'\n",
    "# elif target_dataset == 'HM':\n",
    "#     file_name = './model/pretrained-challenge-front-fill-teacher-2spain'\n",
    "\n",
    "# for each_epoch in range(epochs):\n",
    "\n",
    "#     epoch_loss = []\n",
    "#     counter_batch = 0\n",
    "#     model.train()  \n",
    "\n",
    "#     for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(train_x, train_y, train_mask_x, train_x_len, batch_size, shuffle=True)):  \n",
    "#         optimizer.zero_grad()\n",
    "#         batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "#         batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "#         batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#         batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "# #        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "#         opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "\n",
    "#         BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1)) # b t 1\n",
    "# #             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "#         loss = BCE_Loss #+ 1000 * decov_loss\n",
    "\n",
    "#         epoch_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if step % 20 == 0:\n",
    "#             print('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "#             logger.info('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "\n",
    "#     epoch_loss = np.mean(epoch_loss)\n",
    "#     total_train_loss.append(epoch_loss)\n",
    "\n",
    "#     #Validation\n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         valid_loss = []\n",
    "#         valid_true = []\n",
    "#         valid_pred = []\n",
    "#         for batch_x, batch_y, batch_mask_x, batch_lens in batch_iter(dev_x, dev_y, dev_mask_x, dev_x_len, batch_size):\n",
    "#             batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "#             batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "#             batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#             batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "# #            masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "\n",
    "#             opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "\n",
    "#             BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "# #                 REC_Loss = F.mse_loss(recon, batch_x, reduction='mean').to(device)\n",
    "\n",
    "#             valid_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "\n",
    "#             y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "#             y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "#         valid_loss = np.mean(valid_loss)\n",
    "#         total_valid_loss.append(valid_loss)\n",
    "#         ret = metrics.print_metrics_binary(y_true, y_pred,verbose = 0)\n",
    "#         history.append(ret)\n",
    "#         #print()\n",
    "\n",
    "#         print('Epoch %d: Loss = %.4f Valid loss = %.4f roc = %.4f'%(each_epoch, total_train_loss[-1], total_valid_loss[-1], ret['auroc']))\n",
    "#         logger.info('Epoch %d: Loss = %.4f Valid loss = %.4f roc = %.4f'%(each_epoch, total_train_loss[-1], total_valid_loss[-1], ret['auroc']))\n",
    "#         metrics.print_metrics_binary(y_true, y_pred)\n",
    "\n",
    "#         cur_auroc = ret['auroc']\n",
    "#         if cur_auroc > best_auroc:\n",
    "#             best_auroc = cur_auroc\n",
    "#             best_auprc = ret['auprc']\n",
    "#             best_minpse = ret['minpse']\n",
    "#             state = {\n",
    "#                 'net': model.state_dict(),\n",
    "#                 'optimizer': optimizer.state_dict(),\n",
    "#                 'epoch': each_epoch\n",
    "#             }\n",
    "#             torch.save(state, file_name)\n",
    "#             print('------------ Save best model - AUROC: %.4f ------------'%cur_auroc)       \n",
    "\n",
    "# print('auroc %.4f'%(best_auroc))\n",
    "# print('auprc %.4f'%(best_auprc))\n",
    "# print('minpse %.4f'%(best_minpse))  \n",
    "# logger.info('auroc %.4f'%(best_auroc))\n",
    "# logger.info('auprc %.4f'%(best_auprc))\n",
    "# logger.info('minpse %.4f'%(best_minpse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T10:41:54.613732Z",
     "start_time": "2021-01-30T10:41:54.090698Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:08,549 - INFO - last saved model is in epoch 115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last saved model is in epoch 115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "distcare_teacher(\n",
       "  (PositionalEncoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (GRUs): ModuleList(\n",
       "    (0): GRU(1, 32, batch_first=True)\n",
       "    (1): GRU(1, 32, batch_first=True)\n",
       "    (2): GRU(1, 32, batch_first=True)\n",
       "    (3): GRU(1, 32, batch_first=True)\n",
       "    (4): GRU(1, 32, batch_first=True)\n",
       "    (5): GRU(1, 32, batch_first=True)\n",
       "    (6): GRU(1, 32, batch_first=True)\n",
       "    (7): GRU(1, 32, batch_first=True)\n",
       "    (8): GRU(1, 32, batch_first=True)\n",
       "    (9): GRU(1, 32, batch_first=True)\n",
       "    (10): GRU(1, 32, batch_first=True)\n",
       "    (11): GRU(1, 32, batch_first=True)\n",
       "    (12): GRU(1, 32, batch_first=True)\n",
       "    (13): GRU(1, 32, batch_first=True)\n",
       "    (14): GRU(1, 32, batch_first=True)\n",
       "    (15): GRU(1, 32, batch_first=True)\n",
       "    (16): GRU(1, 32, batch_first=True)\n",
       "    (17): GRU(1, 32, batch_first=True)\n",
       "    (18): GRU(1, 32, batch_first=True)\n",
       "    (19): GRU(1, 32, batch_first=True)\n",
       "    (20): GRU(1, 32, batch_first=True)\n",
       "    (21): GRU(1, 32, batch_first=True)\n",
       "    (22): GRU(1, 32, batch_first=True)\n",
       "    (23): GRU(1, 32, batch_first=True)\n",
       "    (24): GRU(1, 32, batch_first=True)\n",
       "    (25): GRU(1, 32, batch_first=True)\n",
       "    (26): GRU(1, 32, batch_first=True)\n",
       "    (27): GRU(1, 32, batch_first=True)\n",
       "    (28): GRU(1, 32, batch_first=True)\n",
       "    (29): GRU(1, 32, batch_first=True)\n",
       "    (30): GRU(1, 32, batch_first=True)\n",
       "    (31): GRU(1, 32, batch_first=True)\n",
       "    (32): GRU(1, 32, batch_first=True)\n",
       "    (33): GRU(1, 32, batch_first=True)\n",
       "  )\n",
       "  (LastStepAttentions): ModuleList(\n",
       "    (0): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (1): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (2): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (3): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (4): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (5): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (6): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (7): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (8): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (9): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (10): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (11): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (12): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (13): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (14): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (15): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (16): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (17): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (18): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (19): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (20): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (21): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (22): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (23): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (24): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (25): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (26): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (27): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (28): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (29): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (30): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (31): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (32): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (33): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "  )\n",
       "  (FinalAttentionQKV): FinalAttentionQKV(\n",
       "    (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_k): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_v): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_out): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "    (tanh): Tanh()\n",
       "    (softmax): Softmax(dim=None)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (MultiHeadedAttention): MultiHeadedAttention(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (final_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "  )\n",
       "  (SublayerConnection): SublayerConnection(\n",
       "    (norm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "  )\n",
       "  (PositionwiseFeedForward): PositionwiseFeedForward(\n",
       "    (w_1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (w_2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (demo_proj_main): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (demo_proj): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (Linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (output): Linear(in_features=34, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.30000000000000004, inplace=False)\n",
       "  (tanh): Tanh()\n",
       "  (softmax): Softmax(dim=None)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if target_dataset == 'PD':    \n",
    "    file_name = './model/pretrained-challenge-front-fill-teacher-2pd'\n",
    "    \n",
    "checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu') )\n",
    "save_epoch = checkpoint['epoch']\n",
    "print(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "logger.info(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T10:42:04.714142Z",
     "start_time": "2021-01-30T10:41:56.611967Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:09,299 - INFO - Batch 0: Test Loss = 0.0931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Test Loss = 0.0931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:13,110 - INFO - \n",
      "==>Predicting on test\n",
      "2023-08-11 12:04:13,112 - INFO - Test Loss = 0.1227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>Predicting on test\n",
      "Test Loss = 0.1227\n",
      "confusion matrix:\n",
      "[[3712   24]\n",
      " [ 133  164]]\n",
      "accuracy = 0.9610711336135864\n",
      "precision class 0 = 0.9654096364974976\n",
      "precision class 1 = 0.8723404407501221\n",
      "recall class 0 = 0.9935759902000427\n",
      "recall class 1 = 0.5521885752677917\n",
      "AUC of ROC = 0.9417155134499888\n",
      "AUC of PRC = 0.7497692675220317\n",
      "min(+P, Se) = 0.7104377104377104\n",
      "f1_score = 0.6762886533793953\n"
     ]
    }
   ],
   "source": [
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "pad_token = np.zeros(34)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(test_x, test_y, test_mask_x, test_x_len, batch_size, shuffle=True)):  \n",
    "        optimizer.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "        opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "\n",
    "        loss = model_loss\n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "            logger.info('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "        y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "        y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "logger.info(\"\\n==>Predicting on test\")\n",
    "logger.info('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "test_res = metrics.print_metrics_binary(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:02.685007Z",
     "start_time": "2021-02-10T15:06:02.655919Z"
    }
   },
   "outputs": [],
   "source": [
    "class distcare_student(nn.Module):\n",
    "    def __init__(self, input_dim, input_diff_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(distcare_student, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        self.input_diff_dim = input_diff_dim\n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 400)\n",
    "\n",
    "        self.GRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        self.generalGRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_diff_dim)\n",
    "        self.LastStepAttentions = clones(SingleAttention(self.hidden_dim, 8, attention_type='concat', demographic_dim=12, time_aware=True, use_demographic=False),self.input_dim)\n",
    "        \n",
    "        self.FinalAttentionQKV = FinalAttentionQKV(self.hidden_dim, self.hidden_dim, attention_type='mul',dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model,dropout = 1 - self.keep_prob)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.output = nn.Linear(self.input_dim + self.input_diff_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.FC_embed = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.to_MMD = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    def forward(self, input, input_diff, lens, tar, train):\n",
    "        lens = lens.to('cpu')\n",
    "        tar_x, tar_lens = tar\n",
    "        # input shape [batch_size, timestep, feature_dim]\n",
    "#         demo_main = self.tanh(self.demo_proj_main(demo_input)).unsqueeze(1)# b hidden_dim\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        feature_dim_diff = input_diff.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "\n",
    "        # Initialization\n",
    "        #cur_hs = Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0))\n",
    "\n",
    "        # forward\n",
    "        # GRU_embeded_input = self.GRUs[0](input[:,:,0].unsqueeze(-1), Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device))[0] # b t h\n",
    "        # Attention_embeded_input = self.LastStepAttentions[0](GRU_embeded_input)[0].unsqueeze(1)# b 1 h\n",
    "        # for i in range(feature_dim-1):\n",
    "        #     embeded_input = self.GRUs[i+1](input[:,:,i+1].unsqueeze(-1), Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device))[0] # b 1 h\n",
    "        #     embeded_input = self.LastStepAttentions[i+1](embeded_input)[0].unsqueeze(1)# b 1 h\n",
    "        #     Attention_embeded_input = torch.cat((Attention_embeded_input, embeded_input), 1)# b i h\n",
    "\n",
    "        # Attention_embeded_input = torch.cat((Attention_embeded_input, demo_main), 1)# b i+1 h\n",
    "        # posi_input = self.dropout(Attention_embeded_input) # batch_size * d_input+1 * hidden_dim\n",
    "\n",
    "#         input = pack_padded_sequence(input, lens, batch_first=True)\n",
    "        \n",
    "        GRU_embeded_input = self.GRUs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "#         print(GRU_embeded_input.shape)\n",
    "        for i in range(feature_dim-1):\n",
    "            embeded_input = self.GRUs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "            GRU_embeded_input = torch.cat((GRU_embeded_input, embeded_input), 1)\n",
    "\n",
    "#         GRU_embeded_input = torch.cat((GRU_embeded_input, demo_main), 1)# b i+1 h\n",
    "        General_GRU_embeded_input = self.generalGRUs[0](pack_padded_sequence(input_diff[:,:,0].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "        for i in range(feature_dim_diff - 1):\n",
    "            general_embeded_input = self.generalGRUs[i + 1](pack_padded_sequence(input_diff[:,:,i].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "            General_GRU_embeded_input = torch.cat((General_GRU_embeded_input,general_embeded_input), 1)\n",
    "        \n",
    "#         posi_input = self.dropout(GRU_embeded_input) # batch_size * d_input * hidden_dim\n",
    "        posi_input = self.dropout(torch.cat((GRU_embeded_input, General_GRU_embeded_input), 1)) # batch_size * d_input * hidden_dim\n",
    "    \n",
    "        # cul tar loss\n",
    "        if train:\n",
    "            tar_lens = tar_lens.to('cpu')\n",
    "            GRU_tar_input = self.GRUs[0](pack_padded_sequence(tar_x[:,:,0].unsqueeze(-1), tar_lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "            for i in range(feature_dim-1):\n",
    "                tar_input = self.GRUs[i+1](pack_padded_sequence(tar_x[:,:,i+1].unsqueeze(-1), tar_lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "                GRU_tar_input = torch.cat((GRU_tar_input, tar_input), 1)\n",
    "            GRU_embeded_output = self.to_MMD(GRU_embeded_input)\n",
    "            GRU_tar_output = self.to_MMD(GRU_tar_input)\n",
    "        else:\n",
    "            GRU_embeded_output = []\n",
    "            GRU_tar_output = []\n",
    "        \n",
    "        #mask = subsequent_mask(time_step).to(device) # 1 t t 下三角 N to 1任务不用mask\n",
    "        # contexts = self.SublayerConnection(posi_input, lambda x: self.MultiHeadedAttention(posi_input, posi_input, posi_input, None))# # batch_size * d_input * hidden_dim\n",
    "    \n",
    "        # DeCov_loss = contexts[1]\n",
    "        # contexts = contexts[0]\n",
    "\n",
    "        # contexts = self.SublayerConnection(contexts, lambda x: self.PositionwiseFeedForward(contexts))[0]# # batch_size * d_input * hidden_dim\n",
    "        #contexts = contexts.view(batch_size, feature_dim * self.hidden_dim)#\n",
    "        # contexts = torch.matmul(self.Wproj, contexts) + self.bproj\n",
    "        # contexts = contexts.squeeze()\n",
    "        # demo_key = self.demo_proj(demo_input)# b hidden_dim\n",
    "        # demo_key = self.relu(demo_key)\n",
    "        # input_dim_scores = torch.matmul(contexts, demo_key.unsqueeze(-1)).squeeze() # b i\n",
    "        # input_dim_scores = self.dropout(self.sigmoid(input_dim_scores)).unsqueeze(1)# b i\n",
    "        \n",
    "        # weighted_contexts = torch.matmul(input_dim_scores, contexts).squeeze()\n",
    "#         print(contexts.shape)\n",
    "\n",
    "        # weighted_contexts = self.FinalAttentionQKV(contexts)[0]\n",
    "        # output_embed = self.FC_embed(weighted_contexts)\n",
    "        # output = self.output(self.dropout(weighted_contexts))# b 1\n",
    "        # output = self.sigmoid(output)\n",
    "#         print(weighted_contexts.shape)\n",
    "        contexts = self.Linear(posi_input).squeeze()# b i\n",
    "        output = self.output(self.dropout(contexts))# b 1\n",
    "        output = self.sigmoid(output)\n",
    "          \n",
    "        return output, None, contexts, [GRU_embeded_output, GRU_tar_output]\n",
    "    #, self.MultiHeadedAttention.attn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:13,170 - INFO - load target data\n",
      "2023-08-11 12:04:13,187 - INFO - [[-0.84276482  0.37440202  0.67961237 -1.39897541 -0.48314192 -0.21203008\n",
      "   1.58875966  0.79457896 -0.86126933 -0.48197292 -0.67452243  0.71372084\n",
      "  -1.44708995 -0.77101282 -1.42318156 -0.58514053]\n",
      " [-0.84276482  0.37440202  0.67961237 -1.39897541 -0.48314192 -0.21203008\n",
      "   1.58875966  0.79457896 -0.86126933 -0.48197292 -0.67452243  0.71372084\n",
      "  -1.44708995 -0.77101282 -1.42318156 -0.58514053]\n",
      " [-0.53383302  0.43115698  0.89646606 -1.34242175 -0.69460513 -0.21203008\n",
      "   0.89527972  0.03870413 -0.86126933  0.07263348 -0.50511021 -0.28657272\n",
      "   0.39888266 -1.37943777 -0.82370897 -0.92211195]\n",
      " [-0.71138003  0.09062723  0.55763217 -1.39897541 -0.8637757  -0.21203008\n",
      "   0.46852283  0.33565496 -0.86126933 -0.57440732 -0.50142734 -0.17115423\n",
      "  -0.25396131 -0.77101282 -0.67384083 -2.43848334]\n",
      " [-0.71138003  0.09062723  0.55763217 -1.39897541 -0.8637757  -0.21203008\n",
      "   0.46852283  0.33565496 -0.86126933 -0.57440732 -0.50142734 -0.17115423\n",
      "  -0.25396131  0.64864541  0.0754999  -0.58514053]\n",
      " [-0.51252738  0.60142185  0.50341875 -1.45552908 -0.52543456 -0.41638034\n",
      "   0.7885905   0.2816639  -0.58901728  0.59026612 -0.34306374 -0.74824667\n",
      "  -0.18642573  0.64864541  0.0754999  -0.58514053]\n",
      " [-0.46281422 -0.24990251 -0.37754938 -1.11620709 -1.17392174  0.19667043\n",
      "   0.68190128  0.4706326  -0.60716742 -0.14920908 -0.335698   -0.44046404\n",
      "  -1.2444832   0.39513501 -0.52397268 -0.83786909]\n",
      " [-0.46281422 -0.24990251 -0.37754938 -1.11620709 -1.17392174  0.19667043\n",
      "   0.68190128  0.4706326  -0.60716742 -0.14920908 -0.335698   -0.44046404\n",
      "  -1.2444832   0.69934749 -0.67384083 -0.92211195]\n",
      " [-0.64391217  0.03387228 -0.17424904 -1.37069858 -0.55362966  0.40102068\n",
      "   1.24201969  0.4706326  -0.60716742 -0.87019741 -0.4351356   0.02120991\n",
      "  -1.06438832  0.69934749 -0.67384083 -0.92211195]\n",
      " [-0.64391217  0.03387228 -0.17424904 -1.37069858 -0.55362966  0.40102068\n",
      "   1.24201969  0.4706326  -0.60716742 -0.87019741 -0.4351356   0.02120991\n",
      "  -1.06438832  1.05426204 -0.67384083 -0.0796834 ]\n",
      " [-1.03806653  0.26089211  0.16458485 -1.45552908 -1.6391408   0.19667043\n",
      "   1.24201969  0.4706326  -0.66161783 -0.5004598  -0.46091572  0.02120991\n",
      "  -1.17694762  0.09092253 -1.34824748 -1.25908337]\n",
      " [-0.89957987  0.48791194  0.15103149 -1.41311383 -0.65231249  0.19667043\n",
      "   0.14845517  0.4706326  -0.75236851 -0.94414493 -0.950738    0.17510123\n",
      "  -1.04187645  0.54724125 -0.67384083 -1.0063548 ]\n",
      " [-0.5764443   0.31764706 -0.12003562 -1.3000065  -0.63821494  0.19667043\n",
      "   0.86860741  0.4706326  -0.80681892 -0.24164348 -1.00229824 -4.7109481\n",
      "   6.99485797  0.54724125 -0.67384083 -1.0063548 ]\n",
      " [-0.5764443   0.31764706 -0.12003562 -1.3000065  -0.63821494  0.19667043\n",
      "   0.86860741  0.4706326  -0.80681892 -0.24164348 -1.00229824 -4.7109481\n",
      "   6.99485797  0.24302877  0.82484063 -1.42756908]\n",
      " [-0.2320031  -0.64718721 -0.41820945 -1.15862233 -1.32899476 -0.21203008\n",
      "   0.25514439  1.1995119  -0.75236851 -0.33407788 -0.93600651 -0.09420857\n",
      "  -0.09637829  0.24302877  0.82484063 -1.42756908]\n",
      " [-0.2320031  -0.64718721 -0.41820945 -1.15862233 -1.32899476 -0.21203008\n",
      "   0.25514439  1.1995119  -0.75236851 -0.33407788 -0.93600651 -0.09420857\n",
      "  -0.09637829 -1.07522529 -1.04851119  2.27911655]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -1.07522529 -1.04851119  2.27911655]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -0.26399202 -0.67384083  2.27911655]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -0.77101282 -0.67384083 -0.66938338]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -0.77101282 -0.67384083  0.08880231]]\n",
      "2023-08-11 12:04:13,192 - INFO - 16\n",
      "2023-08-11 12:04:13,193 - INFO - 325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.84276482  0.37440202  0.67961237 -1.39897541 -0.48314192 -0.21203008\n",
      "   1.58875966  0.79457896 -0.86126933 -0.48197292 -0.67452243  0.71372084\n",
      "  -1.44708995 -0.77101282 -1.42318156 -0.58514053]\n",
      " [-0.84276482  0.37440202  0.67961237 -1.39897541 -0.48314192 -0.21203008\n",
      "   1.58875966  0.79457896 -0.86126933 -0.48197292 -0.67452243  0.71372084\n",
      "  -1.44708995 -0.77101282 -1.42318156 -0.58514053]\n",
      " [-0.53383302  0.43115698  0.89646606 -1.34242175 -0.69460513 -0.21203008\n",
      "   0.89527972  0.03870413 -0.86126933  0.07263348 -0.50511021 -0.28657272\n",
      "   0.39888266 -1.37943777 -0.82370897 -0.92211195]\n",
      " [-0.71138003  0.09062723  0.55763217 -1.39897541 -0.8637757  -0.21203008\n",
      "   0.46852283  0.33565496 -0.86126933 -0.57440732 -0.50142734 -0.17115423\n",
      "  -0.25396131 -0.77101282 -0.67384083 -2.43848334]\n",
      " [-0.71138003  0.09062723  0.55763217 -1.39897541 -0.8637757  -0.21203008\n",
      "   0.46852283  0.33565496 -0.86126933 -0.57440732 -0.50142734 -0.17115423\n",
      "  -0.25396131  0.64864541  0.0754999  -0.58514053]\n",
      " [-0.51252738  0.60142185  0.50341875 -1.45552908 -0.52543456 -0.41638034\n",
      "   0.7885905   0.2816639  -0.58901728  0.59026612 -0.34306374 -0.74824667\n",
      "  -0.18642573  0.64864541  0.0754999  -0.58514053]\n",
      " [-0.46281422 -0.24990251 -0.37754938 -1.11620709 -1.17392174  0.19667043\n",
      "   0.68190128  0.4706326  -0.60716742 -0.14920908 -0.335698   -0.44046404\n",
      "  -1.2444832   0.39513501 -0.52397268 -0.83786909]\n",
      " [-0.46281422 -0.24990251 -0.37754938 -1.11620709 -1.17392174  0.19667043\n",
      "   0.68190128  0.4706326  -0.60716742 -0.14920908 -0.335698   -0.44046404\n",
      "  -1.2444832   0.69934749 -0.67384083 -0.92211195]\n",
      " [-0.64391217  0.03387228 -0.17424904 -1.37069858 -0.55362966  0.40102068\n",
      "   1.24201969  0.4706326  -0.60716742 -0.87019741 -0.4351356   0.02120991\n",
      "  -1.06438832  0.69934749 -0.67384083 -0.92211195]\n",
      " [-0.64391217  0.03387228 -0.17424904 -1.37069858 -0.55362966  0.40102068\n",
      "   1.24201969  0.4706326  -0.60716742 -0.87019741 -0.4351356   0.02120991\n",
      "  -1.06438832  1.05426204 -0.67384083 -0.0796834 ]\n",
      " [-1.03806653  0.26089211  0.16458485 -1.45552908 -1.6391408   0.19667043\n",
      "   1.24201969  0.4706326  -0.66161783 -0.5004598  -0.46091572  0.02120991\n",
      "  -1.17694762  0.09092253 -1.34824748 -1.25908337]\n",
      " [-0.89957987  0.48791194  0.15103149 -1.41311383 -0.65231249  0.19667043\n",
      "   0.14845517  0.4706326  -0.75236851 -0.94414493 -0.950738    0.17510123\n",
      "  -1.04187645  0.54724125 -0.67384083 -1.0063548 ]\n",
      " [-0.5764443   0.31764706 -0.12003562 -1.3000065  -0.63821494  0.19667043\n",
      "   0.86860741  0.4706326  -0.80681892 -0.24164348 -1.00229824 -4.7109481\n",
      "   6.99485797  0.54724125 -0.67384083 -1.0063548 ]\n",
      " [-0.5764443   0.31764706 -0.12003562 -1.3000065  -0.63821494  0.19667043\n",
      "   0.86860741  0.4706326  -0.80681892 -0.24164348 -1.00229824 -4.7109481\n",
      "   6.99485797  0.24302877  0.82484063 -1.42756908]\n",
      " [-0.2320031  -0.64718721 -0.41820945 -1.15862233 -1.32899476 -0.21203008\n",
      "   0.25514439  1.1995119  -0.75236851 -0.33407788 -0.93600651 -0.09420857\n",
      "  -0.09637829  0.24302877  0.82484063 -1.42756908]\n",
      " [-0.2320031  -0.64718721 -0.41820945 -1.15862233 -1.32899476 -0.21203008\n",
      "   0.25514439  1.1995119  -0.75236851 -0.33407788 -0.93600651 -0.09420857\n",
      "  -0.09637829 -1.07522529 -1.04851119  2.27911655]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -1.07522529 -1.04851119  2.27911655]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -0.26399202 -0.67384083  2.27911655]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -0.77101282 -0.67384083 -0.66938338]\n",
      " [-0.2142484  -0.02288268  0.01549794 -1.08793025 -0.92016589  0.19667043\n",
      "  -1.05179858  1.1995119  -0.75236851  0.96000372 -0.70398543 -0.5174097\n",
      "   0.62400127 -0.77101282 -0.67384083  0.08880231]]\n",
      "16\n",
      "325\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"load target data\")\n",
    "if target_dataset == 'PD':\n",
    "    data_path = './data/PD/'\n",
    "    tar_all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    tar_all_time = pickle.load(open(data_path + 'y_z.pkl', 'rb'))\n",
    "    tar_all_x_len = [len(i) for i in tar_all_x]\n",
    "\n",
    "    tar_subset_idx = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    tar_other_idx = list(range(69))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(tar_all_x)):\n",
    "        cur = np.array(tar_all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        tar_all_x[i] = cur_subset\n",
    "    \n",
    "print(tar_all_x[0])\n",
    "print(len(tar_all_x[0][0]))\n",
    "print(len(tar_all_x))\n",
    "logger.info(tar_all_x[0])\n",
    "logger.info(len(tar_all_x[0][0]))\n",
    "logger.info(len(tar_all_x))\n",
    "\n",
    "assert(subset_cnt == len(tar_subset_idx))\n",
    "\n",
    "examples = []\n",
    "for idx in range(len(tar_all_x)):\n",
    "    examples.append((tar_all_x[idx], tar_all_time[idx], tar_all_x_len[idx]))\n",
    "examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "tar_all_x = [e[0] for e in examples]\n",
    "tar_all_time = [e[1] for e in examples]\n",
    "tar_all_x_len = [e[2] for e in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:06.077135Z",
     "start_time": "2021-02-10T15:06:05.101451Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 256\n",
    "# input_dim = subset_cnt\n",
    "common_dim = subset_cnt \n",
    "diff_dim = input_dim - subset_cnt\n",
    "hidden_dim = 32\n",
    "d_model = 32\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1\n",
    "\n",
    "model_student = distcare_student(input_dim = common_dim, input_diff_dim = diff_dim, hidden_dim = hidden_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, output_dim = output_dim).to(device)\n",
    "\n",
    "# input_dim, d_model, d_k, d_v, MHD_num_head, d_ff, output_dim\n",
    "optimizer_student = torch.optim.Adam(model_student.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T10:43:10.804571Z",
     "start_time": "2021-01-30T10:42:10.264393Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:13,689 - INFO - Batch 0: Test Loss = 0.1131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Test Loss = 0.1131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:19,645 - INFO - Batch 20: Test Loss = 0.1478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20: Test Loss = 0.1478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:24,738 - INFO - Batch 40: Test Loss = 0.1526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40: Test Loss = 0.1526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:29,684 - INFO - Batch 60: Test Loss = 0.1207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 60: Test Loss = 0.1207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:34,092 - INFO - Batch 80: Test Loss = 0.0851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 80: Test Loss = 0.0851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:39,201 - INFO - Batch 100: Test Loss = 0.0724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100: Test Loss = 0.0724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:43,614 - INFO - Batch 120: Test Loss = 0.1097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 120: Test Loss = 0.1097\n"
     ]
    }
   ],
   "source": [
    "#Generate Teacher model embedding\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.eval()\n",
    "\n",
    "train_teacher_emb = []\n",
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "pad_token = np.zeros(34)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(train_x, train_y, train_mask_x, train_x_len, batch_size, shuffle=False)):  \n",
    "        optimizer.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "        opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "        train_teacher_emb.append(emb.cpu().detach().numpy())\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, model_loss.cpu().detach().numpy()))\n",
    "            logger.info('Batch %d: Test Loss = %.4f'%(step, model_loss.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_all_x = torch.tensor(pad_sents(tar_all_x, np.zeros(len(tar_subset_idx))), dtype=torch.float32).to(device)\n",
    "tar_all_x_len = torch.tensor(tar_all_x_len, dtype=torch.float32).to(device).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5, fix_sigma=None, **kwargs):\n",
    "        super(MMDLoss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = None\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul, kernel_num, fix_sigma):\n",
    "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2)\n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
    "                          for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
    "                      for bandwidth_temp in bandwidth_list]\n",
    "        return sum(kernel_val)\n",
    "\n",
    "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
    "        loss = 0.0\n",
    "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
    "        loss = delta.dot(delta.T)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        if self.kernel_type == 'linear':\n",
    "            return self.linear_mmd2(source, target)\n",
    "        elif self.kernel_type == 'rbf':\n",
    "            batch_size = int(source.size()[0])\n",
    "            kernels = self.guassian_kernel(\n",
    "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
    "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
    "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
    "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
    "            loss = torch.mean(XX + YY - XY - YX)\n",
    "            return loss\n",
    "\n",
    "class MultitaskLoss(nn.Module):\n",
    "    def __init__(self, task_num=3):\n",
    "        super(MultitaskLoss, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.alpha = nn.Parameter(torch.ones((task_num)), requires_grad=True)\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.kl = nn.KLDivLoss(reduce=True, size_average=True)\n",
    "        self.tar = MMDLoss()\n",
    "\n",
    "    def forward(self, opt_student, batch_y, emb_student, emb_teacher, tar_source, tar_tar):\n",
    "        BCE_Loss = self.bce(opt_student, batch_y)\n",
    "        emb_Loss = self.kl(emb_student, emb_teacher)\n",
    "        tar_Loss = self.tar(source=tar_source, target=tar_tar)\n",
    "        return BCE_Loss * self.alpha[0] + emb_Loss * self.alpha[1] + tar_Loss * self.alpha[2]\n",
    "\n",
    "def get_multitask_loss(opt_student, batch_y, emb_student, emb_teacher, tar_source, tar_tar):\n",
    "    mtl = MultitaskLoss(task_num=3)\n",
    "    return mtl(opt_student, batch_y, emb_student, emb_teacher, tar_source, tar_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T15:39:24.489286Z",
     "start_time": "2021-01-30T10:43:58.619303Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Training Student\n",
    "# # If you don't want to train Student Model:\n",
    "# # - The pretrained student model is in direcrtory './model/', and can be directly loaded, \n",
    "# # - Simply skip this cell and load the model to validate on Dev Dataset.\n",
    "\n",
    "# logger.info('Training Student')\n",
    "# teacher_flag = True\n",
    "# epochs = 30\n",
    "# total_train_loss = []\n",
    "# total_valid_loss = []\n",
    "# global_best = 0\n",
    "# auroc = []\n",
    "# auprc = []\n",
    "# minpse = []\n",
    "# history = []\n",
    "\n",
    "# pad_token = np.zeros(34)\n",
    "# # begin_time = time.time()\n",
    "# best_auroc = 0\n",
    "# best_auprc = 0\n",
    "# best_minpse = 0\n",
    "\n",
    "# if target_dataset == 'PD':    \n",
    "#     data_str = 'pd'\n",
    "\n",
    "\n",
    "# if teacher_flag:\n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str\n",
    "# else: \n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher'\n",
    "\n",
    "# for each_epoch in range(epochs):\n",
    "\n",
    "#     epoch_loss = []\n",
    "#     counter_batch = 0\n",
    "#     model_student.train()  \n",
    "#     model.eval()\n",
    "#     for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(train_x, train_y, train_mask_x, train_x_len, batch_size, shuffle=False)):  \n",
    "#         optimizer_student.zero_grad()\n",
    "#         batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "#         batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "#         batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#         batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "# #        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "#         opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "#         emb_teacher = torch.tensor(train_teacher_emb[step], dtype=torch.float32).to(device)\n",
    "#         # opt_teacher, decov_loss_teacher, emb_teacher = model(batch_x, batch_lens)\n",
    "# #         BCE_Loss = get_loss(opt_student, batch_y.unsqueeze(-1)) # b t 1\n",
    "#         #emb_Loss = 0.1 * get_re_loss(emb_student, emb_teacher.detach())\n",
    "#         if teacher_flag:\n",
    "#             emb_student = F.log_softmax(emb_student, dim=1)\n",
    "#             emb_teacher = F.softmax(emb_teacher.detach(), dim=1)\n",
    "# #             emb_Loss = get_kl_loss(emb_student, emb_teacher)\n",
    "            \n",
    "#             tar_source = F.softmax(tar_result[0].squeeze(), dim=-1)\n",
    "#             tar_tar = F.softmax(tar_result[1].squeeze(), dim=-1)\n",
    "# #             shrink_indices = torch.tensor(np.random.choice(range(len(tar_tar)), len(tar_source))).to(device)\n",
    "# #             tar_tar = torch.index_select(tar_tar, 0, shrink_indices)\n",
    "# #             tar_Loss = get_kl_loss(tar_source, tar_tar)\n",
    "# #             logger.info(tar_tar)\n",
    "# #             logger.info(tar_source)\n",
    "# #             logger.info(tar_Loss)\n",
    "#             loss = get_multitask_loss(opt_student, batch_y.unsqueeze(-1), emb_student, emb_teacher, tar_source, tar_tar)\n",
    "#         # emb_Loss = 0.1 * get_wass_dist(emb_student, emb_teacher.detach())\n",
    "# #             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "#         else:\n",
    "#             loss = BCE_Loss #+ decov_loss_student\n",
    "\n",
    "#         epoch_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model_student.parameters(), 20)\n",
    "#         optimizer_student.step()\n",
    "\n",
    "#         if step % 20 == 0:\n",
    "#             print('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "#             logger.info('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "\n",
    "#     epoch_loss = np.mean(epoch_loss)\n",
    "#     total_train_loss.append(epoch_loss)\n",
    "\n",
    "#     #Validation\n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         model_student.eval()\n",
    "#         valid_loss = []\n",
    "#         valid_true = []\n",
    "#         valid_pred = []\n",
    "#         for batch_x, batch_y, batch_mask_x, batch_lens in batch_iter(dev_x, dev_y, dev_mask_x, dev_x_len, batch_size):\n",
    "#             batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "#             batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "#             batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#             batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "# #            masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "\n",
    "#             opt_student, decov_loss_student, emb, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [[], []], False)\n",
    "\n",
    "#             BCE_Loss = get_loss(opt_student, batch_y.unsqueeze(-1))\n",
    "# #                 REC_Loss = F.mse_loss(recon, batch_x, reduction='mean').to(device)\n",
    "\n",
    "#             valid_loss.append(BCE_Loss.cpu().detach().numpy())\n",
    "\n",
    "#             y_pred += list(opt_student.cpu().detach().numpy().flatten())\n",
    "#             y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "#         valid_loss = np.mean(valid_loss)\n",
    "#         total_valid_loss.append(valid_loss)\n",
    "#         ret = metrics.print_metrics_binary(y_true, y_pred,verbose = 0)\n",
    "#         history.append(ret)\n",
    "#         #print()\n",
    "\n",
    "#         print('Epoch %d: Loss = %.4f Valid loss = %.4f roc = %.4f'%(each_epoch, total_train_loss[-1], total_valid_loss[-1], ret['auroc']))\n",
    "#         metrics.print_metrics_binary(y_true, y_pred)\n",
    "\n",
    "#         cur_auroc = ret['auroc']\n",
    "#         if cur_auroc > best_auroc:\n",
    "#             best_auroc = cur_auroc\n",
    "#             best_auprc = ret['auprc']\n",
    "#             best_minpse = ret['minpse']\n",
    "#             state = {\n",
    "#                 'net': model_student.state_dict(),\n",
    "#                 'optimizer': optimizer_student.state_dict(),\n",
    "#                 'epoch': each_epoch\n",
    "#             }\n",
    "#             torch.save(state, file_name)\n",
    "#             print('------------ Save best model - AUROC: %.4f ------------'%cur_auroc)\n",
    "#             logger.info('------------ Save best model - AUROC: %.4f ------------'%cur_auroc)\n",
    "\n",
    "# print('auroc %.4f'%(best_auroc))\n",
    "# print('auprc %.4f'%(best_auprc))\n",
    "# print('minpse %.4f'%(best_minpse)) \n",
    "# logger.info('auroc %.4f'%(best_auroc))\n",
    "# logger.info('auprc %.4f'%(best_auprc))\n",
    "# logger.info('minpse %.4f'%(best_minpse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:17.118640Z",
     "start_time": "2021-02-10T15:06:15.660556Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:45,681 - INFO - last saved model is in epoch 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last saved model is in epoch 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "distcare_student(\n",
       "  (PositionalEncoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (GRUs): ModuleList(\n",
       "    (0): GRU(1, 32, batch_first=True)\n",
       "    (1): GRU(1, 32, batch_first=True)\n",
       "    (2): GRU(1, 32, batch_first=True)\n",
       "    (3): GRU(1, 32, batch_first=True)\n",
       "    (4): GRU(1, 32, batch_first=True)\n",
       "    (5): GRU(1, 32, batch_first=True)\n",
       "    (6): GRU(1, 32, batch_first=True)\n",
       "    (7): GRU(1, 32, batch_first=True)\n",
       "    (8): GRU(1, 32, batch_first=True)\n",
       "    (9): GRU(1, 32, batch_first=True)\n",
       "    (10): GRU(1, 32, batch_first=True)\n",
       "    (11): GRU(1, 32, batch_first=True)\n",
       "    (12): GRU(1, 32, batch_first=True)\n",
       "    (13): GRU(1, 32, batch_first=True)\n",
       "    (14): GRU(1, 32, batch_first=True)\n",
       "    (15): GRU(1, 32, batch_first=True)\n",
       "  )\n",
       "  (generalGRUs): ModuleList(\n",
       "    (0): GRU(1, 32, batch_first=True)\n",
       "    (1): GRU(1, 32, batch_first=True)\n",
       "    (2): GRU(1, 32, batch_first=True)\n",
       "    (3): GRU(1, 32, batch_first=True)\n",
       "    (4): GRU(1, 32, batch_first=True)\n",
       "    (5): GRU(1, 32, batch_first=True)\n",
       "    (6): GRU(1, 32, batch_first=True)\n",
       "    (7): GRU(1, 32, batch_first=True)\n",
       "    (8): GRU(1, 32, batch_first=True)\n",
       "    (9): GRU(1, 32, batch_first=True)\n",
       "    (10): GRU(1, 32, batch_first=True)\n",
       "    (11): GRU(1, 32, batch_first=True)\n",
       "    (12): GRU(1, 32, batch_first=True)\n",
       "    (13): GRU(1, 32, batch_first=True)\n",
       "    (14): GRU(1, 32, batch_first=True)\n",
       "    (15): GRU(1, 32, batch_first=True)\n",
       "    (16): GRU(1, 32, batch_first=True)\n",
       "    (17): GRU(1, 32, batch_first=True)\n",
       "  )\n",
       "  (LastStepAttentions): ModuleList(\n",
       "    (0): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (1): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (2): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (3): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (4): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (5): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (6): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (7): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (8): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (9): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (10): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (11): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (12): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (13): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (14): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "    (15): SingleAttention(\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "  )\n",
       "  (FinalAttentionQKV): FinalAttentionQKV(\n",
       "    (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_k): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_v): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (W_out): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (tanh): Tanh()\n",
       "    (softmax): Softmax(dim=None)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (MultiHeadedAttention): MultiHeadedAttention(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (final_linear): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (SublayerConnection): SublayerConnection(\n",
       "    (norm): LayerNorm()\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (PositionwiseFeedForward): PositionwiseFeedForward(\n",
       "    (w_1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (w_2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (demo_proj_main): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (demo_proj): Linear(in_features=12, out_features=32, bias=True)\n",
       "  (Linear): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (output): Linear(in_features=34, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (FC_embed): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (softmax): Softmax(dim=None)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       "  (to_MMD): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_flag = True\n",
    "\n",
    "if target_dataset == 'PD':    \n",
    "    data_str = 'pd'\n",
    "\n",
    "if teacher_flag:\n",
    "    file_name = './model/pretrained-challenge-front-fill-2'+ data_str\n",
    "else: \n",
    "    file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher'\n",
    "\n",
    "checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu') )\n",
    "save_epoch = checkpoint['epoch']\n",
    "print(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "logger.info(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "model_student.load_state_dict(checkpoint['net'])\n",
    "optimizer_student.load_state_dict(checkpoint['optimizer'])\n",
    "model_student.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-30T15:40:05.462185Z",
     "start_time": "2021-01-30T15:40:00.917653Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:46,161 - INFO - Batch 0: Test Loss = 0.2126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Test Loss = 0.2126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:04:50,981 - INFO - \n",
      "==>Predicting on test\n",
      "2023-08-11 12:04:50,983 - INFO - Test Loss = 0.1377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==>Predicting on test\n",
      "Test Loss = 0.1377\n",
      "confusion matrix:\n",
      "[[3713   23]\n",
      " [ 148  149]]\n",
      "accuracy = 0.9575998187065125\n",
      "precision class 0 = 0.9616679549217224\n",
      "precision class 1 = 0.8662790656089783\n",
      "recall class 0 = 0.993843674659729\n",
      "recall class 1 = 0.5016834735870361\n",
      "AUC of ROC = 0.9284574870763308\n",
      "AUC of PRC = 0.7098101411898149\n",
      "min(+P, Se) = 0.6734006734006734\n",
      "f1_score = 0.6353944049515263\n"
     ]
    }
   ],
   "source": [
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    model_student.eval()\n",
    "    for step, (batch_x, batch_y, batch_mask_x, batch_lens) in enumerate(batch_iter(test_x, test_y, test_mask_x, test_x_len, batch_size, shuffle=True)):  \n",
    "        optimizer_student.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "        opt, decov_loss, emb, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [[], []], False)\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "\n",
    "        loss = model_loss\n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "            logger.info('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "        y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "        y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "logger.info(\"\\n==>Predicting on test\")\n",
    "logger.info('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "test_res = metrics.print_metrics_binary(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Target Dataset & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(source_x) is 40336\n",
      "len(source_x_diff) is 40336\n",
      "len(target_x_diff) is 325\n"
     ]
    }
   ],
   "source": [
    "# if target_dataset == 'TJ':\n",
    "#     source_common_idx = [27, 29, 18, 16, 26, 33, 28, 31, 32, 15, 11, 25, 21, 20, 9, 17, 30, 19]\n",
    "#     target_common_idx = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "#     source_data_path = './data/Challenge/'\n",
    "#     source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "#     target_data_path = './data/Tongji/'\n",
    "#     target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "\n",
    "# elif target_dataset == 'HM':\n",
    "#     source_common_idx = [0, 1, 2, 3, 5, 9, 11, 12, 13, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
    "#     target_common_idx = [5, 6, 4, 2, 3, 48, 79, 76, 87, 25, 30, 31, 18, 43, 58, 66, 40, 57, 23, 92, 50, 54, 91, 60, 39, 81]\n",
    "#     source_data_path = './data/Challenge/'\n",
    "#     source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "#     target_data_path = './data/CDSL/'\n",
    "#     target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    source_common_idx = [31, 29, 28, 33, 25, 18, 7, 21, 16, 15, 19, 17, 24, 3, 5, 0]\n",
    "    target_common_idx = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    source_data_path = './data/Challenge/'\n",
    "    source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "    target_data_path = './data/PD/'\n",
    "    target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "\n",
    "assert(len(source_common_idx) == len(target_common_idx))\n",
    "common_len = len(source_common_idx)\n",
    "source_x_diff = []\n",
    "target_x_diff = []\n",
    "\n",
    "source_total_len = 34\n",
    "source_other_idx = list(range(source_total_len))\n",
    "for i in source_common_idx:\n",
    "    source_other_idx.remove(i)\n",
    "\n",
    "# if target_dataset == 'TJ':\n",
    "#     target_other_idx = list(range(75))\n",
    "#     target_total_len = 75\n",
    "# else:\n",
    "#     target_other_idx = list(range(99))\n",
    "#     target_total_len = 99\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    target_total_len = 69\n",
    "    target_other_idx = list(range(target_total_len))\n",
    "    for i in target_common_idx:\n",
    "        target_other_idx.remove(i)\n",
    "\n",
    "print(f'len(source_x) is {len(source_x)}')\n",
    "for i in range(len(source_x)):\n",
    "    cur = np.array(source_x[i], dtype=float)\n",
    "    cur_subset = cur[:, source_common_idx]\n",
    "    cur_other = cur[:, source_other_idx]\n",
    "    source_x_diff.append(cur_other.tolist())\n",
    "    source_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "print(f'len(source_x_diff) is {len(source_x_diff)}')\n",
    "for i in range(len(target_x)):\n",
    "    cur = np.array(target_x[i], dtype=float)\n",
    "    cur_subset = cur[:, target_common_idx]\n",
    "    cur_other = cur[:, target_other_idx]\n",
    "    target_x_diff.append(cur_other.tolist())\n",
    "    target_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "print(f'len(target_x_diff) is {len(target_x_diff)}')\n",
    "\n",
    "source_max = 0\n",
    "for i in range(len(source_x_diff)):\n",
    "    if source_max < len(source_x_diff[i]):\n",
    "        source_max = len(source_x_diff[i])\n",
    "\n",
    "source_x_diff_longest = max(list(len(_) for _ in source_x_diff))\n",
    "source_batch = len(source_x_diff)\n",
    "source_features = source_total_len - common_len\n",
    "source_x_diff_ex = torch.zeros((source_batch, source_x_diff_longest, source_features))\n",
    "\n",
    "for i in range(len(source_x_diff)):\n",
    "    for j in range(source_x_diff_longest):\n",
    "        cur_len = len(source_x_diff[i])\n",
    "        if j < cur_len:\n",
    "            source_x_diff_ex[i,j,:] = torch.Tensor(source_x_diff[i])[j,:]\n",
    "        else:\n",
    "            source_x_diff_ex[i,j,:] = torch.Tensor(source_x_diff[i])[cur_len - 1,:]\n",
    "\n",
    "\n",
    "target_x_diff_longest = max(list(len(_) for _ in target_x_diff))\n",
    "target_batch = len(target_x_diff)\n",
    "target_features = target_total_len - common_len\n",
    "target_x_diff_ex = torch.zeros((target_batch, target_x_diff_longest, target_features))\n",
    "\n",
    "for i in range(len(target_x_diff)):\n",
    "    for j in range(target_x_diff_longest):\n",
    "        cur_len = len(target_x_diff[i])\n",
    "        if j < cur_len:\n",
    "            target_x_diff_ex[i,j,:] = torch.Tensor(target_x_diff[i])[j,:]\n",
    "        else:\n",
    "            target_x_diff_ex[i,j,:] = torch.Tensor(target_x_diff[i])[cur_len - 1,:]\n",
    "\n",
    "# pad_token_source = [0] * (source_total_len - common_len)\n",
    "# source_x_diff_ex = pad_sents(source_x_diff, pad_token_source)\n",
    "# print(f'source_x_diff_ex.shape is {np.array(source_x_diff_ex).shape}, max_len is {source_max}')\n",
    "\n",
    "# target_max = 0\n",
    "# for i in range(len(target_x_diff)):\n",
    "#     if target_max < len(target_x_diff[i]):\n",
    "#         target_max = len(target_x_diff[i])\n",
    "\n",
    "# pad_token_target = [0] * (target_total_len - common_len)\n",
    "# target_x_diff_ex = pad_sents(target_x_diff, pad_token_target)\n",
    "    \n",
    "# print(f'target_x_diff_ex.shape is {np.array(target_x_diff_ex).shape}, max is {target_max}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if target_dataset == 'TJ':\n",
    "#     data_path = './data/Tongji/'\n",
    "#     all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "#     all_y = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "#     all_time = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "#     all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "#     for i in range(len(all_time)):\n",
    "#         for j in range(len(all_time[i])):\n",
    "#             all_time[i][j] = all_time[i][j][-1]\n",
    "#             all_y[i][j] = all_y[i][j][0]\n",
    "\n",
    "#     tar_subset_idx = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "#     tar_other_idx = list(range(75))\n",
    "#     for i in tar_subset_idx:\n",
    "#         tar_other_idx.remove(i)\n",
    "#     for i in range(len(all_x)):\n",
    "#         cur = np.array(all_x[i], dtype=float)\n",
    "#         cur_subset = cur[:, tar_subset_idx]\n",
    "#         cur_other = cur[:, tar_other_idx]\n",
    "#         all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_x_diff_ex.shape is (40336, 336, 18), max_len is 336\n",
      "target_x_diff_ex.shape is (325, 143, 53)\n"
     ]
    }
   ],
   "source": [
    "print(f'source_x_diff_ex.shape is {np.array(source_x_diff_ex).shape}, max_len is {source_max}')\n",
    "print(f'target_x_diff_ex.shape is {np.array(target_x_diff_ex).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((336, 18), (143, 53))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_x_diff_mean = np.mean(np.array(source_x_diff_ex), 0)\n",
    "target_x_diff_mean = np.mean(np.array(target_x_diff_ex), 0)\n",
    "source_x_diff_mean.shape, target_x_diff_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 16,\n",
       " 6,\n",
       " 4,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 16,\n",
       " 6,\n",
       " 13,\n",
       " 8,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 11,\n",
       " 9,\n",
       " 16,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 13,\n",
       " 8,\n",
       " 8,\n",
       " 13,\n",
       " 13,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 10,\n",
       " 8,\n",
       " 0,\n",
       " 13,\n",
       " 8,\n",
       " 8,\n",
       " 13,\n",
       " 13,\n",
       " 8,\n",
       " 13,\n",
       " 3,\n",
       " 10,\n",
       " 2,\n",
       " 11,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 7]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dtw import *\n",
    "latest_idx = []\n",
    "for i in range(target_x_diff_mean.shape[1]):\n",
    "    min_idx = 0\n",
    "    min_distance = float('inf')\n",
    "    for j in range(source_x_diff_mean.shape[1]):\n",
    "        source_feature = source_x_diff_mean[:, j]\n",
    "        target_feature = target_x_diff_mean[:, i]\n",
    "        alignment = dtw(source_feature, target_feature)\n",
    "        distance = alignment.distance\n",
    "        if min_distance > distance:\n",
    "            min_distance = distance\n",
    "            min_idx = j\n",
    "    latest_idx.append(min_idx)\n",
    "latest_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:28.889438Z",
     "start_time": "2021-02-10T15:06:28.597765Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:11:25,629 - INFO - Transfer Target Dataset & Model\n",
      "2023-08-11 12:11:25,691 - INFO - [[-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8285543981909917, 2.770245567717861, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8374745525934485, 2.8093811444689463, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5338330207864587, 0.431156978499758, 0.8964660630930379, -1.342421748214616, -0.6946051308187712, -0.2120300841305121, 0.8952797203562032, 0.0387041303378994, -0.8612693268611251, 0.0726334784031089, -0.5051102137388988, -0.2865727201409924, 0.3988826585206329, -1.3794377690564883, -0.8237089728907875, -0.9221119476695248, -1.6036614531597553, -0.2379932071009605, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -1.1501193786706505, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, 1.0826051686869729, -0.609009148503521, 0.9684393533852332, -0.3880554131475662, 0.8439788318452395, 2.837917502516613, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 0.278477698357045, 1.2505677424119097, -0.6305581644917595, -0.9062745442328174, -0.7536814885080627, -0.4374103947888615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, -0.7710128163592748, -0.6738408269117502, -2.438483340619628, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8608899578998963, 2.912112033440545, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3396023676711391, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8763143915541441, 2.979783968239297, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5125273793649132, 0.601421850901989, 0.5034187451388209, -1.455529079732558, -0.5254345623915906, -0.4163803392884947, 0.7885904984786939, 0.2816638966057675, -0.5890172833864098, 0.5902661209826593, -0.3430637446868887, -0.7482466726318323, -0.1864257310498265, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.2249939001501796, -0.0341172340589738, 0.3466058957088718, 0.3482841380580905, -0.2890718868318484, 0.1923438589112976, -0.2031244129114749, -1.100153472115602, -0.7598199909551685, -0.6868858002659636, 0.3602180776283341, -0.700061185363224, -0.501359972189453, -0.2332733777972843, -1.36588558517574, 0.3571411263970316, -0.7447833078367583, 0.8765002281041955, 2.9805992927549454, 0.0776143028335215, -0.2769313825285214, 0.0755136678180621, -0.1474826847594624, -0.3999358135056357, -0.0169688050451549, -0.0937841116273902, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, -0.0122781725754626, -0.3292808750219816, -0.2769313825285214, -0.107625012968948, -0.1474826847594624, -0.5354516373428909, -0.0002229391773086, -0.0862088042532133, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, 0.0766269712424727, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.3951350096437179, -0.5239726809327129, -0.8378690925056301, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.8988006141103361, 3.0784382346326584, 0.0776143028335215, -0.3487779453829513, 0.070457637721598, -0.1474826847594624, -0.3999358135056357, 0.0045338857876385, -0.0729488956980065, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.1275947723114954, -0.4255010572823463, -0.3487779453829513, -0.1777070069371955, -0.1474826847594624, -0.5354516373428909, 0.029608437271828, -0.0566564538358881, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.0232601678654147, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.9106941533136118, 3.1306190036341057, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9108799898636633, 3.1314343281497536, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 1.0542620417323658, -0.6738408269117502, -0.0796833960305785, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9222160194167848, 3.1811691236042576, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.5107528363891513, 1.4769583166408096, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-1.038066534429697, 0.260892106097527, 0.1645848503507034, -1.455529079732558, -1.6391408045371951, 0.196670426185453, 1.2420196914581063, 0.4706326037029981, -0.6616178283130005, -0.5004598044528213, -0.4609157221792596, 0.021209914852902, -1.1769476210921423, 0.0909225332951111, -1.348247483817418, -1.2590833683251033, -0.9252694826312332, 0.781386658108973, -0.0798138719333467, -0.2032969502372001, -0.5728175028694968, 0.103242147293012, -0.2031244129114749, -0.6504603131201653, -0.5868915885967425, -0.6218024077192129, -0.0284729839577739, -0.3925684926321655, -0.4454382040900255, -0.2791761177909213, -1.5149673075505703, 0.3413397023846657, -0.8352815289623093, 0.9329945393197532, 3.228457945511819, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8995798651896526, 0.4879119359671683, 0.1510314945591795, -1.4131138304133295, -0.6523124887119757, 0.196670426185453, 0.148455167213642, 0.4706326037029981, -0.752368509471239, -0.9441449266638648, -0.9507380036319262, 0.1751012323498492, -1.0418764542681906, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -1.3629417216818924, 0.1697587389830128, -0.364093717028159, -0.2645837378255657, -0.2890718868318484, 0.0364158635792983, -0.2031244129114749, -1.8246591171638051, -0.4061028043129334, -0.5350245509902118, 0.0492652283594477, 0.0357249008146661, -0.4929416415078188, -0.5851943844151671, -1.170932563608653, 0.051337096981241, -0.8763418268751098, 0.9447022419729768, 3.279823389997619, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.962914223877992, 3.359725192531085, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.973321070680858, 3.405383365407351, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9735069072309092, 3.4061986899229986, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9980373318376644, 3.5138215259884835, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0340896225475933, 3.671994482024121, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.2639920224449301, -0.6738408269117502, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0396647190491284, 3.696454217493548, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7418059691584676, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, -0.6693833821778409, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.057505027854041, 3.774725370995719, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5792130238763558, -0.9062745442328174, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, 0.0888023142972107, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0760886828591592, 3.856257822560481, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:11:25,695 - INFO - 69\n",
      "2023-08-11 12:11:25,697 - INFO - 325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8285543981909917, 2.770245567717861, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8374745525934485, 2.8093811444689463, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5338330207864587, 0.431156978499758, 0.8964660630930379, -1.342421748214616, -0.6946051308187712, -0.2120300841305121, 0.8952797203562032, 0.0387041303378994, -0.8612693268611251, 0.0726334784031089, -0.5051102137388988, -0.2865727201409924, 0.3988826585206329, -1.3794377690564883, -0.8237089728907875, -0.9221119476695248, -1.6036614531597553, -0.2379932071009605, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -1.1501193786706505, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, 1.0826051686869729, -0.609009148503521, 0.9684393533852332, -0.3880554131475662, 0.8439788318452395, 2.837917502516613, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 0.278477698357045, 1.2505677424119097, -0.6305581644917595, -0.9062745442328174, -0.7536814885080627, -0.4374103947888615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, -0.7710128163592748, -0.6738408269117502, -2.438483340619628, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8608899578998963, 2.912112033440545, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3396023676711391, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8763143915541441, 2.979783968239297, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5125273793649132, 0.601421850901989, 0.5034187451388209, -1.455529079732558, -0.5254345623915906, -0.4163803392884947, 0.7885904984786939, 0.2816638966057675, -0.5890172833864098, 0.5902661209826593, -0.3430637446868887, -0.7482466726318323, -0.1864257310498265, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.2249939001501796, -0.0341172340589738, 0.3466058957088718, 0.3482841380580905, -0.2890718868318484, 0.1923438589112976, -0.2031244129114749, -1.100153472115602, -0.7598199909551685, -0.6868858002659636, 0.3602180776283341, -0.700061185363224, -0.501359972189453, -0.2332733777972843, -1.36588558517574, 0.3571411263970316, -0.7447833078367583, 0.8765002281041955, 2.9805992927549454, 0.0776143028335215, -0.2769313825285214, 0.0755136678180621, -0.1474826847594624, -0.3999358135056357, -0.0169688050451549, -0.0937841116273902, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, -0.0122781725754626, -0.3292808750219816, -0.2769313825285214, -0.107625012968948, -0.1474826847594624, -0.5354516373428909, -0.0002229391773086, -0.0862088042532133, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, 0.0766269712424727, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.3951350096437179, -0.5239726809327129, -0.8378690925056301, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.8988006141103361, 3.0784382346326584, 0.0776143028335215, -0.3487779453829513, 0.070457637721598, -0.1474826847594624, -0.3999358135056357, 0.0045338857876385, -0.0729488956980065, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.1275947723114954, -0.4255010572823463, -0.3487779453829513, -0.1777070069371955, -0.1474826847594624, -0.5354516373428909, 0.029608437271828, -0.0566564538358881, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.0232601678654147, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.9106941533136118, 3.1306190036341057, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9108799898636633, 3.1314343281497536, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 1.0542620417323658, -0.6738408269117502, -0.0796833960305785, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9222160194167848, 3.1811691236042576, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.5107528363891513, 1.4769583166408096, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-1.038066534429697, 0.260892106097527, 0.1645848503507034, -1.455529079732558, -1.6391408045371951, 0.196670426185453, 1.2420196914581063, 0.4706326037029981, -0.6616178283130005, -0.5004598044528213, -0.4609157221792596, 0.021209914852902, -1.1769476210921423, 0.0909225332951111, -1.348247483817418, -1.2590833683251033, -0.9252694826312332, 0.781386658108973, -0.0798138719333467, -0.2032969502372001, -0.5728175028694968, 0.103242147293012, -0.2031244129114749, -0.6504603131201653, -0.5868915885967425, -0.6218024077192129, -0.0284729839577739, -0.3925684926321655, -0.4454382040900255, -0.2791761177909213, -1.5149673075505703, 0.3413397023846657, -0.8352815289623093, 0.9329945393197532, 3.228457945511819, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8995798651896526, 0.4879119359671683, 0.1510314945591795, -1.4131138304133295, -0.6523124887119757, 0.196670426185453, 0.148455167213642, 0.4706326037029981, -0.752368509471239, -0.9441449266638648, -0.9507380036319262, 0.1751012323498492, -1.0418764542681906, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -1.3629417216818924, 0.1697587389830128, -0.364093717028159, -0.2645837378255657, -0.2890718868318484, 0.0364158635792983, -0.2031244129114749, -1.8246591171638051, -0.4061028043129334, -0.5350245509902118, 0.0492652283594477, 0.0357249008146661, -0.4929416415078188, -0.5851943844151671, -1.170932563608653, 0.051337096981241, -0.8763418268751098, 0.9447022419729768, 3.279823389997619, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.962914223877992, 3.359725192531085, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.973321070680858, 3.405383365407351, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9735069072309092, 3.4061986899229986, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9980373318376644, 3.5138215259884835, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0340896225475933, 3.671994482024121, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.2639920224449301, -0.6738408269117502, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0396647190491284, 3.696454217493548, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7418059691584676, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, -0.6693833821778409, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.057505027854041, 3.774725370995719, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5792130238763558, -0.9062745442328174, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, 0.0888023142972107, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0760886828591592, 3.856257822560481, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "69\n",
      "325\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Transfer Target Dataset & Model\")\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    data_path = './data/PD/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y_z.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    tar_subset_idx = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    tar_other_idx = list(range(69))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    \n",
    "print(all_x[0])\n",
    "print(len(all_x[0][0]))\n",
    "print(len(all_x))\n",
    "logger.info(all_x[0])\n",
    "logger.info(len(all_x[0][0]))\n",
    "logger.info(len(all_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:34.621807Z",
     "start_time": "2021-02-10T15:06:34.616350Z"
    }
   },
   "outputs": [],
   "source": [
    "long_x = all_x\n",
    "long_y = all_y\n",
    "long_y_kfold = [each[-1] for each in all_y]\n",
    "long_time = all_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:34.845288Z",
     "start_time": "2021-02-10T15:06:34.837259Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_n2n_data(x, y, x_len):\n",
    "#     length = len(x)\n",
    "#     assert length == len(y)\n",
    "#     assert length == len(x_len)\n",
    "#     new_x = []\n",
    "#     new_y = []\n",
    "#     new_x_len = []\n",
    "#     for i in range(length):\n",
    "#         for j in range(len(x[i])):\n",
    "#             new_x.append(x[i][:j+1])\n",
    "#             new_y.append(y[i][j])\n",
    "#             new_x_len.append(j+1)\n",
    "#     return new_x, new_y, new_x_len\n",
    "def get_n2n_data(x, y, x_len):\n",
    "    length = len(x)\n",
    "    assert length == len(y)\n",
    "    assert length == len(x_len)\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    new_x_len = []\n",
    "    for i in range(length):\n",
    "        for j in range(len(x[i])):\n",
    "            new_x.append(x[i][:j+1])\n",
    "            new_y.append(y[i][j])\n",
    "            new_x_len.append(j+1)\n",
    "    return new_x, new_y, new_x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:35.791565Z",
     "start_time": "2021-02-10T15:06:35.745700Z"
    }
   },
   "outputs": [],
   "source": [
    "class distcare_target(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(distcare_target, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.PositionalEncoding = PositionalEncoding(self.d_model, dropout = 0, max_len = 400)\n",
    "\n",
    "        self.GRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        \n",
    "        self.LastStepAttentions = clones(SingleAttention(self.hidden_dim, 16, attention_type='concat', demographic_dim=12, time_aware=True, use_demographic=False),self.input_dim)\n",
    "        \n",
    "        self.FinalAttentionQKV = FinalAttentionQKV(self.hidden_dim, self.hidden_dim, attention_type='mul',dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.MultiHeadedAttention = MultiHeadedAttention(self.MHD_num_head, self.d_model,dropout = 1 - self.keep_prob)\n",
    "        self.SublayerConnection = SublayerConnection(self.d_model, dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.PositionwiseFeedForward = PositionwiseFeedForward(self.d_model, self.d_ff, dropout=0.1)\n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.output = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.FC_embed = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.Linear_los = nn.Linear(self.input_dim, self.output_dim)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, input, lens):\n",
    "        lens = lens.to('cpu')\n",
    "        # input shape [batch_size, timestep, feature_dim]\n",
    "#         demo_main = self.tanh(self.demo_proj_main(demo_input)).unsqueeze(1)# b hidden_dim\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "\n",
    "        # Initialization\n",
    "        #cur_hs = Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0))\n",
    "\n",
    "        # forward\n",
    "        # GRU_embeded_input = self.GRUs[0](input[:,:,0].unsqueeze(-1), Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device))[0] # b t h\n",
    "        # Attention_embeded_input = self.LastStepAttentions[0](GRU_embeded_input)[0].unsqueeze(1)# b 1 h\n",
    "        # for i in range(feature_dim-1):\n",
    "        #     embeded_input = self.GRUs[i+1](input[:,:,i+1].unsqueeze(-1), Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device))[0] # b 1 h\n",
    "        #     embeded_input = self.LastStepAttentions[i+1](embeded_input)[0].unsqueeze(1)# b 1 h\n",
    "        #     Attention_embeded_input = torch.cat((Attention_embeded_input, embeded_input), 1)# b i h\n",
    "\n",
    "        # Attention_embeded_input = torch.cat((Attention_embeded_input, demo_main), 1)# b i+1 h\n",
    "        # posi_input = self.dropout(Attention_embeded_input) # batch_size * d_input+1 * hidden_dim\n",
    "\n",
    "#         input = pack_padded_sequence(input, lens, batch_first=True)\n",
    "        \n",
    "        GRU_embeded_input = self.GRUs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "#         print(GRU_embeded_input.shape)\n",
    "        for i in range(feature_dim-1):\n",
    "            embeded_input = self.GRUs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "            GRU_embeded_input = torch.cat((GRU_embeded_input, embeded_input), 1)\n",
    "        \n",
    "\n",
    "#         GRU_embeded_input = torch.cat((GRU_embeded_input, demo_main), 1)# b i+1 h\n",
    "        posi_input = self.dropout(GRU_embeded_input) # batch_size * d_input * hidden_dim\n",
    "        contexts = self.Linear(posi_input).squeeze()# b i\n",
    "        output = self.Linear_los(self.dropout(contexts))# b 1\n",
    "        #mask = subsequent_mask(time_step).to(device) # 1 t t 下三角 N to 1任务不用mask\n",
    "        return output, None, None\n",
    "    #, self.MultiHeadedAttention.attn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:37.622869Z",
     "start_time": "2021-02-10T15:06:37.615260Z"
    }
   },
   "outputs": [],
   "source": [
    "# def transfer_gru_dict(pretrain_dict, model_dict):\n",
    "#     state_dict = {}\n",
    "#     for k, v in pretrain_dict.items():\n",
    "#         if 'GRUs' in k:\n",
    "#             state_dict[k] = v\n",
    "#             print(\"transfered weight: {}\".format(k))\n",
    "#             logger.info(\"transfered weight: {}\".format(k))\n",
    "#         else:\n",
    "#             print(\"Other weight in model_dict: {}\".format(k))\n",
    "#             logger.info(\"Other weight in model_dict: {}\".format(k))\n",
    "#     return state_dict\n",
    "# def transfer_gru_dict(pretrain_dict, model_dict):\n",
    "#     state_dict = {}\n",
    "#     for k, v in model_dict.items():\n",
    "#         if \"GRUs\" in k:\n",
    "#             if k in pretrain_dict:\n",
    "#                 state_dict[k] = pretrain_dict[k]\n",
    "#             else:\n",
    "#                 target_k = \"generalGRU.\"\n",
    "#                 point_position1 = k.find('.')\n",
    "#                 point_position2 = k.find('.', point_position1+1)\n",
    "#                 target_k += k[point_position2+1:]\n",
    "#                 state_dict[k] = pretrain_dict[target_k]\n",
    "#     return state_dict\n",
    "def transfer_gru_dict(pretrain_dict, model_dict, latest_idx, common_len):\n",
    "    state_dict = {}\n",
    "    \n",
    "    for k, v in model_dict.items():\n",
    "        model_point_position1 = k.find('.')\n",
    "        model_module_name = k[:model_point_position1]\n",
    "        if \"GRUs\" == model_module_name:\n",
    "            model_point_position2 = k.find('.', model_point_position1+1)\n",
    "            model_module_idx = int(k[model_point_position1 + 1: model_point_position2])\n",
    "            if model_module_idx < common_len:\n",
    "                state_dict[k] = pretrain_dict[k]\n",
    "            else:\n",
    "                target_module_name = \"generalGRUs\"\n",
    "                target_module_idx = str(latest_idx[model_module_idx - common_len])\n",
    "                target_k = target_module_name +'.' + target_module_idx + '.' + k[model_point_position2+1:]\n",
    "                state_dict[k] = pretrain_dict[target_k]\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:37.864449Z",
     "start_time": "2021-02-10T15:06:37.857471Z"
    }
   },
   "outputs": [],
   "source": [
    "if target_dataset == 'PD':\n",
    "    input_dim = 69\n",
    "    \n",
    "cell = 'GRU'\n",
    "hidden_dim = 32\n",
    "d_model = 32\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:06:39.629505Z",
     "start_time": "2021-02-10T15:06:39.615897Z"
    }
   },
   "outputs": [],
   "source": [
    "def ckd_batch_iter(x, y, lens, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(x) / batch_size) # 向下取整\n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size] #  fetch out all the induces\n",
    "        \n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx],  lens[idx]))\n",
    "       \n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    \n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "#         batch_name = [e[2] for e in examples]\n",
    "        batch_lens = [e[2] for e in examples]\n",
    "       \n",
    "\n",
    "        yield batch_x, batch_y, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetMultitaskLoss(nn.Module):\n",
    "    def __init__(self, task_num=2):\n",
    "        super(TargetMultitaskLoss, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.alpha = nn.Parameter(torch.ones((task_num)), requires_grad=True)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, opt_student, los, outcome, outcome_y):\n",
    "        MSE_Loss = self.mse(opt_student, los)\n",
    "        BCE_Loss = self.bce(outcome, outcome_y)\n",
    "        return MSE_Loss * self.alpha[0] + BCE_Loss * self.alpha[1]\n",
    "\n",
    "def get_target_multitask_loss(opt_student, los, outcome, outcome_y):\n",
    "    mtl = TargetMultitaskLoss(task_num=2)\n",
    "    return mtl(opt_student, los, outcome, outcome_y)\n",
    "\n",
    "def reverse_los(y, los_info):\n",
    "    return y * los_info[\"los_std\"] + los_info[\"los_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:11:25,861 - INFO - {'los_mean': 1055.0307777880782, 'los_std': 799.0879849276147}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'los_mean': 1055.0307777880782, 'los_std': 799.0879849276147}\n"
     ]
    }
   ],
   "source": [
    "los_info = pickle.load(open(data_path + 'los_info.pkl', 'rb'))\n",
    "print(los_info)\n",
    "logger.info(los_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-10T15:08:58.832Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:11:28,612 - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 1.1161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 0 Batch 0: Train Loss = 1.1161\n",
      "Fold 1, epoch 0: Loss = 1.0162 Valid loss = 0.9860 MSE = 667.6894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:11:45,507 - INFO - Fold 1, epoch 0: Loss = 1.0162 Valid loss = 0.9860 MSE = 667.6894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save FOLD-BEST model - MSE: 667.6894 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:11:45,509 - INFO - ------------ Save FOLD-BEST model - MSE: 667.6894 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0  89 177   0]\n",
      " [  0 328 631   0]\n",
      " [  0 164 344   0]\n",
      " [  0  25 317   0]]\n",
      "Mean absolute deviation (MAD) = 21.558128671079686\n",
      "Mean squared error (MSE) = 667.6894238786298\n",
      "Mean absolute percentage error (MAPE) = 273.59363680138557\n",
      "Cohen kappa score = 0.06874567455906722\n",
      "------------ Save best model - MSE: 667.6894 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:11:45,715 - INFO - ------------ Save best model - MSE: 667.6894 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 667.6894, mad = 21.5581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:11:45,717 - INFO - Fold 1, mse = 667.6894, mad = 21.5581\n",
      "2023-08-11 12:11:46,168 - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 0.9547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1 Batch 0: Train Loss = 0.9547\n",
      "------------ Save FOLD-BEST model - MSE: 641.5886 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:12:03,428 - INFO - ------------ Save FOLD-BEST model - MSE: 641.5886 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 158 108   0]\n",
      " [  0 493 466   0]\n",
      " [  0 220 288   0]\n",
      " [  0  95 247   0]]\n",
      "Mean absolute deviation (MAD) = 21.001437309172378\n",
      "Mean squared error (MSE) = 641.5885925117468\n",
      "Mean absolute percentage error (MAPE) = 257.9980001055271\n",
      "Cohen kappa score = 0.09733452476952253\n",
      "------------ Save best model - MSE: 641.5886 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:12:03,733 - INFO - ------------ Save best model - MSE: 641.5886 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 641.5886, mad = 21.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:12:03,735 - INFO - Fold 1, mse = 641.5886, mad = 21.0014\n",
      "2023-08-11 12:12:04,386 - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 0.8635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 2 Batch 0: Train Loss = 0.8635\n",
      "------------ Save FOLD-BEST model - MSE: 618.5652 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:12:21,539 - INFO - ------------ Save FOLD-BEST model - MSE: 618.5652 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 191  75   0]\n",
      " [  0 588 371   0]\n",
      " [  0 230 278   0]\n",
      " [  0  95 247   0]]\n",
      "Mean absolute deviation (MAD) = 20.34841992402983\n",
      "Mean squared error (MSE) = 618.5652169372457\n",
      "Mean absolute percentage error (MAPE) = 236.4660688744861\n",
      "Cohen kappa score = 0.15579108859041602\n",
      "------------ Save best model - MSE: 618.5652 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:12:21,744 - INFO - ------------ Save best model - MSE: 618.5652 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 618.5652, mad = 20.3484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:12:21,745 - INFO - Fold 1, mse = 618.5652, mad = 20.3484\n",
      "2023-08-11 12:12:22,195 - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 0.8453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 3 Batch 0: Train Loss = 0.8453\n",
      "------------ Save FOLD-BEST model - MSE: 606.8200 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:12:41,280 - INFO - ------------ Save FOLD-BEST model - MSE: 606.8200 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  7 180  79   0]\n",
      " [ 10 529 420   0]\n",
      " [  0 208 300   0]\n",
      " [  0  84 258   0]]\n",
      "Mean absolute deviation (MAD) = 20.250756430188208\n",
      "Mean squared error (MSE) = 606.8200077904922\n",
      "Mean absolute percentage error (MAPE) = 239.08625069725466\n",
      "Cohen kappa score = 0.15626823544151947\n",
      "------------ Save best model - MSE: 606.8200 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:12:41,610 - INFO - ------------ Save best model - MSE: 606.8200 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 606.8200, mad = 20.2508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:12:41,611 - INFO - Fold 1, mse = 606.8200, mad = 20.2508\n",
      "2023-08-11 12:12:42,141 - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 0.7566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 4 Batch 0: Train Loss = 0.7566\n",
      "------------ Save FOLD-BEST model - MSE: 596.8588 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:13:02,177 - INFO - ------------ Save FOLD-BEST model - MSE: 596.8588 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  8 175  83   0]\n",
      " [ 13 524 422   0]\n",
      " [  0 209 299   0]\n",
      " [  0  82 260   0]]\n",
      "Mean absolute deviation (MAD) = 20.06241057401641\n",
      "Mean squared error (MSE) = 596.8587992738829\n",
      "Mean absolute percentage error (MAPE) = 236.01003686383083\n",
      "Cohen kappa score = 0.1542135061429034\n",
      "------------ Save best model - MSE: 596.8588 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:13:02,423 - INFO - ------------ Save best model - MSE: 596.8588 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 596.8588, mad = 20.0624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:13:02,425 - INFO - Fold 1, mse = 596.8588, mad = 20.0624\n",
      "2023-08-11 12:13:02,920 - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 0.8653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 5 Batch 0: Train Loss = 0.8653\n",
      "------------ Save FOLD-BEST model - MSE: 589.2749 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:13:21,630 - INFO - ------------ Save FOLD-BEST model - MSE: 589.2749 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[ 13 191  62   0]\n",
      " [ 19 585 355   0]\n",
      " [  0 263 245   0]\n",
      " [  0 112 230   0]]\n",
      "Mean absolute deviation (MAD) = 19.648381228600336\n",
      "Mean squared error (MSE) = 589.274910981622\n",
      "Mean absolute percentage error (MAPE) = 215.33838796632057\n",
      "Cohen kappa score = 0.14424857724304574\n",
      "------------ Save best model - MSE: 589.2749 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:13:21,873 - INFO - ------------ Save best model - MSE: 589.2749 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 589.2749, mad = 19.6484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:13:21,876 - INFO - Fold 1, mse = 589.2749, mad = 19.6484\n",
      "2023-08-11 12:13:22,390 - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 0.7953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 6 Batch 0: Train Loss = 0.7953\n",
      "------------ Save FOLD-BEST model - MSE: 585.3385 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:13:40,895 - INFO - ------------ Save FOLD-BEST model - MSE: 585.3385 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  8 182  76   0]\n",
      " [ 14 530 415   0]\n",
      " [  0 226 282   0]\n",
      " [  0  64 278   0]]\n",
      "Mean absolute deviation (MAD) = 19.883129783954118\n",
      "Mean squared error (MSE) = 585.3384829113978\n",
      "Mean absolute percentage error (MAPE) = 229.5532096410228\n",
      "Cohen kappa score = 0.1618094058036943\n",
      "------------ Save best model - MSE: 585.3385 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:13:41,136 - INFO - ------------ Save best model - MSE: 585.3385 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 585.3385, mad = 19.8831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:13:41,137 - INFO - Fold 1, mse = 585.3385, mad = 19.8831\n",
      "2023-08-11 12:13:41,651 - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 0.7775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 7 Batch 0: Train Loss = 0.7775\n",
      "Fold 1, mse = 587.5375, mad = 19.6908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:14:00,225 - INFO - Fold 1, mse = 587.5375, mad = 19.6908\n",
      "2023-08-11 12:14:00,786 - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 0.7794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 8 Batch 0: Train Loss = 0.7794\n",
      "------------ Save FOLD-BEST model - MSE: 568.6351 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:14:20,305 - INFO - ------------ Save FOLD-BEST model - MSE: 568.6351 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  8 178  80   0]\n",
      " [ 12 527 420   0]\n",
      " [  0 207 301   0]\n",
      " [  0  54 288   0]]\n",
      "Mean absolute deviation (MAD) = 19.60251090490563\n",
      "Mean squared error (MSE) = 568.6350819924273\n",
      "Mean absolute percentage error (MAPE) = 230.95125065041628\n",
      "Cohen kappa score = 0.17768650502152505\n",
      "------------ Save best model - MSE: 568.6351 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:14:20,547 - INFO - ------------ Save best model - MSE: 568.6351 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 568.6351, mad = 19.6025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:14:20,548 - INFO - Fold 1, mse = 568.6351, mad = 19.6025\n",
      "2023-08-11 12:14:21,038 - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 0.6524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 9 Batch 0: Train Loss = 0.6524\n",
      "Fold 1, mse = 569.0229, mad = 19.4185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:14:39,694 - INFO - Fold 1, mse = 569.0229, mad = 19.4185\n",
      "2023-08-11 12:14:40,176 - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 0.6745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 10 Batch 0: Train Loss = 0.6745\n",
      "Fold 1, epoch 10: Loss = 0.7400 Valid loss = 0.8186 MSE = 587.7875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:14:59,824 - INFO - Fold 1, epoch 10: Loss = 0.7400 Valid loss = 0.8186 MSE = 587.7875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 587.7875, mad = 19.7567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:14:59,825 - INFO - Fold 1, mse = 587.7875, mad = 19.7567\n",
      "2023-08-11 12:15:00,371 - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 0.7087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 11 Batch 0: Train Loss = 0.7087\n",
      "Fold 1, mse = 585.3424, mad = 19.6680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:15:18,861 - INFO - Fold 1, mse = 585.3424, mad = 19.6680\n",
      "2023-08-11 12:15:19,415 - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 0.6450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 12 Batch 0: Train Loss = 0.6450\n",
      "Fold 1, mse = 590.6148, mad = 19.8619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:15:37,395 - INFO - Fold 1, mse = 590.6148, mad = 19.8619\n",
      "2023-08-11 12:15:37,840 - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 0.7732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 13 Batch 0: Train Loss = 0.7732\n",
      "Fold 1, mse = 599.8239, mad = 20.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:15:57,017 - INFO - Fold 1, mse = 599.8239, mad = 20.0677\n",
      "2023-08-11 12:15:57,568 - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 0.7471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 14 Batch 0: Train Loss = 0.7471\n",
      "Fold 1, mse = 596.1483, mad = 20.1125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:16:16,085 - INFO - Fold 1, mse = 596.1483, mad = 20.1125\n",
      "2023-08-11 12:16:16,623 - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 0.6426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 15 Batch 0: Train Loss = 0.6426\n",
      "Fold 1, mse = 580.9644, mad = 19.8492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:16:35,022 - INFO - Fold 1, mse = 580.9644, mad = 19.8492\n",
      "2023-08-11 12:16:35,473 - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 0.7916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 16 Batch 0: Train Loss = 0.7916\n",
      "Fold 1, mse = 588.7167, mad = 19.9694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:16:55,371 - INFO - Fold 1, mse = 588.7167, mad = 19.9694\n",
      "2023-08-11 12:16:55,929 - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 0.6858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 17 Batch 0: Train Loss = 0.6858\n",
      "Fold 1, mse = 588.9642, mad = 19.7346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:17:15,591 - INFO - Fold 1, mse = 588.9642, mad = 19.7346\n",
      "2023-08-11 12:17:16,138 - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 0.6370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 18 Batch 0: Train Loss = 0.6370\n",
      "Fold 1, mse = 602.1691, mad = 20.2684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:17:35,907 - INFO - Fold 1, mse = 602.1691, mad = 20.2684\n",
      "2023-08-11 12:17:36,385 - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 0.7038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 19 Batch 0: Train Loss = 0.7038\n",
      "Fold 1, mse = 621.7803, mad = 20.3488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:17:55,713 - INFO - Fold 1, mse = 621.7803, mad = 20.3488\n",
      "2023-08-11 12:17:56,242 - INFO - Fold 1 Epoch 20 Batch 0: Train Loss = 0.5875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 20 Batch 0: Train Loss = 0.5875\n",
      "Fold 1, epoch 20: Loss = 0.6471 Valid loss = 0.8177 MSE = 593.2768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:18:14,661 - INFO - Fold 1, epoch 20: Loss = 0.6471 Valid loss = 0.8177 MSE = 593.2768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, mse = 593.2768, mad = 19.8243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:18:14,663 - INFO - Fold 1, mse = 593.2768, mad = 19.8243\n",
      "2023-08-11 12:18:15,268 - INFO - Fold 1 Epoch 21 Batch 0: Train Loss = 0.5769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 21 Batch 0: Train Loss = 0.5769\n",
      "Fold 1, mse = 601.8840, mad = 19.9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:18:34,187 - INFO - Fold 1, mse = 601.8840, mad = 19.9995\n",
      "2023-08-11 12:18:34,759 - INFO - Fold 1 Epoch 22 Batch 0: Train Loss = 0.6067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 22 Batch 0: Train Loss = 0.6067\n",
      "Fold 1, mse = 595.8202, mad = 19.5826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:18:52,926 - INFO - Fold 1, mse = 595.8202, mad = 19.5826\n",
      "2023-08-11 12:18:53,493 - INFO - Fold 1 Epoch 23 Batch 0: Train Loss = 0.6075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 23 Batch 0: Train Loss = 0.6075\n",
      "Fold 1, mse = 606.2692, mad = 20.2760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:19:12,241 - INFO - Fold 1, mse = 606.2692, mad = 20.2760\n",
      "2023-08-11 12:19:12,793 - INFO - Fold 1 Epoch 24 Batch 0: Train Loss = 0.6779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 24 Batch 0: Train Loss = 0.6779\n",
      "Fold 1, mse = 620.1445, mad = 20.5631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:19:33,251 - INFO - Fold 1, mse = 620.1445, mad = 20.5631\n",
      "2023-08-11 12:19:33,858 - INFO - Fold 1 Epoch 25 Batch 0: Train Loss = 0.6201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 25 Batch 0: Train Loss = 0.6201\n",
      "Fold 1, mse = 638.0536, mad = 20.7209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:19:52,687 - INFO - Fold 1, mse = 638.0536, mad = 20.7209\n",
      "2023-08-11 12:19:53,310 - INFO - Fold 1 Epoch 26 Batch 0: Train Loss = 0.7229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 26 Batch 0: Train Loss = 0.7229\n",
      "Fold 1, mse = 634.3418, mad = 20.4100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:20:13,161 - INFO - Fold 1, mse = 634.3418, mad = 20.4100\n",
      "2023-08-11 12:20:13,711 - INFO - Fold 1 Epoch 27 Batch 0: Train Loss = 0.5752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 27 Batch 0: Train Loss = 0.5752\n",
      "Fold 1, mse = 618.8936, mad = 20.2817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:20:31,951 - INFO - Fold 1, mse = 618.8936, mad = 20.2817\n",
      "2023-08-11 12:20:32,499 - INFO - Fold 1 Epoch 28 Batch 0: Train Loss = 0.5489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 28 Batch 0: Train Loss = 0.5489\n",
      "Fold 1, mse = 615.0261, mad = 20.2190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:20:51,099 - INFO - Fold 1, mse = 615.0261, mad = 20.2190\n",
      "2023-08-11 12:20:51,694 - INFO - Fold 1 Epoch 29 Batch 0: Train Loss = 0.5957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 29 Batch 0: Train Loss = 0.5957\n",
      "Fold 1, mse = 637.6060, mad = 20.6240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:21:10,413 - INFO - Fold 1, mse = 637.6060, mad = 20.6240\n",
      "2023-08-11 12:21:11,146 - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 1.0765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 0 Batch 0: Train Loss = 1.0765\n",
      "Fold 2, epoch 0: Loss = 1.0435 Valid loss = 0.8209 MSE = 584.6664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:21:29,343 - INFO - Fold 2, epoch 0: Loss = 1.0435 Valid loss = 0.8209 MSE = 584.6664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save FOLD-BEST model - MSE: 584.6664 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:21:29,345 - INFO - ------------ Save FOLD-BEST model - MSE: 584.6664 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 129 142   0]\n",
      " [  0 459 514   0]\n",
      " [  0 197 374   0]\n",
      " [  0  31 239   0]]\n",
      "Mean absolute deviation (MAD) = 19.779357591339448\n",
      "Mean squared error (MSE) = 584.6663799292387\n",
      "Mean absolute percentage error (MAPE) = 271.88228191879705\n",
      "Cohen kappa score = 0.12430512511947833\n",
      "Fold 2, mse = 584.6664, mad = 19.7794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:21:29,494 - INFO - Fold 2, mse = 584.6664, mad = 19.7794\n",
      "2023-08-11 12:21:29,951 - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 0.9392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 1 Batch 0: Train Loss = 0.9392\n",
      "------------ Save FOLD-BEST model - MSE: 552.5209 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:21:48,224 - INFO - ------------ Save FOLD-BEST model - MSE: 552.5209 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 176  95   0]\n",
      " [  0 513 460   0]\n",
      " [  0 236 335   0]\n",
      " [  0  36 234   0]]\n",
      "Mean absolute deviation (MAD) = 19.17368093496257\n",
      "Mean squared error (MSE) = 552.5209140464331\n",
      "Mean absolute percentage error (MAPE) = 256.971702854177\n",
      "Cohen kappa score = 0.144600505871747\n",
      "------------ Save best model - MSE: 552.5209 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:21:48,463 - INFO - ------------ Save best model - MSE: 552.5209 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 552.5209, mad = 19.1737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:21:48,465 - INFO - Fold 2, mse = 552.5209, mad = 19.1737\n",
      "2023-08-11 12:21:48,950 - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 1.0134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 2 Batch 0: Train Loss = 1.0134\n",
      "------------ Save FOLD-BEST model - MSE: 534.4235 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:22:07,149 - INFO - ------------ Save FOLD-BEST model - MSE: 534.4235 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 156 115   0]\n",
      " [  0 486 487   0]\n",
      " [  0 233 338   0]\n",
      " [  0   6 264   0]]\n",
      "Mean absolute deviation (MAD) = 18.896352085336222\n",
      "Mean squared error (MSE) = 534.4234949861434\n",
      "Mean absolute percentage error (MAPE) = 256.6314465750925\n",
      "Cohen kappa score = 0.144121719841672\n",
      "------------ Save best model - MSE: 534.4235 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:22:07,378 - INFO - ------------ Save best model - MSE: 534.4235 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 534.4235, mad = 18.8964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:22:07,380 - INFO - Fold 2, mse = 534.4235, mad = 18.8964\n",
      "2023-08-11 12:22:07,879 - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 0.7908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 3 Batch 0: Train Loss = 0.7908\n",
      "------------ Save FOLD-BEST model - MSE: 507.2804 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:22:26,370 - INFO - ------------ Save FOLD-BEST model - MSE: 507.2804 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  7 174  90   0]\n",
      " [  6 583 384   0]\n",
      " [  0 253 318   0]\n",
      " [  0   8 262   0]]\n",
      "Mean absolute deviation (MAD) = 18.277648615069918\n",
      "Mean squared error (MSE) = 507.28042168311396\n",
      "Mean absolute percentage error (MAPE) = 236.00140464527342\n",
      "Cohen kappa score = 0.20079902810668093\n",
      "------------ Save best model - MSE: 507.2804 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:22:26,590 - INFO - ------------ Save best model - MSE: 507.2804 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 507.2804, mad = 18.2776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:22:26,592 - INFO - Fold 2, mse = 507.2804, mad = 18.2776\n",
      "2023-08-11 12:22:27,111 - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 0.8511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 4 Batch 0: Train Loss = 0.8511\n",
      "------------ Save FOLD-BEST model - MSE: 503.2491 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:22:45,371 - INFO - ------------ Save FOLD-BEST model - MSE: 503.2491 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[ 11 167  93   0]\n",
      " [  3 577 393   0]\n",
      " [  0 233 338   0]\n",
      " [  0   3 267   0]]\n",
      "Mean absolute deviation (MAD) = 18.26973286629225\n",
      "Mean squared error (MSE) = 503.24914241824234\n",
      "Mean absolute percentage error (MAPE) = 240.84952404976576\n",
      "Cohen kappa score = 0.21720771401946604\n",
      "------------ Save best model - MSE: 503.2491 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:22:45,616 - INFO - ------------ Save best model - MSE: 503.2491 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 503.2491, mad = 18.2697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:22:45,618 - INFO - Fold 2, mse = 503.2491, mad = 18.2697\n",
      "2023-08-11 12:22:46,118 - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 0.7331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 5 Batch 0: Train Loss = 0.7331\n",
      "Fold 2, mse = 503.8620, mad = 18.3262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:23:05,834 - INFO - Fold 2, mse = 503.8620, mad = 18.3262\n",
      "2023-08-11 12:23:06,454 - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 0.7014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 6 Batch 0: Train Loss = 0.7014\n",
      "------------ Save FOLD-BEST model - MSE: 500.1251 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:23:27,052 - INFO - ------------ Save FOLD-BEST model - MSE: 500.1251 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[ 17 175  79   0]\n",
      " [ 20 600 353   0]\n",
      " [  0 259 312   0]\n",
      " [  0  16 254   0]]\n",
      "Mean absolute deviation (MAD) = 18.0996887866421\n",
      "Mean squared error (MSE) = 500.1250545345138\n",
      "Mean absolute percentage error (MAPE) = 227.14287813039346\n",
      "Cohen kappa score = 0.21923786688937086\n",
      "------------ Save best model - MSE: 500.1251 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:23:27,305 - INFO - ------------ Save best model - MSE: 500.1251 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 500.1251, mad = 18.0997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:23:27,307 - INFO - Fold 2, mse = 500.1251, mad = 18.0997\n",
      "2023-08-11 12:23:27,795 - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 0.7790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 7 Batch 0: Train Loss = 0.7790\n",
      "Fold 2, mse = 518.1722, mad = 18.6140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:23:46,090 - INFO - Fold 2, mse = 518.1722, mad = 18.6140\n",
      "2023-08-11 12:23:46,676 - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 0.6388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 8 Batch 0: Train Loss = 0.6388\n",
      "Fold 2, mse = 519.9511, mad = 18.5120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:24:06,944 - INFO - Fold 2, mse = 519.9511, mad = 18.5120\n",
      "2023-08-11 12:24:07,372 - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 0.7646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 9 Batch 0: Train Loss = 0.7646\n",
      "Fold 2, mse = 517.2045, mad = 18.4499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:24:25,668 - INFO - Fold 2, mse = 517.2045, mad = 18.4499\n",
      "2023-08-11 12:24:26,170 - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 0.6942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 10 Batch 0: Train Loss = 0.6942\n",
      "Fold 2, epoch 10: Loss = 0.7091 Valid loss = 0.6916 MSE = 513.6917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:24:45,488 - INFO - Fold 2, epoch 10: Loss = 0.7091 Valid loss = 0.6916 MSE = 513.6917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 513.6917, mad = 18.4217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:24:45,490 - INFO - Fold 2, mse = 513.6917, mad = 18.4217\n",
      "2023-08-11 12:24:45,978 - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 0.6443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 11 Batch 0: Train Loss = 0.6443\n",
      "Fold 2, mse = 518.7807, mad = 18.5305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:25:07,094 - INFO - Fold 2, mse = 518.7807, mad = 18.5305\n",
      "2023-08-11 12:25:07,611 - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 0.5801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 12 Batch 0: Train Loss = 0.5801\n",
      "Fold 2, mse = 527.3610, mad = 18.7179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:25:25,712 - INFO - Fold 2, mse = 527.3610, mad = 18.7179\n",
      "2023-08-11 12:25:26,265 - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 0.6182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 13 Batch 0: Train Loss = 0.6182\n",
      "Fold 2, mse = 517.5557, mad = 18.4440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:25:43,356 - INFO - Fold 2, mse = 517.5557, mad = 18.4440\n",
      "2023-08-11 12:25:43,902 - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 0.6749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 14 Batch 0: Train Loss = 0.6749\n",
      "Fold 2, mse = 512.0825, mad = 18.3264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:26:02,162 - INFO - Fold 2, mse = 512.0825, mad = 18.3264\n",
      "2023-08-11 12:26:02,752 - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 0.7214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 15 Batch 0: Train Loss = 0.7214\n",
      "Fold 2, mse = 513.2485, mad = 18.3542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:26:20,859 - INFO - Fold 2, mse = 513.2485, mad = 18.3542\n",
      "2023-08-11 12:26:21,370 - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 0.5727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 16 Batch 0: Train Loss = 0.5727\n",
      "Fold 2, mse = 519.2330, mad = 18.4907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:26:40,001 - INFO - Fold 2, mse = 519.2330, mad = 18.4907\n",
      "2023-08-11 12:26:40,507 - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 0.6058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 17 Batch 0: Train Loss = 0.6058\n",
      "Fold 2, mse = 521.0947, mad = 18.3563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:27:00,765 - INFO - Fold 2, mse = 521.0947, mad = 18.3563\n",
      "2023-08-11 12:27:01,314 - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 0.7112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 18 Batch 0: Train Loss = 0.7112\n",
      "Fold 2, mse = 527.4716, mad = 18.6738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:27:19,873 - INFO - Fold 2, mse = 527.4716, mad = 18.6738\n",
      "2023-08-11 12:27:20,427 - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 0.6464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 19 Batch 0: Train Loss = 0.6464\n",
      "Fold 2, mse = 527.9723, mad = 18.6162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:27:38,912 - INFO - Fold 2, mse = 527.9723, mad = 18.6162\n",
      "2023-08-11 12:27:39,472 - INFO - Fold 2 Epoch 20 Batch 0: Train Loss = 0.5964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 20 Batch 0: Train Loss = 0.5964\n",
      "Fold 2, epoch 20: Loss = 0.6159 Valid loss = 0.7031 MSE = 520.8083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:27:57,749 - INFO - Fold 2, epoch 20: Loss = 0.6159 Valid loss = 0.7031 MSE = 520.8083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, mse = 520.8083, mad = 18.2577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:27:57,751 - INFO - Fold 2, mse = 520.8083, mad = 18.2577\n",
      "2023-08-11 12:27:58,279 - INFO - Fold 2 Epoch 21 Batch 0: Train Loss = 0.6065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 21 Batch 0: Train Loss = 0.6065\n",
      "Fold 2, mse = 523.1449, mad = 18.6142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:28:17,597 - INFO - Fold 2, mse = 523.1449, mad = 18.6142\n",
      "2023-08-11 12:28:18,114 - INFO - Fold 2 Epoch 22 Batch 0: Train Loss = 0.6577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 22 Batch 0: Train Loss = 0.6577\n",
      "Fold 2, mse = 525.7952, mad = 18.3605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:28:35,116 - INFO - Fold 2, mse = 525.7952, mad = 18.3605\n",
      "2023-08-11 12:28:35,614 - INFO - Fold 2 Epoch 23 Batch 0: Train Loss = 0.5235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 23 Batch 0: Train Loss = 0.5235\n",
      "Fold 2, mse = 539.8657, mad = 18.8667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:28:54,426 - INFO - Fold 2, mse = 539.8657, mad = 18.8667\n",
      "2023-08-11 12:28:54,966 - INFO - Fold 2 Epoch 24 Batch 0: Train Loss = 0.5524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 24 Batch 0: Train Loss = 0.5524\n",
      "Fold 2, mse = 524.6098, mad = 18.3526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:29:13,239 - INFO - Fold 2, mse = 524.6098, mad = 18.3526\n",
      "2023-08-11 12:29:13,800 - INFO - Fold 2 Epoch 25 Batch 0: Train Loss = 0.6279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 25 Batch 0: Train Loss = 0.6279\n",
      "Fold 2, mse = 542.0366, mad = 18.8664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:29:31,944 - INFO - Fold 2, mse = 542.0366, mad = 18.8664\n",
      "2023-08-11 12:29:32,468 - INFO - Fold 2 Epoch 26 Batch 0: Train Loss = 0.6065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 26 Batch 0: Train Loss = 0.6065\n",
      "Fold 2, mse = 531.7451, mad = 18.4061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:29:50,636 - INFO - Fold 2, mse = 531.7451, mad = 18.4061\n",
      "2023-08-11 12:29:51,182 - INFO - Fold 2 Epoch 27 Batch 0: Train Loss = 0.6127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 27 Batch 0: Train Loss = 0.6127\n",
      "Fold 2, mse = 522.7437, mad = 18.2973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:30:09,623 - INFO - Fold 2, mse = 522.7437, mad = 18.2973\n",
      "2023-08-11 12:30:10,148 - INFO - Fold 2 Epoch 28 Batch 0: Train Loss = 0.5524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 28 Batch 0: Train Loss = 0.5524\n",
      "Fold 2, mse = 524.9176, mad = 18.3670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:30:28,421 - INFO - Fold 2, mse = 524.9176, mad = 18.3670\n",
      "2023-08-11 12:30:28,969 - INFO - Fold 2 Epoch 29 Batch 0: Train Loss = 0.5624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 29 Batch 0: Train Loss = 0.5624\n",
      "Fold 2, mse = 522.1677, mad = 18.1918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:30:47,222 - INFO - Fold 2, mse = 522.1677, mad = 18.1918\n",
      "2023-08-11 12:30:48,024 - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 1.1071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 0 Batch 0: Train Loss = 1.1071\n",
      "Fold 3, epoch 0: Loss = 0.9929 Valid loss = 0.9073 MSE = 646.5132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:31:06,198 - INFO - Fold 3, epoch 0: Loss = 0.9929 Valid loss = 0.9073 MSE = 646.5132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save FOLD-BEST model - MSE: 646.5132 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:31:06,199 - INFO - ------------ Save FOLD-BEST model - MSE: 646.5132 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 140 141   0]\n",
      " [  0 484 635   0]\n",
      " [  0 293 400   0]\n",
      " [  0  99 298   0]]\n",
      "Mean absolute deviation (MAD) = 20.87490222488839\n",
      "Mean squared error (MSE) = 646.5132330618036\n",
      "Mean absolute percentage error (MAPE) = 202.1731357059215\n",
      "Cohen kappa score = 0.054065845416791714\n",
      "Fold 3, mse = 646.5132, mad = 20.8749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:31:06,316 - INFO - Fold 3, mse = 646.5132, mad = 20.8749\n",
      "2023-08-11 12:31:06,824 - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 1.0760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 1 Batch 0: Train Loss = 1.0760\n",
      "------------ Save FOLD-BEST model - MSE: 636.6216 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:31:24,788 - INFO - ------------ Save FOLD-BEST model - MSE: 636.6216 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 152 129   0]\n",
      " [  0 476 643   0]\n",
      " [  0 261 432   0]\n",
      " [  0  76 321   0]]\n",
      "Mean absolute deviation (MAD) = 20.796892192443142\n",
      "Mean squared error (MSE) = 636.6215779132405\n",
      "Mean absolute percentage error (MAPE) = 200.7580844395105\n",
      "Cohen kappa score = 0.08726849050372609\n",
      "Fold 3, mse = 636.6216, mad = 20.7969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:31:24,932 - INFO - Fold 3, mse = 636.6216, mad = 20.7969\n",
      "2023-08-11 12:31:25,453 - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 0.8998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 2 Batch 0: Train Loss = 0.8998\n",
      "------------ Save FOLD-BEST model - MSE: 628.8385 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:31:43,653 - INFO - ------------ Save FOLD-BEST model - MSE: 628.8385 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 161 120   0]\n",
      " [ 17 475 627   0]\n",
      " [  0 253 440   0]\n",
      " [  0  68 329   0]]\n",
      "Mean absolute deviation (MAD) = 20.74569101391571\n",
      "Mean squared error (MSE) = 628.8384590179915\n",
      "Mean absolute percentage error (MAPE) = 195.8764451770355\n",
      "Cohen kappa score = 0.10503162451152437\n",
      "Fold 3, mse = 628.8385, mad = 20.7457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:31:43,768 - INFO - Fold 3, mse = 628.8385, mad = 20.7457\n",
      "2023-08-11 12:31:44,240 - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 0.8775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 3 Batch 0: Train Loss = 0.8775\n",
      "Fold 3, mse = 637.1999, mad = 20.7927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:32:01,697 - INFO - Fold 3, mse = 637.1999, mad = 20.7927\n",
      "2023-08-11 12:32:02,133 - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 0.7711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 4 Batch 0: Train Loss = 0.7711\n",
      "Fold 3, mse = 653.7330, mad = 21.0721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:32:20,462 - INFO - Fold 3, mse = 653.7330, mad = 21.0721\n",
      "2023-08-11 12:32:20,953 - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 0.8118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 5 Batch 0: Train Loss = 0.8118\n",
      "Fold 3, mse = 655.1750, mad = 21.0711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:32:38,585 - INFO - Fold 3, mse = 655.1750, mad = 21.0711\n",
      "2023-08-11 12:32:39,101 - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 0.8035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 6 Batch 0: Train Loss = 0.8035\n",
      "Fold 3, mse = 641.3181, mad = 20.8361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:32:56,702 - INFO - Fold 3, mse = 641.3181, mad = 20.8361\n",
      "2023-08-11 12:32:57,201 - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 0.6761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 7 Batch 0: Train Loss = 0.6761\n",
      "Fold 3, mse = 670.3043, mad = 21.3795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:33:17,168 - INFO - Fold 3, mse = 670.3043, mad = 21.3795\n",
      "2023-08-11 12:33:17,668 - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 0.6874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 8 Batch 0: Train Loss = 0.6874\n",
      "Fold 3, mse = 666.6990, mad = 21.1327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:33:37,043 - INFO - Fold 3, mse = 666.6990, mad = 21.1327\n",
      "2023-08-11 12:33:37,575 - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 0.7824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 9 Batch 0: Train Loss = 0.7824\n",
      "Fold 3, mse = 695.6981, mad = 21.7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:33:55,770 - INFO - Fold 3, mse = 695.6981, mad = 21.7300\n",
      "2023-08-11 12:33:56,332 - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 0.7101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 10 Batch 0: Train Loss = 0.7101\n",
      "Fold 3, epoch 10: Loss = 0.6831 Valid loss = 0.9818 MSE = 700.3259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:34:14,693 - INFO - Fold 3, epoch 10: Loss = 0.6831 Valid loss = 0.9818 MSE = 700.3259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, mse = 700.3259, mad = 22.0475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:34:14,696 - INFO - Fold 3, mse = 700.3259, mad = 22.0475\n",
      "2023-08-11 12:34:15,201 - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 0.6729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 11 Batch 0: Train Loss = 0.6729\n",
      "Fold 3, mse = 682.9727, mad = 21.4776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:34:33,468 - INFO - Fold 3, mse = 682.9727, mad = 21.4776\n",
      "2023-08-11 12:34:33,984 - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 0.6023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 12 Batch 0: Train Loss = 0.6023\n",
      "Fold 3, mse = 697.2176, mad = 21.6285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:34:52,135 - INFO - Fold 3, mse = 697.2176, mad = 21.6285\n",
      "2023-08-11 12:34:52,608 - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 0.7285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 13 Batch 0: Train Loss = 0.7285\n",
      "Fold 3, mse = 691.8326, mad = 21.8253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:35:10,634 - INFO - Fold 3, mse = 691.8326, mad = 21.8253\n",
      "2023-08-11 12:35:11,119 - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 0.5346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 14 Batch 0: Train Loss = 0.5346\n",
      "Fold 3, mse = 709.9386, mad = 22.2623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:35:29,283 - INFO - Fold 3, mse = 709.9386, mad = 22.2623\n",
      "2023-08-11 12:35:29,752 - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 0.6168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 15 Batch 0: Train Loss = 0.6168\n",
      "Fold 3, mse = 704.1267, mad = 21.9649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:35:47,237 - INFO - Fold 3, mse = 704.1267, mad = 21.9649\n",
      "2023-08-11 12:35:47,746 - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 0.5494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 16 Batch 0: Train Loss = 0.5494\n",
      "Fold 3, mse = 705.4687, mad = 21.9245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:36:06,239 - INFO - Fold 3, mse = 705.4687, mad = 21.9245\n",
      "2023-08-11 12:36:06,813 - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 0.6597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 17 Batch 0: Train Loss = 0.6597\n",
      "Fold 3, mse = 717.8708, mad = 22.2036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:36:24,812 - INFO - Fold 3, mse = 717.8708, mad = 22.2036\n",
      "2023-08-11 12:36:25,378 - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 0.6017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 18 Batch 0: Train Loss = 0.6017\n",
      "Fold 3, mse = 706.7032, mad = 21.9696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:36:44,409 - INFO - Fold 3, mse = 706.7032, mad = 21.9696\n",
      "2023-08-11 12:36:44,955 - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 0.7144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 19 Batch 0: Train Loss = 0.7144\n",
      "Fold 3, mse = 738.5687, mad = 22.5223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:37:03,294 - INFO - Fold 3, mse = 738.5687, mad = 22.5223\n",
      "2023-08-11 12:37:03,790 - INFO - Fold 3 Epoch 20 Batch 0: Train Loss = 0.6231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 20 Batch 0: Train Loss = 0.6231\n",
      "Fold 3, epoch 20: Loss = 0.6321 Valid loss = 1.0296 MSE = 732.3301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:37:21,853 - INFO - Fold 3, epoch 20: Loss = 0.6321 Valid loss = 1.0296 MSE = 732.3301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, mse = 732.3301, mad = 22.5737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:37:21,855 - INFO - Fold 3, mse = 732.3301, mad = 22.5737\n",
      "2023-08-11 12:37:22,402 - INFO - Fold 3 Epoch 21 Batch 0: Train Loss = 0.7497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 21 Batch 0: Train Loss = 0.7497\n",
      "Fold 3, mse = 734.7893, mad = 22.6614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:37:41,474 - INFO - Fold 3, mse = 734.7893, mad = 22.6614\n",
      "2023-08-11 12:37:41,977 - INFO - Fold 3 Epoch 22 Batch 0: Train Loss = 0.6361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 22 Batch 0: Train Loss = 0.6361\n",
      "Fold 3, mse = 741.9026, mad = 22.9079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:38:00,025 - INFO - Fold 3, mse = 741.9026, mad = 22.9079\n",
      "2023-08-11 12:38:00,525 - INFO - Fold 3 Epoch 23 Batch 0: Train Loss = 0.6415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 23 Batch 0: Train Loss = 0.6415\n",
      "Fold 3, mse = 719.4886, mad = 22.2075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:38:18,710 - INFO - Fold 3, mse = 719.4886, mad = 22.2075\n",
      "2023-08-11 12:38:19,183 - INFO - Fold 3 Epoch 24 Batch 0: Train Loss = 0.5665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 24 Batch 0: Train Loss = 0.5665\n",
      "Fold 3, mse = 717.3441, mad = 22.1636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:38:37,135 - INFO - Fold 3, mse = 717.3441, mad = 22.1636\n",
      "2023-08-11 12:38:37,643 - INFO - Fold 3 Epoch 25 Batch 0: Train Loss = 0.6235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 25 Batch 0: Train Loss = 0.6235\n",
      "Fold 3, mse = 709.1632, mad = 21.9879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:38:55,722 - INFO - Fold 3, mse = 709.1632, mad = 21.9879\n",
      "2023-08-11 12:38:56,226 - INFO - Fold 3 Epoch 26 Batch 0: Train Loss = 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 26 Batch 0: Train Loss = 0.6562\n",
      "Fold 3, mse = 697.7872, mad = 21.8535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:39:14,078 - INFO - Fold 3, mse = 697.7872, mad = 21.8535\n",
      "2023-08-11 12:39:14,560 - INFO - Fold 3 Epoch 27 Batch 0: Train Loss = 0.5892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 27 Batch 0: Train Loss = 0.5892\n",
      "Fold 3, mse = 648.5876, mad = 20.9225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:39:32,831 - INFO - Fold 3, mse = 648.5876, mad = 20.9225\n",
      "2023-08-11 12:39:33,356 - INFO - Fold 3 Epoch 28 Batch 0: Train Loss = 0.6829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 28 Batch 0: Train Loss = 0.6829\n",
      "Fold 3, mse = 657.0059, mad = 20.8382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:39:50,998 - INFO - Fold 3, mse = 657.0059, mad = 20.8382\n",
      "2023-08-11 12:39:51,477 - INFO - Fold 3 Epoch 29 Batch 0: Train Loss = 0.4986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 29 Batch 0: Train Loss = 0.4986\n",
      "Fold 3, mse = 682.0132, mad = 21.3624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:40:09,538 - INFO - Fold 3, mse = 682.0132, mad = 21.3624\n",
      "2023-08-11 12:40:10,252 - INFO - Fold 4 Epoch 0 Batch 0: Train Loss = 0.9416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 0 Batch 0: Train Loss = 0.9416\n",
      "Fold 4, epoch 0: Loss = 0.9478 Valid loss = 1.1836 MSE = 830.1663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:40:28,169 - INFO - Fold 4, epoch 0: Loss = 0.9478 Valid loss = 1.1836 MSE = 830.1663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save FOLD-BEST model - MSE: 830.1663 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:40:28,171 - INFO - ------------ Save FOLD-BEST model - MSE: 830.1663 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 126 124   0]\n",
      " [  0 437 440   0]\n",
      " [  0 325 230   0]\n",
      " [  0 160 260   0]]\n",
      "Mean absolute deviation (MAD) = 23.231953560455242\n",
      "Mean squared error (MSE) = 830.1663440168611\n",
      "Mean absolute percentage error (MAPE) = 221.83998173557714\n",
      "Cohen kappa score = 0.0012880051343312493\n",
      "Fold 4, mse = 830.1663, mad = 23.2320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:40:28,314 - INFO - Fold 4, mse = 830.1663, mad = 23.2320\n",
      "2023-08-11 12:40:28,819 - INFO - Fold 4 Epoch 1 Batch 0: Train Loss = 0.9226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 1 Batch 0: Train Loss = 0.9226\n",
      "------------ Save FOLD-BEST model - MSE: 781.1693 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:40:47,009 - INFO - ------------ Save FOLD-BEST model - MSE: 781.1693 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 139 111   0]\n",
      " [  0 457 420   0]\n",
      " [  0 289 266   0]\n",
      " [  0 120 300   0]]\n",
      "Mean absolute deviation (MAD) = 22.633660703149705\n",
      "Mean squared error (MSE) = 781.1692606978088\n",
      "Mean absolute percentage error (MAPE) = 218.24614781307346\n",
      "Cohen kappa score = 0.06630204048299737\n",
      "Fold 4, mse = 781.1693, mad = 22.6337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:40:47,146 - INFO - Fold 4, mse = 781.1693, mad = 22.6337\n",
      "2023-08-11 12:40:47,624 - INFO - Fold 4 Epoch 2 Batch 0: Train Loss = 0.7147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 2 Batch 0: Train Loss = 0.7147\n",
      "------------ Save FOLD-BEST model - MSE: 755.7516 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:41:07,213 - INFO - ------------ Save FOLD-BEST model - MSE: 755.7516 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 173  77   0]\n",
      " [  0 587 290   0]\n",
      " [  0 355 200   0]\n",
      " [  0 149 271   0]]\n",
      "Mean absolute deviation (MAD) = 21.912821422997553\n",
      "Mean squared error (MSE) = 755.751616482314\n",
      "Mean absolute percentage error (MAPE) = 190.40275386890113\n",
      "Cohen kappa score = 0.09650431807062776\n",
      "Fold 4, mse = 755.7516, mad = 21.9128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:41:07,354 - INFO - Fold 4, mse = 755.7516, mad = 21.9128\n",
      "2023-08-11 12:41:07,866 - INFO - Fold 4 Epoch 3 Batch 0: Train Loss = 0.8102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 3 Batch 0: Train Loss = 0.8102\n",
      "------------ Save FOLD-BEST model - MSE: 719.6057 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:41:25,835 - INFO - ------------ Save FOLD-BEST model - MSE: 719.6057 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  3 175  72   0]\n",
      " [  0 556 321   0]\n",
      " [  0 297 258   0]\n",
      " [  0 114 306   0]]\n",
      "Mean absolute deviation (MAD) = 21.573327345496708\n",
      "Mean squared error (MSE) = 719.6056995090523\n",
      "Mean absolute percentage error (MAPE) = 187.9505938873417\n",
      "Cohen kappa score = 0.14301813162752286\n",
      "Fold 4, mse = 719.6057, mad = 21.5733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:41:25,979 - INFO - Fold 4, mse = 719.6057, mad = 21.5733\n",
      "2023-08-11 12:41:26,496 - INFO - Fold 4 Epoch 4 Batch 0: Train Loss = 0.9009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 4 Batch 0: Train Loss = 0.9009\n",
      "------------ Save FOLD-BEST model - MSE: 710.7313 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:41:44,899 - INFO - ------------ Save FOLD-BEST model - MSE: 710.7313 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  8 163  79   0]\n",
      " [  1 536 340   0]\n",
      " [  0 285 270   0]\n",
      " [  0  82 338   0]]\n",
      "Mean absolute deviation (MAD) = 21.39380089738875\n",
      "Mean squared error (MSE) = 710.7312984525208\n",
      "Mean absolute percentage error (MAPE) = 184.7205879521794\n",
      "Cohen kappa score = 0.16054683279717963\n",
      "Fold 4, mse = 710.7313, mad = 21.3938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:41:45,041 - INFO - Fold 4, mse = 710.7313, mad = 21.3938\n",
      "2023-08-11 12:41:45,538 - INFO - Fold 4 Epoch 5 Batch 0: Train Loss = 0.7291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 5 Batch 0: Train Loss = 0.7291\n",
      "------------ Save FOLD-BEST model - MSE: 701.9044 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:42:03,440 - INFO - ------------ Save FOLD-BEST model - MSE: 701.9044 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  7 169  74   0]\n",
      " [  0 569 308   0]\n",
      " [  0 299 256   0]\n",
      " [  0  93 327   0]]\n",
      "Mean absolute deviation (MAD) = 21.166526172729398\n",
      "Mean squared error (MSE) = 701.9043820435901\n",
      "Mean absolute percentage error (MAPE) = 178.43862617100555\n",
      "Cohen kappa score = 0.16459126405958036\n",
      "Fold 4, mse = 701.9044, mad = 21.1665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:42:03,581 - INFO - Fold 4, mse = 701.9044, mad = 21.1665\n",
      "2023-08-11 12:42:04,052 - INFO - Fold 4 Epoch 6 Batch 0: Train Loss = 0.8223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 6 Batch 0: Train Loss = 0.8223\n",
      "Fold 4, mse = 706.9534, mad = 21.2835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:42:22,016 - INFO - Fold 4, mse = 706.9534, mad = 21.2835\n",
      "2023-08-11 12:42:22,507 - INFO - Fold 4 Epoch 7 Batch 0: Train Loss = 0.7296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 7 Batch 0: Train Loss = 0.7296\n",
      "Fold 4, mse = 722.1583, mad = 21.4438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:42:40,225 - INFO - Fold 4, mse = 722.1583, mad = 21.4438\n",
      "2023-08-11 12:42:40,757 - INFO - Fold 4 Epoch 8 Batch 0: Train Loss = 0.8271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 8 Batch 0: Train Loss = 0.8271\n",
      "Fold 4, mse = 716.3073, mad = 21.3275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:42:58,709 - INFO - Fold 4, mse = 716.3073, mad = 21.3275\n",
      "2023-08-11 12:42:59,238 - INFO - Fold 4 Epoch 9 Batch 0: Train Loss = 0.6704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 9 Batch 0: Train Loss = 0.6704\n",
      "------------ Save FOLD-BEST model - MSE: 693.5641 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:43:16,983 - INFO - ------------ Save FOLD-BEST model - MSE: 693.5641 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[ 10 173  67   0]\n",
      " [  9 568 300   0]\n",
      " [  0 302 253   0]\n",
      " [  0 100 320   0]]\n",
      "Mean absolute deviation (MAD) = 20.87505838346909\n",
      "Mean squared error (MSE) = 693.5641266139203\n",
      "Mean absolute percentage error (MAPE) = 169.07420913040028\n",
      "Cohen kappa score = 0.1675609758784372\n",
      "Fold 4, mse = 693.5641, mad = 20.8751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:43:17,122 - INFO - Fold 4, mse = 693.5641, mad = 20.8751\n",
      "2023-08-11 12:43:17,581 - INFO - Fold 4 Epoch 10 Batch 0: Train Loss = 0.8017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 10 Batch 0: Train Loss = 0.8017\n",
      "Fold 4, epoch 10: Loss = 0.6903 Valid loss = 0.9785 MSE = 708.4331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:43:35,513 - INFO - Fold 4, epoch 10: Loss = 0.6903 Valid loss = 0.9785 MSE = 708.4331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, mse = 708.4331, mad = 21.2457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:43:35,515 - INFO - Fold 4, mse = 708.4331, mad = 21.2457\n",
      "2023-08-11 12:43:35,968 - INFO - Fold 4 Epoch 11 Batch 0: Train Loss = 0.7004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 11 Batch 0: Train Loss = 0.7004\n",
      "Fold 4, mse = 707.2118, mad = 21.2148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:43:53,967 - INFO - Fold 4, mse = 707.2118, mad = 21.2148\n",
      "2023-08-11 12:43:54,491 - INFO - Fold 4 Epoch 12 Batch 0: Train Loss = 0.6727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 12 Batch 0: Train Loss = 0.6727\n",
      "Fold 4, mse = 740.0629, mad = 21.5944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:44:11,736 - INFO - Fold 4, mse = 740.0629, mad = 21.5944\n",
      "2023-08-11 12:44:12,193 - INFO - Fold 4 Epoch 13 Batch 0: Train Loss = 0.6095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 13 Batch 0: Train Loss = 0.6095\n",
      "Fold 4, mse = 742.4251, mad = 21.6603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:44:28,473 - INFO - Fold 4, mse = 742.4251, mad = 21.6603\n",
      "2023-08-11 12:44:28,982 - INFO - Fold 4 Epoch 14 Batch 0: Train Loss = 0.6810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 14 Batch 0: Train Loss = 0.6810\n",
      "Fold 4, mse = 702.6895, mad = 21.3989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:44:47,084 - INFO - Fold 4, mse = 702.6895, mad = 21.3989\n",
      "2023-08-11 12:44:47,607 - INFO - Fold 4 Epoch 15 Batch 0: Train Loss = 0.6661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 15 Batch 0: Train Loss = 0.6661\n",
      "Fold 4, mse = 715.7376, mad = 21.4427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:45:05,544 - INFO - Fold 4, mse = 715.7376, mad = 21.4427\n",
      "2023-08-11 12:45:06,072 - INFO - Fold 4 Epoch 16 Batch 0: Train Loss = 0.6428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 16 Batch 0: Train Loss = 0.6428\n",
      "Fold 4, mse = 721.9504, mad = 21.8246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:45:24,051 - INFO - Fold 4, mse = 721.9504, mad = 21.8246\n",
      "2023-08-11 12:45:24,506 - INFO - Fold 4 Epoch 17 Batch 0: Train Loss = 0.6614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 17 Batch 0: Train Loss = 0.6614\n",
      "Fold 4, mse = 749.2422, mad = 21.9576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:45:42,918 - INFO - Fold 4, mse = 749.2422, mad = 21.9576\n",
      "2023-08-11 12:45:43,395 - INFO - Fold 4 Epoch 18 Batch 0: Train Loss = 0.6177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 18 Batch 0: Train Loss = 0.6177\n",
      "Fold 4, mse = 721.3989, mad = 21.6094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:46:01,268 - INFO - Fold 4, mse = 721.3989, mad = 21.6094\n",
      "2023-08-11 12:46:01,768 - INFO - Fold 4 Epoch 19 Batch 0: Train Loss = 0.6208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 19 Batch 0: Train Loss = 0.6208\n",
      "Fold 4, mse = 724.5638, mad = 21.5225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:46:19,847 - INFO - Fold 4, mse = 724.5638, mad = 21.5225\n",
      "2023-08-11 12:46:20,407 - INFO - Fold 4 Epoch 20 Batch 0: Train Loss = 0.5221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 20 Batch 0: Train Loss = 0.5221\n",
      "Fold 4, epoch 20: Loss = 0.6093 Valid loss = 1.0042 MSE = 718.7038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:46:38,306 - INFO - Fold 4, epoch 20: Loss = 0.6093 Valid loss = 1.0042 MSE = 718.7038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, mse = 718.7038, mad = 21.5290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:46:38,309 - INFO - Fold 4, mse = 718.7038, mad = 21.5290\n",
      "2023-08-11 12:46:38,801 - INFO - Fold 4 Epoch 21 Batch 0: Train Loss = 0.6466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 21 Batch 0: Train Loss = 0.6466\n",
      "Fold 4, mse = 721.6742, mad = 21.9217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:46:57,703 - INFO - Fold 4, mse = 721.6742, mad = 21.9217\n",
      "2023-08-11 12:46:58,229 - INFO - Fold 4 Epoch 22 Batch 0: Train Loss = 0.6858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 22 Batch 0: Train Loss = 0.6858\n",
      "Fold 4, mse = 712.9040, mad = 21.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:47:16,102 - INFO - Fold 4, mse = 712.9040, mad = 21.4375\n",
      "2023-08-11 12:47:16,600 - INFO - Fold 4 Epoch 23 Batch 0: Train Loss = 0.5729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 23 Batch 0: Train Loss = 0.5729\n",
      "Fold 4, mse = 741.5980, mad = 22.0277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:47:35,256 - INFO - Fold 4, mse = 741.5980, mad = 22.0277\n",
      "2023-08-11 12:47:35,715 - INFO - Fold 4 Epoch 24 Batch 0: Train Loss = 0.5097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 24 Batch 0: Train Loss = 0.5097\n",
      "Fold 4, mse = 752.0515, mad = 22.3819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:47:55,219 - INFO - Fold 4, mse = 752.0515, mad = 22.3819\n",
      "2023-08-11 12:47:55,731 - INFO - Fold 4 Epoch 25 Batch 0: Train Loss = 0.6274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 25 Batch 0: Train Loss = 0.6274\n",
      "Fold 4, mse = 755.2506, mad = 21.9418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:48:13,942 - INFO - Fold 4, mse = 755.2506, mad = 21.9418\n",
      "2023-08-11 12:48:14,364 - INFO - Fold 4 Epoch 26 Batch 0: Train Loss = 0.6004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 26 Batch 0: Train Loss = 0.6004\n",
      "Fold 4, mse = 711.9163, mad = 21.4008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:48:32,501 - INFO - Fold 4, mse = 711.9163, mad = 21.4008\n",
      "2023-08-11 12:48:33,021 - INFO - Fold 4 Epoch 27 Batch 0: Train Loss = 0.4928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 27 Batch 0: Train Loss = 0.4928\n",
      "Fold 4, mse = 738.4656, mad = 21.9261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:48:51,077 - INFO - Fold 4, mse = 738.4656, mad = 21.9261\n",
      "2023-08-11 12:48:51,529 - INFO - Fold 4 Epoch 28 Batch 0: Train Loss = 0.6125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 28 Batch 0: Train Loss = 0.6125\n",
      "Fold 4, mse = 779.6123, mad = 22.4163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:49:11,023 - INFO - Fold 4, mse = 779.6123, mad = 22.4163\n",
      "2023-08-11 12:49:11,542 - INFO - Fold 4 Epoch 29 Batch 0: Train Loss = 0.5909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 29 Batch 0: Train Loss = 0.5909\n",
      "Fold 4, mse = 756.3656, mad = 22.1411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:49:29,590 - INFO - Fold 4, mse = 756.3656, mad = 22.1411\n",
      "2023-08-11 12:49:30,324 - INFO - Fold 5 Epoch 0 Batch 0: Train Loss = 1.1396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 0 Batch 0: Train Loss = 1.1396\n",
      "Fold 5, epoch 0: Loss = 0.9870 Valid loss = 0.9841 MSE = 699.5252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:49:48,990 - INFO - Fold 5, epoch 0: Loss = 0.9870 Valid loss = 0.9841 MSE = 699.5252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Save FOLD-BEST model - MSE: 699.5252 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:49:48,991 - INFO - ------------ Save FOLD-BEST model - MSE: 699.5252 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 179 114   0]\n",
      " [  0 399 511   0]\n",
      " [  0 176 345   0]\n",
      " [  0  73 238   0]]\n",
      "Mean absolute deviation (MAD) = 21.639641715275683\n",
      "Mean squared error (MSE) = 699.5252208200124\n",
      "Mean absolute percentage error (MAPE) = 269.4794290552038\n",
      "Cohen kappa score = 0.10761185226327341\n",
      "Fold 5, mse = 699.5252, mad = 21.6396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:49:49,122 - INFO - Fold 5, mse = 699.5252, mad = 21.6396\n",
      "2023-08-11 12:49:49,622 - INFO - Fold 5 Epoch 1 Batch 0: Train Loss = 1.0436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 1 Batch 0: Train Loss = 1.0436\n",
      "------------ Save FOLD-BEST model - MSE: 689.9164 ------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:50:08,091 - INFO - ------------ Save FOLD-BEST model - MSE: 689.9164 ------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom bins confusion matrix:\n",
      "[[  0 199  94   0]\n",
      " [  0 450 460   0]\n",
      " [  0 226 295   0]\n",
      " [  0 100 211   0]]\n",
      "Mean absolute deviation (MAD) = 21.464991344112214\n",
      "Mean squared error (MSE) = 689.9164444457284\n",
      "Mean absolute percentage error (MAPE) = 259.93748835657817\n",
      "Cohen kappa score = 0.08915041984364447\n",
      "Fold 5, mse = 689.9164, mad = 21.4650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:50:08,227 - INFO - Fold 5, mse = 689.9164, mad = 21.4650\n",
      "2023-08-11 12:50:08,782 - INFO - Fold 5 Epoch 2 Batch 0: Train Loss = 0.9585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 2 Batch 0: Train Loss = 0.9585\n",
      "Fold 5, mse = 709.0048, mad = 21.5314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:50:27,845 - INFO - Fold 5, mse = 709.0048, mad = 21.5314\n",
      "2023-08-11 12:50:28,432 - INFO - Fold 5 Epoch 3 Batch 0: Train Loss = 0.8420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 3 Batch 0: Train Loss = 0.8420\n",
      "Fold 5, mse = 724.1798, mad = 21.5492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:50:49,448 - INFO - Fold 5, mse = 724.1798, mad = 21.5492\n",
      "2023-08-11 12:50:49,953 - INFO - Fold 5 Epoch 4 Batch 0: Train Loss = 0.8116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 4 Batch 0: Train Loss = 0.8116\n",
      "Fold 5, mse = 719.9338, mad = 21.4070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:51:08,527 - INFO - Fold 5, mse = 719.9338, mad = 21.4070\n",
      "2023-08-11 12:51:09,079 - INFO - Fold 5 Epoch 5 Batch 0: Train Loss = 0.8326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 5 Batch 0: Train Loss = 0.8326\n",
      "Fold 5, mse = 735.2393, mad = 21.8232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:51:28,633 - INFO - Fold 5, mse = 735.2393, mad = 21.8232\n",
      "2023-08-11 12:51:29,188 - INFO - Fold 5 Epoch 6 Batch 0: Train Loss = 0.8513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 6 Batch 0: Train Loss = 0.8513\n",
      "Fold 5, mse = 737.0011, mad = 21.3964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:51:49,978 - INFO - Fold 5, mse = 737.0011, mad = 21.3964\n",
      "2023-08-11 12:51:50,564 - INFO - Fold 5 Epoch 7 Batch 0: Train Loss = 0.8195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 7 Batch 0: Train Loss = 0.8195\n",
      "Fold 5, mse = 763.4832, mad = 21.8670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:52:10,061 - INFO - Fold 5, mse = 763.4832, mad = 21.8670\n",
      "2023-08-11 12:52:10,544 - INFO - Fold 5 Epoch 8 Batch 0: Train Loss = 0.7406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 8 Batch 0: Train Loss = 0.7406\n",
      "Fold 5, mse = 774.3513, mad = 22.2997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:52:31,651 - INFO - Fold 5, mse = 774.3513, mad = 22.2997\n",
      "2023-08-11 12:52:32,207 - INFO - Fold 5 Epoch 9 Batch 0: Train Loss = 0.6500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 9 Batch 0: Train Loss = 0.6500\n",
      "Fold 5, mse = 761.8563, mad = 21.9249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:52:51,196 - INFO - Fold 5, mse = 761.8563, mad = 21.9249\n",
      "2023-08-11 12:52:51,766 - INFO - Fold 5 Epoch 10 Batch 0: Train Loss = 0.6824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 10 Batch 0: Train Loss = 0.6824\n",
      "Fold 5, epoch 10: Loss = 0.6718 Valid loss = 1.0946 MSE = 775.1876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:53:10,648 - INFO - Fold 5, epoch 10: Loss = 0.6718 Valid loss = 1.0946 MSE = 775.1876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, mse = 775.1876, mad = 22.2828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:53:10,650 - INFO - Fold 5, mse = 775.1876, mad = 22.2828\n",
      "2023-08-11 12:53:11,126 - INFO - Fold 5 Epoch 11 Batch 0: Train Loss = 0.7133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 11 Batch 0: Train Loss = 0.7133\n",
      "Fold 5, mse = 783.7895, mad = 22.4482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:53:29,719 - INFO - Fold 5, mse = 783.7895, mad = 22.4482\n",
      "2023-08-11 12:53:30,197 - INFO - Fold 5 Epoch 12 Batch 0: Train Loss = 0.6307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 12 Batch 0: Train Loss = 0.6307\n",
      "Fold 5, mse = 773.3265, mad = 22.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:53:50,021 - INFO - Fold 5, mse = 773.3265, mad = 22.0677\n",
      "2023-08-11 12:53:50,540 - INFO - Fold 5 Epoch 13 Batch 0: Train Loss = 0.5896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 13 Batch 0: Train Loss = 0.5896\n",
      "Fold 5, mse = 796.2789, mad = 22.3369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:54:07,438 - INFO - Fold 5, mse = 796.2789, mad = 22.3369\n",
      "2023-08-11 12:54:07,952 - INFO - Fold 5 Epoch 14 Batch 0: Train Loss = 0.7115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 14 Batch 0: Train Loss = 0.7115\n",
      "Fold 5, mse = 798.2354, mad = 22.3007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:54:26,650 - INFO - Fold 5, mse = 798.2354, mad = 22.3007\n",
      "2023-08-11 12:54:27,221 - INFO - Fold 5 Epoch 15 Batch 0: Train Loss = 0.5990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 15 Batch 0: Train Loss = 0.5990\n",
      "Fold 5, mse = 802.8176, mad = 22.6960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:54:45,857 - INFO - Fold 5, mse = 802.8176, mad = 22.6960\n",
      "2023-08-11 12:54:46,446 - INFO - Fold 5 Epoch 16 Batch 0: Train Loss = 0.5620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 16 Batch 0: Train Loss = 0.5620\n",
      "Fold 5, mse = 750.0686, mad = 21.7759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:55:05,872 - INFO - Fold 5, mse = 750.0686, mad = 21.7759\n",
      "2023-08-11 12:55:06,450 - INFO - Fold 5 Epoch 17 Batch 0: Train Loss = 0.6947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 17 Batch 0: Train Loss = 0.6947\n",
      "Fold 5, mse = 777.6747, mad = 22.1352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:55:27,432 - INFO - Fold 5, mse = 777.6747, mad = 22.1352\n",
      "2023-08-11 12:55:27,944 - INFO - Fold 5 Epoch 18 Batch 0: Train Loss = 0.5857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 18 Batch 0: Train Loss = 0.5857\n",
      "Fold 5, mse = 805.2126, mad = 22.4278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:55:46,503 - INFO - Fold 5, mse = 805.2126, mad = 22.4278\n",
      "2023-08-11 12:55:47,029 - INFO - Fold 5 Epoch 19 Batch 0: Train Loss = 0.6405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 19 Batch 0: Train Loss = 0.6405\n",
      "Fold 5, mse = 806.2534, mad = 22.4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:56:05,539 - INFO - Fold 5, mse = 806.2534, mad = 22.4848\n",
      "2023-08-11 12:56:06,057 - INFO - Fold 5 Epoch 20 Batch 0: Train Loss = 0.5503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 20 Batch 0: Train Loss = 0.5503\n",
      "Fold 5, epoch 20: Loss = 0.6225 Valid loss = 1.0799 MSE = 766.3352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:56:24,423 - INFO - Fold 5, epoch 20: Loss = 0.6225 Valid loss = 1.0799 MSE = 766.3352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, mse = 766.3352, mad = 22.3447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:56:24,425 - INFO - Fold 5, mse = 766.3352, mad = 22.3447\n",
      "2023-08-11 12:56:24,889 - INFO - Fold 5 Epoch 21 Batch 0: Train Loss = 0.7181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 21 Batch 0: Train Loss = 0.7181\n",
      "Fold 5, mse = 790.7934, mad = 22.2188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:56:43,484 - INFO - Fold 5, mse = 790.7934, mad = 22.2188\n",
      "2023-08-11 12:56:43,945 - INFO - Fold 5 Epoch 22 Batch 0: Train Loss = 0.5618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 22 Batch 0: Train Loss = 0.5618\n",
      "Fold 5, mse = 817.8837, mad = 22.6455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:57:02,124 - INFO - Fold 5, mse = 817.8837, mad = 22.6455\n",
      "2023-08-11 12:57:02,689 - INFO - Fold 5 Epoch 23 Batch 0: Train Loss = 0.5963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 23 Batch 0: Train Loss = 0.5963\n",
      "Fold 5, mse = 759.4625, mad = 21.9553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:57:21,698 - INFO - Fold 5, mse = 759.4625, mad = 21.9553\n",
      "2023-08-11 12:57:22,262 - INFO - Fold 5 Epoch 24 Batch 0: Train Loss = 0.6285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 24 Batch 0: Train Loss = 0.6285\n",
      "Fold 5, mse = 779.6140, mad = 22.3189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:57:43,740 - INFO - Fold 5, mse = 779.6140, mad = 22.3189\n",
      "2023-08-11 12:57:44,307 - INFO - Fold 5 Epoch 25 Batch 0: Train Loss = 0.5436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 25 Batch 0: Train Loss = 0.5436\n",
      "Fold 5, mse = 790.8243, mad = 22.4480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:58:03,133 - INFO - Fold 5, mse = 790.8243, mad = 22.4480\n",
      "2023-08-11 12:58:03,643 - INFO - Fold 5 Epoch 26 Batch 0: Train Loss = 0.5345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 26 Batch 0: Train Loss = 0.5345\n",
      "Fold 5, mse = 777.3193, mad = 21.8379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:58:21,694 - INFO - Fold 5, mse = 777.3193, mad = 21.8379\n",
      "2023-08-11 12:58:22,188 - INFO - Fold 5 Epoch 27 Batch 0: Train Loss = 0.5764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 27 Batch 0: Train Loss = 0.5764\n",
      "Fold 5, mse = 758.3977, mad = 22.1417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 12:58:40,293 - INFO - Fold 5, mse = 758.3977, mad = 22.1417\n",
      "2023-08-11 12:58:40,869 - INFO - Fold 5 Epoch 28 Batch 0: Train Loss = 0.5993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 28 Batch 0: Train Loss = 0.5993\n"
     ]
    }
   ],
   "source": [
    "if target_dataset == 'PD':\n",
    "    n_splits = 5\n",
    "    epochs = 30\n",
    "\n",
    "teacher_flag = True\n",
    "transfer_flag = True\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "if target_dataset == 'PD':    \n",
    "    data_str = 'pd'\n",
    "\n",
    "if teacher_flag:\n",
    "    file_name = './model/pretrained-challenge-front-fill-2'+ data_str\n",
    "else: \n",
    "    file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher'\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "fold_count = 0\n",
    "total_train_loss = []\n",
    "total_valid_loss = []\n",
    "\n",
    "global_best = 10000\n",
    "mse = []\n",
    "mad = []\n",
    "mape = []\n",
    "kappa = []\n",
    "history = []\n",
    "\n",
    "pad_token = np.zeros(input_dim)\n",
    "# begin_time = time.time()\n",
    "\n",
    "for train, test in kfold.split(long_x):\n",
    "    train_x = [long_x[i] for i in train]\n",
    "    train_y = [long_time[i] for i in train]\n",
    "    train_x_len = [all_x_len[i] for i in train]\n",
    "    #train_static = [long_static[i] for i in train]\n",
    "    \n",
    "    train_x, train_y, train_x_len = get_n2n_data(train_x, train_y, train_x_len)\n",
    "    if len(train_x) % 256 == 1:\n",
    "        print(len(train_x))\n",
    "        print('wrong squeeze!')\n",
    "\n",
    "# for train, test in kfold.split(long_x):\n",
    "for train, test in kfold.split(long_x):\n",
    "    \n",
    "    model = distcare_target(input_dim = input_dim,output_dim=output_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, hidden_dim=hidden_dim).to(device)\n",
    "    \n",
    "    if transfer_flag:\n",
    "        checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu'))\n",
    "        pretrain_dict = checkpoint['net']\n",
    "        model_dict = model.state_dict()\n",
    "        pretrain_dict = transfer_gru_dict(pretrain_dict, model_dict,latest_idx, common_len)\n",
    "        model_dict.update(pretrain_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    fold_count += 1\n",
    "#     print(train)\n",
    "\n",
    "    \n",
    "    train_x = [long_x[i] for i in train]\n",
    "    train_y = [long_time[i] for i in train]\n",
    "    train_x_len = [all_x_len[i] for i in train]\n",
    "    #train_static = [long_static[i] for i in train]\n",
    "    \n",
    "    train_x, train_y, train_x_len = get_n2n_data(train_x, train_y, train_x_len)\n",
    "    \n",
    "    test_x = [long_x[i] for i in test]\n",
    "    test_y = [long_time[i] for i in test]\n",
    "    test_x_len = [all_x_len[i] for i in test]\n",
    "    #test_static = [long_static[i] for i in test]\n",
    "    \n",
    "    test_x, test_y, test_x_len = get_n2n_data(test_x, test_y, test_x_len)\n",
    "    \n",
    "    if not os.path.exists('./model/'+data_str):\n",
    "        os.mkdir('./model/'+data_str)\n",
    "        \n",
    "    if transfer_flag:\n",
    "        target_file_name = './model/'+data_str+'/distcare-trans-'+str(n_splits)+'-fold-LOS-regression' + str(fold_count)#4114\n",
    "    else:\n",
    "        target_file_name = './model/'+data_str+'/distcare-no-trans-'+str(n_splits)+'-fold-LOS-regression' + str(fold_count)#4114\n",
    "    \n",
    "    fold_train_loss = []\n",
    "    fold_valid_loss = []\n",
    "    best_mse = 10000\n",
    "    best_mad = 0\n",
    "    best_mape = 0\n",
    "    best_kappa = 0\n",
    "    \n",
    "    for each_epoch in range(epochs):\n",
    "       \n",
    "        \n",
    "        epoch_loss = []\n",
    "        counter_batch = 0\n",
    "        model.train()  \n",
    "        \n",
    "        for step, (batch_x, batch_y, batch_lens) in enumerate(ckd_batch_iter(train_x, train_y, train_x_len, batch_size, shuffle=True)):  \n",
    "            optimizer.zero_grad()\n",
    "            batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "\n",
    "            masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "            opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "\n",
    "            MSE_Loss = get_re_loss(opt, batch_y.unsqueeze(-1))\n",
    "\n",
    "#             model_loss = pred_loss + 1e7*decov_loss\n",
    "            model_loss = MSE_Loss\n",
    "\n",
    "            loss = model_loss\n",
    "\n",
    "            epoch_loss.append(MSE_Loss.cpu().detach().numpy())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % 50 == 0:\n",
    "                print('Fold %d Epoch %d Batch %d: Train Loss = %.4f'%(fold_count,each_epoch, step, loss.cpu().detach().numpy()))\n",
    "                logger.info('Fold %d Epoch %d Batch %d: Train Loss = %.4f'%(fold_count,each_epoch, step, loss.cpu().detach().numpy()))\n",
    "            \n",
    "        epoch_loss = np.mean(epoch_loss)\n",
    "        fold_train_loss.append(epoch_loss)\n",
    "\n",
    "        #Validation\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_pred_flatten = []\n",
    "        y_true_flatten = []\n",
    "        outcome_pred_flatten = []\n",
    "        outcome_true_flatten = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            valid_true = []\n",
    "            valid_pred = []\n",
    "            for batch_x, batch_y, batch_lens in ckd_batch_iter(test_x, test_y, test_x_len, batch_size):\n",
    "                batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "                batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "                batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "                masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "               \n",
    "                opt, decov_loss, emb = model(batch_x, batch_lens)\n",
    "                \n",
    "                MSE_Loss = get_re_loss(opt, batch_y.unsqueeze(-1))\n",
    "                \n",
    "                valid_loss.append(MSE_Loss.cpu().detach().numpy())\n",
    "\n",
    "                y_pred_flatten += [reverse_los(x, los_info) / 30 for x in list(opt.cpu().detach().numpy().flatten())]\n",
    "                y_true_flatten += [reverse_los(x, los_info) / 30 for x in list(batch_y.cpu().numpy().flatten())]\n",
    "            \n",
    "\n",
    "            valid_loss = np.mean(valid_loss)\n",
    "            fold_valid_loss.append(valid_loss)\n",
    "            ret = metrics.print_metrics_regression(y_true_flatten, y_pred_flatten, verbose=0)\n",
    "            history.append(ret)\n",
    "            #print()\n",
    "\n",
    "            if each_epoch % 10 == 0:\n",
    "                print('Fold %d, epoch %d: Loss = %.4f Valid loss = %.4f MSE = %.4f' % (\n",
    "                    fold_count, each_epoch, fold_train_loss[-1], fold_valid_loss[-1], ret['mse']), flush=True)\n",
    "                logger.info('Fold %d, epoch %d: Loss = %.4f Valid loss = %.4f MSE = %.4f' % (\n",
    "                    fold_count, each_epoch, fold_train_loss[-1], fold_valid_loss[-1], ret['mse']))\n",
    "                # metrics.print_metrics_regression(y_true_flatten, y_pred_flatten)\n",
    "                \n",
    "            cur_mse = ret['mse']\n",
    "            if cur_mse < best_mse:\n",
    "                print('------------ Save FOLD-BEST model - MSE: %.4f ------------' % cur_mse, flush=True)\n",
    "                logger.info('------------ Save FOLD-BEST model - MSE: %.4f ------------' % cur_mse)\n",
    "                metrics.print_metrics_regression(y_true_flatten, y_pred_flatten)\n",
    "                best_mse = cur_mse\n",
    "                best_mad = ret['mad']\n",
    "                state = {\n",
    "                    'net': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': each_epoch\n",
    "                }\n",
    "                torch.save(state, target_file_name + '_' + str(fold_count))\n",
    "\n",
    "                if cur_mse < global_best:\n",
    "                    global_best = cur_mse\n",
    "                    state = {\n",
    "                        'net': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'epoch': each_epoch\n",
    "                    }\n",
    "                    torch.save(state, target_file_name)\n",
    "                    print('------------ Save best model - MSE: %.4f ------------' % cur_mse, flush=True)\n",
    "                    logger.info('------------ Save best model - MSE: %.4f ------------' % cur_mse)\n",
    "\n",
    "        print('Fold %d, mse = %.4f, mad = %.4f' % (fold_count, ret['mse'], ret['mad']), flush=True)\n",
    "        logger.info('Fold %d, mse = %.4f, mad = %.4f' % (fold_count, ret['mse'], ret['mad']))\n",
    "\n",
    "    mse.append(best_mse)\n",
    "    mad.append(best_mad)\n",
    "    total_train_loss.append(fold_train_loss)\n",
    "    total_valid_loss.append(fold_valid_loss)\n",
    "\n",
    "\n",
    "print('mse %.4f(%.4f)' % (np.mean(mse), np.std(mse)))\n",
    "print('mad %.4f(%.4f)' % (np.mean(mad), np.std(mad)))\n",
    "logger.info('mse %.4f(%.4f)' % (np.mean(mse), np.std(mse)))\n",
    "logger.info('mad %.4f(%.4f)' % (np.mean(mad), np.std(mad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
