{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "import pickle5 as pickle\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import logging\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from utils import utils\n",
    "from utils.readers import InHospitalMortalityReader\n",
    "from utils.preprocessing import Discretizer, Normalizer\n",
    "from utils import metrics\n",
    "from utils import common_utils\n",
    "from torch.autograd import Function\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = 'PD' \n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED) #numpy\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED) # cpu\n",
    "torch.cuda.manual_seed(RANDOM_SEED) #gpu\n",
    "torch.backends.cudnn.deterministic=True # cudnn\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() == True else 'cpu')\n",
    "# print(\"available device: {}\".format(device))\n",
    "reverse = True\n",
    "model_name = 'dann'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reverse:\n",
    "    file_name = 'log_file' + '_' + model_name + '_' + target_dataset + '_' + 'reverse' + '.log'\n",
    "else:\n",
    "    file_name = 'log_file' + '_' + model_name + '_' + target_dataset + '.log'\n",
    "def get_logger(name, file_name):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # 以下两行是为了在jupyter notebook 中不重复输出日志\n",
    "    if logger.root.handlers:\n",
    "        logger.root.handlers[0].setLevel(logging.WARNING)\n",
    " \n",
    "    handler_stdout = logging.StreamHandler()\n",
    "    handler_stdout.setLevel(logging.INFO)\n",
    "    handler_stdout.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    # logger.addHandler(handler_stdout)\n",
    " \n",
    "    handler_file = logging.FileHandler(filename=file_name, mode='w', encoding='utf-8')\n",
    "    handler_file.setLevel(logging.DEBUG)\n",
    "    handler_file.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler_file)\n",
    " \n",
    "    return logger\n",
    "\n",
    "logger = get_logger(__name__,file_name)\n",
    "\n",
    "logger.debug('这是希望输出的debug内容')\n",
    "logger.info('这是希望输出的info内容')\n",
    "logger.warning('这是希望输出的warning内容')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get source data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n2n_data(x, y, x_len):\n",
    "    length = len(x)\n",
    "    assert length == len(y)\n",
    "    assert length == len(x_len)\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    new_x_len = []\n",
    "    for i in range(length):\n",
    "        for j in range(len(x[i])):\n",
    "            new_x.append(x[i][:j+1])\n",
    "            new_y.append(y[i][j])\n",
    "            new_x_len.append(j+1)\n",
    "    return new_x, new_y, new_x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data_path = './data/Challenge/'\n",
    "small_part = False\n",
    "arg_timestep = 1.0\n",
    "batch_size = 256\n",
    "epochs = 100\n",
    "all_x_source = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "all_y_source = pickle.load(open(source_data_path + 'new_y_front_fill.dat', 'rb'))\n",
    "all_names_source = pickle.load(open(source_data_path + 'new_name.dat', 'rb'))\n",
    "static_source = pickle.load(open(source_data_path + 'new_demo_front_fill.dat', 'rb'))\n",
    "mask_x_source = pickle.load(open(source_data_path + 'new_mask_x.dat', 'rb'))\n",
    "mask_demo_source = pickle.load(open(source_data_path + 'new_mask_demo.dat', 'rb'))\n",
    "all_x_len_source = [len(i) for i in all_x_source]\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    subset_idx_source = [31, 29, 28, 33, 25, 18, 7, 21, 16, 15, 19, 17, 24, 3, 5, 0]\n",
    "elif target_dataset == 'TJ':\n",
    "    subset_idx_source = [27, 29, 18, 16, 26, 33, 28, 31, 32, 15, 11, 25, 21, 20, 9, 17, 30, 19]\n",
    "elif target_dataset == 'HM':\n",
    "    subset_idx_source = [0, 1, 2, 3, 5, 9, 11, 12, 13, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
    "\n",
    "subset_cnt = len(subset_idx_source)\n",
    "other_idx = []\n",
    "for i in range(len(all_x_source[0][0])):\n",
    "    if i not in subset_idx_source:\n",
    "        other_idx.append(i)\n",
    "\n",
    "for i in range(len(all_x_source)): #将共同特征移动到最开始，非共同特征移动到末尾\n",
    "    cur = np.array(all_x_source[i], dtype=float)\n",
    "    cur_mask = np.array(mask_x_source[i])\n",
    "    cur_subset = cur[:, subset_idx_source]\n",
    "    cur_other = cur[:, other_idx]\n",
    "    cur_mask_subset = cur_mask[:, subset_idx_source]\n",
    "    cur_mask_other = cur_mask[:, other_idx]\n",
    "    all_x_source[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    mask_x_source[i] = np.concatenate((cur_mask_subset, cur_mask_other), axis=1).tolist()\n",
    "\n",
    "\n",
    "train_num_source =int( len(all_x_source) * 0.8) + 1\n",
    "logger.info(train_num_source)\n",
    "dev_num_source =int( len(all_x_source) * 0.1) + 1\n",
    "logger.info(dev_num_source)\n",
    "test_num_source =int( len(all_x_source) * 0.1)\n",
    "logger.info(test_num_source)\n",
    "assert(train_num_source+dev_num_source+test_num_source == len(all_x_source))\n",
    "\n",
    "train_x_source = []\n",
    "train_y_source = []\n",
    "train_names_source = []\n",
    "train_static_source = []\n",
    "train_x_len_source = []\n",
    "train_mask_x_source = []\n",
    "for idx in range(train_num_source):\n",
    "    train_x_source.append(all_x_source[idx])\n",
    "    train_y_source.append(int(all_y_source[idx][-1]))\n",
    "    train_names_source.append(all_names_source[idx])\n",
    "    train_static_source.append(static_source[idx])\n",
    "    train_x_len_source.append(all_x_len_source[idx])\n",
    "    train_mask_x_source.append(mask_x_source[idx])\n",
    "\n",
    "dev_x_source = []\n",
    "dev_y_source = []\n",
    "dev_names_source = []\n",
    "dev_static_source = []\n",
    "dev_x_len_source = []\n",
    "dev_mask_x_source = []\n",
    "for idx in range(train_num_source, train_num_source + dev_num_source):\n",
    "    dev_x_source.append(all_x_source[idx])\n",
    "    dev_y_source.append(int(all_y_source[idx][-1]))\n",
    "    dev_names_source.append(all_names_source[idx])\n",
    "    dev_static_source.append(static_source[idx])\n",
    "    dev_x_len_source.append(all_x_len_source[idx])\n",
    "    dev_mask_x_source.append(mask_x_source[idx])\n",
    "\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "test_names = []\n",
    "test_static = []\n",
    "test_x_len = []\n",
    "test_mask_x = []\n",
    "for idx in range(train_num_source + dev_num_source, train_num_source + dev_num_source + test_num_source):\n",
    "    test_x.append(all_x_source[idx])\n",
    "    test_y.append(int(all_y_source[idx][-1]))\n",
    "    test_names.append(all_names_source[idx])\n",
    "    test_static.append(static_source[idx])\n",
    "    test_x_len.append(all_x_len_source[idx])\n",
    "    test_mask_x.append(mask_x_source[idx])\n",
    "\n",
    "\n",
    "assert(len(train_x_source) == train_num_source)\n",
    "assert(len(dev_x_source) == dev_num_source)\n",
    "assert(len(test_x) == test_num_source)\n",
    "\n",
    "long_x_source = all_x_source\n",
    "long_y_source = [y[-1] for y in all_y_source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(y_pred, y_true):\n",
    "    loss = torch.nn.BCELoss()\n",
    "    return loss(y_pred, y_true)\n",
    "def get_re_loss(y_pred, y_true):\n",
    "    loss = torch.nn.MSELoss()\n",
    "    return loss(y_pred, y_true)\n",
    "def get_kl_loss(x_pred, x_target):\n",
    "    loss = torch.nn.KLDivLoss(reduce=True, size_average=True)\n",
    "    return loss(x_pred, x_target)\n",
    "def get_wass_dist(x_pred, x_target):\n",
    "    m1 = torch.mean(x_pred, dim=0)\n",
    "    m2 = torch.mean(x_target, dim=0)\n",
    "    v1 = torch.var(x_pred, dim=0)\n",
    "    v2 = torch.var(x_target, dim=0)\n",
    "    p1 = torch.sum(torch.pow((m1 - m2), 2))\n",
    "    p2 = torch.sum(torch.pow(torch.pow(v1, 1/2) - torch.pow(v2, 1/2), 2))\n",
    "    return torch.pow(p1+p2, 1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sents(sents, pad_token):\n",
    "#     print(f'len(pad_token) is {len(pad_token)}')\n",
    "#     print(f'sents is {sents}')\n",
    "\n",
    "    sents_padded = []\n",
    "\n",
    "    max_length = max([len(_) for _ in sents])\n",
    "    for i in sents:\n",
    "        padded = list(i) + [pad_token]*(max_length-len(i))\n",
    "#         print(f'padded is {padded}')\n",
    "        sents_padded.append(np.array(padded))\n",
    "\n",
    "\n",
    "    return np.array(sents_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(x, y, lens, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    # batch_num = math.ceil(len(x) / batch_size) # 向下取整\n",
    "    batch_num = len(x) // batch_size if len(x) % batch_size == 0 else len(x) // batch_size + 1\n",
    "    # print(f\"len(x) is {len(x)}, len(y) is {len(y)}, len(lens) is {len(lens)}, batch_size is {batch_size}, batch_num is {batch_num}\")\n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        if (i + 1) * batch_size  < len(x):\n",
    "            indices = index_array[i * batch_size: (i + 1) * batch_size] #  fetch out all the induces\n",
    "        else:\n",
    "            indices = index_array[i * batch_size: ]\n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx],lens[idx]))\n",
    "       \n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    \n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "        batch_lens = [e[2] for e in examples]\n",
    "\n",
    "        yield batch_x, batch_y, batch_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_to_mask(length, max_len=None, dtype=None):\n",
    "    \"\"\"length: B.\n",
    "    return B x max_len.\n",
    "    If max_len is None, then max of length will be used.\n",
    "    \"\"\"\n",
    "    assert len(length.shape) == 1, 'Length shape should be 1 dimensional.'\n",
    "    max_len = max_len or length.max().item()\n",
    "    mask = torch.arange(max_len, device=length.device,\n",
    "                        dtype=length.dtype).expand(len(length), max_len) < length.unsqueeze(1)\n",
    "    if dtype is not None:\n",
    "        mask = torch.as_tensor(mask, dtype=dtype, device=length.device)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class dann(nn.Module):\n",
    "    def __init__(self, common_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(dann, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = common_dim\n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.GRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        \n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.output = nn.Linear(self.input_dim, self.output_dim)\n",
    "\n",
    "        # adversal方法中的域分类器  \n",
    "        self.domain_classifier = nn.Sequential()\n",
    "        self.domain_classifier.add_module('d_fc1', nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(self.hidden_dim))\n",
    "        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))\n",
    "        self.domain_classifier.add_module('d_fc2', nn.Linear(hidden_dim, 2))\n",
    "        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.FC_embed = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.to_MMD = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    def forward(self, input, lens, alpha, is_teacher):\n",
    "        lens = lens.to('cpu')\n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "        GRU_embeded_input = self.GRUs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "        for i in range(feature_dim-1):\n",
    "            embeded_input = self.GRUs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "            GRU_embeded_input = torch.cat((GRU_embeded_input, embeded_input), 1)\n",
    "        # print(f\"GRU_embeded_input.shape is {GRU_embeded_input.shape}\")\n",
    "\n",
    "\n",
    "        if is_teacher: # 来自源数据集\n",
    "            common_input = GRU_embeded_input[:, 0, :]\n",
    "            for i in range(1, feature_dim):\n",
    "                common_input = common_input + GRU_embeded_input[:, i, :]  \n",
    "            # print(f\"common_input1.shape is {common_input.shape}\")\n",
    "            common_input = torch.squeeze(common_input, 1) # batch * hidden\n",
    "            reverse_input = ReverseLayerF.apply(common_input, alpha)\n",
    "            # print(f\"common_input2.shape is {common_input.shape}\")\n",
    "            domain_output = self.domain_classifier(reverse_input)\n",
    "\n",
    "            posi_input = self.dropout(GRU_embeded_input) # batch_size * d_input + d_input_diff * hidden_dim\n",
    "            \n",
    "            contexts = self.Linear(posi_input).squeeze()# b i\n",
    "            output = self.output(self.dropout(contexts))# b 1\n",
    "            output = self.sigmoid(output)\n",
    "            return output, domain_output, contexts\n",
    "        else: # 来自目标数据集，主要是为了混淆domain classifier\n",
    "            common_input = GRU_embeded_input[:, 0, :]\n",
    "            for i in range(1, feature_dim):\n",
    "                common_input = common_input + GRU_embeded_input[:, i, :]  \n",
    "            common_input = torch.squeeze(common_input, 1) # batch * hidden\n",
    "            reverse_input = ReverseLayerF.apply(common_input, alpha)\n",
    "            domain_output = self.domain_classifier(reverse_input)\n",
    "            return domain_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "def getSplitData(x, lens, y):\n",
    "    train_num =int( len(x) * 0.8) + 1\n",
    "    dev_num =int( len(x) * 0.1) + 1\n",
    "    test_num = len(x) - train_num - dev_num\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    train_len = []\n",
    "    for idx in range(train_num):\n",
    "        train_x.append(x[idx])\n",
    "        train_y.append(int(y[idx][-1]))\n",
    "        train_len.append(lens[idx])\n",
    "\n",
    "    dev_x = []\n",
    "    dev_y = []\n",
    "    dev_len = []\n",
    "    for idx in range(train_num, train_num + dev_num):\n",
    "        dev_x.append(x[idx])\n",
    "        dev_y.append(int(y[idx][-1]))\n",
    "        dev_len.append(lens[idx])\n",
    "\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    test_len = []\n",
    "\n",
    "    for idx in range(train_num + dev_num, train_num + dev_num + test_num):\n",
    "        test_x.append(x[idx])\n",
    "        test_y.append(int(y[idx][-1]))\n",
    "        test_len.append(lens[idx])\n",
    "    return train_x, train_y, train_len, dev_x, dev_y, dev_len, test_x, test_y, test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"load target data\")\n",
    "if target_dataset == 'PD':\n",
    "    data_path = './data/PD/'\n",
    "    all_x_target = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_time_target = pickle.load(open(data_path + 'y_z.pkl', 'rb'))\n",
    "    all_x_len_target = [len(i) for i in all_x_target]\n",
    "\n",
    "    subset_idx_target = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    other_idx_target = list(range(69))\n",
    "    for i in subset_idx_target:\n",
    "        other_idx_target.remove(i)\n",
    "    for i in range(len(all_x_target)):\n",
    "        cur = np.array(all_x_target[i], dtype=float)\n",
    "        cur_subset = cur[:, subset_idx_target]\n",
    "        cur_other = cur[:, other_idx_target]\n",
    "        all_x_target[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "elif target_dataset == 'TJ':\n",
    "    data_path = './data/Tongji/'\n",
    "    all_x_target = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y_target = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time_target = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len_target = [len(i) for i in all_x_target]\n",
    "\n",
    "    for i in range(len(all_time_target)):\n",
    "        for j in range(len(all_time_target[i])):\n",
    "            all_time_target[i][j] = all_time_target[i][j][-1]\n",
    "            all_y_target[i][j] = all_y_target[i][j][0]\n",
    "\n",
    "    subset_idx_target = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "    other_idx_target = list(range(75))\n",
    "    for i in subset_idx_target:\n",
    "        other_idx_target.remove(i)\n",
    "    for i in range(len(all_x_target)):\n",
    "        cur = np.array(all_x_target[i], dtype=float)\n",
    "        cur_subset = cur[:, subset_idx_target]\n",
    "        cur_other = cur[:, other_idx_target]\n",
    "        all_x_target[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "elif target_dataset == 'HM':\n",
    "    data_path = './data/CDSL/'\n",
    "    all_x_target = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y_target = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time_target = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len_target = [len(i) for i in all_x_target]\n",
    "\n",
    "    for i in range(len(all_time_target)):\n",
    "        for j in range(len(all_time_target[i])):\n",
    "            all_time_target[i][j] = all_time_target[i][j][-1]\n",
    "            all_y_target[i][j] = all_y_target[i][j][0]\n",
    "\n",
    "    subset_idx_target = [5, 6, 4, 2, 3, 48, 79, 76, 87, 25, 30, 31, 18, 43, 58, 66, 40, 57, 23, 92, 50, 54, 91, 60, 39, 81]\n",
    "    other_idx_target= list(range(99))\n",
    "    for i in subset_idx_target:\n",
    "        other_idx_target.remove(i)\n",
    "    for i in range(len(all_x_target)):\n",
    "        cur = np.array(all_x_target[i], dtype=float)\n",
    "        cur_subset = cur[:, subset_idx_target]\n",
    "        cur_other = cur[:, other_idx_target]\n",
    "    #     tar_all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "        all_x_target[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    \n",
    "if target_dataset == 'PD':\n",
    "    all_x_target = all_x_target\n",
    "    all_y_target = all_time_target\n",
    "elif  target_dataset == 'HM' or target_dataset == 'TJ':\n",
    "    examples = []\n",
    "    for idx in range(len(all_x_target)):\n",
    "        examples.append((all_x_target[idx], all_y_target[idx], all_time_target[idx], all_x_len_target[idx]))\n",
    "    examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    all_x_target = [e[0] for e in examples]\n",
    "    all_y_target = [e[1] for e in examples]\n",
    "    all_time_target = [e[2] for e in examples]\n",
    "    all_x_len_target = [e[3] for e in examples]\n",
    "\n",
    "num_source = len(all_x_source)\n",
    "num_target = len(all_x_target)\n",
    "# print(target_dataset,len(all_x_target), len(all_x_target[0]),len(all_x_target[0][0]))\n",
    "all_x_target_confuse = []\n",
    "all_x_len_target_confuse = []\n",
    "all_y_target_confuse = []\n",
    "all_x_source_confuse = []\n",
    "all_x_len_source_confuse = []\n",
    "all_y_source_confuse = []\n",
    "repeat_times = 0\n",
    "\n",
    "if num_source < num_target:\n",
    "    all_x_target_confuse = all_x_target\n",
    "    all_y_target_confuse = all_y_target\n",
    "    all_x_len_target_confuse = all_x_len_target\n",
    "    while repeat_times * num_source < num_target:\n",
    "        all_x_source_confuse = all_x_source_confuse + all_x_source\n",
    "        all_x_len_source_confuse = all_x_len_source_confuse + all_x_len_source\n",
    "        all_y_source_confuse =  all_y_source_confuse + all_y_source\n",
    "        repeat_times = repeat_times + 1\n",
    "    all_x_source_confuse = all_x_source_confuse[:num_target]\n",
    "    all_x_len_source_confuse = all_x_len_source_confuse[:num_target]\n",
    "    all_y_source_confuse = all_y_source_confuse[:num_target]\n",
    "elif num_target < num_source:\n",
    "    all_x_source_confuse = all_x_source\n",
    "    all_x_len_source_confuse = all_x_len_source\n",
    "    all_y_source_confuse = all_y_source\n",
    "    while repeat_times * num_target < num_source:\n",
    "        all_x_target_confuse = all_x_target_confuse + all_x_target\n",
    "        all_x_len_target_confuse = all_x_len_target_confuse + all_x_len_target\n",
    "        all_y_target_confuse = all_y_target_confuse + all_y_target\n",
    "        repeat_times = repeat_times + 1\n",
    "    all_x_target_confuse = all_x_target_confuse[:num_source]\n",
    "    all_x_len_target_confuse = all_x_len_target_confuse[:num_source]\n",
    "    all_y_target_confuse = all_y_target_confuse[:num_source]\n",
    "\n",
    "# print(f\"len(all_x_source_confuse) is {len(all_x_source_confuse)}, len(all_x_target_confuse) is {len(all_x_target_confuse)}\")\n",
    "\n",
    "#todo 划分train、dev、test \n",
    "# all_x_source_confuse = pad_sents(all_x_source_confuse, pad_token_source)\n",
    "# all_x_target_confuse = pad_sents(all_x_target_confuse, pad_token_target)\n",
    "train_x_source_confuse, train_y_source_confuse, train_len_source_confuse, dev_x_source_confuse, dev_y_source_confuse, dev_len_source_confuse, test_x_source_confuse,\\\n",
    "test_y_source_confuse, test_len_source_confuse = getSplitData(all_x_source_confuse, all_x_len_source_confuse, all_y_source_confuse)\n",
    "\n",
    "train_x_target_confuse, train_y_target_confuse, train_len_target_confuse, dev_x_target_confuse, dev_y_target_confuse, dev_len_target_confuse, test_x_target_confuse,\\\n",
    "test_y_target_confuse, test_len_target_confuse = getSplitData(all_x_target_confuse, all_x_len_target_confuse, all_y_target_confuse)\n",
    "\n",
    "# long_x_source = all_x_source\n",
    "# long_y_source = [y[-1] for y in all_y_source]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_source = np.zeros(34)\n",
    "if target_dataset == 'PD':\n",
    "    pad_token_target = np.zeros(69)\n",
    "elif target_dataset == 'TJ':\n",
    "    pad_token_target = np.zeros(75)\n",
    "elif target_dataset == 'HM':\n",
    "    pad_token_target = np.zeros(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 256\n",
    "common_dim = subset_cnt \n",
    "hidden_dim = 64\n",
    "d_model = 64\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1\n",
    "model_student = dann(common_dim = common_dim, hidden_dim = hidden_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, output_dim = output_dim).to(device)\n",
    "optimizer_student = torch.optim.Adam(model_student.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskLoss(nn.Module):\n",
    "    def __init__(self, task_num=2):\n",
    "        super(MultitaskLoss, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.alpha = nn.Parameter(torch.ones((task_num)), requires_grad=True)\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.kl = nn.KLDivLoss(reduce=True, size_average=True)\n",
    "\n",
    "    def forward(self, opt_student, batch_y, emb_student, emb_teacher, tar_source, tar_tar):\n",
    "        BCE_Loss = self.bce(opt_student, batch_y)\n",
    "        emb_Loss = self.kl(emb_student, emb_teacher)\n",
    "        return BCE_Loss * self.alpha[0] + emb_Loss * self.alpha[1]\n",
    "\n",
    "def get_multitask_loss(opt_student, batch_y, emb_student, emb_teacher):\n",
    "    mtl = MultitaskLoss(task_num=3)\n",
    "    return mtl(opt_student, batch_y, emb_student, emb_teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_dataset == 'PD':\n",
    "    data_str = 'pd'\n",
    "elif target_dataset == 'TJ':\n",
    "    data_str = 'covid'\n",
    "elif target_dataset == 'HM':\n",
    "    data_str = 'spain'\n",
    "\n",
    "\n",
    "# if teacher_flag:\n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str\n",
    "# else: \n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher'\n",
    "\n",
    "file_name = './model/pretrained-dann'+ data_str;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_source_iter) is 32269, len(train_target_iter) is 32269, steps is 127\n",
      "Epoch 0 Batch 0: Train Loss = 1.5256\n",
      "Epoch 0 Batch 20: Train Loss = 1.4485\n",
      "Epoch 0 Batch 40: Train Loss = 1.4229\n",
      "Epoch 0 Batch 60: Train Loss = 1.4197\n",
      "Epoch 0 Batch 80: Train Loss = 1.3795\n",
      "Epoch 0 Batch 100: Train Loss = 1.3363\n",
      "Epoch 0 Batch 120: Train Loss = 1.3573\n",
      "------------ Save best model - TOTAL_LOSS: 1.2985 ------------\n",
      "------------ Save best model - TOTAL_LOSS: 1.2973 ------------\n",
      "------------ Save best model - TOTAL_LOSS: 1.2721 ------------\n",
      "------------ Save best model - TOTAL_LOSS: 1.2684 ------------\n",
      "Epoch 1 Batch 0: Train Loss = 1.3570\n",
      "Epoch 1 Batch 20: Train Loss = 1.4905\n",
      "Epoch 1 Batch 40: Train Loss = 1.4807\n",
      "Epoch 1 Batch 60: Train Loss = 1.4751\n",
      "Epoch 1 Batch 80: Train Loss = 1.4631\n",
      "Epoch 1 Batch 100: Train Loss = 1.4609\n",
      "Epoch 1 Batch 120: Train Loss = 1.4808\n",
      "Epoch 2 Batch 0: Train Loss = 1.4451\n",
      "Epoch 2 Batch 20: Train Loss = 1.4634\n",
      "Epoch 2 Batch 40: Train Loss = 1.4584\n",
      "Epoch 2 Batch 60: Train Loss = 1.4863\n",
      "Epoch 2 Batch 80: Train Loss = 1.4485\n",
      "Epoch 2 Batch 100: Train Loss = 1.4450\n",
      "Epoch 2 Batch 120: Train Loss = 1.4461\n",
      "Epoch 3 Batch 0: Train Loss = 1.4697\n",
      "Epoch 3 Batch 20: Train Loss = 1.4693\n",
      "Epoch 3 Batch 40: Train Loss = 1.4735\n",
      "Epoch 3 Batch 60: Train Loss = 1.4821\n",
      "Epoch 3 Batch 80: Train Loss = 1.4383\n",
      "Epoch 3 Batch 100: Train Loss = 1.4374\n",
      "Epoch 3 Batch 120: Train Loss = 1.4563\n",
      "Epoch 4 Batch 0: Train Loss = 1.4704\n",
      "Epoch 4 Batch 20: Train Loss = 1.4621\n",
      "Epoch 4 Batch 40: Train Loss = 1.4687\n",
      "Epoch 4 Batch 60: Train Loss = 1.4777\n",
      "Epoch 4 Batch 80: Train Loss = 1.4533\n",
      "Epoch 4 Batch 100: Train Loss = 1.4463\n",
      "Epoch 4 Batch 120: Train Loss = 1.4556\n",
      "Epoch 5 Batch 0: Train Loss = 1.4609\n",
      "Epoch 5 Batch 20: Train Loss = 1.4637\n",
      "Epoch 5 Batch 40: Train Loss = 1.4721\n",
      "Epoch 5 Batch 60: Train Loss = 1.4806\n",
      "Epoch 5 Batch 80: Train Loss = 1.4484\n",
      "Epoch 5 Batch 100: Train Loss = 1.4425\n",
      "Epoch 5 Batch 120: Train Loss = 1.4481\n",
      "Epoch 6 Batch 0: Train Loss = 1.4585\n",
      "Epoch 6 Batch 20: Train Loss = 1.4600\n",
      "Epoch 6 Batch 40: Train Loss = 1.4655\n",
      "Epoch 6 Batch 60: Train Loss = 1.4803\n",
      "Epoch 6 Batch 80: Train Loss = 1.4521\n",
      "Epoch 6 Batch 100: Train Loss = 1.4431\n",
      "Epoch 6 Batch 120: Train Loss = 1.4547\n",
      "Epoch 7 Batch 0: Train Loss = 1.4645\n",
      "Epoch 7 Batch 20: Train Loss = 1.4620\n",
      "Epoch 7 Batch 40: Train Loss = 1.4659\n",
      "Epoch 7 Batch 60: Train Loss = 1.4769\n",
      "Epoch 7 Batch 80: Train Loss = 1.4474\n",
      "Epoch 7 Batch 100: Train Loss = 1.4410\n",
      "Epoch 7 Batch 120: Train Loss = 1.4531\n",
      "Epoch 8 Batch 0: Train Loss = 1.4602\n",
      "Epoch 8 Batch 20: Train Loss = 1.4612\n",
      "Epoch 8 Batch 40: Train Loss = 1.4666\n",
      "Epoch 8 Batch 60: Train Loss = 1.4738\n",
      "Epoch 8 Batch 80: Train Loss = 1.4448\n",
      "Epoch 8 Batch 100: Train Loss = 1.4398\n",
      "Epoch 8 Batch 120: Train Loss = 1.4527\n",
      "Epoch 9 Batch 0: Train Loss = 1.4590\n",
      "Epoch 9 Batch 20: Train Loss = 1.4622\n",
      "Epoch 9 Batch 40: Train Loss = 1.4664\n",
      "Epoch 9 Batch 60: Train Loss = 1.4753\n",
      "Epoch 9 Batch 80: Train Loss = 1.4459\n",
      "Epoch 9 Batch 100: Train Loss = 1.4406\n",
      "Epoch 9 Batch 120: Train Loss = 1.4512\n",
      "Epoch 10 Batch 0: Train Loss = 1.4606\n",
      "Epoch 10 Batch 20: Train Loss = 1.4637\n",
      "Epoch 10 Batch 40: Train Loss = 1.4676\n",
      "Epoch 10 Batch 60: Train Loss = 1.4749\n",
      "Epoch 10 Batch 80: Train Loss = 1.4463\n",
      "Epoch 10 Batch 100: Train Loss = 1.4411\n",
      "Epoch 10 Batch 120: Train Loss = 1.4530\n",
      "Epoch 11 Batch 0: Train Loss = 1.4577\n",
      "Epoch 11 Batch 20: Train Loss = 1.4635\n",
      "Epoch 11 Batch 40: Train Loss = 1.4640\n",
      "Epoch 11 Batch 60: Train Loss = 1.4736\n",
      "Epoch 11 Batch 80: Train Loss = 1.4457\n",
      "Epoch 11 Batch 100: Train Loss = 1.4391\n",
      "Epoch 11 Batch 120: Train Loss = 1.4533\n",
      "Epoch 12 Batch 0: Train Loss = 1.4579\n",
      "Epoch 12 Batch 20: Train Loss = 1.4636\n",
      "Epoch 12 Batch 40: Train Loss = 1.4643\n",
      "Epoch 12 Batch 60: Train Loss = 1.4793\n",
      "Epoch 12 Batch 80: Train Loss = 1.4477\n",
      "Epoch 12 Batch 100: Train Loss = 1.4400\n",
      "Epoch 12 Batch 120: Train Loss = 1.4519\n",
      "Epoch 13 Batch 0: Train Loss = 1.4594\n",
      "Epoch 13 Batch 20: Train Loss = 1.4610\n",
      "Epoch 13 Batch 40: Train Loss = 1.4678\n",
      "Epoch 13 Batch 60: Train Loss = 1.4778\n",
      "Epoch 13 Batch 80: Train Loss = 1.4495\n",
      "Epoch 13 Batch 100: Train Loss = 1.4370\n",
      "Epoch 13 Batch 120: Train Loss = 1.4553\n",
      "Epoch 14 Batch 0: Train Loss = 1.4599\n",
      "Epoch 14 Batch 20: Train Loss = 1.4650\n",
      "Epoch 14 Batch 40: Train Loss = 1.4687\n",
      "Epoch 14 Batch 60: Train Loss = 1.4769\n",
      "Epoch 14 Batch 80: Train Loss = 1.4458\n",
      "Epoch 14 Batch 100: Train Loss = 1.4399\n",
      "Epoch 14 Batch 120: Train Loss = 1.4528\n",
      "Epoch 15 Batch 0: Train Loss = 1.4597\n",
      "Epoch 15 Batch 20: Train Loss = 1.4623\n",
      "Epoch 15 Batch 40: Train Loss = 1.4657\n",
      "Epoch 15 Batch 60: Train Loss = 1.4755\n",
      "Epoch 15 Batch 80: Train Loss = 1.4465\n",
      "Epoch 15 Batch 100: Train Loss = 1.4391\n",
      "Epoch 15 Batch 120: Train Loss = 1.4540\n",
      "Epoch 16 Batch 0: Train Loss = 1.4605\n",
      "Epoch 16 Batch 20: Train Loss = 1.4617\n",
      "Epoch 16 Batch 40: Train Loss = 1.4627\n",
      "Epoch 16 Batch 60: Train Loss = 1.4731\n",
      "Epoch 16 Batch 80: Train Loss = 1.4459\n",
      "Epoch 16 Batch 100: Train Loss = 1.4398\n",
      "Epoch 16 Batch 120: Train Loss = 1.4528\n",
      "Epoch 17 Batch 0: Train Loss = 1.4595\n",
      "Epoch 17 Batch 20: Train Loss = 1.4619\n",
      "Epoch 17 Batch 40: Train Loss = 1.4666\n",
      "Epoch 17 Batch 60: Train Loss = 1.4754\n",
      "Epoch 17 Batch 80: Train Loss = 1.4443\n",
      "Epoch 17 Batch 100: Train Loss = 1.4387\n",
      "Epoch 17 Batch 120: Train Loss = 1.4518\n",
      "Epoch 18 Batch 0: Train Loss = 1.4580\n",
      "Epoch 18 Batch 20: Train Loss = 1.4625\n",
      "Epoch 18 Batch 40: Train Loss = 1.4658\n",
      "Epoch 18 Batch 60: Train Loss = 1.4744\n",
      "Epoch 18 Batch 80: Train Loss = 1.4459\n",
      "Epoch 18 Batch 100: Train Loss = 1.4362\n",
      "Epoch 18 Batch 120: Train Loss = 1.4516\n",
      "Epoch 19 Batch 0: Train Loss = 1.4556\n",
      "Epoch 19 Batch 20: Train Loss = 1.4629\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55473/501086163.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_predict\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0merr_domain1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0merr_domain2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_student\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0moptimizer_student\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training dann model\n",
    "# If you don't want to train Student Model:\n",
    "# - The pretrained student model is in direcrtory './model/', and can be directly loaded, \n",
    "# - Simply skip this cell and load the model to validate on Dev Dataset.\n",
    "\n",
    "logger.info('Training Student')\n",
    "teacher_flag = False\n",
    "epochs = 30\n",
    "total_train_loss = []\n",
    "total_valid_loss = []\n",
    "global_best = 0\n",
    "auroc = []\n",
    "auprc = []\n",
    "minpse = []\n",
    "history = []\n",
    "# begin_time = time.time()\n",
    "best_auroc = 0\n",
    "best_auprc = 0\n",
    "best_minpse = 0\n",
    "best_total_loss = 0x3f3f3f3f\n",
    "loss_domain = torch.nn.NLLLoss()\n",
    "loss_predict = torch.nn.MSELoss()\n",
    "loss_embed = nn.KLDivLoss(reduce=True, size_average=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'len(train_source_iter) is {len(train_x_source_confuse)}, len(train_target_iter) is {len(train_x_target_confuse)}, steps is {len(train_x_source_confuse) // batch_size + 1}')\n",
    "\n",
    "\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    train_source_iter = batch_iter(train_x_source_confuse, train_y_source_confuse, train_len_source_confuse, batch_size=batch_size)\n",
    "    dev_source_iter = batch_iter(dev_x_source_confuse, dev_y_source_confuse, dev_len_source_confuse, batch_size=batch_size)\n",
    "    test_source_iter = batch_iter(test_x_source_confuse, test_y_source_confuse, test_len_source_confuse, batch_size=batch_size)\n",
    "    train_target_iter = batch_iter(train_x_target_confuse, train_y_target_confuse, train_len_target_confuse, batch_size=batch_size)\n",
    "    dev_target_iter = batch_iter(dev_x_target_confuse, dev_y_target_confuse, dev_len_target_confuse, batch_size=batch_size)\n",
    "    test_target_iter = batch_iter(test_x_target_confuse, test_y_target_confuse, test_len_target_confuse, batch_size=batch_size)\n",
    "    epoch_loss = []\n",
    "    counter_batch = 0\n",
    "    model_student.train()  \n",
    "    steps = len(train_x_source_confuse) // batch_size + 1 if len(train_x_source_confuse) % batch_size != 0 else len(train_x_source_confuse) // batch_size\n",
    "    for step in range(steps):\n",
    "        # -----source_domain--------\n",
    "        batch_x, batch_y, batch_lens= next(train_source_iter)\n",
    "        p = float(step + each_epoch * steps) / epochs / steps\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        optimizer_student.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token_source), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        # batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "        # opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "        domain_label = torch.zeros(min(batch_size, batch_x.shape[0])).long().to(device)\n",
    "        opt_student, opt_domain, emb_student = model_student(batch_x[:,:,:subset_cnt], batch_lens, alpha, True)\n",
    "        emb_student = F.log_softmax(emb_student, dim=1)\n",
    "        err_predict = loss_predict(opt_student, batch_y)\n",
    "        err_domain1 = loss_domain(opt_domain, domain_label)\n",
    "            # loss = get_multitask_loss(opt_student, batch_y.unsqueeze(-1), emb_student, emb_teacher)\n",
    "\n",
    "        # -----target_domain--------\n",
    "        batch_x, batch_y, batch_lens = next(train_target_iter)\n",
    "        p = float(step + each_epoch * len(train_x_source)) / epochs / len(train_x_len_source)\n",
    "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        optimizer_student.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token_target), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        # batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "        # opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "        domain_label = torch.ones(min(batch_size, batch_x.shape[0])).long().to(device)\n",
    "        opt_domain = model_student(batch_x[:,:,:subset_cnt], batch_lens, alpha, False)\n",
    "        err_domain2 = loss_domain(opt_domain, domain_label)\n",
    "\n",
    "        # -----common--------\n",
    "        loss = err_predict + err_domain1 + err_domain2\n",
    "        epoch_loss.append(loss.cpu().detach().numpy())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_student.parameters(), 20)\n",
    "        optimizer_student.step()\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "            logger.info('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    total_train_loss.append(epoch_loss)\n",
    "\n",
    "\n",
    "    # dev_source_dataset = MyDataset(dev_x_source_confuse, dev_len_source_confuse, dev_y_source_confuse)\n",
    "    # dev_target_dataset = MyDataset(dev_x_target_confuse, dev_len_target_confuse, dev_y_target_confuse)\n",
    "    # dev_source_dataloader = DataLoader(dev_source_dataset, batch_size= batch_size)\n",
    "    # dev_target_dataloader = DataLoader(dev_target_dataset, batch_size=batch_size)\n",
    "    #Validation\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        steps = len(dev_x_source_confuse) // batch_size + 1 if len(dev_x_source_confuse) % batch_size != 0 else len(dev_x_source_confuse) // batch_size\n",
    "        for step in range(steps):\n",
    "            # -----source_domain--------\n",
    "            batch_x, batch_y, batch_lens= next(dev_source_iter)\n",
    "            p = float(step + each_epoch * steps) / epochs / steps\n",
    "            alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "            optimizer_student.zero_grad()\n",
    "            batch_x = torch.tensor(pad_sents(batch_x, pad_token_source), dtype=torch.float32).to(device)\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "            # batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "            # opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "            domain_label = torch.zeros(min(batch_size, batch_x.shape[0])).long().to(device)\n",
    "            opt_student, opt_domain, emb_student = model_student(batch_x[:,:,:subset_cnt], batch_lens, alpha, True)\n",
    "            # emb_teacher = torch.tensor(dev_teacher_emb[step], dtype=torch.float32).to(device)\n",
    "            emb_student = F.log_softmax(emb_student, dim=1)\n",
    "            # err_emb = loss_embed(emb_student, emb_teacher) #todo 是否考虑它\n",
    "            err_predict = loss_predict(opt_student, batch_y)\n",
    "            err_domain1 = loss_domain(opt_domain, domain_label)\n",
    "                # loss = get_multitask_loss(opt_student, batch_y.unsqueeze(-1), emb_student, emb_teacher)\n",
    "\n",
    "            # -----target_domain--------\n",
    "            batch_x, batch_y, batch_lens = next(dev_target_iter)\n",
    "            optimizer_student.zero_grad()\n",
    "            batch_x = torch.tensor(pad_sents(batch_x, pad_token_target), dtype=torch.float32).to(device)\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "            # batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "            # opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "            domain_label = torch.ones(min(batch_size, batch_x.shape[0])).long().to(device)\n",
    "            opt_domain = model_student(batch_x[:,:,:subset_cnt], batch_lens, alpha, False)\n",
    "            err_domain2 = loss_domain(opt_domain, domain_label)\n",
    "\n",
    "            # -----common--------\n",
    "            loss = err_domain1 + err_domain2\n",
    "            if loss < best_total_loss:\n",
    "                best_total_loss = loss\n",
    "                state = {\n",
    "                    'net': model_student.state_dict(),\n",
    "                    'optimizer': optimizer_student.state_dict(),\n",
    "                    'epoch': each_epoch\n",
    "                }\n",
    "                torch.save(state, file_name)\n",
    "                print('------------ Save best model - TOTAL_LOSS: %.4f ------------'%best_total_loss)\n",
    "                logger.info('------------ Save best model - TOTAL_LOSS: %.4f ------------'%best_total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last saved model is in epoch 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dann(\n",
       "  (GRUs): ModuleList(\n",
       "    (0): GRU(1, 64, batch_first=True)\n",
       "    (1): GRU(1, 64, batch_first=True)\n",
       "    (2): GRU(1, 64, batch_first=True)\n",
       "    (3): GRU(1, 64, batch_first=True)\n",
       "    (4): GRU(1, 64, batch_first=True)\n",
       "    (5): GRU(1, 64, batch_first=True)\n",
       "    (6): GRU(1, 64, batch_first=True)\n",
       "    (7): GRU(1, 64, batch_first=True)\n",
       "    (8): GRU(1, 64, batch_first=True)\n",
       "    (9): GRU(1, 64, batch_first=True)\n",
       "    (10): GRU(1, 64, batch_first=True)\n",
       "    (11): GRU(1, 64, batch_first=True)\n",
       "    (12): GRU(1, 64, batch_first=True)\n",
       "    (13): GRU(1, 64, batch_first=True)\n",
       "    (14): GRU(1, 64, batch_first=True)\n",
       "    (15): GRU(1, 64, batch_first=True)\n",
       "  )\n",
       "  (demo_proj_main): Linear(in_features=12, out_features=64, bias=True)\n",
       "  (demo_proj): Linear(in_features=12, out_features=64, bias=True)\n",
       "  (Linear): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (output): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (domain_classifier): Sequential(\n",
       "    (d_fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (d_bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (d_relu1): ReLU(inplace=True)\n",
       "    (d_fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (d_softmax): LogSoftmax(dim=1)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (FC_embed): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (softmax): Softmax(dim=None)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       "  (to_MMD): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:7\" if torch.cuda.is_available() == True else 'cpu') )\n",
    "save_epoch = checkpoint['epoch']\n",
    "print(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "logger.info(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "model_student.load_state_dict(checkpoint['net'])\n",
    "optimizer_student.load_state_dict(checkpoint['optimizer'])\n",
    "model_student.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Test Loss = -2.5771\n",
      "\n",
      "==>Predicting on test\n",
      "Test Loss = -2.5198\n"
     ]
    }
   ],
   "source": [
    "#anchor\n",
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    model_student.eval()\n",
    "    test_target_iter = batch_iter(test_x_target_confuse, test_y_target_confuse, test_len_target_confuse, batch_size=batch_size, shuffle=True)\n",
    "    steps = len(test_x_target_confuse) // batch_size + 1 if len(test_x_target_confuse) % batch_size != 0 else len(test_x_target_confuse) // batch_size\n",
    "    for step in range(steps):\n",
    "        # -----target_domain--------\n",
    "        batch_x, batch_y, batch_lens= next(test_target_iter) \n",
    "        optimizer_student.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token_target), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "        opt, _, _  = model_student(batch_x[:,:,:subset_cnt], batch_lens, 1, True)\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "\n",
    "        loss = model_loss\n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "            logger.info('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "        y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "        y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "logger.info(\"\\n==>Predicting on test\")\n",
    "logger.info('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "# test_res = metrics.print_metrics_binary(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_dataset == 'PD':\n",
    "    source_common_idx = [31, 29, 28, 33, 25, 18, 7, 21, 16, 15, 19, 17, 24, 3, 5, 0]\n",
    "    target_common_idx = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    source_data_path = './data/Challenge/'\n",
    "    source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "    target_data_path = './data/PD/'\n",
    "    target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "elif target_dataset == 'TJ':\n",
    "    source_common_idx = [27, 29, 18, 16, 26, 33, 28, 31, 32, 15, 11, 25, 21, 20, 9, 17, 30, 19]\n",
    "    target_common_idx = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "    source_data_path = './data/Challenge/'\n",
    "    source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "    target_data_path = './data/Tongji/'\n",
    "    target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "\n",
    "elif target_dataset == 'HM':\n",
    "    source_common_idx = [0, 1, 2, 3, 5, 9, 11, 12, 13, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
    "    target_common_idx = [5, 6, 4, 2, 3, 48, 79, 76, 87, 25, 30, 31, 18, 43, 58, 66, 40, 57, 23, 92, 50, 54, 91, 60, 39, 81]\n",
    "    source_data_path = './data/Challenge/'\n",
    "    source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "    target_data_path = './data/CDSL/'\n",
    "    target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "\n",
    "assert(len(source_common_idx) == len(target_common_idx))\n",
    "common_len = len(source_common_idx)\n",
    "source_x_diff = []\n",
    "target_x_diff = []\n",
    "\n",
    "source_total_len = 34\n",
    "source_other_idx = list(range(source_total_len))\n",
    "for i in source_common_idx:\n",
    "    source_other_idx.remove(i)\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    target_total_len = 69\n",
    "    target_other_idx = list(range(target_total_len))\n",
    "    for i in target_common_idx:\n",
    "        target_other_idx.remove(i)\n",
    "elif target_dataset == 'TJ':\n",
    "    target_other_idx = list(range(75))\n",
    "    target_total_len = 75\n",
    "    for i in target_common_idx:\n",
    "        target_other_idx.remove(i)\n",
    "elif target_dataset == 'HM':\n",
    "    target_other_idx = list(range(99))\n",
    "    target_total_len = 99\n",
    "    for i in target_common_idx:\n",
    "        target_other_idx.remove(i)\n",
    "\n",
    "for i in range(len(source_x)):\n",
    "    cur = np.array(source_x[i], dtype=float)\n",
    "    cur_subset = cur[:, source_common_idx]\n",
    "    cur_other = cur[:, source_other_idx]\n",
    "    source_x_diff.append(cur_other.tolist())\n",
    "    source_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "\n",
    "for i in range(len(target_x)):\n",
    "    cur = np.array(target_x[i], dtype=float)\n",
    "    cur_subset = cur[:, target_common_idx]\n",
    "    cur_other = cur[:, target_other_idx]\n",
    "    target_x_diff.append(cur_other.tolist())\n",
    "    target_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "\n",
    "\n",
    "source_max = 0\n",
    "for i in range(len(source_x_diff)):\n",
    "    if source_max < len(source_x_diff[i]):\n",
    "        source_max = len(source_x_diff[i])\n",
    "\n",
    "source_x_diff_longest = max(list(len(_) for _ in source_x_diff))\n",
    "source_x_longest = max(list(len(_) for _ in source_x))\n",
    "source_batch = len(source_x_diff)\n",
    "source_diff_features = source_total_len - common_len\n",
    "source_x_diff_ex = torch.zeros((source_batch, source_x_diff_longest, source_diff_features))\n",
    "source_x_ex = torch.zeros((source_batch, source_x_longest, source_total_len))\n",
    "\n",
    "for i in range(len(source_x_diff)):\n",
    "    for j in range(source_x_diff_longest):\n",
    "        cur_len = len(source_x_diff[i])\n",
    "        if j < cur_len:\n",
    "            source_x_diff_ex[i,j,:] = torch.Tensor(source_x_diff[i])[j,:]\n",
    "        else:\n",
    "            source_x_diff_ex[i,j,:] = torch.Tensor(source_x_diff[i])[cur_len - 1,:]\n",
    "\n",
    "for i in range(len(source_x)):\n",
    "    for j in range(source_x_longest):\n",
    "        cur_len = len(source_x[i])\n",
    "        if j < cur_len:\n",
    "            source_x_ex[i,j,:] = torch.Tensor(source_x[i])[j,:]\n",
    "        else:\n",
    "            source_x_ex[i,j,:] = torch.Tensor(source_x[i])[cur_len - 1,:]\n",
    "\n",
    "target_x_diff_longest = max(list(len(_) for _ in target_x_diff))\n",
    "target_batch = len(target_x_diff)\n",
    "target_features = target_total_len - common_len\n",
    "target_x_diff_ex = torch.zeros((target_batch, target_x_diff_longest, target_features))\n",
    "\n",
    "for i in range(len(target_x_diff)):\n",
    "    for j in range(target_x_diff_longest):\n",
    "        cur_len = len(target_x_diff[i])\n",
    "        if j < cur_len:\n",
    "            target_x_diff_ex[i,j,:] = torch.Tensor(target_x_diff[i])[j,:]\n",
    "        else:\n",
    "            target_x_diff_ex[i,j,:] = torch.Tensor(target_x_diff[i])[cur_len - 1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8285543981909917, 2.770245567717861, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8374745525934485, 2.8093811444689463, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5338330207864587, 0.431156978499758, 0.8964660630930379, -1.342421748214616, -0.6946051308187712, -0.2120300841305121, 0.8952797203562032, 0.0387041303378994, -0.8612693268611251, 0.0726334784031089, -0.5051102137388988, -0.2865727201409924, 0.3988826585206329, -1.3794377690564883, -0.8237089728907875, -0.9221119476695248, -1.6036614531597553, -0.2379932071009605, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -1.1501193786706505, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, 1.0826051686869729, -0.609009148503521, 0.9684393533852332, -0.3880554131475662, 0.8439788318452395, 2.837917502516613, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 0.278477698357045, 1.2505677424119097, -0.6305581644917595, -0.9062745442328174, -0.7536814885080627, -0.4374103947888615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, -0.7710128163592748, -0.6738408269117502, -2.438483340619628, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8608899578998963, 2.912112033440545, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3396023676711391, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8763143915541441, 2.979783968239297, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5125273793649132, 0.601421850901989, 0.5034187451388209, -1.455529079732558, -0.5254345623915906, -0.4163803392884947, 0.7885904984786939, 0.2816638966057675, -0.5890172833864098, 0.5902661209826593, -0.3430637446868887, -0.7482466726318323, -0.1864257310498265, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.2249939001501796, -0.0341172340589738, 0.3466058957088718, 0.3482841380580905, -0.2890718868318484, 0.1923438589112976, -0.2031244129114749, -1.100153472115602, -0.7598199909551685, -0.6868858002659636, 0.3602180776283341, -0.700061185363224, -0.501359972189453, -0.2332733777972843, -1.36588558517574, 0.3571411263970316, -0.7447833078367583, 0.8765002281041955, 2.9805992927549454, 0.0776143028335215, -0.2769313825285214, 0.0755136678180621, -0.1474826847594624, -0.3999358135056357, -0.0169688050451549, -0.0937841116273902, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, -0.0122781725754626, -0.3292808750219816, -0.2769313825285214, -0.107625012968948, -0.1474826847594624, -0.5354516373428909, -0.0002229391773086, -0.0862088042532133, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, 0.0766269712424727, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.3951350096437179, -0.5239726809327129, -0.8378690925056301, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.8988006141103361, 3.0784382346326584, 0.0776143028335215, -0.3487779453829513, 0.070457637721598, -0.1474826847594624, -0.3999358135056357, 0.0045338857876385, -0.0729488956980065, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.1275947723114954, -0.4255010572823463, -0.3487779453829513, -0.1777070069371955, -0.1474826847594624, -0.5354516373428909, 0.029608437271828, -0.0566564538358881, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.0232601678654147, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.9106941533136118, 3.1306190036341057, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9108799898636633, 3.1314343281497536, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 1.0542620417323658, -0.6738408269117502, -0.0796833960305785, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9222160194167848, 3.1811691236042576, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.5107528363891513, 1.4769583166408096, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-1.038066534429697, 0.260892106097527, 0.1645848503507034, -1.455529079732558, -1.6391408045371951, 0.196670426185453, 1.2420196914581063, 0.4706326037029981, -0.6616178283130005, -0.5004598044528213, -0.4609157221792596, 0.021209914852902, -1.1769476210921423, 0.0909225332951111, -1.348247483817418, -1.2590833683251033, -0.9252694826312332, 0.781386658108973, -0.0798138719333467, -0.2032969502372001, -0.5728175028694968, 0.103242147293012, -0.2031244129114749, -0.6504603131201653, -0.5868915885967425, -0.6218024077192129, -0.0284729839577739, -0.3925684926321655, -0.4454382040900255, -0.2791761177909213, -1.5149673075505703, 0.3413397023846657, -0.8352815289623093, 0.9329945393197532, 3.228457945511819, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8995798651896526, 0.4879119359671683, 0.1510314945591795, -1.4131138304133295, -0.6523124887119757, 0.196670426185453, 0.148455167213642, 0.4706326037029981, -0.752368509471239, -0.9441449266638648, -0.9507380036319262, 0.1751012323498492, -1.0418764542681906, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -1.3629417216818924, 0.1697587389830128, -0.364093717028159, -0.2645837378255657, -0.2890718868318484, 0.0364158635792983, -0.2031244129114749, -1.8246591171638051, -0.4061028043129334, -0.5350245509902118, 0.0492652283594477, 0.0357249008146661, -0.4929416415078188, -0.5851943844151671, -1.170932563608653, 0.051337096981241, -0.8763418268751098, 0.9447022419729768, 3.279823389997619, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.962914223877992, 3.359725192531085, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.973321070680858, 3.405383365407351, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9735069072309092, 3.4061986899229986, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9980373318376644, 3.5138215259884835, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0340896225475933, 3.671994482024121, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.2639920224449301, -0.6738408269117502, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0396647190491284, 3.696454217493548, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7418059691584676, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, -0.6693833821778409, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.057505027854041, 3.774725370995719, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5792130238763558, -0.9062745442328174, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, 0.0888023142972107, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0760886828591592, 3.856257822560481, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "69\n",
      "325\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Transfer Target Dataset & Model\")\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    data_path = './data/PD/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y_z.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    tar_subset_idx = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    tar_other_idx = list(range(69))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "elif target_dataset == 'TJ':\n",
    "    data_path = './data/Tongji/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    for i in range(len(all_time)):\n",
    "        for j in range(len(all_time[i])):\n",
    "            all_time[i][j] = all_time[i][j][-1]\n",
    "            all_y[i][j] = all_y[i][j][0]\n",
    "\n",
    "    tar_subset_idx = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "    tar_other_idx = list(range(75))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "elif target_dataset == 'HM':\n",
    "    data_path = './data/CDSL/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    for i in range(len(all_time)):\n",
    "        for j in range(len(all_time[i])):\n",
    "            all_time[i][j] = all_time[i][j][-1]\n",
    "            all_y[i][j] = all_y[i][j][0]\n",
    "\n",
    "    tar_subset_idx = [5, 6, 4, 2, 3, 48, 79, 76, 87, 25, 30, 31, 18, 43, 58, 66, 40, 57, 23, 92, 50, 54, 91, 60, 39, 81]\n",
    "    tar_other_idx = list(range(99))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    \n",
    "print(all_x[0])\n",
    "print(len(all_x[0][0]))\n",
    "print(len(all_x))\n",
    "logger.info(all_x[0])\n",
    "logger.info(len(all_x[0][0]))\n",
    "logger.info(len(all_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_x = all_x\n",
    "# long_y = all_y\n",
    "# long_y_kfold = [each[-1] for each in all_y]\n",
    "long_time = all_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_n2n_data(x, y, x_len):\n",
    "    length = len(x)\n",
    "    assert length == len(y)\n",
    "    assert length == len(x_len)\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    new_x_len = []\n",
    "    for i in range(length):\n",
    "        for j in range(len(x[i])):\n",
    "            new_x.append(x[i][:j+1])\n",
    "            new_y.append(y[i][j])\n",
    "            new_x_len.append(j+1)\n",
    "    return new_x, new_y, new_x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class target_model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(target_model, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.GRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        \n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.output = nn.Linear(self.input_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.FC_embed = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, self.output_dim)\n",
    "        )\n",
    "        self.MLP_outcome = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, self.output_dim)\n",
    "        )\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, input, lens):\n",
    "        lens = lens.to('cpu')\n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "        # print(f'input.shape is {input.shape}, and lens.shape is {lens.shape}')\n",
    "        GRU_embeded_input = self.GRUs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "        for i in range(feature_dim-1):\n",
    "            embeded_input = self.GRUs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "            GRU_embeded_input = torch.cat((GRU_embeded_input, embeded_input), 1)\n",
    "        # print(f\"GRU_embeded_input.shape is {GRU_embeded_input.shape}\")\n",
    "        posi_input = self.dropout(GRU_embeded_input) # batch_size * d_input * hidden_dim\n",
    "\n",
    "        gru_input = GRU_embeded_input[:, 0, :]\n",
    "        for i in range(1, feature_dim):\n",
    "             gru_input =  gru_input + GRU_embeded_input[:, i, :]  \n",
    "        # print(f\"common_input1.shape is {gru_input.shape}\")\n",
    "        gru_input = torch.squeeze(gru_input, 1) # batch * hidden\n",
    "        # print(f\"common_input2.shape is {gru_input.shape}\")\n",
    "\n",
    "        posi_input = self.dropout(gru_input) # batch_size * d_input + d_input_diff * hidden_dim\n",
    "        contexts = self.Linear(posi_input).squeeze()# b i\n",
    "        output = self.MLP(posi_input)\n",
    "        outcome = self.MLP_outcome(posi_input)\n",
    "        outcome = F.sigmoid(outcome)\n",
    "        if self.output_dim != 1:\n",
    "            output = F.softmax(output, dim=1)\n",
    "        return output, outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_gru_dict(pretrain_dict, model_dict):\n",
    "    state_dict = {}\n",
    "    \n",
    "    for k, v in pretrain_dict.items():\n",
    "        model_point_position1 = k.find('.')\n",
    "        model_module_name = k[:model_point_position1]\n",
    "        if \"GRUs\" == model_module_name:\n",
    "            model_point_position2 = k.find('.', model_point_position1+1)\n",
    "            model_module_idx = int(k[model_point_position1 + 1: model_point_position2])\n",
    "            # print(f'model_module_idx is {model_module_idx}')\n",
    "            state_dict[k] = pretrain_dict[k]\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_dataset == 'PD':\n",
    "    input_dim = 69\n",
    "elif target_dataset == 'TJ':\n",
    "    input_dim = 75\n",
    "elif target_dataset == 'HM':\n",
    "    input_dim = 99\n",
    "    \n",
    "cell = 'GRU'\n",
    "hidden_dim = 64\n",
    "d_model = 64\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 256\n",
    "common_dim = subset_cnt \n",
    "hidden_dim = 64\n",
    "d_model = 64\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ckd_batch_iter(x, y, lens, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(x) / batch_size) # 向下取整\n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size] #  fetch out all the induces\n",
    "        \n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx],  lens[idx]))\n",
    "       \n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    \n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "#         batch_name = [e[2] for e in examples]\n",
    "        batch_lens = [e[2] for e in examples]\n",
    "       \n",
    "\n",
    "        yield batch_x, batch_y, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetMultitaskLoss(nn.Module):\n",
    "    def __init__(self, task_num=2):\n",
    "        super(TargetMultitaskLoss, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.alpha = nn.Parameter(torch.ones((task_num)), requires_grad=True)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, opt_student, los, outcome, outcome_y):\n",
    "        MSE_Loss = self.mse(opt_student, los)\n",
    "        BCE_Loss = self.bce(outcome, outcome_y)\n",
    "        return MSE_Loss * self.alpha[0] + BCE_Loss * self.alpha[1]\n",
    "\n",
    "def get_target_multitask_loss(opt_student, los, outcome, outcome_y):\n",
    "    mtl = TargetMultitaskLoss(task_num=2)\n",
    "    return mtl(opt_student, los, outcome, outcome_y)\n",
    "\n",
    "def reverse_los(y, los_info):\n",
    "    return y * los_info[\"los_std\"] + los_info[\"los_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'los_mean': 1055.0307777880782, 'los_std': 799.0879849276147}\n"
     ]
    }
   ],
   "source": [
    "los_info = pickle.load(open(data_path + 'los_info.pkl', 'rb'))\n",
    "print(los_info)\n",
    "logger.info(los_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 0 Batch 0: Train Loss = 1.9969\n",
      "Fold 1, epoch 0: Loss = 1.6106 Valid loss = 1.0472 MSE = 733.3216\n",
      "------------ Save FOLD-BEST model - MSE: 733.3216 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  207  888    0]\n",
      " [   0  598 3281    0]\n",
      " [   0  282 2058    0]\n",
      " [   0  125 1273    0]]\n",
      "Mean absolute deviation (MAD) = 22.697927510372917\n",
      "Mean squared error (MSE) = 733.3216350382578\n",
      "Mean absolute percentage error (MAPE) = 284.01530210323875\n",
      "Cohen kappa score = 0.030986710109085602\n",
      "------------ Save best model - MSE: 733.3216 ------------\n",
      "Fold 1, mse = 733.3216, mad = 22.6979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1 Batch 0: Train Loss = 1.1314\n",
      "------------ Save FOLD-BEST model - MSE: 712.1826 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  787  308    0]\n",
      " [   0 2651 1228    0]\n",
      " [   0 1582  758    0]\n",
      " [   0  851  547    0]]\n",
      "Mean absolute deviation (MAD) = 21.48960611687766\n",
      "Mean squared error (MSE) = 712.1825507962241\n",
      "Mean absolute percentage error (MAPE) = 231.80879105639258\n",
      "Cohen kappa score = 0.02593614348189255\n",
      "------------ Save best model - MSE: 712.1826 ------------\n",
      "Fold 1, mse = 712.1826, mad = 21.4896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 2 Batch 0: Train Loss = 1.0596\n",
      "------------ Save FOLD-BEST model - MSE: 706.8989 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  605  490    0]\n",
      " [   0 2086 1793    0]\n",
      " [   0 1241 1099    0]\n",
      " [   0  659  739    0]]\n",
      "Mean absolute deviation (MAD) = 21.63736677244211\n",
      "Mean squared error (MSE) = 706.8988594170149\n",
      "Mean absolute percentage error (MAPE) = 244.20960219059245\n",
      "Cohen kappa score = 0.020491013948796044\n",
      "------------ Save best model - MSE: 706.8989 ------------\n",
      "Fold 1, mse = 706.8989, mad = 21.6374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 3 Batch 0: Train Loss = 0.9721\n",
      "------------ Save FOLD-BEST model - MSE: 692.8919 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  439  656    0]\n",
      " [   0 1417 2462    0]\n",
      " [   0  742 1598    0]\n",
      " [   0  311 1087    0]]\n",
      "Mean absolute deviation (MAD) = 21.647255417998505\n",
      "Mean squared error (MSE) = 692.8919208635266\n",
      "Mean absolute percentage error (MAPE) = 252.1160745056613\n",
      "Cohen kappa score = 0.055326701702556824\n",
      "------------ Save best model - MSE: 692.8919 ------------\n",
      "Fold 1, mse = 692.8919, mad = 21.6473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 4 Batch 0: Train Loss = 0.9522\n",
      "------------ Save FOLD-BEST model - MSE: 666.9043 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  762  333    0]\n",
      " [   0 2418 1461    0]\n",
      " [   0 1284 1056    0]\n",
      " [   0  538  860    0]]\n",
      "Mean absolute deviation (MAD) = 20.96651137539864\n",
      "Mean squared error (MSE) = 666.904332803897\n",
      "Mean absolute percentage error (MAPE) = 231.47147447225015\n",
      "Cohen kappa score = 0.09594735739229776\n",
      "------------ Save best model - MSE: 666.9043 ------------\n",
      "Fold 1, mse = 666.9043, mad = 20.9665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 5 Batch 0: Train Loss = 1.0202\n",
      "------------ Save FOLD-BEST model - MSE: 651.9625 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   1  719  375    0]\n",
      " [   2 2265 1608    4]\n",
      " [   1 1136 1203    0]\n",
      " [   0  444  953    1]]\n",
      "Mean absolute deviation (MAD) = 20.67771068857013\n",
      "Mean squared error (MSE) = 651.9624722673639\n",
      "Mean absolute percentage error (MAPE) = 225.72679693317593\n",
      "Cohen kappa score = 0.1112495073334413\n",
      "------------ Save best model - MSE: 651.9625 ------------\n",
      "Fold 1, mse = 651.9625, mad = 20.6777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 6 Batch 0: Train Loss = 0.7553\n",
      "Fold 1, mse = 660.8637, mad = 20.4310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 7 Batch 0: Train Loss = 0.7328\n",
      "Fold 1, mse = 654.7805, mad = 20.5451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 8 Batch 0: Train Loss = 0.8768\n",
      "Fold 1, mse = 663.9837, mad = 20.4719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 9 Batch 0: Train Loss = 0.8454\n",
      "Fold 1, mse = 665.2875, mad = 20.6106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 10 Batch 0: Train Loss = 0.6998\n",
      "Fold 1, epoch 10: Loss = 0.7103 Valid loss = 0.9471 MSE = 668.8218\n",
      "Fold 1, mse = 668.8218, mad = 20.7751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 11 Batch 0: Train Loss = 0.7054\n",
      "Fold 1, mse = 687.6098, mad = 21.1811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 12 Batch 0: Train Loss = 0.6115\n",
      "Fold 1, mse = 703.4314, mad = 21.1764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 13 Batch 0: Train Loss = 0.5727\n",
      "Fold 1, mse = 768.6074, mad = 22.0746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 14 Batch 0: Train Loss = 0.4440\n",
      "Fold 1, mse = 834.9606, mad = 23.3840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 15 Batch 0: Train Loss = 0.5606\n",
      "Fold 1, mse = 791.5300, mad = 22.6049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 16 Batch 0: Train Loss = 0.3966\n",
      "Fold 1, mse = 802.1485, mad = 22.7967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 17 Batch 0: Train Loss = 0.4795\n",
      "Fold 1, mse = 805.8884, mad = 22.8537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 18 Batch 0: Train Loss = 0.3678\n",
      "Fold 1, mse = 801.0290, mad = 22.6624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 19 Batch 0: Train Loss = 0.2493\n",
      "Fold 1, mse = 817.8216, mad = 22.9393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 20 Batch 0: Train Loss = 0.3220\n",
      "Fold 1, epoch 20: Loss = 0.3316 Valid loss = 1.1733 MSE = 838.7015\n",
      "Fold 1, mse = 838.7015, mad = 23.1818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 21 Batch 0: Train Loss = 0.3343\n",
      "Fold 1, mse = 857.6031, mad = 23.3794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 22 Batch 0: Train Loss = 0.4058\n",
      "Fold 1, mse = 872.9593, mad = 23.6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 23 Batch 0: Train Loss = 0.3344\n",
      "Fold 1, mse = 861.6811, mad = 23.4344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 24 Batch 0: Train Loss = 0.2982\n",
      "Fold 1, mse = 833.0036, mad = 22.9129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 25 Batch 0: Train Loss = 0.2969\n",
      "Fold 1, mse = 831.0172, mad = 22.9113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 26 Batch 0: Train Loss = 0.2768\n",
      "Fold 1, mse = 855.0684, mad = 23.3043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 27 Batch 0: Train Loss = 0.2418\n",
      "Fold 1, mse = 864.3920, mad = 23.3731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 28 Batch 0: Train Loss = 0.2079\n",
      "Fold 1, mse = 860.4450, mad = 23.3459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 29 Batch 0: Train Loss = 0.2469\n",
      "Fold 1, mse = 873.9788, mad = 23.4116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 0 Batch 0: Train Loss = 2.6445\n",
      "Fold 2, epoch 0: Loss = 1.4270 Valid loss = 1.0392 MSE = 737.2733\n",
      "------------ Save FOLD-BEST model - MSE: 737.2733 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  235  855    0]\n",
      " [   0  792 3073    0]\n",
      " [   0  479 1798    0]\n",
      " [   0  269 1201    0]]\n",
      "Mean absolute deviation (MAD) = 22.655593347961208\n",
      "Mean squared error (MSE) = 737.273325223394\n",
      "Mean absolute percentage error (MAPE) = 271.3064340467659\n",
      "Cohen kappa score = 0.0044846171614100205\n",
      "Fold 2, mse = 737.2733, mad = 22.6556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 1 Batch 0: Train Loss = 0.9878\n",
      "Fold 2, mse = 769.6189, mad = 21.6456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 2 Batch 0: Train Loss = 0.8441\n",
      "Fold 2, mse = 783.3700, mad = 21.6879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 3 Batch 0: Train Loss = 0.7608\n",
      "Fold 2, mse = 779.8354, mad = 21.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 4 Batch 0: Train Loss = 0.8530\n",
      "Fold 2, mse = 764.7247, mad = 21.6379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 5 Batch 0: Train Loss = 0.9305\n",
      "------------ Save FOLD-BEST model - MSE: 734.9472 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0 1032   58    0]\n",
      " [   0 3545  320    0]\n",
      " [   0 2068  209    0]\n",
      " [   0 1259  211    0]]\n",
      "Mean absolute deviation (MAD) = 21.54283596155946\n",
      "Mean squared error (MSE) = 734.9472497156476\n",
      "Mean absolute percentage error (MAPE) = 213.87472664654004\n",
      "Cohen kappa score = 0.023805327859092285\n",
      "Fold 2, mse = 734.9472, mad = 21.5428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 6 Batch 0: Train Loss = 0.6909\n",
      "------------ Save FOLD-BEST model - MSE: 719.5872 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  986  104    0]\n",
      " [   0 3342  523    0]\n",
      " [   0 1936  341    0]\n",
      " [   0 1068  402    0]]\n",
      "Mean absolute deviation (MAD) = 21.463415503598178\n",
      "Mean squared error (MSE) = 719.5871740742596\n",
      "Mean absolute percentage error (MAPE) = 217.47763513739525\n",
      "Cohen kappa score = 0.047125290293473454\n",
      "Fold 2, mse = 719.5872, mad = 21.4634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 7 Batch 0: Train Loss = 0.8138\n",
      "------------ Save FOLD-BEST model - MSE: 717.7732 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0 1014   76    0]\n",
      " [   0 3456  409    0]\n",
      " [   0 1969  308    0]\n",
      " [   0 1099  371    0]]\n",
      "Mean absolute deviation (MAD) = 21.355305038996384\n",
      "Mean squared error (MSE) = 717.7732347518561\n",
      "Mean absolute percentage error (MAPE) = 213.83238085260788\n",
      "Cohen kappa score = 0.05497105274780112\n",
      "Fold 2, mse = 717.7732, mad = 21.3553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 8 Batch 0: Train Loss = 0.7135\n",
      "------------ Save FOLD-BEST model - MSE: 702.1456 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  985  105    0]\n",
      " [   0 3336  529    0]\n",
      " [   0 1865  412    0]\n",
      " [   0  937  533    0]]\n",
      "Mean absolute deviation (MAD) = 21.205177074028335\n",
      "Mean squared error (MSE) = 702.1455852532965\n",
      "Mean absolute percentage error (MAPE) = 214.72389409953342\n",
      "Cohen kappa score = 0.08124101331222766\n",
      "Fold 2, mse = 702.1456, mad = 21.2052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 9 Batch 0: Train Loss = 0.7819\n",
      "------------ Save FOLD-BEST model - MSE: 690.7182 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  968  122    0]\n",
      " [   0 3211  654    0]\n",
      " [   0 1763  514    0]\n",
      " [   0  867  603    0]]\n",
      "Mean absolute deviation (MAD) = 21.121508712752266\n",
      "Mean squared error (MSE) = 690.7182234546416\n",
      "Mean absolute percentage error (MAPE) = 216.31985157234368\n",
      "Cohen kappa score = 0.0919031360677085\n",
      "Fold 2, mse = 690.7182, mad = 21.1215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 10 Batch 0: Train Loss = 0.6917\n",
      "Fold 2, epoch 10: Loss = 0.7243 Valid loss = 0.9703 MSE = 688.4260\n",
      "------------ Save FOLD-BEST model - MSE: 688.4260 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  972  118    0]\n",
      " [   0 3214  651    0]\n",
      " [   0 1741  536    0]\n",
      " [   0  858  605    7]]\n",
      "Mean absolute deviation (MAD) = 21.090715865373348\n",
      "Mean squared error (MSE) = 688.4260137995149\n",
      "Mean absolute percentage error (MAPE) = 215.3233970220498\n",
      "Cohen kappa score = 0.09984422668061321\n",
      "Fold 2, mse = 688.4260, mad = 21.0907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 11 Batch 0: Train Loss = 0.6333\n",
      "------------ Save FOLD-BEST model - MSE: 670.8719 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  940  146    4]\n",
      " [   0 3039  804   22]\n",
      " [   0 1589  682    6]\n",
      " [   0  740  687   43]]\n",
      "Mean absolute deviation (MAD) = 20.831345016675108\n",
      "Mean squared error (MSE) = 670.8719427095361\n",
      "Mean absolute percentage error (MAPE) = 213.4680835556249\n",
      "Cohen kappa score = 0.1248308812372595\n",
      "Fold 2, mse = 670.8719, mad = 20.8313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 12 Batch 0: Train Loss = 0.7514\n",
      "------------ Save FOLD-BEST model - MSE: 670.8063 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  943  146    1]\n",
      " [   0 3059  769   37]\n",
      " [   0 1582  672   23]\n",
      " [   0  737  675   58]]\n",
      "Mean absolute deviation (MAD) = 20.629842571840488\n",
      "Mean squared error (MSE) = 670.8063162161698\n",
      "Mean absolute percentage error (MAPE) = 198.1279965207673\n",
      "Cohen kappa score = 0.13122101570285027\n",
      "Fold 2, mse = 670.8063, mad = 20.6298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 13 Batch 0: Train Loss = 0.6565\n",
      "Fold 2, mse = 683.9297, mad = 20.6232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 14 Batch 0: Train Loss = 0.3913\n",
      "Fold 2, mse = 700.0893, mad = 20.9088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 15 Batch 0: Train Loss = 0.5092\n",
      "Fold 2, mse = 709.2501, mad = 20.9092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 16 Batch 0: Train Loss = 0.4356\n",
      "Fold 2, mse = 710.3388, mad = 20.7865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 17 Batch 0: Train Loss = 0.4662\n",
      "Fold 2, mse = 707.7830, mad = 20.6780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 18 Batch 0: Train Loss = 0.4778\n",
      "Fold 2, mse = 726.6997, mad = 20.8823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 19 Batch 0: Train Loss = 0.3651\n",
      "Fold 2, mse = 721.2229, mad = 20.6962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 20 Batch 0: Train Loss = 0.3552\n",
      "Fold 2, epoch 20: Loss = 0.3754 Valid loss = 1.0518 MSE = 746.2702\n",
      "Fold 2, mse = 746.2702, mad = 20.9172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 21 Batch 0: Train Loss = 0.4103\n",
      "Fold 2, mse = 778.7477, mad = 21.3697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 22 Batch 0: Train Loss = 0.3462\n",
      "Fold 2, mse = 798.7776, mad = 21.6681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 23 Batch 0: Train Loss = 0.3346\n",
      "Fold 2, mse = 794.3725, mad = 21.5618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 24 Batch 0: Train Loss = 0.3181\n",
      "Fold 2, mse = 792.5855, mad = 21.6886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 25 Batch 0: Train Loss = 0.2844\n",
      "Fold 2, mse = 819.3910, mad = 21.9756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 26 Batch 0: Train Loss = 0.2517\n",
      "Fold 2, mse = 859.0794, mad = 22.5626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 27 Batch 0: Train Loss = 0.2634\n",
      "Fold 2, mse = 852.6183, mad = 22.5004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 28 Batch 0: Train Loss = 0.2496\n",
      "Fold 2, mse = 876.9011, mad = 22.7516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 29 Batch 0: Train Loss = 0.3164\n",
      "Fold 2, mse = 850.1002, mad = 22.4512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 0 Batch 0: Train Loss = 2.6640\n",
      "Fold 3, epoch 0: Loss = 1.3494 Valid loss = 1.0228 MSE = 724.6376\n",
      "------------ Save FOLD-BEST model - MSE: 724.6376 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  975  105    0]\n",
      " [   0 3259  460    0]\n",
      " [   0 1951  204    0]\n",
      " [   0 1191  152    0]]\n",
      "Mean absolute deviation (MAD) = 21.8074854224176\n",
      "Mean squared error (MSE) = 724.6375831724397\n",
      "Mean absolute percentage error (MAPE) = 252.6997472697893\n",
      "Cohen kappa score = -0.010647880313701563\n",
      "Fold 3, mse = 724.6376, mad = 21.8075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 1 Batch 0: Train Loss = 0.9661\n",
      "------------ Save FOLD-BEST model - MSE: 721.8136 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  753  327    0]\n",
      " [   0 2592 1127    0]\n",
      " [   0 1607  548    0]\n",
      " [   0  936  407    0]]\n",
      "Mean absolute deviation (MAD) = 21.903049296998553\n",
      "Mean squared error (MSE) = 721.8135741639572\n",
      "Mean absolute percentage error (MAPE) = 260.22453553287505\n",
      "Cohen kappa score = -0.019251465050776462\n",
      "Fold 3, mse = 721.8136, mad = 21.9030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 2 Batch 0: Train Loss = 0.8726\n",
      "------------ Save FOLD-BEST model - MSE: 720.9435 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  147  933    0]\n",
      " [   0  535 3184    0]\n",
      " [   0  299 1856    0]\n",
      " [   0  135 1208    0]]\n",
      "Mean absolute deviation (MAD) = 22.12777395707127\n",
      "Mean squared error (MSE) = 720.9435123243011\n",
      "Mean absolute percentage error (MAPE) = 272.5628724714352\n",
      "Cohen kappa score = 0.010360016650185488\n",
      "Fold 3, mse = 720.9435, mad = 22.1278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 3 Batch 0: Train Loss = 0.9505\n",
      "------------ Save FOLD-BEST model - MSE: 719.6456 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0   99  981    0]\n",
      " [   0  350 3369    0]\n",
      " [   0  153 2002    0]\n",
      " [   0   65 1278    0]]\n",
      "Mean absolute deviation (MAD) = 22.134294039804256\n",
      "Mean squared error (MSE) = 719.6455857186686\n",
      "Mean absolute percentage error (MAPE) = 273.70418556684837\n",
      "Cohen kappa score = 0.017760944133614398\n",
      "Fold 3, mse = 719.6456, mad = 22.1343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 4 Batch 0: Train Loss = 1.0717\n",
      "------------ Save FOLD-BEST model - MSE: 716.4781 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  376  704    0]\n",
      " [   0 1257 2462    0]\n",
      " [   0  658 1497    0]\n",
      " [   0  271 1072    0]]\n",
      "Mean absolute deviation (MAD) = 21.996361585715235\n",
      "Mean squared error (MSE) = 716.4780835198898\n",
      "Mean absolute percentage error (MAPE) = 268.276644784049\n",
      "Cohen kappa score = 0.044319221474208303\n",
      "Fold 3, mse = 716.4781, mad = 21.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 5 Batch 0: Train Loss = 0.8654\n",
      "------------ Save FOLD-BEST model - MSE: 714.0709 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  594  486    0]\n",
      " [   0 1953 1766    0]\n",
      " [   0 1102 1053    0]\n",
      " [   0  512  831    0]]\n",
      "Mean absolute deviation (MAD) = 21.890133257121068\n",
      "Mean squared error (MSE) = 714.070866534199\n",
      "Mean absolute percentage error (MAPE) = 263.89116012581695\n",
      "Cohen kappa score = 0.04270313540310333\n",
      "Fold 3, mse = 714.0709, mad = 21.8901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 6 Batch 0: Train Loss = 1.0559\n",
      "------------ Save FOLD-BEST model - MSE: 711.7580 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  708  372    0]\n",
      " [   0 2296 1423    0]\n",
      " [   0 1280  875    0]\n",
      " [   0  639  704    0]]\n",
      "Mean absolute deviation (MAD) = 21.80631078211289\n",
      "Mean squared error (MSE) = 711.758030967277\n",
      "Mean absolute percentage error (MAPE) = 260.66854697334963\n",
      "Cohen kappa score = 0.048535327987491494\n",
      "Fold 3, mse = 711.7580, mad = 21.8063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 7 Batch 0: Train Loss = 0.9486\n",
      "------------ Save FOLD-BEST model - MSE: 708.8670 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  738  342    0]\n",
      " [   0 2403 1316    0]\n",
      " [   0 1368  787    0]\n",
      " [   0  684  659    0]]\n",
      "Mean absolute deviation (MAD) = 21.725425261477554\n",
      "Mean squared error (MSE) = 708.8669804321994\n",
      "Mean absolute percentage error (MAPE) = 257.8250971424886\n",
      "Cohen kappa score = 0.04287307094551451\n",
      "Fold 3, mse = 708.8670, mad = 21.7254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 8 Batch 0: Train Loss = 0.7991\n",
      "------------ Save FOLD-BEST model - MSE: 705.5631 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  857  223    0]\n",
      " [   0 2760  959    0]\n",
      " [   0 1575  580    0]\n",
      " [   0  864  479    0]]\n",
      "Mean absolute deviation (MAD) = 21.500879571870698\n",
      "Mean squared error (MSE) = 705.5630707120118\n",
      "Mean absolute percentage error (MAPE) = 247.51384254131224\n",
      "Cohen kappa score = 0.03641303485229774\n",
      "Fold 3, mse = 705.5631, mad = 21.5009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 9 Batch 0: Train Loss = 0.8828\n",
      "Fold 3, mse = 710.8039, mad = 21.3152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 10 Batch 0: Train Loss = 0.8726\n",
      "Fold 3, epoch 10: Loss = 0.8159 Valid loss = 1.0273 MSE = 727.5251\n",
      "Fold 3, mse = 727.5251, mad = 21.4077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 11 Batch 0: Train Loss = 0.9238\n",
      "Fold 3, mse = 726.3713, mad = 21.4669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 12 Batch 0: Train Loss = 0.6123\n",
      "Fold 3, mse = 760.2475, mad = 22.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 13 Batch 0: Train Loss = 0.5960\n",
      "Fold 3, mse = 818.5775, mad = 22.8582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 14 Batch 0: Train Loss = 0.5106\n",
      "Fold 3, mse = 788.2917, mad = 22.3087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 15 Batch 0: Train Loss = 0.4455\n",
      "Fold 3, mse = 823.9479, mad = 22.7401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 16 Batch 0: Train Loss = 0.4393\n",
      "Fold 3, mse = 862.2367, mad = 23.1083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 17 Batch 0: Train Loss = 0.4131\n",
      "Fold 3, mse = 892.6720, mad = 23.4604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 18 Batch 0: Train Loss = 0.3795\n",
      "Fold 3, mse = 921.5254, mad = 23.7654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 19 Batch 0: Train Loss = 0.3117\n",
      "Fold 3, mse = 935.4032, mad = 23.7726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 20 Batch 0: Train Loss = 0.3195\n",
      "Fold 3, epoch 20: Loss = 0.3059 Valid loss = 1.3202 MSE = 931.2289\n",
      "Fold 3, mse = 931.2289, mad = 23.7626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 21 Batch 0: Train Loss = 0.2935\n",
      "Fold 3, mse = 944.6989, mad = 23.7364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 22 Batch 0: Train Loss = 0.2917\n",
      "Fold 3, mse = 978.9594, mad = 24.0490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 23 Batch 0: Train Loss = 0.2570\n",
      "Fold 3, mse = 987.8535, mad = 24.1416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 24 Batch 0: Train Loss = 0.2677\n",
      "Fold 3, mse = 977.7295, mad = 24.1109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 25 Batch 0: Train Loss = 0.2588\n",
      "Fold 3, mse = 975.9726, mad = 24.2607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 26 Batch 0: Train Loss = 0.2732\n",
      "Fold 3, mse = 1008.9216, mad = 24.6090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 27 Batch 0: Train Loss = 0.2482\n",
      "Fold 3, mse = 1016.4365, mad = 24.4728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 28 Batch 0: Train Loss = 0.2561\n",
      "Fold 3, mse = 990.5908, mad = 24.3249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 29 Batch 0: Train Loss = 0.2552\n",
      "Fold 3, mse = 1004.1059, mad = 24.4932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 0 Batch 0: Train Loss = 2.8481\n",
      "Fold 4, epoch 0: Loss = 1.8876 Valid loss = 1.0460 MSE = 741.5236\n",
      "------------ Save FOLD-BEST model - MSE: 741.5236 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0   95 1016    0]\n",
      " [   0  358 3603    0]\n",
      " [   0  260 2033    0]\n",
      " [   0  135 1185    0]]\n",
      "Mean absolute deviation (MAD) = 23.128897179450696\n",
      "Mean squared error (MSE) = 741.523593484677\n",
      "Mean absolute percentage error (MAPE) = 309.6041725576002\n",
      "Cohen kappa score = -0.011474177859408519\n",
      "Fold 4, mse = 741.5236, mad = 23.1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 1 Batch 0: Train Loss = 1.2613\n",
      "Fold 4, mse = 770.9723, mad = 23.8464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 2 Batch 0: Train Loss = 1.2348\n",
      "Fold 4, mse = 765.9230, mad = 23.7506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 3 Batch 0: Train Loss = 1.1531\n",
      "Fold 4, mse = 749.1766, mad = 23.4224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 4 Batch 0: Train Loss = 1.1900\n",
      "------------ Save FOLD-BEST model - MSE: 729.8496 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0   12 1099    0]\n",
      " [   0   32 3929    0]\n",
      " [   0    6 2287    0]\n",
      " [   0    3 1317    0]]\n",
      "Mean absolute deviation (MAD) = 23.058713536088515\n",
      "Mean squared error (MSE) = 729.8496239692549\n",
      "Mean absolute percentage error (MAPE) = 312.8371064179653\n",
      "Cohen kappa score = 0.0034822727781980145\n",
      "Fold 4, mse = 729.8496, mad = 23.0587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 5 Batch 0: Train Loss = 1.0868\n",
      "------------ Save FOLD-BEST model - MSE: 683.6904 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  142  969    0]\n",
      " [   0  344 3617    0]\n",
      " [   0   99 2194    0]\n",
      " [   0   32 1288    0]]\n",
      "Mean absolute deviation (MAD) = 22.086479552884526\n",
      "Mean squared error (MSE) = 683.6904302868694\n",
      "Mean absolute percentage error (MAPE) = 289.26544494026734\n",
      "Cohen kappa score = 0.033969158658266\n",
      "Fold 4, mse = 683.6904, mad = 22.0865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 6 Batch 0: Train Loss = 1.0522\n",
      "------------ Save FOLD-BEST model - MSE: 643.4372 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  472  639    0]\n",
      " [   0 1272 2689    0]\n",
      " [   0  488 1805    0]\n",
      " [   0  154 1166    0]]\n",
      "Mean absolute deviation (MAD) = 21.119980768887846\n",
      "Mean squared error (MSE) = 643.4372295071544\n",
      "Mean absolute percentage error (MAPE) = 263.9104760964831\n",
      "Cohen kappa score = 0.09872666880352754\n",
      "------------ Save best model - MSE: 643.4372 ------------\n",
      "Fold 4, mse = 643.4372, mad = 21.1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 7 Batch 0: Train Loss = 1.0793\n",
      "------------ Save FOLD-BEST model - MSE: 637.2630 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  502  609    0]\n",
      " [   0 1425 2536    0]\n",
      " [   0  554 1739    0]\n",
      " [   0  222 1098    0]]\n",
      "Mean absolute deviation (MAD) = 20.917666699233934\n",
      "Mean squared error (MSE) = 637.2629899582821\n",
      "Mean absolute percentage error (MAPE) = 257.1921399905762\n",
      "Cohen kappa score = 0.09886916141845303\n",
      "------------ Save best model - MSE: 637.2630 ------------\n",
      "Fold 4, mse = 637.2630, mad = 20.9177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 8 Batch 0: Train Loss = 0.8313\n",
      "Fold 4, mse = 642.1958, mad = 20.7418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 9 Batch 0: Train Loss = 0.9104\n",
      "Fold 4, mse = 666.5009, mad = 20.7978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 10 Batch 0: Train Loss = 0.6782\n",
      "Fold 4, epoch 10: Loss = 0.7078 Valid loss = 0.9887 MSE = 701.2426\n",
      "Fold 4, mse = 701.2426, mad = 21.3428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 11 Batch 0: Train Loss = 0.6081\n",
      "Fold 4, mse = 688.9318, mad = 21.0896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 12 Batch 0: Train Loss = 0.5635\n",
      "Fold 4, mse = 700.8806, mad = 20.9620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 13 Batch 0: Train Loss = 0.5279\n",
      "Fold 4, mse = 710.3409, mad = 21.1476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 14 Batch 0: Train Loss = 0.4321\n",
      "Fold 4, mse = 727.2381, mad = 21.3203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 15 Batch 0: Train Loss = 0.4377\n",
      "Fold 4, mse = 740.9418, mad = 21.5374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 16 Batch 0: Train Loss = 0.4204\n",
      "Fold 4, mse = 725.0348, mad = 21.3816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 17 Batch 0: Train Loss = 0.3774\n",
      "Fold 4, mse = 706.6315, mad = 21.0327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 18 Batch 0: Train Loss = 0.3533\n",
      "Fold 4, mse = 724.0982, mad = 21.4003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 19 Batch 0: Train Loss = 0.3890\n",
      "Fold 4, mse = 766.1439, mad = 21.8446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 20 Batch 0: Train Loss = 0.3982\n",
      "Fold 4, epoch 20: Loss = 0.3704 Valid loss = 1.0758 MSE = 761.6488\n",
      "Fold 4, mse = 761.6488, mad = 21.7244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 21 Batch 0: Train Loss = 0.3362\n",
      "Fold 4, mse = 780.7475, mad = 21.9418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 22 Batch 0: Train Loss = 0.2844\n",
      "Fold 4, mse = 764.7619, mad = 21.7556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 23 Batch 0: Train Loss = 0.2815\n",
      "Fold 4, mse = 762.6568, mad = 21.7811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 24 Batch 0: Train Loss = 0.2727\n",
      "Fold 4, mse = 761.3436, mad = 21.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 25 Batch 0: Train Loss = 0.2529\n",
      "Fold 4, mse = 795.9733, mad = 22.2245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 26 Batch 0: Train Loss = 0.3340\n",
      "Fold 4, mse = 820.4604, mad = 22.4192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 27 Batch 0: Train Loss = 0.2765\n",
      "Fold 4, mse = 808.4759, mad = 22.1803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 28 Batch 0: Train Loss = 0.2913\n",
      "Fold 4, mse = 828.6896, mad = 22.4564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 29 Batch 0: Train Loss = 0.2077\n",
      "Fold 4, mse = 800.0361, mad = 22.1152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 0 Batch 0: Train Loss = 11.3561\n",
      "Fold 5, epoch 0: Loss = 4.3324 Valid loss = 1.6638 MSE = 1195.8692\n",
      "------------ Save FOLD-BEST model - MSE: 1195.8692 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[ 281  753   34    0]\n",
      " [ 836 2942  150    0]\n",
      " [ 479 1755   93    0]\n",
      " [ 228 1126   75    0]]\n",
      "Mean absolute deviation (MAD) = 25.890845055341696\n",
      "Mean squared error (MSE) = 1195.869165918147\n",
      "Mean absolute percentage error (MAPE) = 113.49012978989215\n",
      "Cohen kappa score = 0.01961924322240316\n",
      "Fold 5, mse = 1195.8692, mad = 25.8908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 1 Batch 0: Train Loss = 1.8537\n",
      "------------ Save FOLD-BEST model - MSE: 754.8935 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  112  956    0]\n",
      " [   0  287 3641    0]\n",
      " [   0  154 2173    0]\n",
      " [   0   81 1348    0]]\n",
      "Mean absolute deviation (MAD) = 23.26533244652332\n",
      "Mean squared error (MSE) = 754.8934678062736\n",
      "Mean absolute percentage error (MAPE) = 297.95545554923154\n",
      "Cohen kappa score = 0.01001917256693119\n",
      "Fold 5, mse = 754.8935, mad = 23.2653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 2 Batch 0: Train Loss = 1.4380\n",
      "------------ Save FOLD-BEST model - MSE: 697.9567 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  235  833    0]\n",
      " [   0  770 3158    0]\n",
      " [   0  353 1974    0]\n",
      " [   0  166 1263    0]]\n",
      "Mean absolute deviation (MAD) = 21.771438974357395\n",
      "Mean squared error (MSE) = 697.9567346982903\n",
      "Mean absolute percentage error (MAPE) = 254.2033594744599\n",
      "Cohen kappa score = 0.037113361659323774\n",
      "Fold 5, mse = 697.9567, mad = 21.7714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 3 Batch 0: Train Loss = 0.9988\n",
      "Fold 5, mse = 700.8994, mad = 21.8699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 4 Batch 0: Train Loss = 1.0900\n",
      "Fold 5, mse = 702.1467, mad = 21.8981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 5 Batch 0: Train Loss = 0.9397\n",
      "Fold 5, mse = 698.6875, mad = 21.7680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 6 Batch 0: Train Loss = 0.9267\n",
      "------------ Save FOLD-BEST model - MSE: 693.3619 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  353  715    0]\n",
      " [   0  950 2978    0]\n",
      " [   0  403 1924    0]\n",
      " [   0  218 1211    0]]\n",
      "Mean absolute deviation (MAD) = 21.566981951171293\n",
      "Mean squared error (MSE) = 693.3619170308084\n",
      "Mean absolute percentage error (MAPE) = 245.69809750309895\n",
      "Cohen kappa score = 0.05670098524874023\n",
      "Fold 5, mse = 693.3619, mad = 21.5670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 7 Batch 0: Train Loss = 0.7954\n",
      "------------ Save FOLD-BEST model - MSE: 689.2944 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   0  543  525    0]\n",
      " [   0 1625 2303    0]\n",
      " [   3  773 1551    0]\n",
      " [   0  332 1097    0]]\n",
      "Mean absolute deviation (MAD) = 21.3731233464878\n",
      "Mean squared error (MSE) = 689.2944219728594\n",
      "Mean absolute percentage error (MAPE) = 239.16561528378435\n",
      "Cohen kappa score = 0.08464058248244322\n",
      "Fold 5, mse = 689.2944, mad = 21.3731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 8 Batch 0: Train Loss = 0.9037\n",
      "------------ Save FOLD-BEST model - MSE: 684.1130 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[   1  738  329    0]\n",
      " [   0 2414 1514    0]\n",
      " [   7 1282 1038    0]\n",
      " [   0  574  855    0]]\n",
      "Mean absolute deviation (MAD) = 20.99750248744731\n",
      "Mean squared error (MSE) = 684.1130162762228\n",
      "Mean absolute percentage error (MAPE) = 224.11421740637368\n",
      "Cohen kappa score = 0.08534759006893977\n",
      "Fold 5, mse = 684.1130, mad = 20.9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 9 Batch 0: Train Loss = 0.9205\n",
      "Fold 5, mse = 693.9452, mad = 20.8677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 10 Batch 0: Train Loss = 0.7915\n",
      "Fold 5, epoch 10: Loss = 0.8543 Valid loss = 0.9932 MSE = 700.0370\n",
      "Fold 5, mse = 700.0370, mad = 20.9004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 11 Batch 0: Train Loss = 0.9121\n",
      "Fold 5, mse = 690.1399, mad = 20.9925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 12 Batch 0: Train Loss = 0.7213\n",
      "Fold 5, mse = 716.7672, mad = 21.0846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 13 Batch 0: Train Loss = 0.6761\n",
      "Fold 5, mse = 729.4101, mad = 21.0784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 14 Batch 0: Train Loss = 0.6201\n",
      "Fold 5, mse = 741.8029, mad = 21.3412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 15 Batch 0: Train Loss = 0.5648\n",
      "Fold 5, mse = 761.2857, mad = 21.6739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 16 Batch 0: Train Loss = 0.5203\n",
      "Fold 5, mse = 764.1130, mad = 21.6571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 17 Batch 0: Train Loss = 0.5386\n",
      "Fold 5, mse = 775.1615, mad = 21.7558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 18 Batch 0: Train Loss = 0.4424\n",
      "Fold 5, mse = 798.7573, mad = 21.9508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 19 Batch 0: Train Loss = 0.4241\n",
      "Fold 5, mse = 799.2299, mad = 21.9380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 20 Batch 0: Train Loss = 0.4088\n",
      "Fold 5, epoch 20: Loss = 0.4260 Valid loss = 1.1265 MSE = 799.6188\n",
      "Fold 5, mse = 799.6188, mad = 22.0401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 21 Batch 0: Train Loss = 0.4254\n",
      "Fold 5, mse = 816.7109, mad = 22.2302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 22 Batch 0: Train Loss = 0.4364\n",
      "Fold 5, mse = 809.1118, mad = 22.1246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 23 Batch 0: Train Loss = 0.4139\n",
      "Fold 5, mse = 842.9683, mad = 22.5006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 24 Batch 0: Train Loss = 0.3950\n",
      "Fold 5, mse = 858.6816, mad = 22.7302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 25 Batch 0: Train Loss = 0.3776\n",
      "Fold 5, mse = 880.3399, mad = 22.9806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 26 Batch 0: Train Loss = 0.3071\n",
      "Fold 5, mse = 896.8445, mad = 23.2178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 27 Batch 0: Train Loss = 0.3289\n",
      "Fold 5, mse = 906.6051, mad = 23.3868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 28 Batch 0: Train Loss = 0.3481\n",
      "Fold 5, mse = 926.1284, mad = 23.5724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 29 Batch 0: Train Loss = 0.3005\n",
      "Fold 5, mse = 939.5606, mad = 23.6715\n",
      "mse 669.9416(23.9232)\n",
      "mad 20.9447(0.3109)\n"
     ]
    }
   ],
   "source": [
    "if target_dataset == 'PD':\n",
    "    n_splits = 5\n",
    "    epochs = 30\n",
    "elif target_dataset == 'TJ':\n",
    "    n_splits = 10\n",
    "    epochs = 150\n",
    "elif target_dataset == 'HM':\n",
    "    n_splits = 3\n",
    "    epochs = 20\n",
    "\n",
    "teacher_flag = True\n",
    "transfer_flag = True\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "if target_dataset == 'PD':    \n",
    "    data_str = 'pd'\n",
    "elif target_dataset == 'TJ':    \n",
    "    data_str = 'covid'\n",
    "elif target_dataset == 'HM':\n",
    "    data_str = 'spain'\n",
    "\n",
    "# if teacher_flag:\n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str\n",
    "# else: \n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher'\n",
    "    \n",
    "file_name = './model/pretrained-dann'+ data_str;\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "fold_count = 0\n",
    "total_train_loss = []\n",
    "total_valid_loss = []\n",
    "\n",
    "global_best = 10000\n",
    "mse = []\n",
    "mad = []\n",
    "mape = []\n",
    "kappa = []\n",
    "history = []\n",
    "\n",
    "pad_token = np.zeros(input_dim)\n",
    "# begin_time = time.time()\n",
    "\n",
    "for train, test in kfold.split(long_x):\n",
    "        \n",
    "    train_x = [long_x[i] for i in train]\n",
    "    train_y = [long_time[i] for i in train]\n",
    "    train_x_len = [all_x_len[i] for i in train]\n",
    "    #train_static = [long_static[i] for i in train]\n",
    "    \n",
    "    train_x, train_y, train_x_len = get_n2n_data(train_x, train_y, train_x_len)\n",
    "    if len(train_x) % 256 == 1:\n",
    "        print(len(train_x))\n",
    "        print('wrong squeeze!')\n",
    "\n",
    "# for train, test in kfold.split(long_x):\n",
    "for train, test in kfold.split(long_x):\n",
    "    if reverse:\n",
    "        temp = train\n",
    "        train = test\n",
    "        test = temp\n",
    "    \n",
    "    model = target_model(input_dim = input_dim,output_dim=output_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, hidden_dim=hidden_dim).to(device)\n",
    "    \n",
    "    if transfer_flag:\n",
    "        checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu'))\n",
    "        pretrain_dict = checkpoint['net']\n",
    "        model_dict = model.state_dict()\n",
    "        pretrain_dict = transfer_gru_dict(pretrain_dict, model_dict)\n",
    "        model_dict.update(pretrain_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    fold_count += 1\n",
    "#     print(train)\n",
    "\n",
    "    \n",
    "    train_x = [long_x[i] for i in train]\n",
    "    train_y = [long_time[i] for i in train]\n",
    "    train_x_len = [all_x_len[i] for i in train]\n",
    "    #train_static = [long_static[i] for i in train]\n",
    "    \n",
    "    train_x, train_y, train_x_len = get_n2n_data(train_x, train_y, train_x_len)\n",
    "    \n",
    "    test_x = [long_x[i] for i in test]\n",
    "    test_y = [long_time[i] for i in test]\n",
    "    test_x_len = [all_x_len[i] for i in test]\n",
    "    #test_static = [long_static[i] for i in test]\n",
    "    \n",
    "    test_x, test_y, test_x_len = get_n2n_data(test_x, test_y, test_x_len)\n",
    "    \n",
    "    if not os.path.exists('./model/'+data_str):\n",
    "        os.mkdir('./model/'+data_str)\n",
    "        \n",
    "    if transfer_flag:\n",
    "        target_file_name = './model/'+data_str+'/distcare-trans-'+str(n_splits)+'-fold-LOS-regression' + str(fold_count)#4114\n",
    "    else:\n",
    "        target_file_name = './model/'+data_str+'/distcare-no-trans-'+str(n_splits)+'-fold-LOS-regression' + str(fold_count)#4114\n",
    "    \n",
    "    fold_train_loss = []\n",
    "    fold_valid_loss = []\n",
    "    best_mse = 10000\n",
    "    best_mad = 0\n",
    "    best_mape = 0\n",
    "    best_kappa = 0\n",
    "    \n",
    "    for each_epoch in range(epochs):\n",
    "       \n",
    "        \n",
    "        epoch_loss = []\n",
    "        counter_batch = 0\n",
    "        model.train()  \n",
    "        \n",
    "        for step, (batch_x, batch_y, batch_lens) in enumerate(ckd_batch_iter(train_x, train_y, train_x_len, batch_size, shuffle=True)):  \n",
    "            optimizer.zero_grad()\n",
    "            batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "\n",
    "            masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "            opt, emb = model(batch_x, batch_lens)\n",
    "\n",
    "            MSE_Loss = get_re_loss(opt, batch_y.unsqueeze(-1))\n",
    "\n",
    "#             model_loss = pred_loss + 1e7*decov_loss\n",
    "            model_loss = MSE_Loss\n",
    "\n",
    "            loss = model_loss\n",
    "\n",
    "            epoch_loss.append(MSE_Loss.cpu().detach().numpy())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % 50 == 0:\n",
    "                print('Fold %d Epoch %d Batch %d: Train Loss = %.4f'%(fold_count,each_epoch, step, loss.cpu().detach().numpy()))\n",
    "                logger.info('Fold %d Epoch %d Batch %d: Train Loss = %.4f'%(fold_count,each_epoch, step, loss.cpu().detach().numpy()))\n",
    "            \n",
    "        epoch_loss = np.mean(epoch_loss)\n",
    "        fold_train_loss.append(epoch_loss)\n",
    "\n",
    "        #Validation\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_pred_flatten = []\n",
    "        y_true_flatten = []\n",
    "        outcome_pred_flatten = []\n",
    "        outcome_true_flatten = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            valid_true = []\n",
    "            valid_pred = []\n",
    "            for batch_x, batch_y, batch_lens in ckd_batch_iter(test_x, test_y, test_x_len, batch_size):\n",
    "                batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "                batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "                batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "                masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "               \n",
    "                opt, emb = model(batch_x, batch_lens)\n",
    "                \n",
    "                MSE_Loss = get_re_loss(opt, batch_y.unsqueeze(-1))\n",
    "                \n",
    "                valid_loss.append(MSE_Loss.cpu().detach().numpy())\n",
    "\n",
    "                y_pred_flatten += [reverse_los(x, los_info) / 30 for x in list(opt.cpu().detach().numpy().flatten())]\n",
    "                y_true_flatten += [reverse_los(x, los_info) / 30 for x in list(batch_y.cpu().numpy().flatten())]\n",
    "            \n",
    "\n",
    "            valid_loss = np.mean(valid_loss)\n",
    "            fold_valid_loss.append(valid_loss)\n",
    "            ret = metrics.print_metrics_regression(y_true_flatten, y_pred_flatten, verbose=0)\n",
    "            history.append(ret)\n",
    "            #print()\n",
    "\n",
    "            if each_epoch % 10 == 0:\n",
    "                print('Fold %d, epoch %d: Loss = %.4f Valid loss = %.4f MSE = %.4f' % (\n",
    "                    fold_count, each_epoch, fold_train_loss[-1], fold_valid_loss[-1], ret['mse']), flush=True)\n",
    "                logger.info('Fold %d, epoch %d: Loss = %.4f Valid loss = %.4f MSE = %.4f' % (\n",
    "                    fold_count, each_epoch, fold_train_loss[-1], fold_valid_loss[-1], ret['mse']))\n",
    "                # metrics.print_metrics_regression(y_true_flatten, y_pred_flatten)\n",
    "                \n",
    "            cur_mse = ret['mse']\n",
    "            if cur_mse < best_mse:\n",
    "                print('------------ Save FOLD-BEST model - MSE: %.4f ------------' % cur_mse, flush=True)\n",
    "                logger.info('------------ Save FOLD-BEST model - MSE: %.4f ------------' % cur_mse)\n",
    "                metrics.print_metrics_regression(y_true_flatten, y_pred_flatten)\n",
    "                best_mse = cur_mse\n",
    "                best_mad = ret['mad']\n",
    "                state = {\n",
    "                    'net': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': each_epoch\n",
    "                }\n",
    "                torch.save(state, target_file_name + '_' + str(fold_count))\n",
    "\n",
    "                if cur_mse < global_best:\n",
    "                    global_best = cur_mse\n",
    "                    state = {\n",
    "                        'net': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'epoch': each_epoch\n",
    "                    }\n",
    "                    torch.save(state, target_file_name)\n",
    "                    print('------------ Save best model - MSE: %.4f ------------' % cur_mse, flush=True)\n",
    "                    logger.info('------------ Save best model - MSE: %.4f ------------' % cur_mse)\n",
    "\n",
    "        print('Fold %d, mse = %.4f, mad = %.4f' % (fold_count, ret['mse'], ret['mad']), flush=True)\n",
    "        logger.info('Fold %d, mse = %.4f, mad = %.4f' % (fold_count, ret['mse'], ret['mad']))\n",
    "\n",
    "    mse.append(best_mse)\n",
    "    mad.append(best_mad)\n",
    "    total_train_loss.append(fold_train_loss)\n",
    "    total_valid_loss.append(fold_valid_loss)\n",
    "\n",
    "\n",
    "print('mse %.4f(%.4f)' % (np.mean(mse), np.std(mse)))\n",
    "print('mad %.4f(%.4f)' % (np.mean(mad), np.std(mad)))\n",
    "logger.info('mse %.4f(%.4f)' % (np.mean(mse), np.std(mse)))\n",
    "logger.info('mad %.4f(%.4f)' % (np.mean(mad), np.std(mad)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
