2024-01-18 01:06:40,948 - __main__ - INFO - 这是希望输出的info内容
2024-01-18 01:06:40,949 - __main__ - WARNING - 这是希望输出的warning内容
2024-01-18 01:07:23,165 - __main__ - INFO - 32269
2024-01-18 01:07:23,165 - __main__ - INFO - 4034
2024-01-18 01:07:23,166 - __main__ - INFO - 4033
2024-01-18 01:07:28,658 - __main__ - INFO - load target data
2024-01-18 01:07:34,360 - __main__ - INFO - Training Student
2024-01-18 01:07:36,177 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 1.7358
2024-01-18 01:07:52,211 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 1.4934
2024-01-18 01:08:09,478 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 1.5245
2024-01-18 01:08:26,669 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 1.4853
2024-01-18 01:08:44,362 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 1.4114
2024-01-18 01:09:01,658 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 1.4137
2024-01-18 01:09:19,083 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 1.4054
2024-01-18 01:09:24,262 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.3356 ------------
2024-01-18 01:09:32,467 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 1.4418
2024-01-18 01:09:49,864 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 1.5487
2024-01-18 01:10:07,018 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 1.5269
2024-01-18 01:10:24,020 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 1.4772
2024-01-18 01:10:41,404 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 1.4600
2024-01-18 01:10:59,064 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 1.4518
2024-01-18 01:11:16,093 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 1.4583
2024-01-18 01:11:30,795 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 1.4705
2024-01-18 01:11:48,879 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 1.4746
2024-01-18 01:12:06,555 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 1.4797
2024-01-18 01:12:24,347 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 1.5061
2024-01-18 01:12:43,083 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 1.4791
2024-01-18 01:13:01,777 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 1.4442
2024-01-18 01:13:22,149 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 1.4700
2024-01-18 01:13:37,743 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 1.4714
2024-01-18 01:13:58,300 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 1.4947
2024-01-18 01:14:15,467 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 1.4851
2024-01-18 01:14:32,125 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 1.4916
2024-01-18 01:14:49,111 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 1.4579
2024-01-18 01:15:05,688 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 1.4472
2024-01-18 01:15:23,092 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 1.4583
2024-01-18 01:15:36,570 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 1.4694
2024-01-18 01:15:53,296 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 1.4698
2024-01-18 01:16:09,954 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 1.4791
2024-01-18 01:16:26,226 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 1.4867
2024-01-18 01:16:43,400 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 1.4520
2024-01-18 01:17:00,611 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 1.4430
2024-01-18 01:17:17,341 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 1.4564
2024-01-18 01:17:31,189 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 1.4649
2024-01-18 01:17:48,824 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 1.4656
2024-01-18 01:18:05,555 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 1.4694
2024-01-18 01:18:22,006 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 1.4810
2024-01-18 01:18:39,359 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 1.4498
2024-01-18 01:18:55,870 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 1.4405
2024-01-18 01:19:11,112 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 1.4562
2024-01-18 01:19:24,521 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 1.4621
2024-01-18 01:19:41,348 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 1.4672
2024-01-18 01:19:59,316 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 1.4683
2024-01-18 01:20:16,429 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 1.4798
2024-01-18 01:20:33,358 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 1.4483
2024-01-18 01:20:50,937 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 1.4406
2024-01-18 01:21:07,361 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 1.4540
2024-01-18 01:21:20,665 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 1.4619
2024-01-18 01:21:37,784 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 1.4674
2024-01-18 01:21:55,246 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 1.4706
2024-01-18 01:22:10,844 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 1.4789
2024-01-18 01:22:27,770 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 1.4486
2024-01-18 01:22:44,200 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 1.4413
2024-01-18 01:23:00,558 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 1.4550
2024-01-18 01:23:14,948 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 1.4608
2024-01-18 01:23:33,281 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 1.4634
2024-01-18 01:23:49,677 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 1.4703
2024-01-18 01:24:06,740 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 1.4794
2024-01-18 01:24:24,116 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 1.4486
2024-01-18 01:24:40,890 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 1.4439
2024-01-18 01:24:57,752 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 1.4544
2024-01-18 01:25:10,408 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 1.4610
2024-01-18 01:25:27,614 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 1.4654
2024-01-18 01:25:43,474 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 1.4715
2024-01-18 01:25:59,297 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 1.4796
2024-01-18 01:26:15,666 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 1.4472
2024-01-18 01:26:32,342 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 1.4416
2024-01-18 01:26:49,895 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 1.4556
2024-01-18 01:27:02,736 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 1.4617
2024-01-18 01:27:20,870 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 1.4660
2024-01-18 01:27:36,977 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 1.4691
2024-01-18 01:27:53,235 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 1.4807
2024-01-18 01:28:10,506 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 1.4489
2024-01-18 01:28:26,531 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 1.4405
2024-01-18 01:28:42,502 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 1.4562
2024-01-18 01:28:55,582 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 1.4629
2024-01-18 01:29:12,272 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 1.4654
2024-01-18 01:29:28,391 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 1.4685
2024-01-18 01:29:44,526 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 1.4803
2024-01-18 01:30:01,684 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 1.4501
2024-01-18 01:30:18,238 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 1.4421
2024-01-18 01:30:34,819 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 1.4567
2024-01-18 01:30:47,606 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 1.4627
2024-01-18 01:31:04,153 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 1.4663
2024-01-18 01:31:20,020 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 1.4687
2024-01-18 01:31:36,426 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 1.4787
2024-01-18 01:31:52,693 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 1.4469
2024-01-18 01:32:08,602 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 1.4408
2024-01-18 01:32:25,270 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 1.4543
2024-01-18 01:32:38,734 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 1.4628
2024-01-18 01:32:56,378 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 1.4672
2024-01-18 01:33:12,792 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 1.4679
2024-01-18 01:33:29,150 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 1.4814
2024-01-18 01:33:46,341 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 1.4482
2024-01-18 01:34:02,662 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 1.4402
2024-01-18 01:34:19,045 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 1.4575
2024-01-18 01:34:32,246 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 1.4619
2024-01-18 01:34:49,393 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 1.4643
2024-01-18 01:35:05,324 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 1.4703
2024-01-18 01:35:21,453 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 1.4793
2024-01-18 01:35:38,736 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 1.4485
2024-01-18 01:35:54,914 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 1.4420
2024-01-18 01:36:11,601 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 1.4541
2024-01-18 01:36:24,059 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 1.4598
2024-01-18 01:36:40,878 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 1.4645
2024-01-18 01:36:57,985 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 1.4688
2024-01-18 01:37:14,222 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 1.4780
2024-01-18 01:37:30,663 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 1.4489
2024-01-18 01:37:47,277 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 1.4432
2024-01-18 01:38:03,967 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 1.4575
2024-01-18 01:38:16,680 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 1.4636
2024-01-18 01:38:34,413 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 1.4657
2024-01-18 01:38:50,345 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 1.4675
2024-01-18 01:39:07,161 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 1.4783
2024-01-18 01:39:22,855 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 1.4487
2024-01-18 01:39:39,659 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 1.4420
2024-01-18 01:39:55,665 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 1.4555
2024-01-18 01:40:09,398 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 1.4614
2024-01-18 01:40:26,801 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 1.4653
2024-01-18 01:40:42,804 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 1.4687
2024-01-18 01:40:59,286 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 1.4819
2024-01-18 01:41:16,100 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 1.4486
2024-01-18 01:41:32,542 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 1.4401
2024-01-18 01:41:48,617 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 1.4549
2024-01-18 01:42:00,739 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 1.4602
2024-01-18 01:42:16,911 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 1.4653
2024-01-18 01:42:32,616 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 1.4703
2024-01-18 01:42:48,430 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 1.4771
2024-01-18 01:43:04,276 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 1.4490
2024-01-18 01:43:20,379 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 1.4420
2024-01-18 01:43:37,407 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 1.4564
2024-01-18 01:43:50,826 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 1.4622
2024-01-18 01:44:07,178 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 1.4643
2024-01-18 01:44:23,662 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 1.4690
2024-01-18 01:44:40,026 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 1.4798
2024-01-18 01:44:56,542 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 1.4480
2024-01-18 01:45:12,236 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 1.4406
2024-01-18 01:45:27,750 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 1.4533
2024-01-18 01:45:40,639 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 1.4624
2024-01-18 01:45:57,273 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 1.4646
2024-01-18 01:46:13,382 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 1.4695
2024-01-18 01:46:30,263 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 1.4789
2024-01-18 01:46:46,994 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 1.4476
2024-01-18 01:47:03,658 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 1.4403
2024-01-18 01:47:20,099 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 1.4530
2024-01-18 01:47:33,067 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 1.4627
2024-01-18 01:47:48,688 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 1.4664
2024-01-18 01:48:04,912 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 1.4672
2024-01-18 01:48:21,008 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 1.4754
2024-01-18 01:48:37,114 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 1.4475
2024-01-18 01:48:52,585 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 1.4394
2024-01-18 01:49:08,906 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 1.4550
2024-01-18 01:49:21,469 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 1.4615
2024-01-18 01:49:38,027 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 1.4642
2024-01-18 01:49:53,847 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 1.4661
2024-01-18 01:50:09,611 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 1.4788
2024-01-18 01:50:27,573 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 1.4445
2024-01-18 01:50:43,972 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 1.4402
2024-01-18 01:50:59,581 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 1.4524
2024-01-18 01:51:11,885 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 1.4621
2024-01-18 01:51:28,019 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 1.4638
2024-01-18 01:51:44,219 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 1.4693
2024-01-18 01:52:00,528 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 1.4774
2024-01-18 01:52:15,279 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 1.4489
2024-01-18 01:52:30,627 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 1.4400
2024-01-18 01:52:45,034 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 1.4552
2024-01-18 01:52:56,861 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 1.4613
2024-01-18 01:53:11,843 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 1.4632
2024-01-18 01:53:25,716 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 1.4658
2024-01-18 01:53:41,314 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 1.4760
2024-01-18 01:53:55,189 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 1.4470
2024-01-18 01:54:09,377 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 1.4376
2024-01-18 01:54:23,662 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 1.4527
2024-01-18 01:54:35,017 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 1.4613
2024-01-18 01:54:49,933 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 1.4681
2024-01-18 01:55:03,793 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 1.4678
2024-01-18 01:55:17,932 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 1.4786
2024-01-18 01:55:32,200 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 1.4474
2024-01-18 01:55:46,276 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 1.4389
2024-01-18 01:56:01,278 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 1.4524
2024-01-18 01:56:13,400 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 1.4620
2024-01-18 01:56:28,565 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 1.4654
2024-01-18 01:56:43,576 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 1.4683
2024-01-18 01:56:58,902 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 1.4798
2024-01-18 01:57:14,081 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 1.4457
2024-01-18 01:57:28,986 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 1.4384
2024-01-18 01:57:42,487 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 1.4499
2024-01-18 01:57:53,897 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 1.4587
2024-01-18 01:58:08,301 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 1.4642
2024-01-18 01:58:22,204 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 1.4653
2024-01-18 01:58:36,486 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 1.4757
2024-01-18 01:58:51,141 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 1.4449
2024-01-18 01:59:04,953 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 1.4366
2024-01-18 01:59:19,227 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 1.4535
2024-01-18 01:59:30,766 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 1.4607
2024-01-18 01:59:45,491 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 1.4620
2024-01-18 01:59:59,639 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 1.4642
2024-01-18 02:00:14,272 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 1.4769
2024-01-18 02:00:30,342 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 1.4486
2024-01-18 02:00:44,536 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 1.4409
2024-01-18 02:00:59,763 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 1.4573
2024-01-18 02:01:11,705 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 1.4621
2024-01-18 02:01:26,128 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 1.4639
2024-01-18 02:01:40,574 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 1.4668
2024-01-18 02:01:54,432 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 1.4761
2024-01-18 02:02:09,089 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 1.4468
2024-01-18 02:02:23,061 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 1.4415
2024-01-18 02:02:37,430 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 1.4530
2024-01-18 02:02:48,300 - __main__ - INFO - last saved model is in epoch 27
2024-01-18 03:17:57,304 - __main__ - INFO - last saved model is in epoch 0
2024-01-18 03:18:01,361 - __main__ - INFO - Batch 0: Test Loss = -2.5771
2024-01-18 03:18:04,577 - __main__ - INFO - 
==>Predicting on test
2024-01-18 03:18:04,578 - __main__ - INFO - Test Loss = -2.5198
2024-01-18 03:41:01,778 - __main__ - INFO - Transfer Target Dataset & Model
2024-01-18 03:41:01,855 - __main__ - INFO - [[-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8285543981909917, 2.770245567717861, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8374745525934485, 2.8093811444689463, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5338330207864587, 0.431156978499758, 0.8964660630930379, -1.342421748214616, -0.6946051308187712, -0.2120300841305121, 0.8952797203562032, 0.0387041303378994, -0.8612693268611251, 0.0726334784031089, -0.5051102137388988, -0.2865727201409924, 0.3988826585206329, -1.3794377690564883, -0.8237089728907875, -0.9221119476695248, -1.6036614531597553, -0.2379932071009605, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -1.1501193786706505, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, 1.0826051686869729, -0.609009148503521, 0.9684393533852332, -0.3880554131475662, 0.8439788318452395, 2.837917502516613, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 0.278477698357045, 1.2505677424119097, -0.6305581644917595, -0.9062745442328174, -0.7536814885080627, -0.4374103947888615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, -0.7710128163592748, -0.6738408269117502, -2.438483340619628, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8608899578998963, 2.912112033440545, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3396023676711391, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8763143915541441, 2.979783968239297, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5125273793649132, 0.601421850901989, 0.5034187451388209, -1.455529079732558, -0.5254345623915906, -0.4163803392884947, 0.7885904984786939, 0.2816638966057675, -0.5890172833864098, 0.5902661209826593, -0.3430637446868887, -0.7482466726318323, -0.1864257310498265, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.2249939001501796, -0.0341172340589738, 0.3466058957088718, 0.3482841380580905, -0.2890718868318484, 0.1923438589112976, -0.2031244129114749, -1.100153472115602, -0.7598199909551685, -0.6868858002659636, 0.3602180776283341, -0.700061185363224, -0.501359972189453, -0.2332733777972843, -1.36588558517574, 0.3571411263970316, -0.7447833078367583, 0.8765002281041955, 2.9805992927549454, 0.0776143028335215, -0.2769313825285214, 0.0755136678180621, -0.1474826847594624, -0.3999358135056357, -0.0169688050451549, -0.0937841116273902, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, -0.0122781725754626, -0.3292808750219816, -0.2769313825285214, -0.107625012968948, -0.1474826847594624, -0.5354516373428909, -0.0002229391773086, -0.0862088042532133, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, 0.0766269712424727, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.3951350096437179, -0.5239726809327129, -0.8378690925056301, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.8988006141103361, 3.0784382346326584, 0.0776143028335215, -0.3487779453829513, 0.070457637721598, -0.1474826847594624, -0.3999358135056357, 0.0045338857876385, -0.0729488956980065, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.1275947723114954, -0.4255010572823463, -0.3487779453829513, -0.1777070069371955, -0.1474826847594624, -0.5354516373428909, 0.029608437271828, -0.0566564538358881, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.0232601678654147, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.9106941533136118, 3.1306190036341057, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9108799898636633, 3.1314343281497536, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 1.0542620417323658, -0.6738408269117502, -0.0796833960305785, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9222160194167848, 3.1811691236042576, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.5107528363891513, 1.4769583166408096, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-1.038066534429697, 0.260892106097527, 0.1645848503507034, -1.455529079732558, -1.6391408045371951, 0.196670426185453, 1.2420196914581063, 0.4706326037029981, -0.6616178283130005, -0.5004598044528213, -0.4609157221792596, 0.021209914852902, -1.1769476210921423, 0.0909225332951111, -1.348247483817418, -1.2590833683251033, -0.9252694826312332, 0.781386658108973, -0.0798138719333467, -0.2032969502372001, -0.5728175028694968, 0.103242147293012, -0.2031244129114749, -0.6504603131201653, -0.5868915885967425, -0.6218024077192129, -0.0284729839577739, -0.3925684926321655, -0.4454382040900255, -0.2791761177909213, -1.5149673075505703, 0.3413397023846657, -0.8352815289623093, 0.9329945393197532, 3.228457945511819, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8995798651896526, 0.4879119359671683, 0.1510314945591795, -1.4131138304133295, -0.6523124887119757, 0.196670426185453, 0.148455167213642, 0.4706326037029981, -0.752368509471239, -0.9441449266638648, -0.9507380036319262, 0.1751012323498492, -1.0418764542681906, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -1.3629417216818924, 0.1697587389830128, -0.364093717028159, -0.2645837378255657, -0.2890718868318484, 0.0364158635792983, -0.2031244129114749, -1.8246591171638051, -0.4061028043129334, -0.5350245509902118, 0.0492652283594477, 0.0357249008146661, -0.4929416415078188, -0.5851943844151671, -1.170932563608653, 0.051337096981241, -0.8763418268751098, 0.9447022419729768, 3.279823389997619, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.962914223877992, 3.359725192531085, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.973321070680858, 3.405383365407351, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9735069072309092, 3.4061986899229986, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9980373318376644, 3.5138215259884835, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0340896225475933, 3.671994482024121, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.2639920224449301, -0.6738408269117502, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0396647190491284, 3.696454217493548, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7418059691584676, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, -0.6693833821778409, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.057505027854041, 3.774725370995719, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5792130238763558, -0.9062745442328174, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, 0.0888023142972107, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0760886828591592, 3.856257822560481, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]
2024-01-18 03:41:01,857 - __main__ - INFO - 69
2024-01-18 03:41:01,858 - __main__ - INFO - 325
2024-01-18 03:41:19,896 - __main__ - INFO - {'los_mean': 1055.0307777880782, 'los_std': 799.0879849276147}
2024-01-18 03:41:22,188 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 2.4444
2024-01-18 03:41:49,022 - __main__ - INFO - Fold 1, epoch 0: Loss = 1.1928 Valid loss = 1.0645 MSE = 712.8670
2024-01-18 03:41:49,023 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 712.8670 ------------
2024-01-18 03:41:49,252 - __main__ - INFO - ------------ Save best model - MSE: 712.8670 ------------
2024-01-18 03:41:49,254 - __main__ - INFO - Fold 1, mse = 712.8670, mad = 22.6365
2024-01-18 03:41:49,965 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 0.9798
2024-01-18 03:42:17,209 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 651.0003 ------------
2024-01-18 03:42:17,476 - __main__ - INFO - ------------ Save best model - MSE: 651.0003 ------------
2024-01-18 03:42:17,478 - __main__ - INFO - Fold 1, mse = 651.0003, mad = 21.1898
2024-01-18 03:42:18,227 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 0.8800
2024-01-18 03:42:45,802 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 604.2220 ------------
2024-01-18 03:42:46,020 - __main__ - INFO - ------------ Save best model - MSE: 604.2220 ------------
2024-01-18 03:42:46,021 - __main__ - INFO - Fold 1, mse = 604.2220, mad = 19.7731
2024-01-18 03:42:46,773 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 0.7498
2024-01-18 03:43:14,085 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 573.8574 ------------
2024-01-18 03:43:14,328 - __main__ - INFO - ------------ Save best model - MSE: 573.8574 ------------
2024-01-18 03:43:14,329 - __main__ - INFO - Fold 1, mse = 573.8574, mad = 19.4100
2024-01-18 03:43:15,078 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 0.7232
2024-01-18 03:43:41,598 - __main__ - INFO - Fold 1, mse = 612.9260, mad = 19.9480
2024-01-18 03:43:42,277 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 0.7705
2024-01-18 03:44:08,690 - __main__ - INFO - Fold 1, mse = 635.1911, mad = 20.1046
2024-01-18 03:44:09,378 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 0.6236
2024-01-18 03:44:36,606 - __main__ - INFO - Fold 1, mse = 632.6206, mad = 20.2395
2024-01-18 03:44:37,314 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 0.5987
2024-01-18 03:45:04,448 - __main__ - INFO - Fold 1, mse = 662.8674, mad = 20.6629
2024-01-18 03:45:05,232 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 0.6400
2024-01-18 03:45:32,379 - __main__ - INFO - Fold 1, mse = 673.6309, mad = 20.9109
2024-01-18 03:45:33,106 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 0.4710
2024-01-18 03:46:01,782 - __main__ - INFO - Fold 1, mse = 701.1772, mad = 21.1974
2024-01-18 03:46:02,469 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 0.4959
2024-01-18 03:46:28,352 - __main__ - INFO - Fold 1, epoch 10: Loss = 0.4719 Valid loss = 0.9859 MSE = 733.1290
2024-01-18 03:46:28,353 - __main__ - INFO - Fold 1, mse = 733.1290, mad = 21.3765
2024-01-18 03:46:29,120 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 0.4647
2024-01-18 03:46:55,457 - __main__ - INFO - Fold 1, mse = 725.7822, mad = 21.7541
2024-01-18 03:46:56,133 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 0.4620
2024-01-18 03:47:22,673 - __main__ - INFO - Fold 1, mse = 704.0913, mad = 21.4255
2024-01-18 03:47:23,345 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 0.4196
2024-01-18 03:47:50,670 - __main__ - INFO - Fold 1, mse = 755.6556, mad = 21.6743
2024-01-18 03:47:51,440 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 0.3287
2024-01-18 03:48:18,697 - __main__ - INFO - Fold 1, mse = 780.9870, mad = 22.0601
2024-01-18 03:48:19,391 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 0.3514
2024-01-18 03:48:47,549 - __main__ - INFO - Fold 1, mse = 780.1214, mad = 22.0391
2024-01-18 03:48:48,200 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 0.3040
2024-01-18 03:49:14,990 - __main__ - INFO - Fold 1, mse = 745.5865, mad = 21.6185
2024-01-18 03:49:15,744 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 0.3319
2024-01-18 03:49:42,988 - __main__ - INFO - Fold 1, mse = 815.4779, mad = 22.5595
2024-01-18 03:49:43,662 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 0.3197
2024-01-18 03:50:09,968 - __main__ - INFO - Fold 1, mse = 774.8775, mad = 21.7205
2024-01-18 03:50:10,756 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 0.2969
2024-01-18 03:50:37,781 - __main__ - INFO - Fold 1, mse = 833.4115, mad = 22.5391
2024-01-18 03:50:38,473 - __main__ - INFO - Fold 1 Epoch 20 Batch 0: Train Loss = 0.3540
2024-01-18 03:51:05,673 - __main__ - INFO - Fold 1, epoch 20: Loss = 0.3175 Valid loss = 1.0846 MSE = 831.2255
2024-01-18 03:51:05,674 - __main__ - INFO - Fold 1, mse = 831.2255, mad = 22.7282
2024-01-18 03:51:06,457 - __main__ - INFO - Fold 1 Epoch 21 Batch 0: Train Loss = 0.3380
2024-01-18 03:51:35,650 - __main__ - INFO - Fold 1, mse = 803.5604, mad = 22.6174
2024-01-18 03:51:36,348 - __main__ - INFO - Fold 1 Epoch 22 Batch 0: Train Loss = 0.2720
2024-01-18 03:52:03,694 - __main__ - INFO - Fold 1, mse = 747.3966, mad = 21.9076
2024-01-18 03:52:04,426 - __main__ - INFO - Fold 1 Epoch 23 Batch 0: Train Loss = 0.3479
2024-01-18 03:52:31,391 - __main__ - INFO - Fold 1, mse = 771.9932, mad = 22.0597
2024-01-18 03:52:32,106 - __main__ - INFO - Fold 1 Epoch 24 Batch 0: Train Loss = 0.3334
2024-01-18 03:52:58,907 - __main__ - INFO - Fold 1, mse = 712.7209, mad = 21.1822
2024-01-18 03:52:59,645 - __main__ - INFO - Fold 1 Epoch 25 Batch 0: Train Loss = 0.2877
2024-01-18 03:53:26,839 - __main__ - INFO - Fold 1, mse = 811.9258, mad = 21.9256
2024-01-18 03:53:27,517 - __main__ - INFO - Fold 1 Epoch 26 Batch 0: Train Loss = 0.3162
2024-01-18 03:53:54,943 - __main__ - INFO - Fold 1, mse = 770.7056, mad = 21.8011
2024-01-18 03:53:55,604 - __main__ - INFO - Fold 1 Epoch 27 Batch 0: Train Loss = 0.2775
2024-01-18 03:54:24,814 - __main__ - INFO - Fold 1, mse = 768.4240, mad = 22.0765
2024-01-18 03:54:25,492 - __main__ - INFO - Fold 1 Epoch 28 Batch 0: Train Loss = 0.2521
2024-01-18 03:54:53,836 - __main__ - INFO - Fold 1, mse = 773.3383, mad = 22.1945
2024-01-18 03:54:54,612 - __main__ - INFO - Fold 1 Epoch 29 Batch 0: Train Loss = 0.2731
2024-01-18 03:55:20,688 - __main__ - INFO - Fold 1, mse = 829.9516, mad = 22.7900
2024-01-18 03:55:21,613 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 3.1219
2024-01-18 03:55:47,792 - __main__ - INFO - Fold 2, epoch 0: Loss = 1.2377 Valid loss = 0.8283 MSE = 617.3108
2024-01-18 03:55:47,793 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 617.3108 ------------
2024-01-18 03:55:47,937 - __main__ - INFO - Fold 2, mse = 617.3108, mad = 19.4566
2024-01-18 03:55:48,582 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 0.9691
2024-01-18 03:56:14,324 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 594.2239 ------------
2024-01-18 03:56:14,491 - __main__ - INFO - Fold 2, mse = 594.2239, mad = 19.6498
2024-01-18 03:56:15,177 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 0.8437
2024-01-18 03:56:42,051 - __main__ - INFO - Fold 2, mse = 604.2441, mad = 20.0802
2024-01-18 03:56:42,711 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 0.7479
2024-01-18 03:57:11,256 - __main__ - INFO - Fold 2, mse = 603.3506, mad = 19.8868
2024-01-18 03:57:12,008 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 0.7505
2024-01-18 03:57:40,998 - __main__ - INFO - Fold 2, mse = 615.5538, mad = 19.9606
2024-01-18 03:57:41,745 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 0.7502
2024-01-18 03:58:10,341 - __main__ - INFO - Fold 2, mse = 610.6928, mad = 19.7925
2024-01-18 03:58:11,010 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 0.5777
2024-01-18 03:58:39,413 - __main__ - INFO - Fold 2, mse = 595.8875, mad = 19.4010
2024-01-18 03:58:40,136 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 0.6348
2024-01-18 03:59:08,620 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 587.9373 ------------
2024-01-18 03:59:08,763 - __main__ - INFO - Fold 2, mse = 587.9373, mad = 19.2046
2024-01-18 03:59:09,647 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 0.6836
2024-01-18 03:59:38,396 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 555.0865 ------------
2024-01-18 03:59:38,694 - __main__ - INFO - ------------ Save best model - MSE: 555.0865 ------------
2024-01-18 03:59:38,695 - __main__ - INFO - Fold 2, mse = 555.0865, mad = 18.4368
2024-01-18 03:59:39,422 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 0.5686
2024-01-18 04:00:09,492 - __main__ - INFO - Fold 2, mse = 574.9761, mad = 18.5478
2024-01-18 04:00:10,230 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 0.4531
2024-01-18 04:00:39,822 - __main__ - INFO - Fold 2, epoch 10: Loss = 0.4845 Valid loss = 0.8390 MSE = 632.2946
2024-01-18 04:00:39,824 - __main__ - INFO - Fold 2, mse = 632.2946, mad = 19.9781
2024-01-18 04:00:40,545 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 0.4588
2024-01-18 04:01:10,809 - __main__ - INFO - Fold 2, mse = 632.8797, mad = 19.7894
2024-01-18 04:01:11,615 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 0.3847
2024-01-18 04:01:39,888 - __main__ - INFO - Fold 2, mse = 695.9867, mad = 21.0038
2024-01-18 04:01:40,579 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 0.4436
2024-01-18 04:02:08,110 - __main__ - INFO - Fold 2, mse = 685.2644, mad = 20.6039
2024-01-18 04:02:08,826 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 0.4079
2024-01-18 04:02:36,151 - __main__ - INFO - Fold 2, mse = 720.8159, mad = 21.2110
2024-01-18 04:02:36,982 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 0.3927
2024-01-18 04:03:04,973 - __main__ - INFO - Fold 2, mse = 716.0256, mad = 21.1415
2024-01-18 04:03:05,615 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 0.3356
2024-01-18 04:03:33,614 - __main__ - INFO - Fold 2, mse = 650.2528, mad = 20.0189
2024-01-18 04:03:34,281 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 0.3047
2024-01-18 04:04:03,072 - __main__ - INFO - Fold 2, mse = 642.7753, mad = 19.8500
2024-01-18 04:04:03,784 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 0.3537
2024-01-18 04:04:31,469 - __main__ - INFO - Fold 2, mse = 695.2128, mad = 21.0597
2024-01-18 04:04:32,226 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 0.3042
2024-01-18 04:05:00,286 - __main__ - INFO - Fold 2, mse = 673.5206, mad = 20.3901
2024-01-18 04:05:01,142 - __main__ - INFO - Fold 2 Epoch 20 Batch 0: Train Loss = 0.2998
2024-01-18 04:05:29,780 - __main__ - INFO - Fold 2, epoch 20: Loss = 0.3290 Valid loss = 1.0229 MSE = 776.9028
2024-01-18 04:05:29,781 - __main__ - INFO - Fold 2, mse = 776.9028, mad = 22.4786
2024-01-18 04:05:30,549 - __main__ - INFO - Fold 2 Epoch 21 Batch 0: Train Loss = 0.2924
2024-01-18 04:05:58,605 - __main__ - INFO - Fold 2, mse = 813.9379, mad = 22.7324
2024-01-18 04:05:59,430 - __main__ - INFO - Fold 2 Epoch 22 Batch 0: Train Loss = 0.3500
2024-01-18 04:06:28,572 - __main__ - INFO - Fold 2, mse = 853.2949, mad = 23.2234
2024-01-18 04:06:29,306 - __main__ - INFO - Fold 2 Epoch 23 Batch 0: Train Loss = 0.2972
2024-01-18 04:06:58,567 - __main__ - INFO - Fold 2, mse = 745.6588, mad = 21.5303
2024-01-18 04:06:59,263 - __main__ - INFO - Fold 2 Epoch 24 Batch 0: Train Loss = 0.2921
2024-01-18 04:07:27,712 - __main__ - INFO - Fold 2, mse = 760.8229, mad = 22.0157
2024-01-18 04:07:28,459 - __main__ - INFO - Fold 2 Epoch 25 Batch 0: Train Loss = 0.3175
2024-01-18 04:07:56,922 - __main__ - INFO - Fold 2, mse = 723.5786, mad = 21.5554
2024-01-18 04:07:57,598 - __main__ - INFO - Fold 2 Epoch 26 Batch 0: Train Loss = 0.2586
2024-01-18 04:08:26,073 - __main__ - INFO - Fold 2, mse = 807.5113, mad = 22.3073
2024-01-18 04:08:26,884 - __main__ - INFO - Fold 2 Epoch 27 Batch 0: Train Loss = 0.2943
2024-01-18 04:08:54,462 - __main__ - INFO - Fold 2, mse = 741.2123, mad = 21.4290
2024-01-18 04:08:55,140 - __main__ - INFO - Fold 2 Epoch 28 Batch 0: Train Loss = 0.2771
2024-01-18 04:09:24,485 - __main__ - INFO - Fold 2, mse = 735.2908, mad = 21.2975
2024-01-18 04:09:25,170 - __main__ - INFO - Fold 2 Epoch 29 Batch 0: Train Loss = 0.2487
2024-01-18 04:09:54,299 - __main__ - INFO - Fold 2, mse = 697.9582, mad = 20.7234
2024-01-18 04:09:55,220 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 2.8337
2024-01-18 04:10:20,944 - __main__ - INFO - Fold 3, epoch 0: Loss = 1.1585 Valid loss = 0.9363 MSE = 667.9150
2024-01-18 04:10:20,944 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 667.9150 ------------
2024-01-18 04:10:21,067 - __main__ - INFO - Fold 3, mse = 667.9150, mad = 21.2183
2024-01-18 04:10:21,715 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 1.1091
2024-01-18 04:10:47,474 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 663.0254 ------------
2024-01-18 04:10:47,586 - __main__ - INFO - Fold 3, mse = 663.0254, mad = 20.8833
2024-01-18 04:10:48,187 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 0.9268
2024-01-18 04:11:12,551 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 651.3325 ------------
2024-01-18 04:11:12,677 - __main__ - INFO - Fold 3, mse = 651.3325, mad = 20.5713
2024-01-18 04:11:13,345 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 0.9297
2024-01-18 04:11:38,160 - __main__ - INFO - Fold 3, mse = 691.0730, mad = 21.4521
2024-01-18 04:11:38,810 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 1.0048
2024-01-18 04:12:02,640 - __main__ - INFO - Fold 3, mse = 695.9465, mad = 21.6844
2024-01-18 04:12:03,282 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 0.8001
2024-01-18 04:12:27,371 - __main__ - INFO - Fold 3, mse = 721.5582, mad = 22.0325
2024-01-18 04:12:28,056 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 0.8016
2024-01-18 04:12:53,805 - __main__ - INFO - Fold 3, mse = 751.3657, mad = 22.4905
2024-01-18 04:12:54,442 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 0.6891
2024-01-18 04:13:20,338 - __main__ - INFO - Fold 3, mse = 786.0536, mad = 22.7657
2024-01-18 04:13:21,042 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 0.5688
2024-01-18 04:13:47,846 - __main__ - INFO - Fold 3, mse = 726.6764, mad = 21.8961
2024-01-18 04:13:48,406 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 0.6548
2024-01-18 04:14:12,398 - __main__ - INFO - Fold 3, mse = 738.1213, mad = 21.7392
2024-01-18 04:14:12,938 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 0.5791
2024-01-18 04:14:38,248 - __main__ - INFO - Fold 3, epoch 10: Loss = 0.5289 Valid loss = 1.1007 MSE = 780.0484
2024-01-18 04:14:38,249 - __main__ - INFO - Fold 3, mse = 780.0484, mad = 22.2823
2024-01-18 04:14:38,797 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 0.4894
2024-01-18 04:15:04,138 - __main__ - INFO - Fold 3, mse = 761.1281, mad = 21.7705
2024-01-18 04:15:04,815 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 0.4274
2024-01-18 04:15:31,061 - __main__ - INFO - Fold 3, mse = 859.1863, mad = 23.5003
2024-01-18 04:15:31,788 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 0.4355
2024-01-18 04:15:57,632 - __main__ - INFO - Fold 3, mse = 919.7292, mad = 23.9223
2024-01-18 04:15:58,343 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 0.4869
2024-01-18 04:16:23,785 - __main__ - INFO - Fold 3, mse = 875.8246, mad = 23.0522
2024-01-18 04:16:24,543 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 0.4493
2024-01-18 04:16:50,552 - __main__ - INFO - Fold 3, mse = 896.1797, mad = 23.1042
2024-01-18 04:16:51,122 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 0.3594
2024-01-18 04:17:16,786 - __main__ - INFO - Fold 3, mse = 857.9232, mad = 22.3019
2024-01-18 04:17:17,426 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 0.3883
2024-01-18 04:17:43,217 - __main__ - INFO - Fold 3, mse = 913.5757, mad = 23.3045
2024-01-18 04:17:43,802 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 0.4473
2024-01-18 04:18:08,369 - __main__ - INFO - Fold 3, mse = 909.4342, mad = 23.3502
2024-01-18 04:18:08,955 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 0.3041
2024-01-18 04:18:33,745 - __main__ - INFO - Fold 3, mse = 854.3530, mad = 22.1621
2024-01-18 04:18:34,478 - __main__ - INFO - Fold 3 Epoch 20 Batch 0: Train Loss = 0.3085
2024-01-18 04:19:00,505 - __main__ - INFO - Fold 3, epoch 20: Loss = 0.3472 Valid loss = 1.2341 MSE = 868.0351
2024-01-18 04:19:00,507 - __main__ - INFO - Fold 3, mse = 868.0351, mad = 23.1204
2024-01-18 04:19:01,252 - __main__ - INFO - Fold 3 Epoch 21 Batch 0: Train Loss = 0.3428
2024-01-18 04:19:27,423 - __main__ - INFO - Fold 3, mse = 912.7227, mad = 22.7488
2024-01-18 04:19:28,091 - __main__ - INFO - Fold 3 Epoch 22 Batch 0: Train Loss = 0.3308
2024-01-18 04:19:53,576 - __main__ - INFO - Fold 3, mse = 932.8720, mad = 22.9459
2024-01-18 04:19:54,120 - __main__ - INFO - Fold 3 Epoch 23 Batch 0: Train Loss = 0.2703
2024-01-18 04:20:18,168 - __main__ - INFO - Fold 3, mse = 908.5816, mad = 22.9179
2024-01-18 04:20:18,757 - __main__ - INFO - Fold 3 Epoch 24 Batch 0: Train Loss = 0.2047
2024-01-18 04:20:43,005 - __main__ - INFO - Fold 3, mse = 864.1048, mad = 23.0641
2024-01-18 04:20:43,632 - __main__ - INFO - Fold 3 Epoch 25 Batch 0: Train Loss = 0.3139
2024-01-18 04:21:07,638 - __main__ - INFO - Fold 3, mse = 908.0766, mad = 23.0330
2024-01-18 04:21:08,270 - __main__ - INFO - Fold 3 Epoch 26 Batch 0: Train Loss = 0.2366
2024-01-18 04:21:32,498 - __main__ - INFO - Fold 3, mse = 908.3441, mad = 22.9470
2024-01-18 04:21:33,213 - __main__ - INFO - Fold 3 Epoch 27 Batch 0: Train Loss = 0.3273
2024-01-18 04:21:59,169 - __main__ - INFO - Fold 3, mse = 875.9701, mad = 22.6731
2024-01-18 04:21:59,901 - __main__ - INFO - Fold 3 Epoch 28 Batch 0: Train Loss = 0.2711
2024-01-18 04:22:25,281 - __main__ - INFO - Fold 3, mse = 819.5135, mad = 22.1105
2024-01-18 04:22:25,998 - __main__ - INFO - Fold 3 Epoch 29 Batch 0: Train Loss = 0.2138
2024-01-18 04:22:51,480 - __main__ - INFO - Fold 3, mse = 793.2231, mad = 21.7918
2024-01-18 04:22:52,230 - __main__ - INFO - Fold 4 Epoch 0 Batch 0: Train Loss = 2.7773
2024-01-18 04:23:16,524 - __main__ - INFO - Fold 4, epoch 0: Loss = 1.1996 Valid loss = 1.1649 MSE = 814.7362
2024-01-18 04:23:16,525 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 814.7362 ------------
2024-01-18 04:23:16,663 - __main__ - INFO - Fold 4, mse = 814.7362, mad = 23.3736
2024-01-18 04:23:17,252 - __main__ - INFO - Fold 4 Epoch 1 Batch 0: Train Loss = 0.9039
2024-01-18 04:23:40,595 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 766.3684 ------------
2024-01-18 04:23:40,732 - __main__ - INFO - Fold 4, mse = 766.3684, mad = 22.0928
2024-01-18 04:23:41,345 - __main__ - INFO - Fold 4 Epoch 2 Batch 0: Train Loss = 0.8299
2024-01-18 04:24:05,307 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 728.0518 ------------
2024-01-18 04:24:05,494 - __main__ - INFO - Fold 4, mse = 728.0518, mad = 21.3949
2024-01-18 04:24:06,099 - __main__ - INFO - Fold 4 Epoch 3 Batch 0: Train Loss = 0.7838
2024-01-18 04:24:28,064 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 720.4616 ------------
2024-01-18 04:24:28,200 - __main__ - INFO - Fold 4, mse = 720.4616, mad = 21.0418
2024-01-18 04:24:28,829 - __main__ - INFO - Fold 4 Epoch 4 Batch 0: Train Loss = 0.6907
2024-01-18 04:24:50,961 - __main__ - INFO - Fold 4, mse = 738.2530, mad = 21.2898
2024-01-18 04:24:51,650 - __main__ - INFO - Fold 4 Epoch 5 Batch 0: Train Loss = 0.6992
2024-01-18 04:25:13,364 - __main__ - INFO - Fold 4, mse = 726.7340, mad = 21.2623
2024-01-18 04:25:13,920 - __main__ - INFO - Fold 4 Epoch 6 Batch 0: Train Loss = 0.5825
2024-01-18 04:25:37,180 - __main__ - INFO - Fold 4, mse = 733.5908, mad = 21.2519
2024-01-18 04:25:37,723 - __main__ - INFO - Fold 4 Epoch 7 Batch 0: Train Loss = 0.5983
2024-01-18 04:25:59,610 - __main__ - INFO - Fold 4, mse = 795.9987, mad = 21.8466
2024-01-18 04:26:00,386 - __main__ - INFO - Fold 4 Epoch 8 Batch 0: Train Loss = 0.6453
2024-01-18 04:26:22,829 - __main__ - INFO - Fold 4, mse = 776.8979, mad = 21.5629
2024-01-18 04:26:23,382 - __main__ - INFO - Fold 4 Epoch 9 Batch 0: Train Loss = 0.5352
2024-01-18 04:26:45,044 - __main__ - INFO - Fold 4, mse = 833.3473, mad = 22.4214
2024-01-18 04:26:45,605 - __main__ - INFO - Fold 4 Epoch 10 Batch 0: Train Loss = 0.5273
2024-01-18 04:27:06,554 - __main__ - INFO - Fold 4, epoch 10: Loss = 0.5149 Valid loss = 1.2006 MSE = 885.3663
2024-01-18 04:27:06,555 - __main__ - INFO - Fold 4, mse = 885.3663, mad = 23.7331
2024-01-18 04:27:07,156 - __main__ - INFO - Fold 4 Epoch 11 Batch 0: Train Loss = 0.4979
2024-01-18 04:27:29,961 - __main__ - INFO - Fold 4, mse = 887.7238, mad = 23.6891
2024-01-18 04:27:30,543 - __main__ - INFO - Fold 4 Epoch 12 Batch 0: Train Loss = 0.4822
2024-01-18 04:27:53,235 - __main__ - INFO - Fold 4, mse = 822.3675, mad = 22.5056
2024-01-18 04:27:53,839 - __main__ - INFO - Fold 4 Epoch 13 Batch 0: Train Loss = 0.4191
2024-01-18 04:28:16,325 - __main__ - INFO - Fold 4, mse = 832.8831, mad = 22.5213
2024-01-18 04:28:16,950 - __main__ - INFO - Fold 4 Epoch 14 Batch 0: Train Loss = 0.4435
2024-01-18 04:28:38,679 - __main__ - INFO - Fold 4, mse = 865.5543, mad = 23.1524
2024-01-18 04:28:39,294 - __main__ - INFO - Fold 4 Epoch 15 Batch 0: Train Loss = 0.4437
2024-01-18 04:29:03,124 - __main__ - INFO - Fold 4, mse = 983.3482, mad = 24.6223
2024-01-18 04:29:03,735 - __main__ - INFO - Fold 4 Epoch 16 Batch 0: Train Loss = 0.4259
2024-01-18 04:29:27,704 - __main__ - INFO - Fold 4, mse = 972.9542, mad = 24.6439
2024-01-18 04:29:28,280 - __main__ - INFO - Fold 4 Epoch 17 Batch 0: Train Loss = 0.4340
2024-01-18 04:29:52,493 - __main__ - INFO - Fold 4, mse = 988.1112, mad = 24.8161
2024-01-18 04:29:53,113 - __main__ - INFO - Fold 4 Epoch 18 Batch 0: Train Loss = 0.3732
2024-01-18 04:30:18,721 - __main__ - INFO - Fold 4, mse = 1022.5471, mad = 25.2577
2024-01-18 04:30:19,456 - __main__ - INFO - Fold 4 Epoch 19 Batch 0: Train Loss = 0.3536
2024-01-18 04:30:45,761 - __main__ - INFO - Fold 4, mse = 937.3035, mad = 24.0850
2024-01-18 04:30:46,483 - __main__ - INFO - Fold 4 Epoch 20 Batch 0: Train Loss = 0.2675
2024-01-18 04:31:11,952 - __main__ - INFO - Fold 4, epoch 20: Loss = 0.3209 Valid loss = 1.4126 MSE = 1003.7426
2024-01-18 04:31:11,954 - __main__ - INFO - Fold 4, mse = 1003.7426, mad = 24.8558
2024-01-18 04:31:12,796 - __main__ - INFO - Fold 4 Epoch 21 Batch 0: Train Loss = 0.3594
2024-01-18 04:31:37,724 - __main__ - INFO - Fold 4, mse = 924.8432, mad = 23.8374
2024-01-18 04:31:38,330 - __main__ - INFO - Fold 4 Epoch 22 Batch 0: Train Loss = 0.2922
2024-01-18 04:32:02,223 - __main__ - INFO - Fold 4, mse = 956.7878, mad = 24.3153
2024-01-18 04:32:02,835 - __main__ - INFO - Fold 4 Epoch 23 Batch 0: Train Loss = 0.3520
2024-01-18 04:32:27,000 - __main__ - INFO - Fold 4, mse = 1000.8610, mad = 24.7619
2024-01-18 04:32:27,667 - __main__ - INFO - Fold 4 Epoch 24 Batch 0: Train Loss = 0.3486
2024-01-18 04:32:52,129 - __main__ - INFO - Fold 4, mse = 1033.8463, mad = 25.0739
2024-01-18 04:32:52,778 - __main__ - INFO - Fold 4 Epoch 25 Batch 0: Train Loss = 0.2896
2024-01-18 04:33:18,890 - __main__ - INFO - Fold 4, mse = 1004.8170, mad = 24.5429
2024-01-18 04:33:19,562 - __main__ - INFO - Fold 4 Epoch 26 Batch 0: Train Loss = 0.2845
2024-01-18 04:33:42,643 - __main__ - INFO - Fold 4, mse = 1006.4578, mad = 24.8003
2024-01-18 04:33:43,246 - __main__ - INFO - Fold 4 Epoch 27 Batch 0: Train Loss = 0.2821
2024-01-18 04:34:05,895 - __main__ - INFO - Fold 4, mse = 994.4954, mad = 24.5294
2024-01-18 04:34:06,521 - __main__ - INFO - Fold 4 Epoch 28 Batch 0: Train Loss = 0.2935
2024-01-18 04:34:28,353 - __main__ - INFO - Fold 4, mse = 978.0365, mad = 24.2936
2024-01-18 04:34:28,982 - __main__ - INFO - Fold 4 Epoch 29 Batch 0: Train Loss = 0.2556
2024-01-18 04:34:51,231 - __main__ - INFO - Fold 4, mse = 995.4212, mad = 24.5149
2024-01-18 04:34:51,957 - __main__ - INFO - Fold 5 Epoch 0 Batch 0: Train Loss = 11.6836
2024-01-18 04:35:15,778 - __main__ - INFO - Fold 5, epoch 0: Loss = 1.9403 Valid loss = 1.0243 MSE = 727.5351
2024-01-18 04:35:15,779 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 727.5351 ------------
2024-01-18 04:35:15,898 - __main__ - INFO - Fold 5, mse = 727.5351, mad = 22.4998
2024-01-18 04:35:16,543 - __main__ - INFO - Fold 5 Epoch 1 Batch 0: Train Loss = 0.9441
2024-01-18 04:35:42,150 - __main__ - INFO - Fold 5, mse = 731.1314, mad = 22.5651
2024-01-18 04:35:42,805 - __main__ - INFO - Fold 5 Epoch 2 Batch 0: Train Loss = 0.9235
2024-01-18 04:36:08,094 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 709.3266 ------------
2024-01-18 04:36:08,210 - __main__ - INFO - Fold 5, mse = 709.3266, mad = 22.0324
2024-01-18 04:36:08,903 - __main__ - INFO - Fold 5 Epoch 3 Batch 0: Train Loss = 0.9287
2024-01-18 04:36:32,785 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 684.3042 ------------
2024-01-18 04:36:32,905 - __main__ - INFO - Fold 5, mse = 684.3042, mad = 21.4376
2024-01-18 04:36:33,463 - __main__ - INFO - Fold 5 Epoch 4 Batch 0: Train Loss = 0.8901
2024-01-18 04:36:59,271 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 664.5212 ------------
2024-01-18 04:36:59,413 - __main__ - INFO - Fold 5, mse = 664.5212, mad = 20.9047
2024-01-18 04:37:00,140 - __main__ - INFO - Fold 5 Epoch 5 Batch 0: Train Loss = 0.8279
2024-01-18 04:37:25,263 - __main__ - INFO - Fold 5, mse = 699.4239, mad = 20.5319
2024-01-18 04:37:25,942 - __main__ - INFO - Fold 5 Epoch 6 Batch 0: Train Loss = 0.7861
2024-01-18 04:37:50,813 - __main__ - INFO - Fold 5, mse = 670.1216, mad = 20.2697
2024-01-18 04:37:51,385 - __main__ - INFO - Fold 5 Epoch 7 Batch 0: Train Loss = 0.7726
2024-01-18 04:38:17,332 - __main__ - INFO - Fold 5, mse = 685.0527, mad = 20.4384
2024-01-18 04:38:18,069 - __main__ - INFO - Fold 5 Epoch 8 Batch 0: Train Loss = 0.6364
2024-01-18 04:38:43,764 - __main__ - INFO - Fold 5, mse = 698.1619, mad = 20.5557
2024-01-18 04:38:44,463 - __main__ - INFO - Fold 5 Epoch 9 Batch 0: Train Loss = 0.5894
2024-01-18 04:39:12,033 - __main__ - INFO - Fold 5, mse = 712.6076, mad = 20.6583
2024-01-18 04:39:12,731 - __main__ - INFO - Fold 5 Epoch 10 Batch 0: Train Loss = 0.5450
2024-01-18 04:39:38,429 - __main__ - INFO - Fold 5, epoch 10: Loss = 0.5429 Valid loss = 1.0601 MSE = 754.7579
2024-01-18 04:39:38,430 - __main__ - INFO - Fold 5, mse = 754.7579, mad = 21.0663
2024-01-18 04:39:39,167 - __main__ - INFO - Fold 5 Epoch 11 Batch 0: Train Loss = 0.5268
2024-01-18 04:40:06,416 - __main__ - INFO - Fold 5, mse = 765.8580, mad = 21.0601
2024-01-18 04:40:07,110 - __main__ - INFO - Fold 5 Epoch 12 Batch 0: Train Loss = 0.4199
2024-01-18 04:40:32,940 - __main__ - INFO - Fold 5, mse = 714.4261, mad = 20.4555
2024-01-18 04:40:33,543 - __main__ - INFO - Fold 5 Epoch 13 Batch 0: Train Loss = 0.4891
2024-01-18 04:40:58,623 - __main__ - INFO - Fold 5, mse = 803.3657, mad = 21.8162
2024-01-18 04:40:59,242 - __main__ - INFO - Fold 5 Epoch 14 Batch 0: Train Loss = 0.4052
2024-01-18 04:41:24,382 - __main__ - INFO - Fold 5, mse = 821.5710, mad = 21.9022
2024-01-18 04:41:25,005 - __main__ - INFO - Fold 5 Epoch 15 Batch 0: Train Loss = 0.3017
2024-01-18 04:41:51,184 - __main__ - INFO - Fold 5, mse = 777.8456, mad = 21.9159
2024-01-18 04:41:51,801 - __main__ - INFO - Fold 5 Epoch 16 Batch 0: Train Loss = 0.3989
2024-01-18 04:42:16,653 - __main__ - INFO - Fold 5, mse = 780.7434, mad = 21.4408
2024-01-18 04:42:17,226 - __main__ - INFO - Fold 5 Epoch 17 Batch 0: Train Loss = 0.4045
2024-01-18 04:42:43,139 - __main__ - INFO - Fold 5, mse = 819.1740, mad = 21.7509
2024-01-18 04:42:43,927 - __main__ - INFO - Fold 5 Epoch 18 Batch 0: Train Loss = 0.3571
2024-01-18 04:43:07,910 - __main__ - INFO - Fold 5, mse = 888.6389, mad = 22.8976
2024-01-18 04:43:08,436 - __main__ - INFO - Fold 5 Epoch 19 Batch 0: Train Loss = 0.3752
2024-01-18 04:43:29,682 - __main__ - INFO - Fold 5, mse = 825.5850, mad = 21.5607
2024-01-18 04:43:30,436 - __main__ - INFO - Fold 5 Epoch 20 Batch 0: Train Loss = 0.3410
2024-01-18 04:43:52,417 - __main__ - INFO - Fold 5, epoch 20: Loss = 0.3266 Valid loss = 1.2443 MSE = 876.5461
2024-01-18 04:43:52,418 - __main__ - INFO - Fold 5, mse = 876.5461, mad = 22.3708
2024-01-18 04:43:52,968 - __main__ - INFO - Fold 5 Epoch 21 Batch 0: Train Loss = 0.4027
2024-01-18 04:44:15,154 - __main__ - INFO - Fold 5, mse = 871.7607, mad = 22.5842
2024-01-18 04:44:15,894 - __main__ - INFO - Fold 5 Epoch 22 Batch 0: Train Loss = 0.3168
2024-01-18 04:44:36,585 - __main__ - INFO - Fold 5, mse = 953.9296, mad = 23.0041
2024-01-18 04:44:37,155 - __main__ - INFO - Fold 5 Epoch 23 Batch 0: Train Loss = 0.2765
2024-01-18 04:44:57,691 - __main__ - INFO - Fold 5, mse = 916.2847, mad = 23.0687
2024-01-18 04:44:58,241 - __main__ - INFO - Fold 5 Epoch 24 Batch 0: Train Loss = 0.2346
2024-01-18 04:45:19,568 - __main__ - INFO - Fold 5, mse = 928.9622, mad = 23.3516
2024-01-18 04:45:20,139 - __main__ - INFO - Fold 5 Epoch 25 Batch 0: Train Loss = 0.3012
2024-01-18 04:45:42,614 - __main__ - INFO - Fold 5, mse = 961.7237, mad = 23.2242
2024-01-18 04:45:43,179 - __main__ - INFO - Fold 5 Epoch 26 Batch 0: Train Loss = 0.3144
2024-01-18 04:46:04,726 - __main__ - INFO - Fold 5, mse = 958.5640, mad = 23.0638
2024-01-18 04:46:05,416 - __main__ - INFO - Fold 5 Epoch 27 Batch 0: Train Loss = 0.1974
2024-01-18 04:46:27,288 - __main__ - INFO - Fold 5, mse = 958.3108, mad = 23.3932
2024-01-18 04:46:27,879 - __main__ - INFO - Fold 5 Epoch 28 Batch 0: Train Loss = 0.2059
2024-01-18 04:46:50,594 - __main__ - INFO - Fold 5, mse = 968.6820, mad = 23.4663
2024-01-18 04:46:51,158 - __main__ - INFO - Fold 5 Epoch 29 Batch 0: Train Loss = 0.2192
2024-01-18 04:47:11,869 - __main__ - INFO - Fold 5, mse = 986.8008, mad = 23.8058
2024-01-18 04:47:11,870 - __main__ - INFO - mse 633.0518(60.9058)
2024-01-18 04:47:11,871 - __main__ - INFO - mad 20.0729(0.9995)
