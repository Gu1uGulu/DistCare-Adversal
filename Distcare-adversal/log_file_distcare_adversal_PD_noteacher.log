2024-03-21 10:30:12,555 - __main__ - INFO - 这是希望输出的info内容
2024-03-21 10:30:12,555 - __main__ - WARNING - 这是希望输出的warning内容
2024-03-21 10:31:24,358 - __main__ - INFO - 32269
2024-03-21 10:31:24,358 - __main__ - INFO - 4034
2024-03-21 10:31:24,358 - __main__ - INFO - 4033
2024-03-21 10:31:44,199 - __main__ - INFO - Batch 0: Test Loss = 0.0931
2024-03-21 10:31:49,098 - __main__ - INFO - 
==>Predicting on test
2024-03-21 10:31:49,098 - __main__ - INFO - Test Loss = 0.1227
2024-03-21 10:31:49,236 - __main__ - INFO - load target data
2024-03-21 10:31:49,929 - __main__ - INFO - Batch 0: Test Loss = 0.1131
2024-03-21 10:31:55,886 - __main__ - INFO - Batch 20: Test Loss = 0.1478
2024-03-21 10:32:02,076 - __main__ - INFO - Batch 40: Test Loss = 0.1526
2024-03-21 10:32:08,184 - __main__ - INFO - Batch 60: Test Loss = 0.1207
2024-03-21 10:32:14,437 - __main__ - INFO - Batch 80: Test Loss = 0.0851
2024-03-21 10:32:20,872 - __main__ - INFO - Batch 100: Test Loss = 0.0724
2024-03-21 10:32:26,690 - __main__ - INFO - Batch 120: Test Loss = 0.1097
2024-03-21 10:32:28,131 - __main__ - INFO - Training Student
2024-03-21 10:32:29,242 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 1.6692
2024-03-21 10:32:48,187 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 1.4538
2024-03-21 10:33:05,411 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 1.5292
2024-03-21 10:33:24,163 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 1.5048
2024-03-21 10:33:43,427 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 1.4668
2024-03-21 10:34:01,185 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 1.4565
2024-03-21 10:34:19,580 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 1.4831
2024-03-21 10:34:25,218 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4566 ------------
2024-03-21 10:34:27,456 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4565 ------------
2024-03-21 10:34:29,475 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4483 ------------
2024-03-21 10:34:31,528 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4445 ------------
2024-03-21 10:34:35,334 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 1.4684
2024-03-21 10:34:55,258 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 1.5233
2024-03-21 10:35:13,529 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 1.4927
2024-03-21 10:35:31,593 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 1.4909
2024-03-21 10:35:49,957 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 1.4460
2024-03-21 10:36:10,297 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 1.4483
2024-03-21 10:36:31,651 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 1.4637
2024-03-21 10:36:49,728 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 1.4769
2024-03-21 10:37:09,648 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 1.4709
2024-03-21 10:37:28,286 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 1.4787
2024-03-21 10:37:46,996 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 1.4897
2024-03-21 10:38:05,570 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 1.4633
2024-03-21 10:38:24,105 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 1.4623
2024-03-21 10:38:41,853 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 1.4785
2024-03-21 10:38:56,770 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 1.4901
2024-03-21 10:39:15,429 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 1.4814
2024-03-21 10:39:33,721 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 1.4825
2024-03-21 10:39:52,302 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 1.4911
2024-03-21 10:40:10,926 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 1.4549
2024-03-21 10:40:28,927 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 1.4511
2024-03-21 10:40:48,319 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 1.4759
2024-03-21 10:41:05,024 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 1.4741
2024-03-21 10:41:26,550 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 1.4627
2024-03-21 10:41:45,825 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 1.4807
2024-03-21 10:42:03,806 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 1.4927
2024-03-21 10:42:22,507 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 1.4553
2024-03-21 10:42:41,127 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 1.4417
2024-03-21 10:43:00,638 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 1.4587
2024-03-21 10:43:06,251 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4428 ------------
2024-03-21 10:43:12,161 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4422 ------------
2024-03-21 10:43:15,611 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 1.4646
2024-03-21 10:43:34,403 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 1.4618
2024-03-21 10:43:52,465 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 1.4680
2024-03-21 10:44:10,139 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 1.4816
2024-03-21 10:44:29,591 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 1.4505
2024-03-21 10:44:48,195 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 1.4398
2024-03-21 10:45:07,353 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 1.4576
2024-03-21 10:45:22,198 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 1.4626
2024-03-21 10:45:41,232 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 1.4646
2024-03-21 10:45:59,029 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 1.4655
2024-03-21 10:46:18,135 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 1.4810
2024-03-21 10:46:36,222 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 1.4472
2024-03-21 10:46:54,508 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 1.4407
2024-03-21 10:47:12,977 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 1.4540
2024-03-21 10:47:18,578 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4419 ------------
2024-03-21 10:47:27,991 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 1.4623
2024-03-21 10:47:47,298 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 1.4647
2024-03-21 10:48:05,210 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 1.4683
2024-03-21 10:48:22,941 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 1.4779
2024-03-21 10:48:41,068 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 1.4468
2024-03-21 10:48:58,634 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 1.4396
2024-03-21 10:49:17,867 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 1.4545
2024-03-21 10:49:23,222 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4409 ------------
2024-03-21 10:49:29,112 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4404 ------------
2024-03-21 10:49:32,614 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 1.4615
2024-03-21 10:49:51,716 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 1.4625
2024-03-21 10:50:09,424 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 1.4668
2024-03-21 10:50:27,713 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 1.4765
2024-03-21 10:50:45,475 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 1.4468
2024-03-21 10:51:03,005 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 1.4406
2024-03-21 10:51:22,050 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 1.4553
2024-03-21 10:51:37,001 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 1.4617
2024-03-21 10:51:56,115 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 1.4653
2024-03-21 10:52:13,962 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 1.4673
2024-03-21 10:52:32,066 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 1.4778
2024-03-21 10:52:50,011 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 1.4483
2024-03-21 10:53:08,957 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 1.4403
2024-03-21 10:53:27,685 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 1.4544
2024-03-21 10:53:33,493 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4387 ------------
2024-03-21 10:53:43,792 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 1.4600
2024-03-21 10:54:03,645 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 1.4639
2024-03-21 10:54:21,925 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 1.4676
2024-03-21 10:54:39,943 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 1.4774
2024-03-21 10:54:58,416 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 1.4465
2024-03-21 10:55:17,941 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 1.4410
2024-03-21 10:55:35,907 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 1.4544
2024-03-21 10:55:50,994 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 1.4613
2024-03-21 10:56:09,325 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 1.4618
2024-03-21 10:56:26,699 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 1.4646
2024-03-21 10:56:45,229 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 1.4795
2024-03-21 10:57:03,909 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 1.4472
2024-03-21 10:57:23,475 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 1.4415
2024-03-21 10:57:41,717 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 1.4532
2024-03-21 10:57:56,498 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 1.4598
2024-03-21 10:58:15,068 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 1.4650
2024-03-21 10:58:33,884 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 1.4666
2024-03-21 10:58:52,500 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 1.4768
2024-03-21 10:59:10,921 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 1.4455
2024-03-21 10:59:30,155 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 1.4407
2024-03-21 10:59:48,734 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 1.4534
2024-03-21 10:59:54,484 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4383 ------------
2024-03-21 11:00:04,435 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 1.4600
2024-03-21 11:00:23,459 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 1.4651
2024-03-21 11:00:41,494 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 1.4688
2024-03-21 11:01:00,629 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 1.4766
2024-03-21 11:01:19,731 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 1.4450
2024-03-21 11:01:40,256 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 1.4401
2024-03-21 11:01:59,647 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 1.4538
2024-03-21 11:02:15,057 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 1.4606
2024-03-21 11:02:34,962 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 1.4640
2024-03-21 11:02:53,970 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 1.4673
2024-03-21 11:03:13,421 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 1.4788
2024-03-21 11:03:35,879 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 1.4466
2024-03-21 11:03:57,651 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 1.4396
2024-03-21 11:04:18,272 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 1.4534
2024-03-21 11:04:24,045 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4382 ------------
2024-03-21 11:04:30,275 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4367 ------------
2024-03-21 11:04:34,804 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 1.4596
2024-03-21 11:04:55,466 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 1.4637
2024-03-21 11:05:14,651 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 1.4687
2024-03-21 11:05:34,101 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 1.4787
2024-03-21 11:05:53,706 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 1.4462
2024-03-21 11:06:13,706 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 1.4394
2024-03-21 11:06:34,422 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 1.4545
2024-03-21 11:06:50,579 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 1.4610
2024-03-21 11:07:11,348 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 1.4651
2024-03-21 11:07:31,891 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 1.4646
2024-03-21 11:07:52,024 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 1.4766
2024-03-21 11:08:14,205 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 1.4464
2024-03-21 11:08:33,175 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 1.4402
2024-03-21 11:08:51,793 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 1.4538
2024-03-21 11:09:07,180 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 1.4591
2024-03-21 11:09:26,501 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 1.4641
2024-03-21 11:09:45,130 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 1.4649
2024-03-21 11:10:05,324 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 1.4785
2024-03-21 11:10:26,132 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 1.4468
2024-03-21 11:10:45,939 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 1.4410
2024-03-21 11:11:05,124 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 1.4526
2024-03-21 11:11:19,517 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 1.4608
2024-03-21 11:11:39,180 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 1.4632
2024-03-21 11:11:59,417 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 1.4680
2024-03-21 11:12:18,949 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 1.4776
2024-03-21 11:12:37,734 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 1.4464
2024-03-21 11:12:56,399 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 1.4386
2024-03-21 11:13:14,930 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 1.4531
2024-03-21 11:13:30,074 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 1.4588
2024-03-21 11:13:50,056 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 1.4622
2024-03-21 11:14:08,828 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 1.4659
2024-03-21 11:14:27,496 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 1.4763
2024-03-21 11:14:45,417 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 1.4478
2024-03-21 11:15:03,379 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 1.4407
2024-03-21 11:15:20,824 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 1.4540
2024-03-21 11:15:34,362 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 1.4619
2024-03-21 11:15:52,040 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 1.4629
2024-03-21 11:16:10,722 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 1.4646
2024-03-21 11:16:29,362 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 1.4782
2024-03-21 11:16:47,101 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 1.4472
2024-03-21 11:17:04,255 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 1.4386
2024-03-21 11:17:21,535 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 1.4517
2024-03-21 11:17:35,913 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 1.4585
2024-03-21 11:17:54,170 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 1.4647
2024-03-21 11:18:11,856 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 1.4654
2024-03-21 11:18:29,055 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 1.4768
2024-03-21 11:18:46,770 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 1.4438
2024-03-21 11:19:03,639 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 1.4410
2024-03-21 11:19:20,427 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 1.4546
2024-03-21 11:19:25,856 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4357 ------------
2024-03-21 11:19:31,146 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4348 ------------
2024-03-21 11:19:34,435 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 1.4566
2024-03-21 11:19:51,285 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 1.4632
2024-03-21 11:20:08,601 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 1.4685
2024-03-21 11:20:25,728 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 1.4779
2024-03-21 11:20:42,504 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 1.4460
2024-03-21 11:20:59,814 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 1.4375
2024-03-21 11:21:16,707 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 1.4476
2024-03-21 11:21:21,906 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4346 ------------
2024-03-21 11:21:30,769 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 1.4555
2024-03-21 11:21:49,030 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 1.4628
2024-03-21 11:22:05,836 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 1.4694
2024-03-21 11:22:22,742 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 1.4782
2024-03-21 11:22:39,131 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 1.4465
2024-03-21 11:22:56,688 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 1.4415
2024-03-21 11:23:14,350 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 1.4519
2024-03-21 11:23:28,353 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 1.4585
2024-03-21 11:23:46,817 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 1.4637
2024-03-21 11:24:03,739 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 1.4680
2024-03-21 11:24:20,486 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 1.4762
2024-03-21 11:24:37,101 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 1.4423
2024-03-21 11:24:54,523 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 1.4420
2024-03-21 11:25:11,581 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 1.4512
2024-03-21 11:25:25,180 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 1.4576
2024-03-21 11:25:43,019 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 1.4604
2024-03-21 11:26:00,327 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 1.4648
2024-03-21 11:26:16,201 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 1.4731
2024-03-21 11:26:33,153 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 1.4478
2024-03-21 11:26:50,286 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 1.4450
2024-03-21 11:27:07,286 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 1.4546
2024-03-21 11:27:21,024 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 1.4609
2024-03-21 11:27:38,698 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 1.4619
2024-03-21 11:27:55,906 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 1.4646
2024-03-21 11:28:12,641 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 1.4752
2024-03-21 11:28:29,958 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 1.4478
2024-03-21 11:28:48,466 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 1.4391
2024-03-21 11:29:06,004 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 1.4475
2024-03-21 11:29:21,122 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 1.4555
2024-03-21 11:29:38,902 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 1.4635
2024-03-21 11:29:56,293 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 1.4704
2024-03-21 11:30:13,899 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 1.4777
2024-03-21 11:30:31,744 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 1.4459
2024-03-21 11:30:49,544 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 1.4394
2024-03-21 11:31:07,285 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 1.4532
2024-03-21 11:31:21,900 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 1.4574
2024-03-21 11:31:39,924 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 1.4593
2024-03-21 11:31:57,237 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 1.4656
2024-03-21 11:32:15,193 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 1.4798
2024-03-21 11:32:33,375 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 1.4466
2024-03-21 11:32:52,272 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 1.4419
2024-03-21 11:33:09,885 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 1.4528
2024-03-21 11:33:25,386 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 1.4588
2024-03-21 11:33:45,263 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 1.4606
2024-03-21 11:34:05,953 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 1.4634
2024-03-21 11:34:26,788 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 1.4741
2024-03-21 11:34:48,651 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 1.4453
2024-03-21 11:35:10,988 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 1.4412
2024-03-21 11:35:31,296 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 1.4527
2024-03-21 11:35:45,702 - __main__ - INFO - last saved model is in epoch 22
2024-03-21 11:35:46,160 - __main__ - INFO - Batch 0: Test Loss = 0.3426
2024-03-21 11:35:50,989 - __main__ - INFO - 
==>Predicting on test
2024-03-21 11:35:50,989 - __main__ - INFO - Test Loss = 0.2649
2024-03-21 12:06:01,331 - __main__ - INFO - Transfer Target Dataset & Model
2024-03-21 12:06:01,433 - __main__ - INFO - [[-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8285543981909917, 2.770245567717861, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8374745525934485, 2.8093811444689463, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5338330207864587, 0.431156978499758, 0.8964660630930379, -1.342421748214616, -0.6946051308187712, -0.2120300841305121, 0.8952797203562032, 0.0387041303378994, -0.8612693268611251, 0.0726334784031089, -0.5051102137388988, -0.2865727201409924, 0.3988826585206329, -1.3794377690564883, -0.8237089728907875, -0.9221119476695248, -1.6036614531597553, -0.2379932071009605, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -1.1501193786706505, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, 1.0826051686869729, -0.609009148503521, 0.9684393533852332, -0.3880554131475662, 0.8439788318452395, 2.837917502516613, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 0.278477698357045, 1.2505677424119097, -0.6305581644917595, -0.9062745442328174, -0.7536814885080627, -0.4374103947888615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, -0.7710128163592748, -0.6738408269117502, -2.438483340619628, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8608899578998963, 2.912112033440545, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3396023676711391, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8763143915541441, 2.979783968239297, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5125273793649132, 0.601421850901989, 0.5034187451388209, -1.455529079732558, -0.5254345623915906, -0.4163803392884947, 0.7885904984786939, 0.2816638966057675, -0.5890172833864098, 0.5902661209826593, -0.3430637446868887, -0.7482466726318323, -0.1864257310498265, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.2249939001501796, -0.0341172340589738, 0.3466058957088718, 0.3482841380580905, -0.2890718868318484, 0.1923438589112976, -0.2031244129114749, -1.100153472115602, -0.7598199909551685, -0.6868858002659636, 0.3602180776283341, -0.700061185363224, -0.501359972189453, -0.2332733777972843, -1.36588558517574, 0.3571411263970316, -0.7447833078367583, 0.8765002281041955, 2.9805992927549454, 0.0776143028335215, -0.2769313825285214, 0.0755136678180621, -0.1474826847594624, -0.3999358135056357, -0.0169688050451549, -0.0937841116273902, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, -0.0122781725754626, -0.3292808750219816, -0.2769313825285214, -0.107625012968948, -0.1474826847594624, -0.5354516373428909, -0.0002229391773086, -0.0862088042532133, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, 0.0766269712424727, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.3951350096437179, -0.5239726809327129, -0.8378690925056301, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.8988006141103361, 3.0784382346326584, 0.0776143028335215, -0.3487779453829513, 0.070457637721598, -0.1474826847594624, -0.3999358135056357, 0.0045338857876385, -0.0729488956980065, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.1275947723114954, -0.4255010572823463, -0.3487779453829513, -0.1777070069371955, -0.1474826847594624, -0.5354516373428909, 0.029608437271828, -0.0566564538358881, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.0232601678654147, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.9106941533136118, 3.1306190036341057, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9108799898636633, 3.1314343281497536, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 1.0542620417323658, -0.6738408269117502, -0.0796833960305785, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9222160194167848, 3.1811691236042576, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.5107528363891513, 1.4769583166408096, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-1.038066534429697, 0.260892106097527, 0.1645848503507034, -1.455529079732558, -1.6391408045371951, 0.196670426185453, 1.2420196914581063, 0.4706326037029981, -0.6616178283130005, -0.5004598044528213, -0.4609157221792596, 0.021209914852902, -1.1769476210921423, 0.0909225332951111, -1.348247483817418, -1.2590833683251033, -0.9252694826312332, 0.781386658108973, -0.0798138719333467, -0.2032969502372001, -0.5728175028694968, 0.103242147293012, -0.2031244129114749, -0.6504603131201653, -0.5868915885967425, -0.6218024077192129, -0.0284729839577739, -0.3925684926321655, -0.4454382040900255, -0.2791761177909213, -1.5149673075505703, 0.3413397023846657, -0.8352815289623093, 0.9329945393197532, 3.228457945511819, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8995798651896526, 0.4879119359671683, 0.1510314945591795, -1.4131138304133295, -0.6523124887119757, 0.196670426185453, 0.148455167213642, 0.4706326037029981, -0.752368509471239, -0.9441449266638648, -0.9507380036319262, 0.1751012323498492, -1.0418764542681906, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -1.3629417216818924, 0.1697587389830128, -0.364093717028159, -0.2645837378255657, -0.2890718868318484, 0.0364158635792983, -0.2031244129114749, -1.8246591171638051, -0.4061028043129334, -0.5350245509902118, 0.0492652283594477, 0.0357249008146661, -0.4929416415078188, -0.5851943844151671, -1.170932563608653, 0.051337096981241, -0.8763418268751098, 0.9447022419729768, 3.279823389997619, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.962914223877992, 3.359725192531085, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.973321070680858, 3.405383365407351, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9735069072309092, 3.4061986899229986, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9980373318376644, 3.5138215259884835, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0340896225475933, 3.671994482024121, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.2639920224449301, -0.6738408269117502, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0396647190491284, 3.696454217493548, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7418059691584676, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, -0.6693833821778409, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.057505027854041, 3.774725370995719, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5792130238763558, -0.9062745442328174, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, 0.0888023142972107, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0760886828591592, 3.856257822560481, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]
2024-03-21 12:06:01,435 - __main__ - INFO - 69
2024-03-21 12:06:01,435 - __main__ - INFO - 325
2024-03-21 12:06:01,560 - __main__ - INFO - {'los_mean': 1055.0307777880782, 'los_std': 799.0879849276147}
2024-03-21 12:06:03,519 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 1.2583
2024-03-21 12:06:41,058 - __main__ - INFO - Fold 1, epoch 0: Loss = 1.0346 Valid loss = 0.9887 MSE = 656.2734
2024-03-21 12:06:41,062 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 656.2734 ------------
2024-03-21 12:06:41,642 - __main__ - INFO - ------------ Save best model - MSE: 656.2734 ------------
2024-03-21 12:06:41,643 - __main__ - INFO - Fold 1, mse = 656.2734, mad = 21.5287
2024-03-21 12:06:42,683 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 0.9671
2024-03-21 12:07:22,461 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 609.8471 ------------
2024-03-21 12:07:23,137 - __main__ - INFO - ------------ Save best model - MSE: 609.8471 ------------
2024-03-21 12:07:23,138 - __main__ - INFO - Fold 1, mse = 609.8471, mad = 20.2145
2024-03-21 12:07:24,301 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 0.8888
2024-03-21 12:08:03,081 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 602.9647 ------------
2024-03-21 12:08:03,612 - __main__ - INFO - ------------ Save best model - MSE: 602.9647 ------------
2024-03-21 12:08:03,614 - __main__ - INFO - Fold 1, mse = 602.9647, mad = 20.0740
2024-03-21 12:08:04,669 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 0.9030
2024-03-21 12:08:42,286 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 585.4449 ------------
2024-03-21 12:08:42,842 - __main__ - INFO - ------------ Save best model - MSE: 585.4449 ------------
2024-03-21 12:08:42,843 - __main__ - INFO - Fold 1, mse = 585.4449, mad = 19.8457
2024-03-21 12:08:43,577 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 0.7232
2024-03-21 12:09:22,566 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 581.3693 ------------
2024-03-21 12:09:22,973 - __main__ - INFO - ------------ Save best model - MSE: 581.3693 ------------
2024-03-21 12:09:22,976 - __main__ - INFO - Fold 1, mse = 581.3693, mad = 19.7236
2024-03-21 12:09:24,025 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 0.8632
2024-03-21 12:10:02,119 - __main__ - INFO - Fold 1, mse = 582.9546, mad = 19.3978
2024-03-21 12:10:03,368 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 0.8170
2024-03-21 12:10:40,197 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 574.2313 ------------
2024-03-21 12:10:40,770 - __main__ - INFO - ------------ Save best model - MSE: 574.2313 ------------
2024-03-21 12:10:40,772 - __main__ - INFO - Fold 1, mse = 574.2313, mad = 19.4714
2024-03-21 12:10:41,954 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 0.8083
2024-03-21 12:11:18,987 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 568.9509 ------------
2024-03-21 12:11:19,386 - __main__ - INFO - ------------ Save best model - MSE: 568.9509 ------------
2024-03-21 12:11:19,387 - __main__ - INFO - Fold 1, mse = 568.9509, mad = 19.1620
2024-03-21 12:11:20,274 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 0.8447
2024-03-21 12:11:56,459 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 561.8643 ------------
2024-03-21 12:11:56,897 - __main__ - INFO - ------------ Save best model - MSE: 561.8643 ------------
2024-03-21 12:11:56,898 - __main__ - INFO - Fold 1, mse = 561.8643, mad = 19.1210
2024-03-21 12:11:57,821 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 0.6684
2024-03-21 12:12:36,212 - __main__ - INFO - Fold 1, mse = 578.0146, mad = 19.2094
2024-03-21 12:12:37,267 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 0.7934
2024-03-21 12:13:15,556 - __main__ - INFO - Fold 1, epoch 10: Loss = 0.7637 Valid loss = 0.7845 MSE = 566.2375
2024-03-21 12:13:15,558 - __main__ - INFO - Fold 1, mse = 566.2375, mad = 19.1527
2024-03-21 12:13:16,619 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 0.7223
2024-03-21 12:13:55,091 - __main__ - INFO - Fold 1, mse = 593.0455, mad = 19.5390
2024-03-21 12:13:56,076 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 0.7031
2024-03-21 12:14:34,769 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 550.0942 ------------
2024-03-21 12:14:35,296 - __main__ - INFO - ------------ Save best model - MSE: 550.0942 ------------
2024-03-21 12:14:35,297 - __main__ - INFO - Fold 1, mse = 550.0942, mad = 19.1887
2024-03-21 12:14:36,076 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 0.7376
2024-03-21 12:15:14,953 - __main__ - INFO - Fold 1, mse = 572.8418, mad = 19.2834
2024-03-21 12:15:16,102 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 0.7676
2024-03-21 12:15:53,665 - __main__ - INFO - Fold 1, mse = 565.4675, mad = 19.4622
2024-03-21 12:15:54,488 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 0.6835
2024-03-21 12:16:33,826 - __main__ - INFO - Fold 1, mse = 553.5371, mad = 18.8936
2024-03-21 12:16:34,932 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 0.7834
2024-03-21 12:17:12,705 - __main__ - INFO - Fold 1, mse = 560.2295, mad = 19.2208
2024-03-21 12:17:13,554 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 0.7384
2024-03-21 12:17:49,810 - __main__ - INFO - Fold 1, mse = 597.8985, mad = 20.0868
2024-03-21 12:17:50,746 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 0.7061
2024-03-21 12:18:29,410 - __main__ - INFO - Fold 1, mse = 580.1356, mad = 19.6742
2024-03-21 12:18:30,544 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 0.7084
2024-03-21 12:19:06,428 - __main__ - INFO - Fold 1, mse = 578.6004, mad = 19.5515
2024-03-21 12:19:07,421 - __main__ - INFO - Fold 1 Epoch 20 Batch 0: Train Loss = 0.6293
2024-03-21 12:19:41,630 - __main__ - INFO - Fold 1, epoch 20: Loss = 0.6545 Valid loss = 0.7878 MSE = 574.2056
2024-03-21 12:19:41,631 - __main__ - INFO - Fold 1, mse = 574.2056, mad = 19.5128
2024-03-21 12:19:42,378 - __main__ - INFO - Fold 1 Epoch 21 Batch 0: Train Loss = 0.6416
2024-03-21 12:20:11,039 - __main__ - INFO - Fold 1, mse = 570.7969, mad = 19.4772
2024-03-21 12:20:11,876 - __main__ - INFO - Fold 1 Epoch 22 Batch 0: Train Loss = 0.7172
2024-03-21 12:20:40,519 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 538.0670 ------------
2024-03-21 12:20:40,868 - __main__ - INFO - ------------ Save best model - MSE: 538.0670 ------------
2024-03-21 12:20:40,871 - __main__ - INFO - Fold 1, mse = 538.0670, mad = 18.7884
2024-03-21 12:20:41,697 - __main__ - INFO - Fold 1 Epoch 23 Batch 0: Train Loss = 0.6009
2024-03-21 12:21:09,921 - __main__ - INFO - Fold 1, mse = 561.7890, mad = 19.4175
2024-03-21 12:21:10,760 - __main__ - INFO - Fold 1 Epoch 24 Batch 0: Train Loss = 0.7495
2024-03-21 12:21:40,589 - __main__ - INFO - Fold 1, mse = 550.1491, mad = 19.2511
2024-03-21 12:21:41,402 - __main__ - INFO - Fold 1 Epoch 25 Batch 0: Train Loss = 0.6961
2024-03-21 12:22:10,702 - __main__ - INFO - Fold 1, mse = 585.4612, mad = 19.6749
2024-03-21 12:22:11,649 - __main__ - INFO - Fold 1 Epoch 26 Batch 0: Train Loss = 0.6994
2024-03-21 12:22:40,337 - __main__ - INFO - Fold 1, mse = 583.7748, mad = 19.8099
2024-03-21 12:22:41,156 - __main__ - INFO - Fold 1 Epoch 27 Batch 0: Train Loss = 0.6048
2024-03-21 12:23:08,617 - __main__ - INFO - Fold 1, mse = 578.8418, mad = 19.3914
2024-03-21 12:23:09,494 - __main__ - INFO - Fold 1 Epoch 28 Batch 0: Train Loss = 0.4804
2024-03-21 12:23:36,697 - __main__ - INFO - Fold 1, mse = 585.5426, mad = 19.6033
2024-03-21 12:23:37,294 - __main__ - INFO - Fold 1 Epoch 29 Batch 0: Train Loss = 0.6660
2024-03-21 12:24:05,323 - __main__ - INFO - Fold 1, mse = 570.1339, mad = 19.2887
2024-03-21 12:24:06,492 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 1.1011
2024-03-21 12:24:34,041 - __main__ - INFO - Fold 2, epoch 0: Loss = 1.0974 Valid loss = 0.8166 MSE = 579.9695
2024-03-21 12:24:34,042 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 579.9695 ------------
2024-03-21 12:24:34,244 - __main__ - INFO - Fold 2, mse = 579.9695, mad = 19.6832
2024-03-21 12:24:34,824 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 0.9861
2024-03-21 12:25:01,906 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 559.2812 ------------
2024-03-21 12:25:02,085 - __main__ - INFO - Fold 2, mse = 559.2812, mad = 19.3369
2024-03-21 12:25:02,784 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 0.9952
2024-03-21 12:25:27,792 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 546.3327 ------------
2024-03-21 12:25:27,977 - __main__ - INFO - Fold 2, mse = 546.3327, mad = 18.8514
2024-03-21 12:25:28,612 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 0.8822
2024-03-21 12:25:53,381 - __main__ - INFO - Fold 2, mse = 552.4271, mad = 18.9507
2024-03-21 12:25:54,134 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 0.9549
2024-03-21 12:26:19,198 - __main__ - INFO - Fold 2, mse = 549.4240, mad = 18.7446
2024-03-21 12:26:19,867 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 0.8044
2024-03-21 12:26:44,571 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 545.8031 ------------
2024-03-21 12:26:44,771 - __main__ - INFO - Fold 2, mse = 545.8031, mad = 18.6214
2024-03-21 12:26:45,415 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 0.7721
2024-03-21 12:27:10,844 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 537.3057 ------------
2024-03-21 12:27:11,192 - __main__ - INFO - ------------ Save best model - MSE: 537.3057 ------------
2024-03-21 12:27:11,193 - __main__ - INFO - Fold 2, mse = 537.3057, mad = 18.4444
2024-03-21 12:27:11,848 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 0.8353
2024-03-21 12:27:37,722 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 526.7255 ------------
2024-03-21 12:27:38,032 - __main__ - INFO - ------------ Save best model - MSE: 526.7255 ------------
2024-03-21 12:27:38,033 - __main__ - INFO - Fold 2, mse = 526.7255, mad = 18.3211
2024-03-21 12:27:38,767 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 0.6605
2024-03-21 12:28:03,939 - __main__ - INFO - Fold 2, mse = 527.3504, mad = 18.3784
2024-03-21 12:28:04,676 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 0.8205
2024-03-21 12:28:28,657 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 523.2817 ------------
2024-03-21 12:28:28,998 - __main__ - INFO - ------------ Save best model - MSE: 523.2817 ------------
2024-03-21 12:28:28,999 - __main__ - INFO - Fold 2, mse = 523.2817, mad = 18.2715
2024-03-21 12:28:29,596 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 0.7052
2024-03-21 12:28:54,470 - __main__ - INFO - Fold 2, epoch 10: Loss = 0.7476 Valid loss = 0.7177 MSE = 530.9358
2024-03-21 12:28:54,471 - __main__ - INFO - Fold 2, mse = 530.9358, mad = 18.5323
2024-03-21 12:28:55,099 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 0.6617
2024-03-21 12:29:19,759 - __main__ - INFO - Fold 2, mse = 561.9886, mad = 19.2681
2024-03-21 12:29:20,486 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 0.6312
2024-03-21 12:29:44,910 - __main__ - INFO - Fold 2, mse = 525.0289, mad = 18.2666
2024-03-21 12:29:45,758 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 0.6963
2024-03-21 12:30:10,630 - __main__ - INFO - Fold 2, mse = 539.2546, mad = 18.3788
2024-03-21 12:30:11,330 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 0.7588
2024-03-21 12:30:35,365 - __main__ - INFO - Fold 2, mse = 567.3467, mad = 19.0588
2024-03-21 12:30:35,993 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 0.7552
2024-03-21 12:31:00,109 - __main__ - INFO - Fold 2, mse = 557.4347, mad = 19.0679
2024-03-21 12:31:00,804 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 0.5382
2024-03-21 12:31:25,616 - __main__ - INFO - Fold 2, mse = 529.3028, mad = 18.3745
2024-03-21 12:31:26,332 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 0.6885
2024-03-21 12:31:51,111 - __main__ - INFO - Fold 2, mse = 571.9628, mad = 19.4898
2024-03-21 12:31:51,788 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 0.6373
2024-03-21 12:32:16,404 - __main__ - INFO - Fold 2, mse = 568.7600, mad = 19.5304
2024-03-21 12:32:17,081 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 0.6517
2024-03-21 12:32:41,682 - __main__ - INFO - Fold 2, mse = 585.1413, mad = 19.7495
2024-03-21 12:32:42,387 - __main__ - INFO - Fold 2 Epoch 20 Batch 0: Train Loss = 0.5684
2024-03-21 12:33:06,776 - __main__ - INFO - Fold 2, epoch 20: Loss = 0.6278 Valid loss = 0.7726 MSE = 579.7681
2024-03-21 12:33:06,777 - __main__ - INFO - Fold 2, mse = 579.7681, mad = 19.6349
2024-03-21 12:33:07,460 - __main__ - INFO - Fold 2 Epoch 21 Batch 0: Train Loss = 0.5839
2024-03-21 12:33:32,451 - __main__ - INFO - Fold 2, mse = 563.7277, mad = 19.3860
2024-03-21 12:33:33,101 - __main__ - INFO - Fold 2 Epoch 22 Batch 0: Train Loss = 0.6740
2024-03-21 12:33:58,284 - __main__ - INFO - Fold 2, mse = 566.5547, mad = 19.4216
2024-03-21 12:33:58,944 - __main__ - INFO - Fold 2 Epoch 23 Batch 0: Train Loss = 0.5346
2024-03-21 12:34:23,394 - __main__ - INFO - Fold 2, mse = 616.4059, mad = 20.6139
2024-03-21 12:34:24,165 - __main__ - INFO - Fold 2 Epoch 24 Batch 0: Train Loss = 0.5391
2024-03-21 12:34:48,317 - __main__ - INFO - Fold 2, mse = 608.5400, mad = 20.3680
2024-03-21 12:34:49,048 - __main__ - INFO - Fold 2 Epoch 25 Batch 0: Train Loss = 0.6090
2024-03-21 12:35:13,101 - __main__ - INFO - Fold 2, mse = 629.7390, mad = 20.8203
2024-03-21 12:35:13,632 - __main__ - INFO - Fold 2 Epoch 26 Batch 0: Train Loss = 0.6851
2024-03-21 12:35:37,625 - __main__ - INFO - Fold 2, mse = 610.9219, mad = 20.4459
2024-03-21 12:35:38,297 - __main__ - INFO - Fold 2 Epoch 27 Batch 0: Train Loss = 0.6050
2024-03-21 12:36:03,811 - __main__ - INFO - Fold 2, mse = 599.1054, mad = 20.1784
2024-03-21 12:36:04,459 - __main__ - INFO - Fold 2 Epoch 28 Batch 0: Train Loss = 0.6166
2024-03-21 12:36:29,940 - __main__ - INFO - Fold 2, mse = 599.5120, mad = 20.1043
2024-03-21 12:36:30,684 - __main__ - INFO - Fold 2 Epoch 29 Batch 0: Train Loss = 0.5834
2024-03-21 12:36:56,676 - __main__ - INFO - Fold 2, mse = 600.0471, mad = 20.2765
2024-03-21 12:36:57,621 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 1.2205
2024-03-21 12:37:20,603 - __main__ - INFO - Fold 3, epoch 0: Loss = 1.0579 Valid loss = 0.9069 MSE = 646.0877
2024-03-21 12:37:20,603 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 646.0877 ------------
2024-03-21 12:37:20,755 - __main__ - INFO - Fold 3, mse = 646.0877, mad = 20.7803
2024-03-21 12:37:21,382 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 1.1471
2024-03-21 12:37:44,999 - __main__ - INFO - Fold 3, mse = 654.4508, mad = 20.8040
2024-03-21 12:37:45,580 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 0.9040
2024-03-21 12:38:08,851 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 635.3236 ------------
2024-03-21 12:38:08,996 - __main__ - INFO - Fold 3, mse = 635.3236, mad = 20.8384
2024-03-21 12:38:09,607 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 0.9052
2024-03-21 12:38:33,435 - __main__ - INFO - Fold 3, mse = 635.8261, mad = 20.7802
2024-03-21 12:38:34,074 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 0.7789
2024-03-21 12:38:58,029 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 630.5189 ------------
2024-03-21 12:38:58,173 - __main__ - INFO - Fold 3, mse = 630.5189, mad = 20.8287
2024-03-21 12:38:58,759 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 0.8366
2024-03-21 12:39:22,617 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 616.7073 ------------
2024-03-21 12:39:22,797 - __main__ - INFO - Fold 3, mse = 616.7073, mad = 20.4807
2024-03-21 12:39:23,457 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 0.8728
2024-03-21 12:39:47,461 - __main__ - INFO - Fold 3, mse = 620.6770, mad = 20.4882
2024-03-21 12:39:48,014 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 0.7235
2024-03-21 12:40:12,061 - __main__ - INFO - Fold 3, mse = 629.0073, mad = 20.5044
2024-03-21 12:40:12,638 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 0.7336
2024-03-21 12:40:37,062 - __main__ - INFO - Fold 3, mse = 625.6047, mad = 20.4196
2024-03-21 12:40:37,727 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 0.7933
2024-03-21 12:41:02,063 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 611.5036 ------------
2024-03-21 12:41:02,208 - __main__ - INFO - Fold 3, mse = 611.5036, mad = 20.2572
2024-03-21 12:41:02,910 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 0.7215
2024-03-21 12:41:27,823 - __main__ - INFO - Fold 3, epoch 10: Loss = 0.7165 Valid loss = 0.8899 MSE = 635.9744
2024-03-21 12:41:27,824 - __main__ - INFO - Fold 3, mse = 635.9744, mad = 20.6477
2024-03-21 12:41:28,466 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 0.7469
2024-03-21 12:41:52,422 - __main__ - INFO - Fold 3, mse = 626.8257, mad = 20.5526
2024-03-21 12:41:53,092 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 0.6459
2024-03-21 12:42:16,839 - __main__ - INFO - Fold 3, mse = 623.4563, mad = 20.6885
2024-03-21 12:42:17,407 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 0.7335
2024-03-21 12:42:41,399 - __main__ - INFO - Fold 3, mse = 645.1841, mad = 21.1693
2024-03-21 12:42:42,007 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 0.5652
2024-03-21 12:43:05,640 - __main__ - INFO - Fold 3, mse = 640.1531, mad = 21.1431
2024-03-21 12:43:06,301 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 0.6067
2024-03-21 12:43:29,654 - __main__ - INFO - Fold 3, mse = 612.9669, mad = 20.6924
2024-03-21 12:43:30,299 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 0.5670
2024-03-21 12:43:53,107 - __main__ - INFO - Fold 3, mse = 624.4495, mad = 20.6573
2024-03-21 12:43:53,623 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 0.6751
2024-03-21 12:44:17,168 - __main__ - INFO - Fold 3, mse = 632.4881, mad = 20.8963
2024-03-21 12:44:17,831 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 0.5820
2024-03-21 12:44:41,903 - __main__ - INFO - Fold 3, mse = 649.9159, mad = 21.3064
2024-03-21 12:44:42,438 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 0.7121
2024-03-21 12:45:07,327 - __main__ - INFO - Fold 3, mse = 656.4200, mad = 21.3208
2024-03-21 12:45:07,967 - __main__ - INFO - Fold 3 Epoch 20 Batch 0: Train Loss = 0.6433
2024-03-21 12:45:32,437 - __main__ - INFO - Fold 3, epoch 20: Loss = 0.6432 Valid loss = 0.9338 MSE = 663.9078
2024-03-21 12:45:32,438 - __main__ - INFO - Fold 3, mse = 663.9078, mad = 21.5431
2024-03-21 12:45:33,075 - __main__ - INFO - Fold 3 Epoch 21 Batch 0: Train Loss = 0.7770
2024-03-21 12:45:57,316 - __main__ - INFO - Fold 3, mse = 656.2697, mad = 21.1558
2024-03-21 12:45:57,975 - __main__ - INFO - Fold 3 Epoch 22 Batch 0: Train Loss = 0.6836
2024-03-21 12:46:22,528 - __main__ - INFO - Fold 3, mse = 677.3985, mad = 21.7764
2024-03-21 12:46:23,186 - __main__ - INFO - Fold 3 Epoch 23 Batch 0: Train Loss = 0.6049
2024-03-21 12:46:46,633 - __main__ - INFO - Fold 3, mse = 677.0091, mad = 21.6459
2024-03-21 12:46:47,209 - __main__ - INFO - Fold 3 Epoch 24 Batch 0: Train Loss = 0.5893
2024-03-21 12:47:10,326 - __main__ - INFO - Fold 3, mse = 638.4089, mad = 20.8786
2024-03-21 12:47:10,943 - __main__ - INFO - Fold 3 Epoch 25 Batch 0: Train Loss = 0.7047
2024-03-21 12:47:36,198 - __main__ - INFO - Fold 3, mse = 651.0134, mad = 21.1387
2024-03-21 12:47:36,905 - __main__ - INFO - Fold 3 Epoch 26 Batch 0: Train Loss = 0.6805
2024-03-21 12:48:01,042 - __main__ - INFO - Fold 3, mse = 655.1409, mad = 21.2576
2024-03-21 12:48:01,717 - __main__ - INFO - Fold 3 Epoch 27 Batch 0: Train Loss = 0.6281
2024-03-21 12:48:25,219 - __main__ - INFO - Fold 3, mse = 668.4164, mad = 21.6140
2024-03-21 12:48:25,818 - __main__ - INFO - Fold 3 Epoch 28 Batch 0: Train Loss = 0.6098
2024-03-21 12:48:49,184 - __main__ - INFO - Fold 3, mse = 669.9370, mad = 21.6101
2024-03-21 12:48:49,783 - __main__ - INFO - Fold 3 Epoch 29 Batch 0: Train Loss = 0.5651
2024-03-21 12:49:14,101 - __main__ - INFO - Fold 3, mse = 655.7169, mad = 21.3548
2024-03-21 12:49:15,081 - __main__ - INFO - Fold 4 Epoch 0 Batch 0: Train Loss = 1.0460
2024-03-21 12:49:39,716 - __main__ - INFO - Fold 4, epoch 0: Loss = 0.9937 Valid loss = 1.1742 MSE = 828.7555
2024-03-21 12:49:39,718 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 828.7555 ------------
2024-03-21 12:49:39,978 - __main__ - INFO - Fold 4, mse = 828.7555, mad = 23.0837
2024-03-21 12:49:40,613 - __main__ - INFO - Fold 4 Epoch 1 Batch 0: Train Loss = 0.9667
2024-03-21 12:50:05,530 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 748.9138 ------------
2024-03-21 12:50:05,703 - __main__ - INFO - Fold 4, mse = 748.9138, mad = 21.9613
2024-03-21 12:50:06,385 - __main__ - INFO - Fold 4 Epoch 2 Batch 0: Train Loss = 0.7487
2024-03-21 12:50:30,143 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 720.4055 ------------
2024-03-21 12:50:30,300 - __main__ - INFO - Fold 4, mse = 720.4055, mad = 21.2876
2024-03-21 12:50:31,018 - __main__ - INFO - Fold 4 Epoch 3 Batch 0: Train Loss = 0.7689
2024-03-21 12:50:55,521 - __main__ - INFO - Fold 4, mse = 722.0243, mad = 21.3967
2024-03-21 12:50:56,212 - __main__ - INFO - Fold 4 Epoch 4 Batch 0: Train Loss = 0.9480
2024-03-21 12:51:20,091 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 711.9274 ------------
2024-03-21 12:51:20,271 - __main__ - INFO - Fold 4, mse = 711.9274, mad = 21.2137
2024-03-21 12:51:21,039 - __main__ - INFO - Fold 4 Epoch 5 Batch 0: Train Loss = 0.7190
2024-03-21 12:51:44,946 - __main__ - INFO - Fold 4, mse = 715.5193, mad = 21.2765
2024-03-21 12:51:45,644 - __main__ - INFO - Fold 4 Epoch 6 Batch 0: Train Loss = 0.8282
2024-03-21 12:52:08,937 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 709.1204 ------------
2024-03-21 12:52:09,096 - __main__ - INFO - Fold 4, mse = 709.1204, mad = 21.3117
2024-03-21 12:52:09,695 - __main__ - INFO - Fold 4 Epoch 7 Batch 0: Train Loss = 0.7404
2024-03-21 12:52:33,467 - __main__ - INFO - Fold 4, mse = 743.1726, mad = 21.6515
2024-03-21 12:52:34,121 - __main__ - INFO - Fold 4 Epoch 8 Batch 0: Train Loss = 0.9067
2024-03-21 12:52:57,875 - __main__ - INFO - Fold 4, mse = 771.4812, mad = 22.3600
2024-03-21 12:52:58,593 - __main__ - INFO - Fold 4 Epoch 9 Batch 0: Train Loss = 0.6900
2024-03-21 12:53:22,979 - __main__ - INFO - Fold 4, mse = 742.8376, mad = 22.0312
2024-03-21 12:53:23,632 - __main__ - INFO - Fold 4 Epoch 10 Batch 0: Train Loss = 0.7513
2024-03-21 12:53:46,246 - __main__ - INFO - Fold 4, epoch 10: Loss = 0.7063 Valid loss = 1.0608 MSE = 760.3726
2024-03-21 12:53:46,247 - __main__ - INFO - Fold 4, mse = 760.3726, mad = 22.3174
2024-03-21 12:53:46,890 - __main__ - INFO - Fold 4 Epoch 11 Batch 0: Train Loss = 0.6944
2024-03-21 12:54:10,224 - __main__ - INFO - Fold 4, mse = 750.7370, mad = 22.0390
2024-03-21 12:54:10,856 - __main__ - INFO - Fold 4 Epoch 12 Batch 0: Train Loss = 0.6739
2024-03-21 12:54:35,797 - __main__ - INFO - Fold 4, mse = 776.1987, mad = 22.4322
2024-03-21 12:54:36,467 - __main__ - INFO - Fold 4 Epoch 13 Batch 0: Train Loss = 0.6402
2024-03-21 12:54:59,889 - __main__ - INFO - Fold 4, mse = 794.2451, mad = 22.6092
2024-03-21 12:55:00,609 - __main__ - INFO - Fold 4 Epoch 14 Batch 0: Train Loss = 0.7021
2024-03-21 12:55:23,348 - __main__ - INFO - Fold 4, mse = 746.3010, mad = 22.1978
2024-03-21 12:55:23,916 - __main__ - INFO - Fold 4 Epoch 15 Batch 0: Train Loss = 0.6905
2024-03-21 12:55:47,220 - __main__ - INFO - Fold 4, mse = 736.2111, mad = 21.9674
2024-03-21 12:55:47,855 - __main__ - INFO - Fold 4 Epoch 16 Batch 0: Train Loss = 0.6569
2024-03-21 12:56:11,571 - __main__ - INFO - Fold 4, mse = 739.1658, mad = 21.8800
2024-03-21 12:56:12,201 - __main__ - INFO - Fold 4 Epoch 17 Batch 0: Train Loss = 0.6142
2024-03-21 12:56:35,680 - __main__ - INFO - Fold 4, mse = 724.1161, mad = 21.3197
2024-03-21 12:56:36,318 - __main__ - INFO - Fold 4 Epoch 18 Batch 0: Train Loss = 0.6716
2024-03-21 12:56:59,919 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 708.2386 ------------
2024-03-21 12:57:00,078 - __main__ - INFO - Fold 4, mse = 708.2386, mad = 21.5599
2024-03-21 12:57:00,624 - __main__ - INFO - Fold 4 Epoch 19 Batch 0: Train Loss = 0.5887
2024-03-21 12:57:23,904 - __main__ - INFO - Fold 4, mse = 768.7036, mad = 22.2079
2024-03-21 12:57:24,636 - __main__ - INFO - Fold 4 Epoch 20 Batch 0: Train Loss = 0.5741
2024-03-21 12:57:49,032 - __main__ - INFO - Fold 4, epoch 20: Loss = 0.6299 Valid loss = 0.9967 MSE = 733.0427
2024-03-21 12:57:49,033 - __main__ - INFO - Fold 4, mse = 733.0427, mad = 21.7503
2024-03-21 12:57:49,590 - __main__ - INFO - Fold 4 Epoch 21 Batch 0: Train Loss = 0.6230
2024-03-21 12:58:13,517 - __main__ - INFO - Fold 4, mse = 777.6561, mad = 22.5340
2024-03-21 12:58:14,261 - __main__ - INFO - Fold 4 Epoch 22 Batch 0: Train Loss = 0.6216
2024-03-21 12:58:37,303 - __main__ - INFO - Fold 4, mse = 815.2833, mad = 22.9237
2024-03-21 12:58:38,049 - __main__ - INFO - Fold 4 Epoch 23 Batch 0: Train Loss = 0.5706
2024-03-21 12:59:02,786 - __main__ - INFO - Fold 4, mse = 805.6992, mad = 22.8871
2024-03-21 12:59:03,541 - __main__ - INFO - Fold 4 Epoch 24 Batch 0: Train Loss = 0.5232
2024-03-21 12:59:27,722 - __main__ - INFO - Fold 4, mse = 812.8318, mad = 22.8791
2024-03-21 12:59:28,327 - __main__ - INFO - Fold 4 Epoch 25 Batch 0: Train Loss = 0.5635
2024-03-21 12:59:52,765 - __main__ - INFO - Fold 4, mse = 815.2480, mad = 22.8751
2024-03-21 12:59:53,437 - __main__ - INFO - Fold 4 Epoch 26 Batch 0: Train Loss = 0.5559
2024-03-21 13:00:18,262 - __main__ - INFO - Fold 4, mse = 815.7253, mad = 22.8199
2024-03-21 13:00:18,815 - __main__ - INFO - Fold 4 Epoch 27 Batch 0: Train Loss = 0.5096
2024-03-21 13:00:43,080 - __main__ - INFO - Fold 4, mse = 793.2677, mad = 22.3112
2024-03-21 13:00:43,781 - __main__ - INFO - Fold 4 Epoch 28 Batch 0: Train Loss = 0.5593
2024-03-21 13:01:06,033 - __main__ - INFO - Fold 4, mse = 839.2309, mad = 22.9360
2024-03-21 13:01:06,725 - __main__ - INFO - Fold 4 Epoch 29 Batch 0: Train Loss = 0.5166
2024-03-21 13:01:30,306 - __main__ - INFO - Fold 4, mse = 808.3431, mad = 22.4643
2024-03-21 13:01:31,362 - __main__ - INFO - Fold 5 Epoch 0 Batch 0: Train Loss = 1.1369
2024-03-21 13:01:55,797 - __main__ - INFO - Fold 5, epoch 0: Loss = 1.0273 Valid loss = 0.9429 MSE = 670.1478
2024-03-21 13:01:55,798 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 670.1478 ------------
2024-03-21 13:01:55,939 - __main__ - INFO - Fold 5, mse = 670.1478, mad = 21.2981
2024-03-21 13:01:56,499 - __main__ - INFO - Fold 5 Epoch 1 Batch 0: Train Loss = 0.9977
2024-03-21 13:02:20,651 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 648.7756 ------------
2024-03-21 13:02:20,792 - __main__ - INFO - Fold 5, mse = 648.7756, mad = 21.0723
2024-03-21 13:02:21,459 - __main__ - INFO - Fold 5 Epoch 2 Batch 0: Train Loss = 0.9603
2024-03-21 13:02:47,421 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 647.0331 ------------
2024-03-21 13:02:47,573 - __main__ - INFO - Fold 5, mse = 647.0331, mad = 20.7382
2024-03-21 13:02:48,221 - __main__ - INFO - Fold 5 Epoch 3 Batch 0: Train Loss = 0.8373
2024-03-21 13:03:14,340 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 634.7926 ------------
2024-03-21 13:03:14,477 - __main__ - INFO - Fold 5, mse = 634.7926, mad = 20.2694
2024-03-21 13:03:15,107 - __main__ - INFO - Fold 5 Epoch 4 Batch 0: Train Loss = 0.8253
2024-03-21 13:03:41,002 - __main__ - INFO - Fold 5, mse = 637.5699, mad = 20.1073
2024-03-21 13:03:41,597 - __main__ - INFO - Fold 5 Epoch 5 Batch 0: Train Loss = 0.8936
2024-03-21 13:04:05,880 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 620.9071 ------------
2024-03-21 13:04:06,092 - __main__ - INFO - Fold 5, mse = 620.9071, mad = 20.0194
2024-03-21 13:04:06,762 - __main__ - INFO - Fold 5 Epoch 6 Batch 0: Train Loss = 0.9417
2024-03-21 13:04:31,145 - __main__ - INFO - Fold 5, mse = 628.9110, mad = 19.8436
2024-03-21 13:04:31,860 - __main__ - INFO - Fold 5 Epoch 7 Batch 0: Train Loss = 0.9184
2024-03-21 13:04:56,618 - __main__ - INFO - Fold 5, mse = 658.9833, mad = 20.6725
2024-03-21 13:04:57,279 - __main__ - INFO - Fold 5 Epoch 8 Batch 0: Train Loss = 0.7620
2024-03-21 13:05:20,975 - __main__ - INFO - Fold 5, mse = 660.8738, mad = 20.4427
2024-03-21 13:05:21,706 - __main__ - INFO - Fold 5 Epoch 9 Batch 0: Train Loss = 0.7354
2024-03-21 13:05:45,939 - __main__ - INFO - Fold 5, mse = 670.9369, mad = 20.6002
2024-03-21 13:05:46,570 - __main__ - INFO - Fold 5 Epoch 10 Batch 0: Train Loss = 0.7000
2024-03-21 13:06:11,296 - __main__ - INFO - Fold 5, epoch 10: Loss = 0.7118 Valid loss = 0.9097 MSE = 647.8376
2024-03-21 13:06:11,297 - __main__ - INFO - Fold 5, mse = 647.8376, mad = 20.6798
2024-03-21 13:06:11,904 - __main__ - INFO - Fold 5 Epoch 11 Batch 0: Train Loss = 0.7265
2024-03-21 13:06:36,621 - __main__ - INFO - Fold 5, mse = 647.7219, mad = 20.6265
2024-03-21 13:06:37,251 - __main__ - INFO - Fold 5 Epoch 12 Batch 0: Train Loss = 0.7414
2024-03-21 13:07:02,487 - __main__ - INFO - Fold 5, mse = 661.2612, mad = 20.6241
2024-03-21 13:07:03,149 - __main__ - INFO - Fold 5 Epoch 13 Batch 0: Train Loss = 0.6597
2024-03-21 13:07:27,204 - __main__ - INFO - Fold 5, mse = 664.5108, mad = 20.7708
2024-03-21 13:07:27,745 - __main__ - INFO - Fold 5 Epoch 14 Batch 0: Train Loss = 0.7871
2024-03-21 13:07:54,215 - __main__ - INFO - Fold 5, mse = 641.3874, mad = 20.8143
2024-03-21 13:07:54,940 - __main__ - INFO - Fold 5 Epoch 15 Batch 0: Train Loss = 0.6826
2024-03-21 13:08:20,818 - __main__ - INFO - Fold 5, mse = 657.5092, mad = 20.6834
2024-03-21 13:08:21,609 - __main__ - INFO - Fold 5 Epoch 16 Batch 0: Train Loss = 0.5642
2024-03-21 13:08:49,193 - __main__ - INFO - Fold 5, mse = 656.5100, mad = 20.9209
2024-03-21 13:08:50,064 - __main__ - INFO - Fold 5 Epoch 17 Batch 0: Train Loss = 0.7097
2024-03-21 13:09:14,126 - __main__ - INFO - Fold 5, mse = 666.7135, mad = 20.8645
2024-03-21 13:09:14,833 - __main__ - INFO - Fold 5 Epoch 18 Batch 0: Train Loss = 0.6288
2024-03-21 13:09:39,878 - __main__ - INFO - Fold 5, mse = 675.5470, mad = 20.7084
2024-03-21 13:09:40,525 - __main__ - INFO - Fold 5 Epoch 19 Batch 0: Train Loss = 0.6901
2024-03-21 13:10:05,036 - __main__ - INFO - Fold 5, mse = 700.6729, mad = 21.5387
2024-03-21 13:10:05,665 - __main__ - INFO - Fold 5 Epoch 20 Batch 0: Train Loss = 0.5649
2024-03-21 13:10:29,570 - __main__ - INFO - Fold 5, epoch 20: Loss = 0.6214 Valid loss = 0.9718 MSE = 691.2483
2024-03-21 13:10:29,572 - __main__ - INFO - Fold 5, mse = 691.2483, mad = 21.2118
2024-03-21 13:10:30,225 - __main__ - INFO - Fold 5 Epoch 21 Batch 0: Train Loss = 0.7208
2024-03-21 13:10:54,101 - __main__ - INFO - Fold 5, mse = 704.6027, mad = 21.6075
2024-03-21 13:10:54,759 - __main__ - INFO - Fold 5 Epoch 22 Batch 0: Train Loss = 0.5564
2024-03-21 13:11:19,166 - __main__ - INFO - Fold 5, mse = 703.1762, mad = 21.7929
2024-03-21 13:11:19,903 - __main__ - INFO - Fold 5 Epoch 23 Batch 0: Train Loss = 0.5940
2024-03-21 13:11:44,610 - __main__ - INFO - Fold 5, mse = 694.5690, mad = 21.6089
2024-03-21 13:11:45,277 - __main__ - INFO - Fold 5 Epoch 24 Batch 0: Train Loss = 0.5971
2024-03-21 13:12:10,944 - __main__ - INFO - Fold 5, mse = 687.0286, mad = 21.3934
2024-03-21 13:12:11,681 - __main__ - INFO - Fold 5 Epoch 25 Batch 0: Train Loss = 0.5787
2024-03-21 13:12:36,951 - __main__ - INFO - Fold 5, mse = 702.7838, mad = 21.8175
2024-03-21 13:12:37,696 - __main__ - INFO - Fold 5 Epoch 26 Batch 0: Train Loss = 0.4946
2024-03-21 13:13:01,792 - __main__ - INFO - Fold 5, mse = 717.0077, mad = 21.8279
2024-03-21 13:13:02,390 - __main__ - INFO - Fold 5 Epoch 27 Batch 0: Train Loss = 0.5318
2024-03-21 13:13:26,839 - __main__ - INFO - Fold 5, mse = 717.5681, mad = 21.9251
2024-03-21 13:13:27,505 - __main__ - INFO - Fold 5 Epoch 28 Batch 0: Train Loss = 0.5230
2024-03-21 13:13:51,120 - __main__ - INFO - Fold 5, mse = 637.5928, mad = 20.7417
2024-03-21 13:13:51,720 - __main__ - INFO - Fold 5 Epoch 29 Batch 0: Train Loss = 0.5849
2024-03-21 13:14:15,068 - __main__ - INFO - Fold 5, mse = 676.4658, mad = 21.1575
2024-03-21 13:14:15,071 - __main__ - INFO - mse 600.3996(66.3410)
2024-03-21 13:14:15,071 - __main__ - INFO - mad 19.7793(1.1586)
