2024-01-18 03:33:44,845 - __main__ - INFO - 这是希望输出的info内容
2024-01-18 03:33:44,845 - __main__ - WARNING - 这是希望输出的warning内容
2024-01-18 03:34:30,740 - __main__ - INFO - 32269
2024-01-18 03:34:30,740 - __main__ - INFO - 4034
2024-01-18 03:34:30,740 - __main__ - INFO - 4033
2024-01-18 03:34:42,953 - __main__ - INFO - load target data
2024-01-18 03:34:45,447 - __main__ - INFO - last saved model is in epoch 27
2024-01-18 03:34:45,726 - __main__ - INFO - Batch 0: Test Loss = -2.8663
2024-01-18 03:34:48,729 - __main__ - INFO - 
==>Predicting on test
2024-01-18 03:34:48,730 - __main__ - INFO - Test Loss = -2.9028
2024-01-18 03:57:21,413 - __main__ - INFO - Transfer Target Dataset & Model
2024-01-18 03:57:21,475 - __main__ - INFO - [[-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8285543981909917, 2.770245567717861, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8374745525934485, 2.8093811444689463, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5338330207864587, 0.431156978499758, 0.8964660630930379, -1.342421748214616, -0.6946051308187712, -0.2120300841305121, 0.8952797203562032, 0.0387041303378994, -0.8612693268611251, 0.0726334784031089, -0.5051102137388988, -0.2865727201409924, 0.3988826585206329, -1.3794377690564883, -0.8237089728907875, -0.9221119476695248, -1.6036614531597553, -0.2379932071009605, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -1.1501193786706505, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, 1.0826051686869729, -0.609009148503521, 0.9684393533852332, -0.3880554131475662, 0.8439788318452395, 2.837917502516613, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 0.278477698357045, 1.2505677424119097, -0.6305581644917595, -0.9062745442328174, -0.7536814885080627, -0.4374103947888615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, -0.7710128163592748, -0.6738408269117502, -2.438483340619628, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8608899578998963, 2.912112033440545, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3396023676711391, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8763143915541441, 2.979783968239297, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5125273793649132, 0.601421850901989, 0.5034187451388209, -1.455529079732558, -0.5254345623915906, -0.4163803392884947, 0.7885904984786939, 0.2816638966057675, -0.5890172833864098, 0.5902661209826593, -0.3430637446868887, -0.7482466726318323, -0.1864257310498265, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.2249939001501796, -0.0341172340589738, 0.3466058957088718, 0.3482841380580905, -0.2890718868318484, 0.1923438589112976, -0.2031244129114749, -1.100153472115602, -0.7598199909551685, -0.6868858002659636, 0.3602180776283341, -0.700061185363224, -0.501359972189453, -0.2332733777972843, -1.36588558517574, 0.3571411263970316, -0.7447833078367583, 0.8765002281041955, 2.9805992927549454, 0.0776143028335215, -0.2769313825285214, 0.0755136678180621, -0.1474826847594624, -0.3999358135056357, -0.0169688050451549, -0.0937841116273902, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, -0.0122781725754626, -0.3292808750219816, -0.2769313825285214, -0.107625012968948, -0.1474826847594624, -0.5354516373428909, -0.0002229391773086, -0.0862088042532133, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, 0.0766269712424727, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.3951350096437179, -0.5239726809327129, -0.8378690925056301, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.8988006141103361, 3.0784382346326584, 0.0776143028335215, -0.3487779453829513, 0.070457637721598, -0.1474826847594624, -0.3999358135056357, 0.0045338857876385, -0.0729488956980065, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.1275947723114954, -0.4255010572823463, -0.3487779453829513, -0.1777070069371955, -0.1474826847594624, -0.5354516373428909, 0.029608437271828, -0.0566564538358881, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.0232601678654147, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.9106941533136118, 3.1306190036341057, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9108799898636633, 3.1314343281497536, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 1.0542620417323658, -0.6738408269117502, -0.0796833960305785, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9222160194167848, 3.1811691236042576, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.5107528363891513, 1.4769583166408096, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-1.038066534429697, 0.260892106097527, 0.1645848503507034, -1.455529079732558, -1.6391408045371951, 0.196670426185453, 1.2420196914581063, 0.4706326037029981, -0.6616178283130005, -0.5004598044528213, -0.4609157221792596, 0.021209914852902, -1.1769476210921423, 0.0909225332951111, -1.348247483817418, -1.2590833683251033, -0.9252694826312332, 0.781386658108973, -0.0798138719333467, -0.2032969502372001, -0.5728175028694968, 0.103242147293012, -0.2031244129114749, -0.6504603131201653, -0.5868915885967425, -0.6218024077192129, -0.0284729839577739, -0.3925684926321655, -0.4454382040900255, -0.2791761177909213, -1.5149673075505703, 0.3413397023846657, -0.8352815289623093, 0.9329945393197532, 3.228457945511819, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8995798651896526, 0.4879119359671683, 0.1510314945591795, -1.4131138304133295, -0.6523124887119757, 0.196670426185453, 0.148455167213642, 0.4706326037029981, -0.752368509471239, -0.9441449266638648, -0.9507380036319262, 0.1751012323498492, -1.0418764542681906, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -1.3629417216818924, 0.1697587389830128, -0.364093717028159, -0.2645837378255657, -0.2890718868318484, 0.0364158635792983, -0.2031244129114749, -1.8246591171638051, -0.4061028043129334, -0.5350245509902118, 0.0492652283594477, 0.0357249008146661, -0.4929416415078188, -0.5851943844151671, -1.170932563608653, 0.051337096981241, -0.8763418268751098, 0.9447022419729768, 3.279823389997619, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.962914223877992, 3.359725192531085, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.973321070680858, 3.405383365407351, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9735069072309092, 3.4061986899229986, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9980373318376644, 3.5138215259884835, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0340896225475933, 3.671994482024121, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.2639920224449301, -0.6738408269117502, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0396647190491284, 3.696454217493548, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7418059691584676, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, -0.6693833821778409, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.057505027854041, 3.774725370995719, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5792130238763558, -0.9062745442328174, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, 0.0888023142972107, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0760886828591592, 3.856257822560481, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]
2024-01-18 03:57:21,479 - __main__ - INFO - 69
2024-01-18 03:57:21,479 - __main__ - INFO - 325
2024-01-18 03:57:21,622 - __main__ - INFO - {'los_mean': 1055.0307777880782, 'los_std': 799.0879849276147}
2024-01-18 03:57:24,082 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 72.1071
2024-01-18 03:57:43,299 - __main__ - INFO - Fold 1, epoch 0: Loss = 7.8526 Valid loss = 1.1793 MSE = 814.2098
2024-01-18 03:57:43,301 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 814.2098 ------------
2024-01-18 03:57:43,453 - __main__ - INFO - ------------ Save best model - MSE: 814.2098 ------------
2024-01-18 03:57:43,454 - __main__ - INFO - Fold 1, mse = 814.2098, mad = 23.6208
2024-01-18 03:57:43,942 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 1.3365
2024-01-18 03:58:02,554 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 750.5942 ------------
2024-01-18 03:58:02,679 - __main__ - INFO - ------------ Save best model - MSE: 750.5942 ------------
2024-01-18 03:58:02,680 - __main__ - INFO - Fold 1, mse = 750.5942, mad = 23.6324
2024-01-18 03:58:03,299 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 1.0165
2024-01-18 03:58:21,347 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 749.3143 ------------
2024-01-18 03:58:21,438 - __main__ - INFO - ------------ Save best model - MSE: 749.3143 ------------
2024-01-18 03:58:21,440 - __main__ - INFO - Fold 1, mse = 749.3143, mad = 23.6773
2024-01-18 03:58:21,956 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 0.8943
2024-01-18 03:58:39,004 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 677.1827 ------------
2024-01-18 03:58:39,117 - __main__ - INFO - ------------ Save best model - MSE: 677.1827 ------------
2024-01-18 03:58:39,119 - __main__ - INFO - Fold 1, mse = 677.1827, mad = 21.9326
2024-01-18 03:58:39,693 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 0.9905
2024-01-18 03:58:57,765 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 633.4753 ------------
2024-01-18 03:58:57,879 - __main__ - INFO - ------------ Save best model - MSE: 633.4753 ------------
2024-01-18 03:58:57,880 - __main__ - INFO - Fold 1, mse = 633.4753, mad = 20.5950
2024-01-18 03:58:58,360 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 1.1202
2024-01-18 03:59:15,829 - __main__ - INFO - Fold 1, mse = 659.8279, mad = 20.2928
2024-01-18 03:59:16,450 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 0.8713
2024-01-18 03:59:34,106 - __main__ - INFO - Fold 1, mse = 650.0807, mad = 20.2310
2024-01-18 03:59:34,620 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 1.0459
2024-01-18 03:59:52,712 - __main__ - INFO - Fold 1, mse = 635.1553, mad = 20.5666
2024-01-18 03:59:53,154 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 1.0840
2024-01-18 04:00:12,108 - __main__ - INFO - Fold 1, mse = 662.8716, mad = 20.3004
2024-01-18 04:00:12,689 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 1.0117
2024-01-18 04:00:31,194 - __main__ - INFO - Fold 1, mse = 680.9585, mad = 20.5112
2024-01-18 04:00:31,762 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 0.9691
2024-01-18 04:00:51,329 - __main__ - INFO - Fold 1, epoch 10: Loss = 0.9452 Valid loss = 0.9515 MSE = 710.1398
2024-01-18 04:00:51,330 - __main__ - INFO - Fold 1, mse = 710.1398, mad = 20.8788
2024-01-18 04:00:51,878 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 0.9234
2024-01-18 04:01:11,324 - __main__ - INFO - Fold 1, mse = 765.2659, mad = 21.4794
2024-01-18 04:01:11,833 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 1.0027
2024-01-18 04:01:29,580 - __main__ - INFO - Fold 1, mse = 721.8142, mad = 21.0009
2024-01-18 04:01:30,156 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 0.9516
2024-01-18 04:01:48,828 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 625.8400 ------------
2024-01-18 04:01:48,933 - __main__ - INFO - ------------ Save best model - MSE: 625.8400 ------------
2024-01-18 04:01:48,934 - __main__ - INFO - Fold 1, mse = 625.8400, mad = 20.1941
2024-01-18 04:01:49,343 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 0.8635
2024-01-18 04:02:07,160 - __main__ - INFO - Fold 1, mse = 712.3837, mad = 20.8776
2024-01-18 04:02:07,682 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 0.8531
2024-01-18 04:02:25,846 - __main__ - INFO - Fold 1, mse = 737.8831, mad = 21.1351
2024-01-18 04:02:26,366 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 0.8397
2024-01-18 04:02:44,757 - __main__ - INFO - Fold 1, mse = 882.2655, mad = 23.1343
2024-01-18 04:02:45,312 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 0.9895
2024-01-18 04:03:03,958 - __main__ - INFO - Fold 1, mse = 696.2643, mad = 20.6858
2024-01-18 04:03:04,416 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 1.0658
2024-01-18 04:03:23,077 - __main__ - INFO - Fold 1, mse = 766.2148, mad = 21.4898
2024-01-18 04:03:23,605 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 0.8657
2024-01-18 04:03:43,123 - __main__ - INFO - Fold 1, mse = 631.4930, mad = 20.8332
2024-01-18 04:03:43,625 - __main__ - INFO - Fold 1 Epoch 20 Batch 0: Train Loss = 0.9311
2024-01-18 04:04:02,936 - __main__ - INFO - Fold 1, epoch 20: Loss = 0.9341 Valid loss = 1.0218 MSE = 751.7872
2024-01-18 04:04:02,937 - __main__ - INFO - Fold 1, mse = 751.7872, mad = 21.3301
2024-01-18 04:04:03,526 - __main__ - INFO - Fold 1 Epoch 21 Batch 0: Train Loss = 0.9216
2024-01-18 04:04:21,434 - __main__ - INFO - Fold 1, mse = 854.9420, mad = 22.8062
2024-01-18 04:04:21,906 - __main__ - INFO - Fold 1 Epoch 22 Batch 0: Train Loss = 0.8922
2024-01-18 04:04:40,832 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 613.5950 ------------
2024-01-18 04:04:40,955 - __main__ - INFO - ------------ Save best model - MSE: 613.5950 ------------
2024-01-18 04:04:40,957 - __main__ - INFO - Fold 1, mse = 613.5950, mad = 19.9444
2024-01-18 04:04:41,420 - __main__ - INFO - Fold 1 Epoch 23 Batch 0: Train Loss = 1.1818
2024-01-18 04:05:00,641 - __main__ - INFO - Fold 1, mse = 832.0229, mad = 22.4699
2024-01-18 04:05:01,102 - __main__ - INFO - Fold 1 Epoch 24 Batch 0: Train Loss = 0.9975
2024-01-18 04:05:19,993 - __main__ - INFO - Fold 1, mse = 933.6638, mad = 23.7635
2024-01-18 04:05:20,545 - __main__ - INFO - Fold 1 Epoch 25 Batch 0: Train Loss = 0.9944
2024-01-18 04:05:38,873 - __main__ - INFO - Fold 1, mse = 779.7489, mad = 21.6395
2024-01-18 04:05:39,294 - __main__ - INFO - Fold 1 Epoch 26 Batch 0: Train Loss = 1.0959
2024-01-18 04:05:57,938 - __main__ - INFO - Fold 1, mse = 748.5878, mad = 21.1958
2024-01-18 04:05:58,508 - __main__ - INFO - Fold 1 Epoch 27 Batch 0: Train Loss = 0.9423
2024-01-18 04:06:17,551 - __main__ - INFO - Fold 1, mse = 807.3810, mad = 21.9651
2024-01-18 04:06:18,115 - __main__ - INFO - Fold 1 Epoch 28 Batch 0: Train Loss = 0.9169
2024-01-18 04:06:37,625 - __main__ - INFO - Fold 1, mse = 684.4929, mad = 20.5588
2024-01-18 04:06:38,118 - __main__ - INFO - Fold 1 Epoch 29 Batch 0: Train Loss = 0.9781
2024-01-18 04:06:57,803 - __main__ - INFO - Fold 1, mse = 757.8957, mad = 21.4504
2024-01-18 04:06:59,404 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 17.3569
2024-01-18 04:07:17,576 - __main__ - INFO - Fold 2, epoch 0: Loss = 5.9792 Valid loss = 0.7772 MSE = 569.8493
2024-01-18 04:07:17,577 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 569.8493 ------------
2024-01-18 04:07:17,680 - __main__ - INFO - ------------ Save best model - MSE: 569.8493 ------------
2024-01-18 04:07:17,682 - __main__ - INFO - Fold 2, mse = 569.8493, mad = 19.3414
2024-01-18 04:07:18,271 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 1.0209
2024-01-18 04:07:37,276 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 545.6855 ------------
2024-01-18 04:07:37,398 - __main__ - INFO - ------------ Save best model - MSE: 545.6855 ------------
2024-01-18 04:07:37,399 - __main__ - INFO - Fold 2, mse = 545.6855, mad = 18.4713
2024-01-18 04:07:37,911 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 0.9966
2024-01-18 04:07:55,262 - __main__ - INFO - Fold 2, mse = 549.1325, mad = 18.3344
2024-01-18 04:07:55,797 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 0.9447
2024-01-18 04:08:14,060 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 542.6418 ------------
2024-01-18 04:08:14,186 - __main__ - INFO - ------------ Save best model - MSE: 542.6418 ------------
2024-01-18 04:08:14,187 - __main__ - INFO - Fold 2, mse = 542.6418, mad = 18.2424
2024-01-18 04:08:14,692 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 0.9414
2024-01-18 04:08:32,795 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 541.9739 ------------
2024-01-18 04:08:32,899 - __main__ - INFO - ------------ Save best model - MSE: 541.9739 ------------
2024-01-18 04:08:32,900 - __main__ - INFO - Fold 2, mse = 541.9739, mad = 18.4351
2024-01-18 04:08:33,415 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 1.0121
2024-01-18 04:08:51,751 - __main__ - INFO - Fold 2, mse = 551.1479, mad = 18.4690
2024-01-18 04:08:52,189 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 0.8236
2024-01-18 04:09:10,690 - __main__ - INFO - Fold 2, mse = 543.4236, mad = 18.3083
2024-01-18 04:09:11,282 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 1.0509
2024-01-18 04:09:31,409 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 538.3216 ------------
2024-01-18 04:09:31,519 - __main__ - INFO - ------------ Save best model - MSE: 538.3216 ------------
2024-01-18 04:09:31,520 - __main__ - INFO - Fold 2, mse = 538.3216, mad = 18.2261
2024-01-18 04:09:32,131 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 0.9894
2024-01-18 04:09:50,560 - __main__ - INFO - Fold 2, mse = 557.2650, mad = 18.5608
2024-01-18 04:09:50,974 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 0.9863
2024-01-18 04:10:08,455 - __main__ - INFO - Fold 2, mse = 660.2055, mad = 20.4778
2024-01-18 04:10:08,916 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 0.9578
2024-01-18 04:10:26,829 - __main__ - INFO - Fold 2, epoch 10: Loss = 0.9835 Valid loss = 0.7344 MSE = 553.7648
2024-01-18 04:10:26,831 - __main__ - INFO - Fold 2, mse = 553.7648, mad = 18.5832
2024-01-18 04:10:27,310 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 0.9607
2024-01-18 04:10:44,869 - __main__ - INFO - Fold 2, mse = 608.6864, mad = 19.6581
2024-01-18 04:10:45,332 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 0.9533
2024-01-18 04:11:02,626 - __main__ - INFO - Fold 2, mse = 709.3873, mad = 21.2633
2024-01-18 04:11:03,086 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 1.0500
2024-01-18 04:11:19,484 - __main__ - INFO - Fold 2, mse = 582.1337, mad = 19.0246
2024-01-18 04:11:19,909 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 0.9214
2024-01-18 04:11:36,603 - __main__ - INFO - Fold 2, mse = 615.6527, mad = 19.7716
2024-01-18 04:11:37,123 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 1.0217
2024-01-18 04:11:53,552 - __main__ - INFO - Fold 2, mse = 607.8264, mad = 19.5673
2024-01-18 04:11:54,055 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 0.8610
2024-01-18 04:12:10,895 - __main__ - INFO - Fold 2, mse = 560.6755, mad = 18.7379
2024-01-18 04:12:11,405 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 0.9867
2024-01-18 04:12:29,144 - __main__ - INFO - Fold 2, mse = 615.5615, mad = 19.8348
2024-01-18 04:12:29,690 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 0.9148
2024-01-18 04:12:47,096 - __main__ - INFO - Fold 2, mse = 770.6163, mad = 22.1817
2024-01-18 04:12:47,586 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 1.0319
2024-01-18 04:13:05,391 - __main__ - INFO - Fold 2, mse = 781.2954, mad = 22.3610
2024-01-18 04:13:05,818 - __main__ - INFO - Fold 2 Epoch 20 Batch 0: Train Loss = 0.9752
2024-01-18 04:13:23,999 - __main__ - INFO - Fold 2, epoch 20: Loss = 0.9582 Valid loss = 1.7205 MSE = 775.3091
2024-01-18 04:13:24,000 - __main__ - INFO - Fold 2, mse = 775.3091, mad = 22.2714
2024-01-18 04:13:24,450 - __main__ - INFO - Fold 2 Epoch 21 Batch 0: Train Loss = 1.1316
2024-01-18 04:13:41,617 - __main__ - INFO - Fold 2, mse = 605.1253, mad = 19.7284
2024-01-18 04:13:42,044 - __main__ - INFO - Fold 2 Epoch 22 Batch 0: Train Loss = 0.8920
2024-01-18 04:13:59,090 - __main__ - INFO - Fold 2, mse = 594.7267, mad = 19.5065
2024-01-18 04:13:59,592 - __main__ - INFO - Fold 2 Epoch 23 Batch 0: Train Loss = 0.9956
2024-01-18 04:14:16,380 - __main__ - INFO - Fold 2, mse = 797.3380, mad = 22.5208
2024-01-18 04:14:16,859 - __main__ - INFO - Fold 2 Epoch 24 Batch 0: Train Loss = 1.0019
2024-01-18 04:14:33,626 - __main__ - INFO - Fold 2, mse = 820.7901, mad = 22.6070
2024-01-18 04:14:34,115 - __main__ - INFO - Fold 2 Epoch 25 Batch 0: Train Loss = 1.0222
2024-01-18 04:14:51,159 - __main__ - INFO - Fold 2, mse = 745.8438, mad = 21.6058
2024-01-18 04:14:51,604 - __main__ - INFO - Fold 2 Epoch 26 Batch 0: Train Loss = 0.9849
2024-01-18 04:15:08,786 - __main__ - INFO - Fold 2, mse = 707.3613, mad = 21.2887
2024-01-18 04:15:09,319 - __main__ - INFO - Fold 2 Epoch 27 Batch 0: Train Loss = 0.9128
2024-01-18 04:15:25,862 - __main__ - INFO - Fold 2, mse = 806.3321, mad = 22.5521
2024-01-18 04:15:26,384 - __main__ - INFO - Fold 2 Epoch 28 Batch 0: Train Loss = 0.9415
2024-01-18 04:15:43,590 - __main__ - INFO - Fold 2, mse = 999.0415, mad = 24.7160
2024-01-18 04:15:44,104 - __main__ - INFO - Fold 2 Epoch 29 Batch 0: Train Loss = 1.0053
2024-01-18 04:16:02,290 - __main__ - INFO - Fold 2, mse = 703.0652, mad = 21.2001
2024-01-18 04:16:03,594 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 19.6781
2024-01-18 04:16:21,137 - __main__ - INFO - Fold 3, epoch 0: Loss = 3.8732 Valid loss = 0.9116 MSE = 651.3079
2024-01-18 04:16:21,138 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 651.3079 ------------
2024-01-18 04:16:21,213 - __main__ - INFO - Fold 3, mse = 651.3079, mad = 20.8448
2024-01-18 04:16:21,693 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 1.1399
2024-01-18 04:16:38,499 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 624.2762 ------------
2024-01-18 04:16:38,546 - __main__ - INFO - Fold 3, mse = 624.2762, mad = 19.9272
2024-01-18 04:16:39,033 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 0.9251
2024-01-18 04:16:55,314 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 618.9336 ------------
2024-01-18 04:16:55,375 - __main__ - INFO - Fold 3, mse = 618.9336, mad = 19.9294
2024-01-18 04:16:55,808 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 0.9643
2024-01-18 04:17:12,099 - __main__ - INFO - Fold 3, mse = 631.4752, mad = 19.9394
2024-01-18 04:17:12,527 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 1.0566
2024-01-18 04:17:29,161 - __main__ - INFO - Fold 3, mse = 690.8201, mad = 20.7467
2024-01-18 04:17:29,622 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 0.9990
2024-01-18 04:17:46,072 - __main__ - INFO - Fold 3, mse = 656.2347, mad = 20.3240
2024-01-18 04:17:46,522 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 0.9353
2024-01-18 04:18:03,622 - __main__ - INFO - Fold 3, mse = 640.4016, mad = 20.2078
2024-01-18 04:18:04,078 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 1.0733
2024-01-18 04:18:20,974 - __main__ - INFO - Fold 3, mse = 627.8571, mad = 20.1888
2024-01-18 04:18:21,444 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 0.9703
2024-01-18 04:18:38,628 - __main__ - INFO - Fold 3, mse = 629.4040, mad = 20.1573
2024-01-18 04:18:39,207 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 1.1132
2024-01-18 04:18:56,189 - __main__ - INFO - Fold 3, mse = 730.8017, mad = 21.4309
2024-01-18 04:18:56,688 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 1.0242
2024-01-18 04:19:14,917 - __main__ - INFO - Fold 3, epoch 10: Loss = 0.9710 Valid loss = 0.8968 MSE = 638.1349
2024-01-18 04:19:14,919 - __main__ - INFO - Fold 3, mse = 638.1349, mad = 20.2042
2024-01-18 04:19:15,392 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 1.0593
2024-01-18 04:19:32,654 - __main__ - INFO - Fold 3, mse = 675.7176, mad = 20.7740
2024-01-18 04:19:33,102 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 1.0377
2024-01-18 04:19:50,163 - __main__ - INFO - Fold 3, mse = 645.7640, mad = 20.3258
2024-01-18 04:19:50,640 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 1.1186
2024-01-18 04:20:07,019 - __main__ - INFO - Fold 3, mse = 679.9009, mad = 20.6887
2024-01-18 04:20:07,551 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 1.0065
2024-01-18 04:20:24,301 - __main__ - INFO - Fold 3, mse = 667.9975, mad = 20.5931
2024-01-18 04:20:24,786 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 0.9175
2024-01-18 04:20:41,364 - __main__ - INFO - Fold 3, mse = 673.4206, mad = 20.5926
2024-01-18 04:20:41,856 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 0.9251
2024-01-18 04:20:58,660 - __main__ - INFO - Fold 3, mse = 641.3312, mad = 20.2139
2024-01-18 04:20:59,104 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 0.9262
2024-01-18 04:21:15,756 - __main__ - INFO - Fold 3, mse = 675.5153, mad = 20.7260
2024-01-18 04:21:16,275 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 1.1502
2024-01-18 04:21:33,498 - __main__ - INFO - Fold 3, mse = 807.6460, mad = 22.2971
2024-01-18 04:21:33,976 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 0.9321
2024-01-18 04:21:52,611 - __main__ - INFO - Fold 3, mse = 717.1849, mad = 21.0736
2024-01-18 04:21:53,188 - __main__ - INFO - Fold 3 Epoch 20 Batch 0: Train Loss = 0.9346
2024-01-18 04:22:10,979 - __main__ - INFO - Fold 3, epoch 20: Loss = 0.9548 Valid loss = 1.3530 MSE = 820.0933
2024-01-18 04:22:10,980 - __main__ - INFO - Fold 3, mse = 820.0933, mad = 22.2767
2024-01-18 04:22:11,514 - __main__ - INFO - Fold 3 Epoch 21 Batch 0: Train Loss = 0.8687
2024-01-18 04:22:29,306 - __main__ - INFO - Fold 3, mse = 739.5975, mad = 21.2385
2024-01-18 04:22:29,747 - __main__ - INFO - Fold 3 Epoch 22 Batch 0: Train Loss = 0.8696
2024-01-18 04:22:47,093 - __main__ - INFO - Fold 3, mse = 843.2729, mad = 22.5037
2024-01-18 04:22:47,563 - __main__ - INFO - Fold 3 Epoch 23 Batch 0: Train Loss = 1.0094
2024-01-18 04:23:04,107 - __main__ - INFO - Fold 3, mse = 858.5342, mad = 22.7345
2024-01-18 04:23:04,541 - __main__ - INFO - Fold 3 Epoch 24 Batch 0: Train Loss = 0.8898
2024-01-18 04:23:21,175 - __main__ - INFO - Fold 3, mse = 770.6214, mad = 21.7128
2024-01-18 04:23:21,657 - __main__ - INFO - Fold 3 Epoch 25 Batch 0: Train Loss = 1.1575
2024-01-18 04:23:38,229 - __main__ - INFO - Fold 3, mse = 785.6282, mad = 21.9452
2024-01-18 04:23:38,739 - __main__ - INFO - Fold 3 Epoch 26 Batch 0: Train Loss = 0.9572
2024-01-18 04:23:55,182 - __main__ - INFO - Fold 3, mse = 817.1407, mad = 22.4509
2024-01-18 04:23:55,586 - __main__ - INFO - Fold 3 Epoch 27 Batch 0: Train Loss = 0.9755
2024-01-18 04:24:11,466 - __main__ - INFO - Fold 3, mse = 825.2895, mad = 22.3501
2024-01-18 04:24:11,899 - __main__ - INFO - Fold 3 Epoch 28 Batch 0: Train Loss = 1.0590
2024-01-18 04:24:27,088 - __main__ - INFO - Fold 3, mse = 779.2282, mad = 21.9237
2024-01-18 04:24:27,576 - __main__ - INFO - Fold 3 Epoch 29 Batch 0: Train Loss = 0.9004
2024-01-18 04:24:42,925 - __main__ - INFO - Fold 3, mse = 947.0709, mad = 23.8943
2024-01-18 04:24:44,305 - __main__ - INFO - Fold 4 Epoch 0 Batch 0: Train Loss = 18.1816
2024-01-18 04:25:00,130 - __main__ - INFO - Fold 4, epoch 0: Loss = 2.4800 Valid loss = 1.1380 MSE = 816.2749
2024-01-18 04:25:00,132 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 816.2749 ------------
2024-01-18 04:25:00,207 - __main__ - INFO - Fold 4, mse = 816.2749, mad = 23.1134
2024-01-18 04:25:00,669 - __main__ - INFO - Fold 4 Epoch 1 Batch 0: Train Loss = 0.8474
2024-01-18 04:25:16,872 - __main__ - INFO - Fold 4, mse = 841.3248, mad = 23.3272
2024-01-18 04:25:17,303 - __main__ - INFO - Fold 4 Epoch 2 Batch 0: Train Loss = 0.9318
2024-01-18 04:25:33,011 - __main__ - INFO - Fold 4, mse = 851.7866, mad = 23.2881
2024-01-18 04:25:33,462 - __main__ - INFO - Fold 4 Epoch 3 Batch 0: Train Loss = 0.8632
2024-01-18 04:25:49,217 - __main__ - INFO - Fold 4, mse = 831.7455, mad = 23.2595
2024-01-18 04:25:49,693 - __main__ - INFO - Fold 4 Epoch 4 Batch 0: Train Loss = 0.8539
2024-01-18 04:26:04,797 - __main__ - INFO - Fold 4, mse = 846.6688, mad = 23.2745
2024-01-18 04:26:05,241 - __main__ - INFO - Fold 4 Epoch 5 Batch 0: Train Loss = 0.9331
2024-01-18 04:26:20,461 - __main__ - INFO - Fold 4, mse = 849.8091, mad = 23.3133
2024-01-18 04:26:20,985 - __main__ - INFO - Fold 4 Epoch 6 Batch 0: Train Loss = 0.8918
2024-01-18 04:26:35,731 - __main__ - INFO - Fold 4, mse = 864.5068, mad = 23.2790
2024-01-18 04:26:36,146 - __main__ - INFO - Fold 4 Epoch 7 Batch 0: Train Loss = 0.7644
2024-01-18 04:26:51,425 - __main__ - INFO - Fold 4, mse = 837.6782, mad = 23.3000
2024-01-18 04:26:51,988 - __main__ - INFO - Fold 4 Epoch 8 Batch 0: Train Loss = 0.9790
2024-01-18 04:27:07,634 - __main__ - INFO - Fold 4, mse = 876.5536, mad = 23.2834
2024-01-18 04:27:08,104 - __main__ - INFO - Fold 4 Epoch 9 Batch 0: Train Loss = 0.8811
2024-01-18 04:27:23,795 - __main__ - INFO - Fold 4, mse = 870.3937, mad = 23.2902
2024-01-18 04:27:24,261 - __main__ - INFO - Fold 4 Epoch 10 Batch 0: Train Loss = 0.9348
2024-01-18 04:27:40,022 - __main__ - INFO - Fold 4, epoch 10: Loss = 0.9125 Valid loss = 1.1937 MSE = 875.0414
2024-01-18 04:27:40,023 - __main__ - INFO - Fold 4, mse = 875.0414, mad = 23.2987
2024-01-18 04:27:40,508 - __main__ - INFO - Fold 4 Epoch 11 Batch 0: Train Loss = 1.0012
2024-01-18 04:27:56,672 - __main__ - INFO - Fold 4, mse = 892.8378, mad = 23.3228
2024-01-18 04:27:57,072 - __main__ - INFO - Fold 4 Epoch 12 Batch 0: Train Loss = 0.9545
2024-01-18 04:28:12,710 - __main__ - INFO - Fold 4, mse = 901.5189, mad = 23.3594
2024-01-18 04:28:13,252 - __main__ - INFO - Fold 4 Epoch 13 Batch 0: Train Loss = 0.9683
2024-01-18 04:28:29,647 - __main__ - INFO - Fold 4, mse = 879.5098, mad = 23.3446
2024-01-18 04:28:30,121 - __main__ - INFO - Fold 4 Epoch 14 Batch 0: Train Loss = 0.8630
2024-01-18 04:28:45,789 - __main__ - INFO - Fold 4, mse = 869.7195, mad = 23.3537
2024-01-18 04:28:46,260 - __main__ - INFO - Fold 4 Epoch 15 Batch 0: Train Loss = 1.0236
2024-01-18 04:29:02,304 - __main__ - INFO - Fold 4, mse = 916.1129, mad = 23.4351
2024-01-18 04:29:02,758 - __main__ - INFO - Fold 4 Epoch 16 Batch 0: Train Loss = 1.0029
2024-01-18 04:29:19,149 - __main__ - INFO - Fold 4, mse = 899.6968, mad = 23.4019
2024-01-18 04:29:19,597 - __main__ - INFO - Fold 4 Epoch 17 Batch 0: Train Loss = 0.8462
2024-01-18 04:29:35,757 - __main__ - INFO - Fold 4, mse = 878.7817, mad = 23.4739
2024-01-18 04:29:36,247 - __main__ - INFO - Fold 4 Epoch 18 Batch 0: Train Loss = 0.9480
2024-01-18 04:29:52,653 - __main__ - INFO - Fold 4, mse = 897.3107, mad = 23.4621
2024-01-18 04:29:53,124 - __main__ - INFO - Fold 4 Epoch 19 Batch 0: Train Loss = 0.7857
2024-01-18 04:30:10,220 - __main__ - INFO - Fold 4, mse = 907.6589, mad = 23.4667
2024-01-18 04:30:10,634 - __main__ - INFO - Fold 4 Epoch 20 Batch 0: Train Loss = 0.8125
2024-01-18 04:30:28,092 - __main__ - INFO - Fold 4, epoch 20: Loss = 0.8852 Valid loss = 1.2483 MSE = 925.4428
2024-01-18 04:30:28,101 - __main__ - INFO - Fold 4, mse = 925.4428, mad = 23.5659
2024-01-18 04:30:28,706 - __main__ - INFO - Fold 4 Epoch 21 Batch 0: Train Loss = 0.9953
2024-01-18 04:30:46,058 - __main__ - INFO - Fold 4, mse = 955.4614, mad = 23.6197
2024-01-18 04:30:46,622 - __main__ - INFO - Fold 4 Epoch 22 Batch 0: Train Loss = 0.8781
2024-01-18 04:31:03,512 - __main__ - INFO - Fold 4, mse = 953.4800, mad = 23.7293
2024-01-18 04:31:03,935 - __main__ - INFO - Fold 4 Epoch 23 Batch 0: Train Loss = 1.0178
2024-01-18 04:31:21,234 - __main__ - INFO - Fold 4, mse = 985.6396, mad = 23.7532
2024-01-18 04:31:21,701 - __main__ - INFO - Fold 4 Epoch 24 Batch 0: Train Loss = 0.8773
2024-01-18 04:31:37,952 - __main__ - INFO - Fold 4, mse = 903.9036, mad = 23.6375
2024-01-18 04:31:38,458 - __main__ - INFO - Fold 4 Epoch 25 Batch 0: Train Loss = 0.8402
2024-01-18 04:31:54,525 - __main__ - INFO - Fold 4, mse = 934.6306, mad = 23.6397
2024-01-18 04:31:54,987 - __main__ - INFO - Fold 4 Epoch 26 Batch 0: Train Loss = 0.8374
2024-01-18 04:32:11,476 - __main__ - INFO - Fold 4, mse = 946.0298, mad = 23.6979
2024-01-18 04:32:11,969 - __main__ - INFO - Fold 4 Epoch 27 Batch 0: Train Loss = 0.8029
2024-01-18 04:32:28,761 - __main__ - INFO - Fold 4, mse = 955.3393, mad = 23.7572
2024-01-18 04:32:29,280 - __main__ - INFO - Fold 4 Epoch 28 Batch 0: Train Loss = 0.8884
2024-01-18 04:32:45,331 - __main__ - INFO - Fold 4, mse = 982.6415, mad = 23.7852
2024-01-18 04:32:45,843 - __main__ - INFO - Fold 4 Epoch 29 Batch 0: Train Loss = 0.7636
2024-01-18 04:33:03,194 - __main__ - INFO - Fold 4, mse = 975.6768, mad = 23.7367
2024-01-18 04:33:04,698 - __main__ - INFO - Fold 5 Epoch 0 Batch 0: Train Loss = 9.4128
2024-01-18 04:33:21,962 - __main__ - INFO - Fold 5, epoch 0: Loss = 1.8103 Valid loss = 1.0857 MSE = 770.1818
2024-01-18 04:33:21,963 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 770.1818 ------------
2024-01-18 04:33:22,019 - __main__ - INFO - Fold 5, mse = 770.1818, mad = 23.6003
2024-01-18 04:33:22,452 - __main__ - INFO - Fold 5 Epoch 1 Batch 0: Train Loss = 0.9771
2024-01-18 04:33:39,031 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 759.9734 ------------
2024-01-18 04:33:39,074 - __main__ - INFO - Fold 5, mse = 759.9734, mad = 23.3655
2024-01-18 04:33:39,544 - __main__ - INFO - Fold 5 Epoch 2 Batch 0: Train Loss = 0.9551
2024-01-18 04:33:55,675 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 751.1380 ------------
2024-01-18 04:33:55,745 - __main__ - INFO - Fold 5, mse = 751.1380, mad = 23.1570
2024-01-18 04:33:56,185 - __main__ - INFO - Fold 5 Epoch 3 Batch 0: Train Loss = 0.9572
2024-01-18 04:34:12,787 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 668.0501 ------------
2024-01-18 04:34:12,834 - __main__ - INFO - Fold 5, mse = 668.0501, mad = 21.2132
2024-01-18 04:34:13,220 - __main__ - INFO - Fold 5 Epoch 4 Batch 0: Train Loss = 0.9695
2024-01-18 04:34:29,331 - __main__ - INFO - Fold 5, mse = 705.6052, mad = 20.8135
2024-01-18 04:34:29,787 - __main__ - INFO - Fold 5 Epoch 5 Batch 0: Train Loss = 0.9488
2024-01-18 04:34:45,408 - __main__ - INFO - Fold 5, mse = 684.6701, mad = 20.4734
2024-01-18 04:34:45,754 - __main__ - INFO - Fold 5 Epoch 6 Batch 0: Train Loss = 0.8956
2024-01-18 04:35:01,796 - __main__ - INFO - Fold 5, mse = 766.3085, mad = 21.0966
2024-01-18 04:35:02,264 - __main__ - INFO - Fold 5 Epoch 7 Batch 0: Train Loss = 1.0961
2024-01-18 04:35:19,097 - __main__ - INFO - Fold 5, mse = 885.9864, mad = 22.8099
2024-01-18 04:35:19,551 - __main__ - INFO - Fold 5 Epoch 8 Batch 0: Train Loss = 0.9247
2024-01-18 04:35:36,317 - __main__ - INFO - Fold 5, mse = 853.0717, mad = 22.3373
2024-01-18 04:35:36,809 - __main__ - INFO - Fold 5 Epoch 9 Batch 0: Train Loss = 0.8312
2024-01-18 04:35:54,326 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 655.1040 ------------
2024-01-18 04:35:54,395 - __main__ - INFO - Fold 5, mse = 655.1040, mad = 20.6255
2024-01-18 04:35:54,854 - __main__ - INFO - Fold 5 Epoch 10 Batch 0: Train Loss = 0.9083
2024-01-18 04:36:11,015 - __main__ - INFO - Fold 5, epoch 10: Loss = 0.9483 Valid loss = 1.1313 MSE = 774.6718
2024-01-18 04:36:11,017 - __main__ - INFO - Fold 5, mse = 774.6718, mad = 21.1729
2024-01-18 04:36:11,553 - __main__ - INFO - Fold 5 Epoch 11 Batch 0: Train Loss = 1.0426
2024-01-18 04:36:27,541 - __main__ - INFO - Fold 5, mse = 806.7620, mad = 21.6726
2024-01-18 04:36:27,990 - __main__ - INFO - Fold 5 Epoch 12 Batch 0: Train Loss = 0.9108
2024-01-18 04:36:44,885 - __main__ - INFO - Fold 5, mse = 725.1805, mad = 20.6897
2024-01-18 04:36:45,438 - __main__ - INFO - Fold 5 Epoch 13 Batch 0: Train Loss = 1.0442
2024-01-18 04:37:02,816 - __main__ - INFO - Fold 5, mse = 837.5602, mad = 21.9958
2024-01-18 04:37:03,260 - __main__ - INFO - Fold 5 Epoch 14 Batch 0: Train Loss = 0.9121
2024-01-18 04:37:19,873 - __main__ - INFO - Fold 5, mse = 780.8209, mad = 21.2200
2024-01-18 04:37:20,331 - __main__ - INFO - Fold 5 Epoch 15 Batch 0: Train Loss = 0.9053
2024-01-18 04:37:37,325 - __main__ - INFO - Fold 5, mse = 889.8362, mad = 22.8526
2024-01-18 04:37:37,765 - __main__ - INFO - Fold 5 Epoch 16 Batch 0: Train Loss = 0.9262
2024-01-18 04:37:54,780 - __main__ - INFO - Fold 5, mse = 1027.7036, mad = 24.7703
2024-01-18 04:37:55,260 - __main__ - INFO - Fold 5 Epoch 17 Batch 0: Train Loss = 1.0287
2024-01-18 04:38:11,641 - __main__ - INFO - Fold 5, mse = 822.3946, mad = 21.6961
2024-01-18 04:38:12,075 - __main__ - INFO - Fold 5 Epoch 18 Batch 0: Train Loss = 1.0090
2024-01-18 04:38:29,165 - __main__ - INFO - Fold 5, mse = 882.0251, mad = 22.4194
2024-01-18 04:38:29,592 - __main__ - INFO - Fold 5 Epoch 19 Batch 0: Train Loss = 1.0251
2024-01-18 04:38:47,415 - __main__ - INFO - Fold 5, mse = 1034.6094, mad = 24.7585
2024-01-18 04:38:47,885 - __main__ - INFO - Fold 5 Epoch 20 Batch 0: Train Loss = 0.8348
2024-01-18 04:39:05,986 - __main__ - INFO - Fold 5, epoch 20: Loss = 0.9143 Valid loss = 1.0042 MSE = 703.9792
2024-01-18 04:39:05,987 - __main__ - INFO - Fold 5, mse = 703.9792, mad = 20.1546
2024-01-18 04:39:06,522 - __main__ - INFO - Fold 5 Epoch 21 Batch 0: Train Loss = 1.0813
2024-01-18 04:39:24,093 - __main__ - INFO - Fold 5, mse = 874.7063, mad = 22.5167
2024-01-18 04:39:24,585 - __main__ - INFO - Fold 5 Epoch 22 Batch 0: Train Loss = 0.9057
2024-01-18 04:39:42,729 - __main__ - INFO - Fold 5, mse = 752.8681, mad = 20.9972
2024-01-18 04:39:43,234 - __main__ - INFO - Fold 5 Epoch 23 Batch 0: Train Loss = 0.8785
2024-01-18 04:40:01,152 - __main__ - INFO - Fold 5, mse = 902.3720, mad = 23.0314
2024-01-18 04:40:01,614 - __main__ - INFO - Fold 5 Epoch 24 Batch 0: Train Loss = 0.7134
2024-01-18 04:40:18,436 - __main__ - INFO - Fold 5, mse = 724.9596, mad = 20.5189
2024-01-18 04:40:18,867 - __main__ - INFO - Fold 5 Epoch 25 Batch 0: Train Loss = 0.9268
2024-01-18 04:40:35,921 - __main__ - INFO - Fold 5, mse = 878.9907, mad = 22.6766
2024-01-18 04:40:36,400 - __main__ - INFO - Fold 5 Epoch 26 Batch 0: Train Loss = 1.0435
2024-01-18 04:40:53,019 - __main__ - INFO - Fold 5, mse = 831.5833, mad = 21.9807
2024-01-18 04:40:53,460 - __main__ - INFO - Fold 5 Epoch 27 Batch 0: Train Loss = 0.9216
2024-01-18 04:41:10,388 - __main__ - INFO - Fold 5, mse = 1065.6843, mad = 25.2957
2024-01-18 04:41:10,885 - __main__ - INFO - Fold 5 Epoch 28 Batch 0: Train Loss = 0.8886
2024-01-18 04:41:27,946 - __main__ - INFO - Fold 5, mse = 904.7855, mad = 22.9090
2024-01-18 04:41:28,378 - __main__ - INFO - Fold 5 Epoch 29 Batch 0: Train Loss = 0.7771
2024-01-18 04:41:46,022 - __main__ - INFO - Fold 5, mse = 695.2265, mad = 20.3360
2024-01-18 04:41:46,024 - __main__ - INFO - mse 648.4458(92.1128)
2024-01-18 04:41:46,024 - __main__ - INFO - mad 20.3678(1.5851)
07
2024-01-18 04:40:53,460 - __main__ - INFO - Fold 5 Epoch 27 Batch 0: Train Loss = 0.9216
2024-01-18 04:41:10,388 - __main__ - INFO - Fold 5, mse = 1065.6843, mad = 25.2957
2024-01-18 04:41:10,885 - __main__ - INFO - Fold 5 Epoch 28 Batch 0: Train Loss = 0.8886
2024-01-18 04:41:27,946 - __main__ - INFO - Fold 5, mse = 904.7855, mad = 22.9090
2024-01-18 04:41:28,378 - __main__ - INFO - Fold 5 Epoch 29 Batch 0: Train Loss = 0.7771
2024-01-18 04:41:46,022 - __main__ - INFO - Fold 5, mse = 695.2265, mad = 20.3360
2024-01-18 04:41:46,024 - __main__ - INFO - mse 648.4458(92.1128)
2024-01-18 04:41:46,024 - __main__ - INFO - mad 20.3678(1.5851)
