2024-03-21 10:32:14,774 - __main__ - INFO - 这是希望输出的info内容
2024-03-21 10:32:14,774 - __main__ - WARNING - 这是希望输出的warning内容
2024-03-21 10:33:25,600 - __main__ - INFO - 32269
2024-03-21 10:33:25,600 - __main__ - INFO - 4034
2024-03-21 10:33:25,601 - __main__ - INFO - 4033
2024-03-21 10:33:48,478 - __main__ - INFO - Batch 0: Test Loss = 0.0866
2024-03-21 10:33:53,025 - __main__ - INFO - 
==>Predicting on test
2024-03-21 10:33:53,026 - __main__ - INFO - Test Loss = 0.1194
2024-03-21 10:33:53,169 - __main__ - INFO - load target data
2024-03-21 10:33:59,426 - __main__ - INFO - Batch 0: Test Loss = 0.1309
2024-03-21 10:34:05,836 - __main__ - INFO - Batch 20: Test Loss = 0.1464
2024-03-21 10:34:12,060 - __main__ - INFO - Batch 40: Test Loss = 0.1320
2024-03-21 10:34:18,211 - __main__ - INFO - Batch 60: Test Loss = 0.1008
2024-03-21 10:34:24,465 - __main__ - INFO - Batch 80: Test Loss = 0.0734
2024-03-21 10:34:30,170 - __main__ - INFO - Batch 100: Test Loss = 0.0563
2024-03-21 10:34:36,390 - __main__ - INFO - Batch 120: Test Loss = 0.0909
2024-03-21 10:34:38,165 - __main__ - INFO - Training Student
2024-03-21 10:34:39,758 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 1.6680
2024-03-21 10:34:58,764 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 1.4843
2024-03-21 10:35:16,826 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 1.4869
2024-03-21 10:35:34,146 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 1.4856
2024-03-21 10:35:51,186 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 1.4324
2024-03-21 10:36:10,307 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 1.4493
2024-03-21 10:36:30,556 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 1.4898
2024-03-21 10:36:36,455 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4533 ------------
2024-03-21 10:36:37,447 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4493 ------------
2024-03-21 10:36:38,385 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4181 ------------
2024-03-21 10:36:48,916 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 1.5030
2024-03-21 10:37:08,111 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 1.4983
2024-03-21 10:37:25,726 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 1.4917
2024-03-21 10:37:42,976 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 1.4818
2024-03-21 10:38:00,854 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 1.4871
2024-03-21 10:38:19,057 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 1.4447
2024-03-21 10:38:37,355 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 1.4694
2024-03-21 10:38:51,717 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 1.4673
2024-03-21 10:39:10,634 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 1.4642
2024-03-21 10:39:27,670 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 1.4705
2024-03-21 10:39:45,514 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 1.4797
2024-03-21 10:40:04,205 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 1.4521
2024-03-21 10:40:22,630 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 1.4456
2024-03-21 10:40:41,094 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 1.4630
2024-03-21 10:40:56,306 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 1.4675
2024-03-21 10:41:17,026 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 1.4744
2024-03-21 10:41:36,372 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 1.4717
2024-03-21 10:41:53,411 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 1.4811
2024-03-21 10:42:11,568 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 1.4694
2024-03-21 10:42:29,560 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 1.4516
2024-03-21 10:42:47,559 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 1.4625
2024-03-21 10:43:02,399 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 1.4741
2024-03-21 10:43:21,704 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 1.4575
2024-03-21 10:43:39,782 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 1.4781
2024-03-21 10:43:57,846 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 1.4757
2024-03-21 10:44:15,843 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 1.4403
2024-03-21 10:44:35,475 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 1.4475
2024-03-21 10:44:53,610 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 1.4559
2024-03-21 10:45:07,995 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 1.4616
2024-03-21 10:45:27,198 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 1.4657
2024-03-21 10:45:44,856 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 1.4650
2024-03-21 10:46:02,940 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 1.4778
2024-03-21 10:46:20,886 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 1.4490
2024-03-21 10:46:39,578 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 1.4419
2024-03-21 10:46:57,694 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 1.4556
2024-03-21 10:47:12,010 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 1.4554
2024-03-21 10:47:31,026 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 1.4640
2024-03-21 10:47:47,998 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 1.4635
2024-03-21 10:48:05,121 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 1.4772
2024-03-21 10:48:22,390 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 1.4465
2024-03-21 10:48:40,150 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 1.4406
2024-03-21 10:48:58,745 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 1.4540
2024-03-21 10:49:12,663 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 1.4598
2024-03-21 10:49:31,742 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 1.4644
2024-03-21 10:49:49,336 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 1.4667
2024-03-21 10:50:06,863 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 1.4781
2024-03-21 10:50:23,647 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 1.4480
2024-03-21 10:50:42,701 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 1.4388
2024-03-21 10:51:01,368 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 1.4541
2024-03-21 10:51:15,222 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 1.4604
2024-03-21 10:51:33,454 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 1.4635
2024-03-21 10:51:50,993 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 1.4682
2024-03-21 10:52:08,278 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 1.4793
2024-03-21 10:52:25,494 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 1.4453
2024-03-21 10:52:43,226 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 1.4447
2024-03-21 10:53:01,612 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 1.4541
2024-03-21 10:53:15,360 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 1.4610
2024-03-21 10:53:33,956 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 1.4655
2024-03-21 10:53:52,257 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 1.4670
2024-03-21 10:54:09,421 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 1.4789
2024-03-21 10:54:27,413 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 1.4474
2024-03-21 10:54:45,639 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 1.4408
2024-03-21 10:55:04,010 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 1.4527
2024-03-21 10:55:18,296 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 1.4566
2024-03-21 10:55:36,808 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 1.4617
2024-03-21 10:55:54,628 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 1.4632
2024-03-21 10:56:11,994 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 1.4768
2024-03-21 10:56:30,702 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 1.4461
2024-03-21 10:56:49,089 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 1.4404
2024-03-21 10:57:07,684 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 1.4561
2024-03-21 10:57:22,211 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 1.4612
2024-03-21 10:57:41,232 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 1.4637
2024-03-21 10:57:58,740 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 1.4670
2024-03-21 10:58:16,002 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 1.4778
2024-03-21 10:58:34,217 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 1.4479
2024-03-21 10:58:52,164 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 1.4404
2024-03-21 10:59:11,012 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 1.4535
2024-03-21 10:59:25,997 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 1.4573
2024-03-21 10:59:45,250 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 1.4640
2024-03-21 11:00:03,594 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 1.4643
2024-03-21 11:00:21,263 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 1.4761
2024-03-21 11:00:38,981 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 1.4485
2024-03-21 11:00:58,045 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 1.4413
2024-03-21 11:01:17,432 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 1.4533
2024-03-21 11:01:33,086 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 1.4595
2024-03-21 11:01:53,438 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 1.4638
2024-03-21 11:02:12,324 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 1.4641
2024-03-21 11:02:30,797 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 1.4777
2024-03-21 11:02:50,198 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 1.4469
2024-03-21 11:03:10,345 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 1.4412
2024-03-21 11:03:30,277 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 1.4524
2024-03-21 11:03:47,209 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 1.4611
2024-03-21 11:04:08,303 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 1.4629
2024-03-21 11:04:27,760 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 1.4673
2024-03-21 11:04:46,198 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 1.4760
2024-03-21 11:05:04,474 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 1.4459
2024-03-21 11:05:23,736 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 1.4399
2024-03-21 11:05:42,964 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 1.4547
2024-03-21 11:05:59,282 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 1.4621
2024-03-21 11:06:19,770 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 1.4639
2024-03-21 11:06:39,911 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 1.4663
2024-03-21 11:06:59,658 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 1.4769
2024-03-21 11:07:18,851 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 1.4448
2024-03-21 11:07:38,819 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 1.4415
2024-03-21 11:07:58,750 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 1.4539
2024-03-21 11:08:15,435 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 1.4587
2024-03-21 11:08:34,357 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 1.4631
2024-03-21 11:08:50,926 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 1.4663
2024-03-21 11:09:08,663 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 1.4775
2024-03-21 11:09:26,260 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 1.4471
2024-03-21 11:09:44,324 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 1.4404
2024-03-21 11:10:03,660 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 1.4559
2024-03-21 11:10:19,561 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 1.4607
2024-03-21 11:10:39,931 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 1.4633
2024-03-21 11:10:57,551 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 1.4655
2024-03-21 11:11:14,827 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 1.4774
2024-03-21 11:11:32,011 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 1.4440
2024-03-21 11:11:50,531 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 1.4378
2024-03-21 11:12:09,813 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 1.4536
2024-03-21 11:12:24,876 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 1.4632
2024-03-21 11:12:44,514 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 1.4637
2024-03-21 11:13:01,662 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 1.4662
2024-03-21 11:13:18,674 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 1.4766
2024-03-21 11:13:36,453 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 1.4472
2024-03-21 11:13:54,288 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 1.4406
2024-03-21 11:14:12,436 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 1.4515
2024-03-21 11:14:26,636 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 1.4615
2024-03-21 11:14:44,252 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 1.4619
2024-03-21 11:14:59,651 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 1.4670
2024-03-21 11:15:14,959 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 1.4801
2024-03-21 11:15:31,129 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 1.4450
2024-03-21 11:15:46,626 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 1.4361
2024-03-21 11:16:04,471 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 1.4548
2024-03-21 11:16:19,504 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 1.4624
2024-03-21 11:16:37,410 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 1.4628
2024-03-21 11:16:53,169 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 1.4675
2024-03-21 11:17:08,860 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 1.4769
2024-03-21 11:17:24,392 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 1.4488
2024-03-21 11:17:41,593 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 1.4404
2024-03-21 11:17:59,266 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 1.4517
2024-03-21 11:18:12,706 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 1.4642
2024-03-21 11:18:29,958 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 1.4649
2024-03-21 11:18:44,999 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 1.4676
2024-03-21 11:19:00,408 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 1.4802
2024-03-21 11:19:15,948 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 1.4475
2024-03-21 11:19:32,067 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 1.4428
2024-03-21 11:19:48,018 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 1.4547
2024-03-21 11:20:01,352 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 1.4591
2024-03-21 11:20:18,240 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 1.4642
2024-03-21 11:20:34,128 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 1.4660
2024-03-21 11:20:48,963 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 1.4769
2024-03-21 11:21:03,914 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 1.4508
2024-03-21 11:21:19,571 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 1.4396
2024-03-21 11:21:36,349 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 1.4505
2024-03-21 11:21:49,904 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 1.4612
2024-03-21 11:22:06,568 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 1.4668
2024-03-21 11:22:22,248 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 1.4660
2024-03-21 11:22:37,043 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 1.4782
2024-03-21 11:22:52,392 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 1.4478
2024-03-21 11:23:09,037 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 1.4435
2024-03-21 11:23:26,163 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 1.4488
2024-03-21 11:23:39,485 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 1.4612
2024-03-21 11:23:56,428 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 1.4629
2024-03-21 11:24:12,060 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 1.4679
2024-03-21 11:24:27,530 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 1.4763
2024-03-21 11:24:43,228 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 1.4481
2024-03-21 11:24:59,019 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 1.4409
2024-03-21 11:25:15,210 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 1.4503
2024-03-21 11:25:28,371 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 1.4622
2024-03-21 11:25:44,732 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 1.4616
2024-03-21 11:25:59,979 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 1.4682
2024-03-21 11:26:15,335 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 1.4777
2024-03-21 11:26:30,407 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 1.4485
2024-03-21 11:26:45,997 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 1.4401
2024-03-21 11:27:02,491 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 1.4514
2024-03-21 11:27:15,136 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 1.4633
2024-03-21 11:27:31,558 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 1.4649
2024-03-21 11:27:47,124 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 1.4694
2024-03-21 11:28:02,930 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 1.4775
2024-03-21 11:28:18,611 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 1.4498
2024-03-21 11:28:35,139 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 1.4375
2024-03-21 11:28:51,592 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 1.4575
2024-03-21 11:29:05,093 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 1.4607
2024-03-21 11:29:23,616 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 1.4637
2024-03-21 11:29:40,024 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 1.4657
2024-03-21 11:29:55,619 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 1.4781
2024-03-21 11:30:11,869 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 1.4422
2024-03-21 11:30:28,490 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 1.4429
2024-03-21 11:30:45,527 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 1.4538
2024-03-21 11:30:59,507 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 1.4623
2024-03-21 11:31:17,052 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 1.4656
2024-03-21 11:31:33,443 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 1.4661
2024-03-21 11:31:50,322 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 1.4800
2024-03-21 11:32:06,245 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 1.4431
2024-03-21 11:32:23,378 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 1.4404
2024-03-21 11:32:39,011 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 1.4533
2024-03-21 11:32:52,257 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 1.4602
2024-03-21 11:33:09,339 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 1.4632
2024-03-21 11:33:27,248 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 1.4677
2024-03-21 11:33:44,258 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 1.4739
2024-03-21 11:34:02,036 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 1.4472
2024-03-21 11:34:21,466 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 1.4449
2024-03-21 11:34:41,277 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 1.4514
2024-03-21 11:34:56,131 - __main__ - INFO - last saved model is in epoch 0
2024-03-21 11:34:56,665 - __main__ - INFO - Batch 0: Test Loss = 0.3411
2024-03-21 11:35:02,349 - __main__ - INFO - 
==>Predicting on test
2024-03-21 11:35:02,359 - __main__ - INFO - Test Loss = 0.2615
2024-03-21 12:02:59,747 - __main__ - INFO - Transfer Target Dataset & Model
2024-03-21 12:03:04,556 - __main__ - INFO - [[0.35672856748565834, 0.362193782095267, -0.0415507190085758, -0.39134565918228126, 0.7630646387621486, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [2.891396229598206, -0.1650564631881193, -0.7197426028629784, -0.5274138023823791, 0.7630646387621486, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.7295575734197193, 0.8894440273786534, 1.5408970099849981, -1.1397204467828197, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.7295575734197193, 0.362193782095267, 1.5408970099849981, -1.1397204467828197, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.09589065789158233, 1.4166942726620397, 0.6366411648458108, -0.9356182319826729, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [2.076681623919173, 1.4166942726620397, 0.6366411648458108, -1.7520270911832603, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.08515703225931394, 0.8894440273786534, 0.18451324227620902, -0.32331158758223233, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [1.0809193280892433, 1.4166942726620397, 0.41057720356100985, 0.9013017012186487, 1.1922324705555387, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.548509883268823, 1.943944517945426, 0.18451324227620902, 1.989846846819432, 1.0849405126071912, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.548509883268823, 1.4166942726620397, -1.6239984480021659, -1.0036523035827218, -1.1681906043081072, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.7253198692496488, 1.4166942726620397, 0.41057720356100985, 0.28899505681820825, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.17568087733476206, 1.4166942726620397, 0.18451324227620902, 0.561131343218404, 2.050568134142319, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.6390337283442711, 1.4166942726620397, -1.171870525432564, -1.0036523035827218, 0.5484807228654535, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 1.4166942726620397, -2.5282542931413534, 0.8332676296185998, -0.30985494072132685, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 0.8894440273786534, -2.5282542931413534, 0.8332676296185998, -0.30985494072132685, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.18217679879696, 1.943944517945426, -0.0415507190085758, 0.3570291284182572, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.09589065789158233, 1.4166942726620397, -0.49367864157817754, 0.4250632000183061, 0.11931289107206332, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 0.8894440273786534, -0.0415507190085758, 0.8332676296185998, 0.11931289107206332, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.17568087733476206, 1.943944517945426, 0.8627051261306116, -0.7995500887825749, 0.9776485546588437, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.9106052635706156, 1.943944517945426, -1.171870525432564, 0.15292691361811034, 1.1922324705555387, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.2769383480424786, 0.8894440273786534, -0.9458065641477793, -2.092197449183505, -1.3827745202048023, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.5442721790987524, 0.8894440273786534, 0.18451324227620902, 1.5816424172191383, -1.1681906043081072, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.2769383480424786, 1.4166942726620397, -0.9458065641477793, 0.22096098521815927, 0.44118876491710596, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-3.6263206158340595, 0.362193782095267, -1.3979344867173649, -1.8880952343833581, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.6390337283442711, 1.4166942726620397, -1.171870525432564, 0.01685877041801243, -0.8463147304630646, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.9106052635706156, 1.4166942726620397, -0.26761468029337665, 0.4930972716183551, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.0916529537215118, 0.8894440273786534, 0.41057720356100985, 1.5136083456190894, 0.22660484902041086, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-2.3589867847777857, 1.4166942726620397, 0.41057720356100985, 1.3775402024189913, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185]]
2024-03-21 12:03:04,560 - __main__ - INFO - 99
2024-03-21 12:03:04,561 - __main__ - INFO - 4255
2024-03-21 12:03:05,179 - __main__ - INFO - {'los_mean': 5.363315937659429, 'los_std': 4.099244607743503, 'los_median': 4.333333333333333, 'large_los': 21.291666666666668, 'threshold': 3.7605509886094994}
2024-03-21 12:03:07,936 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 25453.5820
2024-03-21 12:04:00,125 - __main__ - INFO - Fold 1 Epoch 0 Batch 50: Train Loss = 4.2662
2024-03-21 12:04:50,248 - __main__ - INFO - Fold 1 Epoch 0 Batch 100: Train Loss = 2.1828
2024-03-21 12:05:44,981 - __main__ - INFO - Fold 1 Epoch 0 Batch 150: Train Loss = 3.1293
2024-03-21 12:06:39,168 - __main__ - INFO - Fold 1 Epoch 0 Batch 200: Train Loss = 2.2020
2024-03-21 12:07:33,625 - __main__ - INFO - Fold 1 Epoch 0 Batch 250: Train Loss = 2.4866
2024-03-21 12:08:27,182 - __main__ - INFO - Fold 1 Epoch 0 Batch 300: Train Loss = 2.5440
2024-03-21 12:10:00,882 - __main__ - INFO - Fold 1, epoch 0: Loss = 2.3798 Valid loss = 2.4901 MSE = 36.7151 AUROC = 0.8125
2024-03-21 12:10:00,883 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.7151 ------------
2024-03-21 12:10:01,919 - __main__ - INFO - ------------ Save best model - MSE: 36.7151 ------------
2024-03-21 12:10:01,920 - __main__ - INFO - Fold 1, mse = 36.7151, mad = 4.2591
2024-03-21 12:10:02,926 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 2.6741
2024-03-21 12:10:54,722 - __main__ - INFO - Fold 1 Epoch 1 Batch 50: Train Loss = 2.7986
2024-03-21 12:11:48,056 - __main__ - INFO - Fold 1 Epoch 1 Batch 100: Train Loss = 2.5234
2024-03-21 12:12:40,428 - __main__ - INFO - Fold 1 Epoch 1 Batch 150: Train Loss = 1.7172
2024-03-21 12:13:34,432 - __main__ - INFO - Fold 1 Epoch 1 Batch 200: Train Loss = 1.8578
2024-03-21 12:14:27,403 - __main__ - INFO - Fold 1 Epoch 1 Batch 250: Train Loss = 1.4470
2024-03-21 12:15:21,940 - __main__ - INFO - Fold 1 Epoch 1 Batch 300: Train Loss = 2.0371
2024-03-21 12:16:59,482 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.9671 ------------
2024-03-21 12:17:00,514 - __main__ - INFO - ------------ Save best model - MSE: 34.9671 ------------
2024-03-21 12:17:00,517 - __main__ - INFO - Fold 1, mse = 34.9671, mad = 4.1192
2024-03-21 12:17:01,577 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 1.6976
2024-03-21 12:17:52,783 - __main__ - INFO - Fold 1 Epoch 2 Batch 50: Train Loss = 2.8312
2024-03-21 12:18:44,486 - __main__ - INFO - Fold 1 Epoch 2 Batch 100: Train Loss = 2.3675
2024-03-21 12:19:33,750 - __main__ - INFO - Fold 1 Epoch 2 Batch 150: Train Loss = 2.3014
2024-03-21 12:20:15,650 - __main__ - INFO - Fold 1 Epoch 2 Batch 200: Train Loss = 2.2900
2024-03-21 12:20:57,139 - __main__ - INFO - Fold 1 Epoch 2 Batch 250: Train Loss = 2.3428
2024-03-21 12:21:41,965 - __main__ - INFO - Fold 1 Epoch 2 Batch 300: Train Loss = 2.0708
2024-03-21 12:22:56,001 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.4294 ------------
2024-03-21 12:22:56,696 - __main__ - INFO - ------------ Save best model - MSE: 34.4294 ------------
2024-03-21 12:22:56,697 - __main__ - INFO - Fold 1, mse = 34.4294, mad = 3.8992
2024-03-21 12:22:57,671 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 1.6307
2024-03-21 12:23:37,273 - __main__ - INFO - Fold 1 Epoch 3 Batch 50: Train Loss = 2.0475
2024-03-21 12:24:18,105 - __main__ - INFO - Fold 1 Epoch 3 Batch 100: Train Loss = 2.0577
2024-03-21 12:24:57,669 - __main__ - INFO - Fold 1 Epoch 3 Batch 150: Train Loss = 2.1002
2024-03-21 12:25:32,501 - __main__ - INFO - Fold 1 Epoch 3 Batch 200: Train Loss = 1.9099
2024-03-21 12:26:05,810 - __main__ - INFO - Fold 1 Epoch 3 Batch 250: Train Loss = 2.4559
2024-03-21 12:26:40,916 - __main__ - INFO - Fold 1 Epoch 3 Batch 300: Train Loss = 2.6432
2024-03-21 12:27:44,286 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.3257 ------------
2024-03-21 12:27:44,900 - __main__ - INFO - ------------ Save best model - MSE: 33.3257 ------------
2024-03-21 12:27:44,901 - __main__ - INFO - Fold 1, mse = 33.3257, mad = 4.1055
2024-03-21 12:27:45,472 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 2.0046
2024-03-21 12:28:19,883 - __main__ - INFO - Fold 1 Epoch 4 Batch 50: Train Loss = 1.9425
2024-03-21 12:28:53,572 - __main__ - INFO - Fold 1 Epoch 4 Batch 100: Train Loss = 2.5042
2024-03-21 12:29:26,592 - __main__ - INFO - Fold 1 Epoch 4 Batch 150: Train Loss = 2.0829
2024-03-21 12:29:59,960 - __main__ - INFO - Fold 1 Epoch 4 Batch 200: Train Loss = 1.6720
2024-03-21 12:30:33,909 - __main__ - INFO - Fold 1 Epoch 4 Batch 250: Train Loss = 1.3766
2024-03-21 12:31:08,251 - __main__ - INFO - Fold 1 Epoch 4 Batch 300: Train Loss = 1.7953
2024-03-21 12:32:12,625 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.7201 ------------
2024-03-21 12:32:13,238 - __main__ - INFO - ------------ Save best model - MSE: 32.7201 ------------
2024-03-21 12:32:13,239 - __main__ - INFO - Fold 1, mse = 32.7201, mad = 3.9957
2024-03-21 12:32:13,920 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 2.0819
2024-03-21 12:32:47,218 - __main__ - INFO - Fold 1 Epoch 5 Batch 50: Train Loss = 1.7945
2024-03-21 12:33:21,637 - __main__ - INFO - Fold 1 Epoch 5 Batch 100: Train Loss = 2.0555
2024-03-21 12:33:54,452 - __main__ - INFO - Fold 1 Epoch 5 Batch 150: Train Loss = 1.8809
2024-03-21 12:34:29,112 - __main__ - INFO - Fold 1 Epoch 5 Batch 200: Train Loss = 2.2169
2024-03-21 12:35:03,262 - __main__ - INFO - Fold 1 Epoch 5 Batch 250: Train Loss = 2.3079
2024-03-21 12:35:38,118 - __main__ - INFO - Fold 1 Epoch 5 Batch 300: Train Loss = 1.7358
2024-03-21 12:36:43,720 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.4684 ------------
2024-03-21 12:36:44,502 - __main__ - INFO - ------------ Save best model - MSE: 32.4684 ------------
2024-03-21 12:36:44,503 - __main__ - INFO - Fold 1, mse = 32.4684, mad = 3.8852
2024-03-21 12:36:45,279 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 1.9065
2024-03-21 12:37:19,935 - __main__ - INFO - Fold 1 Epoch 6 Batch 50: Train Loss = 1.7648
2024-03-21 12:37:55,202 - __main__ - INFO - Fold 1 Epoch 6 Batch 100: Train Loss = 2.4814
2024-03-21 12:38:29,052 - __main__ - INFO - Fold 1 Epoch 6 Batch 150: Train Loss = 1.8806
2024-03-21 12:39:03,421 - __main__ - INFO - Fold 1 Epoch 6 Batch 200: Train Loss = 2.1444
2024-03-21 12:39:37,493 - __main__ - INFO - Fold 1 Epoch 6 Batch 250: Train Loss = 1.9158
2024-03-21 12:40:10,509 - __main__ - INFO - Fold 1 Epoch 6 Batch 300: Train Loss = 1.7947
2024-03-21 12:41:14,740 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.2858 ------------
2024-03-21 12:41:15,472 - __main__ - INFO - ------------ Save best model - MSE: 32.2858 ------------
2024-03-21 12:41:15,473 - __main__ - INFO - Fold 1, mse = 32.2858, mad = 3.9022
2024-03-21 12:41:16,276 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 1.6977
2024-03-21 12:41:51,672 - __main__ - INFO - Fold 1 Epoch 7 Batch 50: Train Loss = 1.7685
2024-03-21 12:42:25,188 - __main__ - INFO - Fold 1 Epoch 7 Batch 100: Train Loss = 2.0749
2024-03-21 12:42:59,302 - __main__ - INFO - Fold 1 Epoch 7 Batch 150: Train Loss = 1.8409
2024-03-21 12:43:33,580 - __main__ - INFO - Fold 1 Epoch 7 Batch 200: Train Loss = 2.0197
2024-03-21 12:44:07,230 - __main__ - INFO - Fold 1 Epoch 7 Batch 250: Train Loss = 1.8515
2024-03-21 12:44:41,279 - __main__ - INFO - Fold 1 Epoch 7 Batch 300: Train Loss = 1.9050
2024-03-21 12:45:45,200 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.5724 ------------
2024-03-21 12:45:45,875 - __main__ - INFO - ------------ Save best model - MSE: 31.5724 ------------
2024-03-21 12:45:45,876 - __main__ - INFO - Fold 1, mse = 31.5724, mad = 3.9087
2024-03-21 12:45:46,612 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 1.2746
2024-03-21 12:46:23,183 - __main__ - INFO - Fold 1 Epoch 8 Batch 50: Train Loss = 1.8024
2024-03-21 12:46:57,619 - __main__ - INFO - Fold 1 Epoch 8 Batch 100: Train Loss = 1.2724
2024-03-21 12:47:31,644 - __main__ - INFO - Fold 1 Epoch 8 Batch 150: Train Loss = 1.8766
2024-03-21 12:48:06,819 - __main__ - INFO - Fold 1 Epoch 8 Batch 200: Train Loss = 1.7792
2024-03-21 12:48:41,385 - __main__ - INFO - Fold 1 Epoch 8 Batch 250: Train Loss = 1.7915
2024-03-21 12:49:15,688 - __main__ - INFO - Fold 1 Epoch 8 Batch 300: Train Loss = 1.7457
2024-03-21 12:50:18,367 - __main__ - INFO - Fold 1, mse = 34.5646, mad = 3.8500
2024-03-21 12:50:19,109 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 1.7235
2024-03-21 12:50:54,758 - __main__ - INFO - Fold 1 Epoch 9 Batch 50: Train Loss = 1.8181
2024-03-21 12:51:29,476 - __main__ - INFO - Fold 1 Epoch 9 Batch 100: Train Loss = 1.6854
2024-03-21 12:52:03,920 - __main__ - INFO - Fold 1 Epoch 9 Batch 150: Train Loss = 1.8926
2024-03-21 12:52:38,538 - __main__ - INFO - Fold 1 Epoch 9 Batch 200: Train Loss = 1.9247
2024-03-21 12:53:12,950 - __main__ - INFO - Fold 1 Epoch 9 Batch 250: Train Loss = 2.1439
2024-03-21 12:53:46,926 - __main__ - INFO - Fold 1 Epoch 9 Batch 300: Train Loss = 1.9836
2024-03-21 12:54:48,723 - __main__ - INFO - Fold 1, mse = 31.7976, mad = 3.8094
2024-03-21 12:54:49,397 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 1.8580
2024-03-21 12:55:24,763 - __main__ - INFO - Fold 1 Epoch 10 Batch 50: Train Loss = 1.6475
2024-03-21 12:56:00,715 - __main__ - INFO - Fold 1 Epoch 10 Batch 100: Train Loss = 2.3827
2024-03-21 12:56:34,034 - __main__ - INFO - Fold 1 Epoch 10 Batch 150: Train Loss = 2.0131
2024-03-21 12:57:07,760 - __main__ - INFO - Fold 1 Epoch 10 Batch 200: Train Loss = 1.7974
2024-03-21 12:57:41,701 - __main__ - INFO - Fold 1 Epoch 10 Batch 250: Train Loss = 1.3425
2024-03-21 12:58:15,185 - __main__ - INFO - Fold 1 Epoch 10 Batch 300: Train Loss = 1.6498
2024-03-21 12:59:17,681 - __main__ - INFO - Fold 1, epoch 10: Loss = 1.7961 Valid loss = 2.1521 MSE = 31.7466 AUROC = 0.8756
2024-03-21 12:59:17,682 - __main__ - INFO - Fold 1, mse = 31.7466, mad = 3.8494
2024-03-21 12:59:18,539 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 1.4775
2024-03-21 12:59:51,079 - __main__ - INFO - Fold 1 Epoch 11 Batch 50: Train Loss = 1.6388
2024-03-21 13:00:26,910 - __main__ - INFO - Fold 1 Epoch 11 Batch 100: Train Loss = 1.6710
2024-03-21 13:01:01,994 - __main__ - INFO - Fold 1 Epoch 11 Batch 150: Train Loss = 1.8543
2024-03-21 13:01:36,520 - __main__ - INFO - Fold 1 Epoch 11 Batch 200: Train Loss = 1.8026
2024-03-21 13:02:10,156 - __main__ - INFO - Fold 1 Epoch 11 Batch 250: Train Loss = 1.7037
2024-03-21 13:02:43,871 - __main__ - INFO - Fold 1 Epoch 11 Batch 300: Train Loss = 1.6680
2024-03-21 13:03:44,933 - __main__ - INFO - Fold 1, mse = 31.9072, mad = 3.9464
2024-03-21 13:03:45,630 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 1.6778
2024-03-21 13:04:18,820 - __main__ - INFO - Fold 1 Epoch 12 Batch 50: Train Loss = 1.4506
2024-03-21 13:04:51,231 - __main__ - INFO - Fold 1 Epoch 12 Batch 100: Train Loss = 1.8364
2024-03-21 13:05:26,850 - __main__ - INFO - Fold 1 Epoch 12 Batch 150: Train Loss = 1.3776
2024-03-21 13:06:02,218 - __main__ - INFO - Fold 1 Epoch 12 Batch 200: Train Loss = 1.9926
2024-03-21 13:06:36,534 - __main__ - INFO - Fold 1 Epoch 12 Batch 250: Train Loss = 1.9955
2024-03-21 13:07:09,626 - __main__ - INFO - Fold 1 Epoch 12 Batch 300: Train Loss = 2.1151
2024-03-21 13:08:12,144 - __main__ - INFO - Fold 1, mse = 33.0175, mad = 3.8069
2024-03-21 13:08:12,789 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 2.5065
2024-03-21 13:08:48,896 - __main__ - INFO - Fold 1 Epoch 13 Batch 50: Train Loss = 1.7063
2024-03-21 13:09:21,337 - __main__ - INFO - Fold 1 Epoch 13 Batch 100: Train Loss = 1.3389
2024-03-21 13:09:54,182 - __main__ - INFO - Fold 1 Epoch 13 Batch 150: Train Loss = 1.2507
2024-03-21 13:10:31,007 - __main__ - INFO - Fold 1 Epoch 13 Batch 200: Train Loss = 1.6196
2024-03-21 13:11:04,305 - __main__ - INFO - Fold 1 Epoch 13 Batch 250: Train Loss = 1.7005
2024-03-21 13:11:36,657 - __main__ - INFO - Fold 1 Epoch 13 Batch 300: Train Loss = 1.6703
2024-03-21 13:12:38,124 - __main__ - INFO - Fold 1, mse = 32.5212, mad = 3.8305
2024-03-21 13:12:38,883 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 1.5536
2024-03-21 13:13:11,556 - __main__ - INFO - Fold 1 Epoch 14 Batch 50: Train Loss = 1.6163
2024-03-21 13:13:44,758 - __main__ - INFO - Fold 1 Epoch 14 Batch 100: Train Loss = 1.8740
2024-03-21 13:14:17,518 - __main__ - INFO - Fold 1 Epoch 14 Batch 150: Train Loss = 1.5849
2024-03-21 13:14:48,780 - __main__ - INFO - Fold 1 Epoch 14 Batch 200: Train Loss = 1.6892
2024-03-21 13:15:20,886 - __main__ - INFO - Fold 1 Epoch 14 Batch 250: Train Loss = 2.2529
2024-03-21 13:15:50,981 - __main__ - INFO - Fold 1 Epoch 14 Batch 300: Train Loss = 1.4635
2024-03-21 13:16:52,949 - __main__ - INFO - Fold 1, mse = 31.9374, mad = 3.8691
2024-03-21 13:16:53,551 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 1.8564
2024-03-21 13:17:24,452 - __main__ - INFO - Fold 1 Epoch 15 Batch 50: Train Loss = 1.4260
2024-03-21 13:17:55,289 - __main__ - INFO - Fold 1 Epoch 15 Batch 100: Train Loss = 1.8233
2024-03-21 13:18:28,679 - __main__ - INFO - Fold 1 Epoch 15 Batch 150: Train Loss = 1.4852
2024-03-21 13:19:02,444 - __main__ - INFO - Fold 1 Epoch 15 Batch 200: Train Loss = 1.2414
2024-03-21 13:19:36,972 - __main__ - INFO - Fold 1 Epoch 15 Batch 250: Train Loss = 1.7524
2024-03-21 13:20:10,312 - __main__ - INFO - Fold 1 Epoch 15 Batch 300: Train Loss = 1.4398
2024-03-21 13:21:07,268 - __main__ - INFO - Fold 1, mse = 32.0706, mad = 3.8383
2024-03-21 13:21:07,867 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 2.2879
2024-03-21 13:21:38,160 - __main__ - INFO - Fold 1 Epoch 16 Batch 50: Train Loss = 1.0979
2024-03-21 13:22:10,566 - __main__ - INFO - Fold 1 Epoch 16 Batch 100: Train Loss = 2.5250
2024-03-21 13:22:41,142 - __main__ - INFO - Fold 1 Epoch 16 Batch 150: Train Loss = 1.9042
2024-03-21 13:23:17,986 - __main__ - INFO - Fold 1 Epoch 16 Batch 200: Train Loss = 1.6265
2024-03-21 13:23:49,339 - __main__ - INFO - Fold 1 Epoch 16 Batch 250: Train Loss = 1.4636
2024-03-21 13:24:18,351 - __main__ - INFO - Fold 1 Epoch 16 Batch 300: Train Loss = 1.2697
2024-03-21 13:25:16,826 - __main__ - INFO - Fold 1, mse = 32.4899, mad = 3.8301
2024-03-21 13:25:17,436 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 1.5618
2024-03-21 13:25:48,252 - __main__ - INFO - Fold 1 Epoch 17 Batch 50: Train Loss = 1.2096
2024-03-21 13:26:18,571 - __main__ - INFO - Fold 1 Epoch 17 Batch 100: Train Loss = 1.6955
2024-03-21 13:26:49,371 - __main__ - INFO - Fold 1 Epoch 17 Batch 150: Train Loss = 1.4752
2024-03-21 13:27:20,605 - __main__ - INFO - Fold 1 Epoch 17 Batch 200: Train Loss = 1.4958
2024-03-21 13:27:50,416 - __main__ - INFO - Fold 1 Epoch 17 Batch 250: Train Loss = 1.5412
2024-03-21 13:28:20,173 - __main__ - INFO - Fold 1 Epoch 17 Batch 300: Train Loss = 1.7721
2024-03-21 13:29:21,825 - __main__ - INFO - Fold 1, mse = 31.8043, mad = 3.8383
2024-03-21 13:29:22,434 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 1.6288
2024-03-21 13:29:55,558 - __main__ - INFO - Fold 1 Epoch 18 Batch 50: Train Loss = 1.7003
2024-03-21 13:30:28,392 - __main__ - INFO - Fold 1 Epoch 18 Batch 100: Train Loss = 1.6773
2024-03-21 13:30:58,492 - __main__ - INFO - Fold 1 Epoch 18 Batch 150: Train Loss = 1.6188
2024-03-21 13:31:32,222 - __main__ - INFO - Fold 1 Epoch 18 Batch 200: Train Loss = 1.1728
2024-03-21 13:32:04,584 - __main__ - INFO - Fold 1 Epoch 18 Batch 250: Train Loss = 1.3767
2024-03-21 13:32:35,637 - __main__ - INFO - Fold 1 Epoch 18 Batch 300: Train Loss = 1.7588
2024-03-21 13:33:34,750 - __main__ - INFO - Fold 1, mse = 32.4646, mad = 3.8448
2024-03-21 13:33:35,456 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 1.6346
2024-03-21 13:34:05,991 - __main__ - INFO - Fold 1 Epoch 19 Batch 50: Train Loss = 1.4734
2024-03-21 13:34:35,667 - __main__ - INFO - Fold 1 Epoch 19 Batch 100: Train Loss = 1.4640
2024-03-21 13:35:07,765 - __main__ - INFO - Fold 1 Epoch 19 Batch 150: Train Loss = 1.3013
2024-03-21 13:35:41,068 - __main__ - INFO - Fold 1 Epoch 19 Batch 200: Train Loss = 1.6896
2024-03-21 13:36:15,107 - __main__ - INFO - Fold 1 Epoch 19 Batch 250: Train Loss = 1.4468
2024-03-21 13:36:45,675 - __main__ - INFO - Fold 1 Epoch 19 Batch 300: Train Loss = 1.8183
2024-03-21 13:37:40,388 - __main__ - INFO - Fold 1, mse = 31.7545, mad = 3.8302
2024-03-21 13:37:41,456 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 21139.9355
2024-03-21 13:38:13,261 - __main__ - INFO - Fold 2 Epoch 0 Batch 50: Train Loss = 4.5315
2024-03-21 13:38:45,113 - __main__ - INFO - Fold 2 Epoch 0 Batch 100: Train Loss = 2.4218
2024-03-21 13:39:17,251 - __main__ - INFO - Fold 2 Epoch 0 Batch 150: Train Loss = 2.6323
2024-03-21 13:39:50,520 - __main__ - INFO - Fold 2 Epoch 0 Batch 200: Train Loss = 2.0733
2024-03-21 13:40:21,419 - __main__ - INFO - Fold 2 Epoch 0 Batch 250: Train Loss = 2.9951
2024-03-21 13:40:51,704 - __main__ - INFO - Fold 2 Epoch 0 Batch 300: Train Loss = 2.4619
2024-03-21 13:41:51,039 - __main__ - INFO - Fold 2, epoch 0: Loss = 2.5120 Valid loss = 2.2954 MSE = 33.2397 AUROC = 0.7638
2024-03-21 13:41:51,041 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.2397 ------------
2024-03-21 13:42:04,643 - __main__ - INFO - Fold 2, mse = 33.2397, mad = 4.2237
2024-03-21 13:42:05,259 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 2.4450
2024-03-21 13:42:37,403 - __main__ - INFO - Fold 2 Epoch 1 Batch 50: Train Loss = 3.4639
2024-03-21 13:43:10,066 - __main__ - INFO - Fold 2 Epoch 1 Batch 100: Train Loss = 1.8930
2024-03-21 13:43:43,601 - __main__ - INFO - Fold 2 Epoch 1 Batch 150: Train Loss = 2.1680
2024-03-21 13:44:16,068 - __main__ - INFO - Fold 2 Epoch 1 Batch 200: Train Loss = 2.4028
2024-03-21 13:44:46,249 - __main__ - INFO - Fold 2 Epoch 1 Batch 250: Train Loss = 3.4172
2024-03-21 13:45:18,698 - __main__ - INFO - Fold 2 Epoch 1 Batch 300: Train Loss = 2.3158
2024-03-21 13:46:17,912 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.4145 ------------
2024-03-21 13:46:18,506 - __main__ - INFO - ------------ Save best model - MSE: 31.4145 ------------
2024-03-21 13:46:18,508 - __main__ - INFO - Fold 2, mse = 31.4145, mad = 4.1459
2024-03-21 13:46:19,211 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 1.8582
2024-03-21 13:46:51,006 - __main__ - INFO - Fold 2 Epoch 2 Batch 50: Train Loss = 2.6305
2024-03-21 13:47:24,287 - __main__ - INFO - Fold 2 Epoch 2 Batch 100: Train Loss = 2.3665
2024-03-21 13:47:58,984 - __main__ - INFO - Fold 2 Epoch 2 Batch 150: Train Loss = 1.9801
2024-03-21 13:48:34,601 - __main__ - INFO - Fold 2 Epoch 2 Batch 200: Train Loss = 1.8215
2024-03-21 13:49:05,938 - __main__ - INFO - Fold 2 Epoch 2 Batch 250: Train Loss = 1.5580
2024-03-21 13:49:37,520 - __main__ - INFO - Fold 2 Epoch 2 Batch 300: Train Loss = 1.9744
2024-03-21 13:50:38,124 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 29.8390 ------------
2024-03-21 13:50:38,776 - __main__ - INFO - ------------ Save best model - MSE: 29.8390 ------------
2024-03-21 13:50:38,778 - __main__ - INFO - Fold 2, mse = 29.8390, mad = 3.7968
2024-03-21 13:50:39,511 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 1.7355
2024-03-21 13:51:11,221 - __main__ - INFO - Fold 2 Epoch 3 Batch 50: Train Loss = 2.1931
2024-03-21 13:51:41,770 - __main__ - INFO - Fold 2 Epoch 3 Batch 100: Train Loss = 2.8039
2024-03-21 13:52:15,740 - __main__ - INFO - Fold 2 Epoch 3 Batch 150: Train Loss = 2.2102
2024-03-21 13:52:48,044 - __main__ - INFO - Fold 2 Epoch 3 Batch 200: Train Loss = 2.5339
2024-03-21 13:53:19,889 - __main__ - INFO - Fold 2 Epoch 3 Batch 250: Train Loss = 2.1300
2024-03-21 13:53:53,073 - __main__ - INFO - Fold 2 Epoch 3 Batch 300: Train Loss = 1.7795
2024-03-21 13:54:52,101 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 28.7801 ------------
2024-03-21 13:54:52,646 - __main__ - INFO - ------------ Save best model - MSE: 28.7801 ------------
2024-03-21 13:54:52,647 - __main__ - INFO - Fold 2, mse = 28.7801, mad = 3.9413
2024-03-21 13:54:53,321 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 2.0147
2024-03-21 13:55:24,925 - __main__ - INFO - Fold 2 Epoch 4 Batch 50: Train Loss = 2.5294
2024-03-21 13:55:58,848 - __main__ - INFO - Fold 2 Epoch 4 Batch 100: Train Loss = 1.7317
2024-03-21 13:56:33,213 - __main__ - INFO - Fold 2 Epoch 4 Batch 150: Train Loss = 2.5882
2024-03-21 13:57:03,747 - __main__ - INFO - Fold 2 Epoch 4 Batch 200: Train Loss = 2.5360
2024-03-21 13:57:36,445 - __main__ - INFO - Fold 2 Epoch 4 Batch 250: Train Loss = 1.8992
2024-03-21 13:58:08,363 - __main__ - INFO - Fold 2 Epoch 4 Batch 300: Train Loss = 1.6596
2024-03-21 13:59:08,607 - __main__ - INFO - Fold 2, mse = 29.5313, mad = 3.7163
2024-03-21 13:59:09,378 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 2.6374
2024-03-21 13:59:42,700 - __main__ - INFO - Fold 2 Epoch 5 Batch 50: Train Loss = 2.0008
2024-03-21 14:00:13,615 - __main__ - INFO - Fold 2 Epoch 5 Batch 100: Train Loss = 2.0382
2024-03-21 14:00:47,868 - __main__ - INFO - Fold 2 Epoch 5 Batch 150: Train Loss = 2.2930
2024-03-21 14:01:20,784 - __main__ - INFO - Fold 2 Epoch 5 Batch 200: Train Loss = 2.1982
2024-03-21 14:01:52,528 - __main__ - INFO - Fold 2 Epoch 5 Batch 250: Train Loss = 1.7477
2024-03-21 14:02:24,008 - __main__ - INFO - Fold 2 Epoch 5 Batch 300: Train Loss = 1.5901
2024-03-21 14:03:30,092 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.7657 ------------
2024-03-21 14:03:30,749 - __main__ - INFO - ------------ Save best model - MSE: 27.7657 ------------
2024-03-21 14:03:30,751 - __main__ - INFO - Fold 2, mse = 27.7657, mad = 3.8455
2024-03-21 14:03:31,438 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 1.6090
2024-03-21 14:04:02,461 - __main__ - INFO - Fold 2 Epoch 6 Batch 50: Train Loss = 1.8534
2024-03-21 14:04:34,764 - __main__ - INFO - Fold 2 Epoch 6 Batch 100: Train Loss = 1.3749
2024-03-21 14:05:10,231 - __main__ - INFO - Fold 2 Epoch 6 Batch 150: Train Loss = 1.6514
2024-03-21 14:05:43,054 - __main__ - INFO - Fold 2 Epoch 6 Batch 200: Train Loss = 2.4471
2024-03-21 14:06:15,302 - __main__ - INFO - Fold 2 Epoch 6 Batch 250: Train Loss = 1.9739
2024-03-21 14:06:48,986 - __main__ - INFO - Fold 2 Epoch 6 Batch 300: Train Loss = 2.0701
2024-03-21 14:07:50,334 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.4391 ------------
2024-03-21 14:07:50,977 - __main__ - INFO - ------------ Save best model - MSE: 27.4391 ------------
2024-03-21 14:07:50,978 - __main__ - INFO - Fold 2, mse = 27.4391, mad = 3.7722
2024-03-21 14:07:51,693 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 1.3723
2024-03-21 14:08:22,705 - __main__ - INFO - Fold 2 Epoch 7 Batch 50: Train Loss = 2.0045
2024-03-21 14:08:56,169 - __main__ - INFO - Fold 2 Epoch 7 Batch 100: Train Loss = 1.8609
2024-03-21 14:09:29,937 - __main__ - INFO - Fold 2 Epoch 7 Batch 150: Train Loss = 2.2712
2024-03-21 14:10:01,461 - __main__ - INFO - Fold 2 Epoch 7 Batch 200: Train Loss = 2.8163
2024-03-21 14:10:32,330 - __main__ - INFO - Fold 2 Epoch 7 Batch 250: Train Loss = 1.6105
2024-03-21 14:11:05,515 - __main__ - INFO - Fold 2 Epoch 7 Batch 300: Train Loss = 1.9123
2024-03-21 14:12:04,530 - __main__ - INFO - Fold 2, mse = 27.8002, mad = 3.7328
2024-03-21 14:12:05,221 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 1.7587
2024-03-21 14:12:37,771 - __main__ - INFO - Fold 2 Epoch 8 Batch 50: Train Loss = 1.7260
2024-03-21 14:13:11,031 - __main__ - INFO - Fold 2 Epoch 8 Batch 100: Train Loss = 2.4150
2024-03-21 14:13:45,300 - __main__ - INFO - Fold 2 Epoch 8 Batch 150: Train Loss = 2.0619
2024-03-21 14:14:17,543 - __main__ - INFO - Fold 2 Epoch 8 Batch 200: Train Loss = 1.6974
2024-03-21 14:14:48,692 - __main__ - INFO - Fold 2 Epoch 8 Batch 250: Train Loss = 2.0535
2024-03-21 14:15:21,595 - __main__ - INFO - Fold 2 Epoch 8 Batch 300: Train Loss = 1.9661
2024-03-21 14:16:19,806 - __main__ - INFO - Fold 2, mse = 27.6528, mad = 3.6633
2024-03-21 14:16:20,441 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 2.2321
2024-03-21 14:16:53,521 - __main__ - INFO - Fold 2 Epoch 9 Batch 50: Train Loss = 2.0943
2024-03-21 14:17:25,374 - __main__ - INFO - Fold 2 Epoch 9 Batch 100: Train Loss = 1.9144
2024-03-21 14:17:58,903 - __main__ - INFO - Fold 2 Epoch 9 Batch 150: Train Loss = 2.0772
2024-03-21 14:18:32,719 - __main__ - INFO - Fold 2 Epoch 9 Batch 200: Train Loss = 2.2778
2024-03-21 14:19:02,750 - __main__ - INFO - Fold 2 Epoch 9 Batch 250: Train Loss = 2.9925
2024-03-21 14:19:34,102 - __main__ - INFO - Fold 2 Epoch 9 Batch 300: Train Loss = 2.1909
2024-03-21 14:20:33,442 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.0229 ------------
2024-03-21 14:20:34,133 - __main__ - INFO - ------------ Save best model - MSE: 27.0229 ------------
2024-03-21 14:20:34,134 - __main__ - INFO - Fold 2, mse = 27.0229, mad = 3.6899
2024-03-21 14:20:34,892 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 1.9766
2024-03-21 14:21:06,564 - __main__ - INFO - Fold 2 Epoch 10 Batch 50: Train Loss = 2.1270
2024-03-21 14:21:41,953 - __main__ - INFO - Fold 2 Epoch 10 Batch 100: Train Loss = 1.8597
2024-03-21 14:22:15,761 - __main__ - INFO - Fold 2 Epoch 10 Batch 150: Train Loss = 1.5004
2024-03-21 14:22:46,223 - __main__ - INFO - Fold 2 Epoch 10 Batch 200: Train Loss = 2.0959
2024-03-21 14:23:18,589 - __main__ - INFO - Fold 2 Epoch 10 Batch 250: Train Loss = 1.7784
2024-03-21 14:23:48,466 - __main__ - INFO - Fold 2 Epoch 10 Batch 300: Train Loss = 2.7009
2024-03-21 14:24:49,217 - __main__ - INFO - Fold 2, epoch 10: Loss = 1.9255 Valid loss = 1.8220 MSE = 26.3054 AUROC = 0.8670
2024-03-21 14:24:49,218 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.3054 ------------
2024-03-21 14:24:49,890 - __main__ - INFO - ------------ Save best model - MSE: 26.3054 ------------
2024-03-21 14:24:49,891 - __main__ - INFO - Fold 2, mse = 26.3054, mad = 3.7237
2024-03-21 14:24:50,712 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 1.9738
2024-03-21 14:25:23,184 - __main__ - INFO - Fold 2 Epoch 11 Batch 50: Train Loss = 1.6937
2024-03-21 14:25:57,332 - __main__ - INFO - Fold 2 Epoch 11 Batch 100: Train Loss = 1.5211
2024-03-21 14:26:30,456 - __main__ - INFO - Fold 2 Epoch 11 Batch 150: Train Loss = 1.5854
2024-03-21 14:27:02,655 - __main__ - INFO - Fold 2 Epoch 11 Batch 200: Train Loss = 1.6553
2024-03-21 14:27:33,816 - __main__ - INFO - Fold 2 Epoch 11 Batch 250: Train Loss = 1.6000
2024-03-21 14:28:05,280 - __main__ - INFO - Fold 2 Epoch 11 Batch 300: Train Loss = 1.5322
2024-03-21 14:29:06,474 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.2727 ------------
2024-03-21 14:29:07,146 - __main__ - INFO - ------------ Save best model - MSE: 26.2727 ------------
2024-03-21 14:29:07,148 - __main__ - INFO - Fold 2, mse = 26.2727, mad = 3.6267
2024-03-21 14:29:07,867 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 1.7681
2024-03-21 14:29:39,159 - __main__ - INFO - Fold 2 Epoch 12 Batch 50: Train Loss = 1.6612
2024-03-21 14:30:13,625 - __main__ - INFO - Fold 2 Epoch 12 Batch 100: Train Loss = 1.5633
2024-03-21 14:30:46,226 - __main__ - INFO - Fold 2 Epoch 12 Batch 150: Train Loss = 1.9829
2024-03-21 14:31:17,297 - __main__ - INFO - Fold 2 Epoch 12 Batch 200: Train Loss = 1.7398
2024-03-21 14:31:48,469 - __main__ - INFO - Fold 2 Epoch 12 Batch 250: Train Loss = 1.7774
2024-03-21 14:32:18,968 - __main__ - INFO - Fold 2 Epoch 12 Batch 300: Train Loss = 1.3584
2024-03-21 14:33:24,529 - __main__ - INFO - Fold 2, mse = 26.6510, mad = 3.6812
2024-03-21 14:33:25,187 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 1.8634
2024-03-21 14:33:58,012 - __main__ - INFO - Fold 2 Epoch 13 Batch 50: Train Loss = 2.3087
2024-03-21 14:34:33,172 - __main__ - INFO - Fold 2 Epoch 13 Batch 100: Train Loss = 1.5580
2024-03-21 14:35:05,704 - __main__ - INFO - Fold 2 Epoch 13 Batch 150: Train Loss = 3.6825
2024-03-21 14:35:37,769 - __main__ - INFO - Fold 2 Epoch 13 Batch 200: Train Loss = 1.8064
2024-03-21 14:36:10,195 - __main__ - INFO - Fold 2 Epoch 13 Batch 250: Train Loss = 1.5825
2024-03-21 14:36:41,963 - __main__ - INFO - Fold 2 Epoch 13 Batch 300: Train Loss = 1.6058
2024-03-21 14:37:38,897 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.0617 ------------
2024-03-21 14:37:39,440 - __main__ - INFO - ------------ Save best model - MSE: 26.0617 ------------
2024-03-21 14:37:39,441 - __main__ - INFO - Fold 2, mse = 26.0617, mad = 3.6493
2024-03-21 14:37:40,148 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 1.6846
2024-03-21 14:38:14,557 - __main__ - INFO - Fold 2 Epoch 14 Batch 50: Train Loss = 3.4631
2024-03-21 14:38:49,816 - __main__ - INFO - Fold 2 Epoch 14 Batch 100: Train Loss = 2.0586
2024-03-21 14:39:21,309 - __main__ - INFO - Fold 2 Epoch 14 Batch 150: Train Loss = 1.4984
2024-03-21 14:39:53,245 - __main__ - INFO - Fold 2 Epoch 14 Batch 200: Train Loss = 1.5330
2024-03-21 14:40:24,637 - __main__ - INFO - Fold 2 Epoch 14 Batch 250: Train Loss = 2.1690
2024-03-21 14:40:56,320 - __main__ - INFO - Fold 2 Epoch 14 Batch 300: Train Loss = 1.7982
2024-03-21 14:41:59,205 - __main__ - INFO - Fold 2, mse = 26.2094, mad = 3.7171
2024-03-21 14:41:59,891 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 1.6639
2024-03-21 14:42:34,631 - __main__ - INFO - Fold 2 Epoch 15 Batch 50: Train Loss = 1.7474
2024-03-21 14:43:09,257 - __main__ - INFO - Fold 2 Epoch 15 Batch 100: Train Loss = 1.6034
2024-03-21 14:43:42,666 - __main__ - INFO - Fold 2 Epoch 15 Batch 150: Train Loss = 1.8149
2024-03-21 14:44:15,608 - __main__ - INFO - Fold 2 Epoch 15 Batch 200: Train Loss = 1.5150
2024-03-21 14:44:48,408 - __main__ - INFO - Fold 2 Epoch 15 Batch 250: Train Loss = 1.5105
2024-03-21 14:45:19,694 - __main__ - INFO - Fold 2 Epoch 15 Batch 300: Train Loss = 1.6930
2024-03-21 14:46:19,368 - __main__ - INFO - Fold 2, mse = 27.4070, mad = 3.6245
2024-03-21 14:46:20,092 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 2.0431
2024-03-21 14:46:50,752 - __main__ - INFO - Fold 2 Epoch 16 Batch 50: Train Loss = 1.8778
2024-03-21 14:47:24,775 - __main__ - INFO - Fold 2 Epoch 16 Batch 100: Train Loss = 1.5122
2024-03-21 14:47:56,115 - __main__ - INFO - Fold 2 Epoch 16 Batch 150: Train Loss = 2.1819
2024-03-21 14:48:28,634 - __main__ - INFO - Fold 2 Epoch 16 Batch 200: Train Loss = 2.2121
2024-03-21 14:49:01,921 - __main__ - INFO - Fold 2 Epoch 16 Batch 250: Train Loss = 1.5023
2024-03-21 14:49:33,618 - __main__ - INFO - Fold 2 Epoch 16 Batch 300: Train Loss = 1.4962
2024-03-21 14:50:31,377 - __main__ - INFO - Fold 2, mse = 26.5161, mad = 3.6554
2024-03-21 14:50:32,049 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 2.2682
2024-03-21 14:51:05,973 - __main__ - INFO - Fold 2 Epoch 17 Batch 50: Train Loss = 1.7391
2024-03-21 14:51:40,731 - __main__ - INFO - Fold 2 Epoch 17 Batch 100: Train Loss = 1.4045
2024-03-21 14:52:13,837 - __main__ - INFO - Fold 2 Epoch 17 Batch 150: Train Loss = 1.5404
2024-03-21 14:52:45,938 - __main__ - INFO - Fold 2 Epoch 17 Batch 200: Train Loss = 1.3634
2024-03-21 14:53:17,990 - __main__ - INFO - Fold 2 Epoch 17 Batch 250: Train Loss = 1.5222
2024-03-21 14:53:49,433 - __main__ - INFO - Fold 2 Epoch 17 Batch 300: Train Loss = 2.2900
2024-03-21 14:54:50,382 - __main__ - INFO - Fold 2, mse = 26.9651, mad = 3.6363
2024-03-21 14:54:51,006 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 1.7422
2024-03-21 14:55:23,144 - __main__ - INFO - Fold 2 Epoch 18 Batch 50: Train Loss = 1.4134
2024-03-21 14:55:56,890 - __main__ - INFO - Fold 2 Epoch 18 Batch 100: Train Loss = 2.7163
2024-03-21 14:56:30,020 - __main__ - INFO - Fold 2 Epoch 18 Batch 150: Train Loss = 1.4299
2024-03-21 14:57:02,365 - __main__ - INFO - Fold 2 Epoch 18 Batch 200: Train Loss = 1.7039
2024-03-21 14:57:35,252 - __main__ - INFO - Fold 2 Epoch 18 Batch 250: Train Loss = 1.8799
2024-03-21 14:58:05,893 - __main__ - INFO - Fold 2 Epoch 18 Batch 300: Train Loss = 1.7790
2024-03-21 14:59:08,349 - __main__ - INFO - Fold 2, mse = 26.6554, mad = 3.6504
2024-03-21 14:59:08,995 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 1.5578
2024-03-21 14:59:40,646 - __main__ - INFO - Fold 2 Epoch 19 Batch 50: Train Loss = 1.5065
2024-03-21 15:00:15,140 - __main__ - INFO - Fold 2 Epoch 19 Batch 100: Train Loss = 1.4802
2024-03-21 15:00:48,114 - __main__ - INFO - Fold 2 Epoch 19 Batch 150: Train Loss = 1.5881
2024-03-21 15:01:19,256 - __main__ - INFO - Fold 2 Epoch 19 Batch 200: Train Loss = 1.8350
2024-03-21 15:01:49,818 - __main__ - INFO - Fold 2 Epoch 19 Batch 250: Train Loss = 2.1636
2024-03-21 15:02:23,504 - __main__ - INFO - Fold 2 Epoch 19 Batch 300: Train Loss = 1.4978
2024-03-21 15:03:20,783 - __main__ - INFO - Fold 2, mse = 27.1774, mad = 3.6768
2024-03-21 15:03:21,931 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 15784.5674
2024-03-21 15:03:53,846 - __main__ - INFO - Fold 3 Epoch 0 Batch 50: Train Loss = 6.2682
2024-03-21 15:04:29,026 - __main__ - INFO - Fold 3 Epoch 0 Batch 100: Train Loss = 2.5284
2024-03-21 15:05:01,648 - __main__ - INFO - Fold 3 Epoch 0 Batch 150: Train Loss = 2.1693
2024-03-21 15:05:34,360 - __main__ - INFO - Fold 3 Epoch 0 Batch 200: Train Loss = 2.8224
2024-03-21 15:06:05,602 - __main__ - INFO - Fold 3 Epoch 0 Batch 250: Train Loss = 2.1027
2024-03-21 15:06:36,717 - __main__ - INFO - Fold 3 Epoch 0 Batch 300: Train Loss = 2.8887
2024-03-21 15:07:35,724 - __main__ - INFO - Fold 3, epoch 0: Loss = 2.5106 Valid loss = 2.2603 MSE = 32.4817 AUROC = 0.7991
2024-03-21 15:07:35,725 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.4817 ------------
2024-03-21 15:07:36,095 - __main__ - INFO - Fold 3, mse = 32.4817, mad = 4.2564
2024-03-21 15:07:36,802 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 3.1102
2024-03-21 15:08:09,201 - __main__ - INFO - Fold 3 Epoch 1 Batch 50: Train Loss = 2.5628
2024-03-21 15:08:43,731 - __main__ - INFO - Fold 3 Epoch 1 Batch 100: Train Loss = 3.2564
2024-03-21 15:09:16,509 - __main__ - INFO - Fold 3 Epoch 1 Batch 150: Train Loss = 2.2462
2024-03-21 15:09:49,981 - __main__ - INFO - Fold 3 Epoch 1 Batch 200: Train Loss = 2.7167
2024-03-21 15:10:22,591 - __main__ - INFO - Fold 3 Epoch 1 Batch 250: Train Loss = 1.8464
2024-03-21 15:10:53,940 - __main__ - INFO - Fold 3 Epoch 1 Batch 300: Train Loss = 2.1018
2024-03-21 15:11:50,899 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.4213 ------------
2024-03-21 15:11:51,305 - __main__ - INFO - Fold 3, mse = 30.4213, mad = 4.0608
2024-03-21 15:11:51,982 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 2.0822
2024-03-21 15:12:24,143 - __main__ - INFO - Fold 3 Epoch 2 Batch 50: Train Loss = 2.1267
2024-03-21 15:12:59,660 - __main__ - INFO - Fold 3 Epoch 2 Batch 100: Train Loss = 2.4574
2024-03-21 15:13:31,966 - __main__ - INFO - Fold 3 Epoch 2 Batch 150: Train Loss = 3.1988
2024-03-21 15:14:04,801 - __main__ - INFO - Fold 3 Epoch 2 Batch 200: Train Loss = 3.5213
2024-03-21 15:14:37,629 - __main__ - INFO - Fold 3 Epoch 2 Batch 250: Train Loss = 3.0759
2024-03-21 15:15:08,233 - __main__ - INFO - Fold 3 Epoch 2 Batch 300: Train Loss = 3.9850
2024-03-21 15:16:07,639 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.3310 ------------
2024-03-21 15:16:08,026 - __main__ - INFO - Fold 3, mse = 30.3310, mad = 4.1017
2024-03-21 15:16:08,884 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 2.5073
2024-03-21 15:16:42,352 - __main__ - INFO - Fold 3 Epoch 3 Batch 50: Train Loss = 1.6397
2024-03-21 15:17:14,666 - __main__ - INFO - Fold 3 Epoch 3 Batch 100: Train Loss = 1.6425
2024-03-21 15:17:47,191 - __main__ - INFO - Fold 3 Epoch 3 Batch 150: Train Loss = 2.0711
2024-03-21 15:18:19,681 - __main__ - INFO - Fold 3 Epoch 3 Batch 200: Train Loss = 2.4001
2024-03-21 15:18:51,061 - __main__ - INFO - Fold 3 Epoch 3 Batch 250: Train Loss = 2.4942
2024-03-21 15:19:25,439 - __main__ - INFO - Fold 3 Epoch 3 Batch 300: Train Loss = 1.4897
2024-03-21 15:20:24,349 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 28.8073 ------------
2024-03-21 15:20:24,782 - __main__ - INFO - Fold 3, mse = 28.8073, mad = 3.8686
2024-03-21 15:20:25,375 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 1.9177
2024-03-21 15:20:57,774 - __main__ - INFO - Fold 3 Epoch 4 Batch 50: Train Loss = 2.5074
2024-03-21 15:21:32,392 - __main__ - INFO - Fold 3 Epoch 4 Batch 100: Train Loss = 1.9942
2024-03-21 15:22:05,151 - __main__ - INFO - Fold 3 Epoch 4 Batch 150: Train Loss = 3.3998
2024-03-21 15:22:36,040 - __main__ - INFO - Fold 3 Epoch 4 Batch 200: Train Loss = 2.5790
2024-03-21 15:23:08,515 - __main__ - INFO - Fold 3 Epoch 4 Batch 250: Train Loss = 2.3208
2024-03-21 15:23:40,525 - __main__ - INFO - Fold 3 Epoch 4 Batch 300: Train Loss = 1.8488
2024-03-21 15:24:38,435 - __main__ - INFO - Fold 3, mse = 29.0912, mad = 3.7290
2024-03-21 15:24:39,084 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 2.2053
2024-03-21 15:25:12,326 - __main__ - INFO - Fold 3 Epoch 5 Batch 50: Train Loss = 1.8131
2024-03-21 15:25:48,300 - __main__ - INFO - Fold 3 Epoch 5 Batch 100: Train Loss = 2.0298
2024-03-21 15:26:20,623 - __main__ - INFO - Fold 3 Epoch 5 Batch 150: Train Loss = 1.4787
2024-03-21 15:26:51,841 - __main__ - INFO - Fold 3 Epoch 5 Batch 200: Train Loss = 1.9167
2024-03-21 15:27:24,841 - __main__ - INFO - Fold 3 Epoch 5 Batch 250: Train Loss = 2.0983
2024-03-21 15:27:56,043 - __main__ - INFO - Fold 3 Epoch 5 Batch 300: Train Loss = 2.0752
2024-03-21 15:28:53,539 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 28.2714 ------------
2024-03-21 15:28:53,863 - __main__ - INFO - Fold 3, mse = 28.2714, mad = 3.7942
2024-03-21 15:28:54,516 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 1.7834
2024-03-21 15:29:23,801 - __main__ - INFO - Fold 3 Epoch 6 Batch 50: Train Loss = 1.7065
2024-03-21 15:29:54,687 - __main__ - INFO - Fold 3 Epoch 6 Batch 100: Train Loss = 2.0942
2024-03-21 15:30:23,969 - __main__ - INFO - Fold 3 Epoch 6 Batch 150: Train Loss = 1.6174
2024-03-21 15:30:52,886 - __main__ - INFO - Fold 3 Epoch 6 Batch 200: Train Loss = 1.9281
2024-03-21 15:31:22,348 - __main__ - INFO - Fold 3 Epoch 6 Batch 250: Train Loss = 1.8451
2024-03-21 15:31:52,730 - __main__ - INFO - Fold 3 Epoch 6 Batch 300: Train Loss = 2.1137
2024-03-21 15:32:56,259 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.7220 ------------
2024-03-21 15:32:56,619 - __main__ - INFO - Fold 3, mse = 27.7220, mad = 3.7555
2024-03-21 15:32:57,379 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 1.7428
2024-03-21 15:33:29,733 - __main__ - INFO - Fold 3 Epoch 7 Batch 50: Train Loss = 1.9010
2024-03-21 15:34:03,539 - __main__ - INFO - Fold 3 Epoch 7 Batch 100: Train Loss = 2.1193
2024-03-21 15:34:36,985 - __main__ - INFO - Fold 3 Epoch 7 Batch 150: Train Loss = 2.1841
2024-03-21 15:35:08,117 - __main__ - INFO - Fold 3 Epoch 7 Batch 200: Train Loss = 1.9064
2024-03-21 15:35:38,677 - __main__ - INFO - Fold 3 Epoch 7 Batch 250: Train Loss = 2.3737
2024-03-21 15:36:09,832 - __main__ - INFO - Fold 3 Epoch 7 Batch 300: Train Loss = 1.8646
2024-03-21 15:37:04,029 - __main__ - INFO - Fold 3, mse = 28.0683, mad = 3.9164
2024-03-21 15:37:04,692 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 1.7504
2024-03-21 15:37:35,740 - __main__ - INFO - Fold 3 Epoch 8 Batch 50: Train Loss = 1.9743
2024-03-21 15:38:11,574 - __main__ - INFO - Fold 3 Epoch 8 Batch 100: Train Loss = 1.6101
2024-03-21 15:38:43,021 - __main__ - INFO - Fold 3 Epoch 8 Batch 150: Train Loss = 1.4981
2024-03-21 15:39:13,017 - __main__ - INFO - Fold 3 Epoch 8 Batch 200: Train Loss = 2.1236
2024-03-21 15:39:45,978 - __main__ - INFO - Fold 3 Epoch 8 Batch 250: Train Loss = 1.9588
2024-03-21 15:40:17,766 - __main__ - INFO - Fold 3 Epoch 8 Batch 300: Train Loss = 2.1973
2024-03-21 15:41:17,824 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.5751 ------------
2024-03-21 15:41:18,273 - __main__ - INFO - Fold 3, mse = 27.5751, mad = 3.8133
2024-03-21 15:41:19,001 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 2.1340
2024-03-21 15:41:51,760 - __main__ - INFO - Fold 3 Epoch 9 Batch 50: Train Loss = 1.5301
2024-03-21 15:42:23,830 - __main__ - INFO - Fold 3 Epoch 9 Batch 100: Train Loss = 2.0208
2024-03-21 15:42:56,962 - __main__ - INFO - Fold 3 Epoch 9 Batch 150: Train Loss = 1.8436
2024-03-21 15:43:28,630 - __main__ - INFO - Fold 3 Epoch 9 Batch 200: Train Loss = 1.5511
2024-03-21 15:44:00,395 - __main__ - INFO - Fold 3 Epoch 9 Batch 250: Train Loss = 3.6754
2024-03-21 15:44:30,186 - __main__ - INFO - Fold 3 Epoch 9 Batch 300: Train Loss = 1.4463
2024-03-21 15:45:25,262 - __main__ - INFO - Fold 3, mse = 27.8774, mad = 3.9280
2024-03-21 15:45:26,005 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 1.6690
2024-03-21 15:46:00,043 - __main__ - INFO - Fold 3 Epoch 10 Batch 50: Train Loss = 1.8448
2024-03-21 15:46:29,123 - __main__ - INFO - Fold 3 Epoch 10 Batch 100: Train Loss = 1.7106
2024-03-21 15:46:59,911 - __main__ - INFO - Fold 3 Epoch 10 Batch 150: Train Loss = 1.5821
2024-03-21 15:47:31,270 - __main__ - INFO - Fold 3 Epoch 10 Batch 200: Train Loss = 1.7621
2024-03-21 15:48:01,937 - __main__ - INFO - Fold 3 Epoch 10 Batch 250: Train Loss = 1.9142
2024-03-21 15:48:32,149 - __main__ - INFO - Fold 3 Epoch 10 Batch 300: Train Loss = 3.2427
2024-03-21 15:49:32,196 - __main__ - INFO - Fold 3, epoch 10: Loss = 1.9516 Valid loss = 1.8851 MSE = 27.3085 AUROC = 0.8684
2024-03-21 15:49:32,197 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.3085 ------------
2024-03-21 15:49:32,538 - __main__ - INFO - Fold 3, mse = 27.3085, mad = 3.7847
2024-03-21 15:49:33,223 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 1.9109
2024-03-21 15:50:06,404 - __main__ - INFO - Fold 3 Epoch 11 Batch 50: Train Loss = 1.8037
2024-03-21 15:50:35,138 - __main__ - INFO - Fold 3 Epoch 11 Batch 100: Train Loss = 1.9817
2024-03-21 15:51:03,657 - __main__ - INFO - Fold 3 Epoch 11 Batch 150: Train Loss = 2.0984
2024-03-21 15:51:32,307 - __main__ - INFO - Fold 3 Epoch 11 Batch 200: Train Loss = 1.3933
2024-03-21 15:51:59,768 - __main__ - INFO - Fold 3 Epoch 11 Batch 250: Train Loss = 2.2531
2024-03-21 15:52:28,046 - __main__ - INFO - Fold 3 Epoch 11 Batch 300: Train Loss = 1.8249
2024-03-21 15:53:28,383 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.7860 ------------
2024-03-21 15:53:28,719 - __main__ - INFO - Fold 3, mse = 26.7860, mad = 3.6964
2024-03-21 15:53:29,389 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 1.4225
2024-03-21 15:54:01,980 - __main__ - INFO - Fold 3 Epoch 12 Batch 50: Train Loss = 2.2454
2024-03-21 15:54:34,837 - __main__ - INFO - Fold 3 Epoch 12 Batch 100: Train Loss = 1.5378
2024-03-21 15:55:07,588 - __main__ - INFO - Fold 3 Epoch 12 Batch 150: Train Loss = 2.5678
2024-03-21 15:55:40,055 - __main__ - INFO - Fold 3 Epoch 12 Batch 200: Train Loss = 2.5585
2024-03-21 15:56:13,048 - __main__ - INFO - Fold 3 Epoch 12 Batch 250: Train Loss = 1.5797
2024-03-21 15:56:45,623 - __main__ - INFO - Fold 3 Epoch 12 Batch 300: Train Loss = 1.7771
2024-03-21 15:57:42,187 - __main__ - INFO - Fold 3, mse = 26.8845, mad = 3.7270
2024-03-21 15:57:42,869 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 1.8992
2024-03-21 15:58:15,741 - __main__ - INFO - Fold 3 Epoch 13 Batch 50: Train Loss = 1.6723
2024-03-21 15:58:44,205 - __main__ - INFO - Fold 3 Epoch 13 Batch 100: Train Loss = 1.4919
2024-03-21 15:59:12,314 - __main__ - INFO - Fold 3 Epoch 13 Batch 150: Train Loss = 2.0110
2024-03-21 15:59:40,361 - __main__ - INFO - Fold 3 Epoch 13 Batch 200: Train Loss = 1.9085
2024-03-21 16:00:08,316 - __main__ - INFO - Fold 3 Epoch 13 Batch 250: Train Loss = 1.6610
2024-03-21 16:00:35,825 - __main__ - INFO - Fold 3 Epoch 13 Batch 300: Train Loss = 1.4924
2024-03-21 16:01:37,356 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.6964 ------------
2024-03-21 16:01:37,693 - __main__ - INFO - Fold 3, mse = 26.6964, mad = 3.7600
2024-03-21 16:01:38,369 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 2.5041
2024-03-21 16:02:10,944 - __main__ - INFO - Fold 3 Epoch 14 Batch 50: Train Loss = 1.7845
2024-03-21 16:02:41,808 - __main__ - INFO - Fold 3 Epoch 14 Batch 100: Train Loss = 1.7121
2024-03-21 16:03:10,347 - __main__ - INFO - Fold 3 Epoch 14 Batch 150: Train Loss = 1.5727
2024-03-21 16:03:38,999 - __main__ - INFO - Fold 3 Epoch 14 Batch 200: Train Loss = 1.8336
2024-03-21 16:04:07,727 - __main__ - INFO - Fold 3 Epoch 14 Batch 250: Train Loss = 2.1386
2024-03-21 16:04:36,242 - __main__ - INFO - Fold 3 Epoch 14 Batch 300: Train Loss = 1.6651
2024-03-21 16:05:40,036 - __main__ - INFO - Fold 3, mse = 26.8072, mad = 3.6974
2024-03-21 16:05:40,724 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 1.9640
2024-03-21 16:06:13,605 - __main__ - INFO - Fold 3 Epoch 15 Batch 50: Train Loss = 1.4074
2024-03-21 16:06:46,225 - __main__ - INFO - Fold 3 Epoch 15 Batch 100: Train Loss = 1.3118
2024-03-21 16:07:19,011 - __main__ - INFO - Fold 3 Epoch 15 Batch 150: Train Loss = 1.7397
2024-03-21 16:07:51,527 - __main__ - INFO - Fold 3 Epoch 15 Batch 200: Train Loss = 1.3981
2024-03-21 16:08:24,174 - __main__ - INFO - Fold 3 Epoch 15 Batch 250: Train Loss = 1.6883
2024-03-21 16:08:56,863 - __main__ - INFO - Fold 3 Epoch 15 Batch 300: Train Loss = 1.5967
2024-03-21 16:10:02,898 - __main__ - INFO - Fold 3, mse = 26.7417, mad = 3.6695
2024-03-21 16:10:03,577 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 2.8805
2024-03-21 16:10:36,282 - __main__ - INFO - Fold 3 Epoch 16 Batch 50: Train Loss = 1.4915
2024-03-21 16:11:08,897 - __main__ - INFO - Fold 3 Epoch 16 Batch 100: Train Loss = 2.3454
2024-03-21 16:11:41,481 - __main__ - INFO - Fold 3 Epoch 16 Batch 150: Train Loss = 1.7965
2024-03-21 16:12:14,442 - __main__ - INFO - Fold 3 Epoch 16 Batch 200: Train Loss = 1.8812
2024-03-21 16:12:47,092 - __main__ - INFO - Fold 3 Epoch 16 Batch 250: Train Loss = 1.5624
2024-03-21 16:13:19,972 - __main__ - INFO - Fold 3 Epoch 16 Batch 300: Train Loss = 1.4564
2024-03-21 16:14:23,726 - __main__ - INFO - Fold 3, mse = 26.9060, mad = 3.6898
2024-03-21 16:14:24,399 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 1.4114
2024-03-21 16:14:57,194 - __main__ - INFO - Fold 3 Epoch 17 Batch 50: Train Loss = 2.0408
2024-03-21 16:15:29,797 - __main__ - INFO - Fold 3 Epoch 17 Batch 100: Train Loss = 1.5780
2024-03-21 16:16:02,559 - __main__ - INFO - Fold 3 Epoch 17 Batch 150: Train Loss = 1.2927
2024-03-21 16:16:29,841 - __main__ - INFO - Fold 3 Epoch 17 Batch 200: Train Loss = 1.6677
2024-03-21 16:16:58,363 - __main__ - INFO - Fold 3 Epoch 17 Batch 250: Train Loss = 2.1548
2024-03-21 16:17:26,749 - __main__ - INFO - Fold 3 Epoch 17 Batch 300: Train Loss = 1.6381
2024-03-21 16:18:30,554 - __main__ - INFO - Fold 3, mse = 26.9703, mad = 3.7777
2024-03-21 16:18:31,226 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 2.0424
2024-03-21 16:19:03,831 - __main__ - INFO - Fold 3 Epoch 18 Batch 50: Train Loss = 1.8533
2024-03-21 16:19:36,557 - __main__ - INFO - Fold 3 Epoch 18 Batch 100: Train Loss = 1.6827
2024-03-21 16:20:09,254 - __main__ - INFO - Fold 3 Epoch 18 Batch 150: Train Loss = 1.8417
2024-03-21 16:20:41,996 - __main__ - INFO - Fold 3 Epoch 18 Batch 200: Train Loss = 1.9617
2024-03-21 16:21:14,801 - __main__ - INFO - Fold 3 Epoch 18 Batch 250: Train Loss = 1.7595
2024-03-21 16:21:44,374 - __main__ - INFO - Fold 3 Epoch 18 Batch 300: Train Loss = 1.8234
2024-03-21 16:22:48,414 - __main__ - INFO - Fold 3, mse = 26.9866, mad = 3.7462
2024-03-21 16:22:49,090 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 2.1213
2024-03-21 16:23:22,057 - __main__ - INFO - Fold 3 Epoch 19 Batch 50: Train Loss = 1.9047
2024-03-21 16:23:54,084 - __main__ - INFO - Fold 3 Epoch 19 Batch 100: Train Loss = 1.2902
2024-03-21 16:24:26,812 - __main__ - INFO - Fold 3 Epoch 19 Batch 150: Train Loss = 1.9875
2024-03-21 16:24:59,566 - __main__ - INFO - Fold 3 Epoch 19 Batch 200: Train Loss = 1.1290
2024-03-21 16:25:32,187 - __main__ - INFO - Fold 3 Epoch 19 Batch 250: Train Loss = 1.4380
2024-03-21 16:26:04,916 - __main__ - INFO - Fold 3 Epoch 19 Batch 300: Train Loss = 1.8163
2024-03-21 16:27:10,899 - __main__ - INFO - Fold 3, mse = 27.7152, mad = 3.6889
2024-03-21 16:27:10,900 - __main__ - INFO - mse 28.1102(2.4619)
2024-03-21 16:27:10,901 - __main__ - INFO - mad 3.7727(0.1063)
2024-03-21 16:27:10,901 - __main__ - INFO - auroc 0.8679(0.0066)
2024-03-21 16:27:10,901 - __main__ - INFO - auprc 0.5374(0.0049)
