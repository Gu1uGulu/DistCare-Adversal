{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import imp\n",
    "import re\n",
    "import pickle5 as pickle\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import logging\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from utils import utils\n",
    "from utils.readers import InHospitalMortalityReader\n",
    "from utils.preprocessing import Discretizer, Normalizer\n",
    "from utils import metrics\n",
    "from utils import common_utils\n",
    "from torch.autograd import Function\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = 'PD' \n",
    "RANDOM_SEED = 43\n",
    "np.random.seed(RANDOM_SEED) #numpy\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED) # cpu\n",
    "torch.cuda.manual_seed(RANDOM_SEED) #gpu\n",
    "torch.backends.cudnn.deterministic=True # cudnn\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() == True else 'cpu')\n",
    "# print(\"available device: {}\".format(device))\n",
    "reverse = False\n",
    "model_name = 'codats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reverse:\n",
    "    file_name = 'log_file' + '_' + model_name + '_' + target_dataset + '_' + 'reverse' + '.log'\n",
    "else:\n",
    "    file_name = 'log_file' + '_' + model_name + '_' + target_dataset + '.log'\n",
    "def get_logger(name, file_name):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # 以下两行是为了在jupyter notebook 中不重复输出日志\n",
    "    if logger.root.handlers:\n",
    "        logger.root.handlers[0].setLevel(logging.WARNING)\n",
    " \n",
    "    handler_stdout = logging.StreamHandler()\n",
    "    handler_stdout.setLevel(logging.INFO)\n",
    "    handler_stdout.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    # logger.addHandler(handler_stdout)\n",
    " \n",
    "    handler_file = logging.FileHandler(filename=file_name, mode='w', encoding='utf-8')\n",
    "    handler_file.setLevel(logging.DEBUG)\n",
    "    handler_file.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(handler_file)\n",
    " \n",
    "    return logger\n",
    "\n",
    "logger = get_logger(__name__,file_name)\n",
    "\n",
    "logger.debug('这是希望输出的debug内容')\n",
    "logger.info('这是希望输出的info内容')\n",
    "logger.warning('这是希望输出的warning内容')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get source data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n2n_data(x, y, x_len):\n",
    "    length = len(x)\n",
    "    assert length == len(y)\n",
    "    assert length == len(x_len)\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    new_x_len = []\n",
    "    for i in range(length):\n",
    "        for j in range(len(x[i])):\n",
    "            new_x.append(x[i][:j+1])\n",
    "            new_y.append(y[i][j])\n",
    "            new_x_len.append(j+1)\n",
    "    return new_x, new_y, new_x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data_path = './data/Challenge/'\n",
    "small_part = False\n",
    "arg_timestep = 1.0\n",
    "batch_size = 256\n",
    "epochs = 100\n",
    "all_x_source = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "all_y_source = pickle.load(open(source_data_path + 'new_y_front_fill.dat', 'rb'))\n",
    "all_names_source = pickle.load(open(source_data_path + 'new_name.dat', 'rb'))\n",
    "static_source = pickle.load(open(source_data_path + 'new_demo_front_fill.dat', 'rb'))\n",
    "mask_x_source = pickle.load(open(source_data_path + 'new_mask_x.dat', 'rb'))\n",
    "mask_demo_source = pickle.load(open(source_data_path + 'new_mask_demo.dat', 'rb'))\n",
    "all_x_len_source = [len(i) for i in all_x_source]\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    subset_idx_source = [31, 29, 28, 33, 25, 18, 7, 21, 16, 15, 19, 17, 24, 3, 5, 0]\n",
    "elif target_dataset == 'TJ':\n",
    "    subset_idx_source = [27, 29, 18, 16, 26, 33, 28, 31, 32, 15, 11, 25, 21, 20, 9, 17, 30, 19]\n",
    "elif target_dataset == 'HM':\n",
    "    subset_idx_source = [0, 1, 2, 3, 5, 9, 11, 12, 13, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
    "\n",
    "subset_cnt = len(subset_idx_source)\n",
    "other_idx = []\n",
    "for i in range(len(all_x_source[0][0])):\n",
    "    if i not in subset_idx_source:\n",
    "        other_idx.append(i)\n",
    "\n",
    "for i in range(len(all_x_source)): #将共同特征移动到最开始，非共同特征移动到末尾\n",
    "    cur = np.array(all_x_source[i], dtype=float)\n",
    "    cur_mask = np.array(mask_x_source[i])\n",
    "    cur_subset = cur[:, subset_idx_source]\n",
    "    cur_other = cur[:, other_idx]\n",
    "    cur_mask_subset = cur_mask[:, subset_idx_source]\n",
    "    cur_mask_other = cur_mask[:, other_idx]\n",
    "    all_x_source[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    mask_x_source[i] = np.concatenate((cur_mask_subset, cur_mask_other), axis=1).tolist()\n",
    "\n",
    "\n",
    "train_num_source =int( len(all_x_source) * 0.8) + 1\n",
    "logger.info(train_num_source)\n",
    "dev_num_source =int( len(all_x_source) * 0.1) + 1\n",
    "logger.info(dev_num_source)\n",
    "test_num_source =int( len(all_x_source) * 0.1)\n",
    "logger.info(test_num_source)\n",
    "assert(train_num_source+dev_num_source+test_num_source == len(all_x_source))\n",
    "\n",
    "train_x_source = []\n",
    "train_y_source = []\n",
    "train_names_source = []\n",
    "train_static_source = []\n",
    "train_x_len_source = []\n",
    "train_mask_x_source = []\n",
    "for idx in range(train_num_source):\n",
    "    train_x_source.append(all_x_source[idx])\n",
    "    train_y_source.append(int(all_y_source[idx][-1]))\n",
    "    train_names_source.append(all_names_source[idx])\n",
    "    train_static_source.append(static_source[idx])\n",
    "    train_x_len_source.append(all_x_len_source[idx])\n",
    "    train_mask_x_source.append(mask_x_source[idx])\n",
    "\n",
    "dev_x_source = []\n",
    "dev_y_source = []\n",
    "dev_names_source = []\n",
    "dev_static_source = []\n",
    "dev_x_len_source = []\n",
    "dev_mask_x_source = []\n",
    "for idx in range(train_num_source, train_num_source + dev_num_source):\n",
    "    dev_x_source.append(all_x_source[idx])\n",
    "    dev_y_source.append(int(all_y_source[idx][-1]))\n",
    "    dev_names_source.append(all_names_source[idx])\n",
    "    dev_static_source.append(static_source[idx])\n",
    "    dev_x_len_source.append(all_x_len_source[idx])\n",
    "    dev_mask_x_source.append(mask_x_source[idx])\n",
    "\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "test_names = []\n",
    "test_static = []\n",
    "test_x_len = []\n",
    "test_mask_x = []\n",
    "for idx in range(train_num_source + dev_num_source, train_num_source + dev_num_source + test_num_source):\n",
    "    test_x.append(all_x_source[idx])\n",
    "    test_y.append(int(all_y_source[idx][-1]))\n",
    "    test_names.append(all_names_source[idx])\n",
    "    test_static.append(static_source[idx])\n",
    "    test_x_len.append(all_x_len_source[idx])\n",
    "    test_mask_x.append(mask_x_source[idx])\n",
    "\n",
    "\n",
    "assert(len(train_x_source) == train_num_source)\n",
    "assert(len(dev_x_source) == dev_num_source)\n",
    "assert(len(test_x) == test_num_source)\n",
    "\n",
    "long_x_source = all_x_source\n",
    "long_y_source = [y[-1] for y in all_y_source]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(y_pred, y_true):\n",
    "    loss = torch.nn.BCELoss()\n",
    "    return loss(y_pred, y_true)\n",
    "def get_re_loss(y_pred, y_true):\n",
    "    loss = torch.nn.MSELoss()\n",
    "    return loss(y_pred, y_true)\n",
    "def get_kl_loss(x_pred, x_target):\n",
    "    loss = torch.nn.KLDivLoss(reduce=True, size_average=True)\n",
    "    return loss(x_pred, x_target)\n",
    "def get_wass_dist(x_pred, x_target):\n",
    "    m1 = torch.mean(x_pred, dim=0)\n",
    "    m2 = torch.mean(x_target, dim=0)\n",
    "    v1 = torch.var(x_pred, dim=0)\n",
    "    v2 = torch.var(x_target, dim=0)\n",
    "    p1 = torch.sum(torch.pow((m1 - m2), 2))\n",
    "    p2 = torch.sum(torch.pow(torch.pow(v1, 1/2) - torch.pow(v2, 1/2), 2))\n",
    "    return torch.pow(p1+p2, 1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sents(sents, pad_token):\n",
    "#     print(f'len(pad_token) is {len(pad_token)}')\n",
    "#     print(f'sents is {sents}')\n",
    "\n",
    "    sents_padded = []\n",
    "\n",
    "    max_length = max([len(_) for _ in sents])\n",
    "    for i in sents:\n",
    "        padded = list(i) + [pad_token]*(max_length-len(i))\n",
    "#         print(f'padded is {padded}')\n",
    "        sents_padded.append(np.array(padded))\n",
    "\n",
    "\n",
    "    return np.array(sents_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(x, y, lens, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    # batch_num = math.ceil(len(x) / batch_size) # 向下取整\n",
    "    batch_num = len(x) // batch_size if len(x) % batch_size == 0 else len(x) // batch_size + 1\n",
    "    # print(f\"len(x) is {len(x)}, len(y) is {len(y)}, len(lens) is {len(lens)}, batch_size is {batch_size}, batch_num is {batch_num}\")\n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        if (i + 1) * batch_size  < len(x):\n",
    "            indices = index_array[i * batch_size: (i + 1) * batch_size] #  fetch out all the induces\n",
    "        else:\n",
    "            indices = index_array[i * batch_size: ]\n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx],lens[idx]))\n",
    "       \n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    \n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "        batch_lens = [e[2] for e in examples]\n",
    "\n",
    "        yield batch_x, batch_y, batch_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_to_mask(length, max_len=None, dtype=None):\n",
    "    \"\"\"length: B.\n",
    "    return B x max_len.\n",
    "    If max_len is None, then max of length will be used.\n",
    "    \"\"\"\n",
    "    assert len(length.shape) == 1, 'Length shape should be 1 dimensional.'\n",
    "    max_len = max_len or length.max().item()\n",
    "    mask = torch.arange(max_len, device=length.device,\n",
    "                        dtype=length.dtype).expand(len(length), max_len) < length.unsqueeze(1)\n",
    "    if dtype is not None:\n",
    "        mask = torch.as_tensor(mask, dtype=dtype, device=length.device)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class codats(nn.Module):\n",
    "    def __init__(self, common_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(codats, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = common_dim\n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.layers = nn.Sequential(\n",
    "                nn.Conv1d(1, hidden_dim, kernel_size=8, padding=4, bias=False),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Conv1d(hidden_dim, 2*hidden_dim, kernel_size=5, padding=2, bias=False),\n",
    "                nn.BatchNorm1d(2*hidden_dim),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Conv1d(2*hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.AdaptiveAvgPool1d(1),\n",
    "            )\n",
    "        \n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.output = nn.Linear(self.input_dim, self.output_dim)\n",
    "\n",
    "        # adversal方法中的域分类器  \n",
    "        self.domain_classifier = nn.Sequential()\n",
    "        self.domain_classifier.add_module('d_fc1', nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(self.hidden_dim))\n",
    "        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))\n",
    "        self.domain_classifier.add_module('d_fc2', nn.Linear(hidden_dim, 2))\n",
    "        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.FC_embed = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.to_MMD = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    def forward(self, input, lens, alpha, is_teacher):\n",
    "        lens = lens.to('cpu')\n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "        \n",
    "        codats_embeded_input = self.layers(input[:,:,0].squeeze().unsqueeze(1)).unsqueeze(1)\n",
    "        # print(f'input[:,:,i].shape is {input[:,:,0].shape}')\n",
    "        for i in range(1, feature_dim):\n",
    "            embeded_input = self.layers(input[:,:,i].squeeze().unsqueeze(1)).unsqueeze(1)\n",
    "            codats_embeded_input = torch.cat((codats_embeded_input, embeded_input), 1)\n",
    "            # print(f'docats_embeded_input.shape is {codats_embeded_input.shape}')\n",
    "        codats_embeded_input = codats_embeded_input.squeeze()\n",
    "        # print(f'codats_embeded_input.shape is {codats_embeded_input.shape}')\n",
    "        # GRU_embeded_input = self.GRUs[0](pack_padded_sequence(input[:,:,0].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "        # for i in range(feature_dim-1):\n",
    "        #     embeded_input = self.GRUs[i+1](pack_padded_sequence(input[:,:,i+1].unsqueeze(-1), lens, batch_first=True))[1].squeeze().unsqueeze(1) # b 1 h\n",
    "        #     GRU_embeded_input = torch.cat((GRU_embeded_input, embeded_input), 1)\n",
    "        # print(f\"GRU_embeded_input.shape is {GRU_embeded_input.shape}\")\n",
    "        \n",
    "\n",
    "\n",
    "        if is_teacher: # 来自源数据集\n",
    "            common_input = codats_embeded_input[:, 0, :]\n",
    "            for i in range(1, feature_dim):\n",
    "                common_input = common_input + codats_embeded_input[:, i, :]  \n",
    "            # print(f\"common_input1.shape is {common_input.shape}\")\n",
    "            common_input = torch.squeeze(common_input, 1) # batch * hidden\n",
    "            reverse_input = ReverseLayerF.apply(common_input, alpha)\n",
    "            # print(f\"common_input2.shape is {common_input.shape}\")\n",
    "            domain_output = self.domain_classifier(reverse_input)\n",
    "\n",
    "            posi_input = self.dropout(codats_embeded_input) # batch_size * d_input + d_input_diff * hidden_dim\n",
    "            \n",
    "            contexts = self.Linear(posi_input).squeeze()# b i\n",
    "            output = self.output(self.dropout(contexts))# b 1\n",
    "            output = self.sigmoid(output)\n",
    "            return output, domain_output, contexts\n",
    "        else: # 来自目标数据集，主要是为了混淆domain classifier\n",
    "            common_input = codats_embeded_input[:, 0, :]\n",
    "            for i in range(1, feature_dim):\n",
    "                common_input = common_input + codats_embeded_input[:, i, :]  \n",
    "            common_input = torch.squeeze(common_input, 1) # batch * hidden\n",
    "            reverse_input = ReverseLayerF.apply(common_input, alpha)\n",
    "            domain_output = self.domain_classifier(reverse_input)\n",
    "            return domain_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "def getSplitData(x, lens, y):\n",
    "    train_num =int( len(x) * 0.8) + 1\n",
    "    dev_num =int( len(x) * 0.1) + 1\n",
    "    test_num = len(x) - train_num - dev_num\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    train_len = []\n",
    "    for idx in range(train_num):\n",
    "        train_x.append(x[idx])\n",
    "        train_y.append(int(y[idx][-1]))\n",
    "        train_len.append(lens[idx])\n",
    "\n",
    "    dev_x = []\n",
    "    dev_y = []\n",
    "    dev_len = []\n",
    "    for idx in range(train_num, train_num + dev_num):\n",
    "        dev_x.append(x[idx])\n",
    "        dev_y.append(int(y[idx][-1]))\n",
    "        dev_len.append(lens[idx])\n",
    "\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    test_len = []\n",
    "\n",
    "    for idx in range(train_num + dev_num, train_num + dev_num + test_num):\n",
    "        test_x.append(x[idx])\n",
    "        test_y.append(int(y[idx][-1]))\n",
    "        test_len.append(lens[idx])\n",
    "    return train_x, train_y, train_len, dev_x, dev_y, dev_len, test_x, test_y, test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"load target data\")\n",
    "if target_dataset == 'PD':\n",
    "    data_path = './data/PD/'\n",
    "    all_x_target = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_time_target = pickle.load(open(data_path + 'y_z.pkl', 'rb'))\n",
    "    all_x_len_target = [len(i) for i in all_x_target]\n",
    "\n",
    "    subset_idx_target = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    other_idx_target = list(range(69))\n",
    "    for i in subset_idx_target:\n",
    "        other_idx_target.remove(i)\n",
    "    for i in range(len(all_x_target)):\n",
    "        cur = np.array(all_x_target[i], dtype=float)\n",
    "        cur_subset = cur[:, subset_idx_target]\n",
    "        cur_other = cur[:, other_idx_target]\n",
    "        all_x_target[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "elif target_dataset == 'TJ':\n",
    "    data_path = './data/Tongji/'\n",
    "    all_x_target = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y_target = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time_target = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len_target = [len(i) for i in all_x_target]\n",
    "\n",
    "    for i in range(len(all_time_target)):\n",
    "        for j in range(len(all_time_target[i])):\n",
    "            all_time_target[i][j] = all_time_target[i][j][-1]\n",
    "            all_y_target[i][j] = all_y_target[i][j][0]\n",
    "\n",
    "    subset_idx_target = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "    other_idx_target = list(range(75))\n",
    "    for i in subset_idx_target:\n",
    "        other_idx_target.remove(i)\n",
    "    for i in range(len(all_x_target)):\n",
    "        cur = np.array(all_x_target[i], dtype=float)\n",
    "        cur_subset = cur[:, subset_idx_target]\n",
    "        cur_other = cur[:, other_idx_target]\n",
    "        all_x_target[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "elif target_dataset == 'HM':\n",
    "    data_path = './data/CDSL/'\n",
    "    all_x_target = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y_target = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time_target = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len_target = [len(i) for i in all_x_target]\n",
    "\n",
    "    for i in range(len(all_time_target)):\n",
    "        for j in range(len(all_time_target[i])):\n",
    "            all_time_target[i][j] = all_time_target[i][j][-1]\n",
    "            all_y_target[i][j] = all_y_target[i][j][0]\n",
    "\n",
    "    subset_idx_target = [5, 6, 4, 2, 3, 48, 79, 76, 87, 25, 30, 31, 18, 43, 58, 66, 40, 57, 23, 92, 50, 54, 91, 60, 39, 81]\n",
    "    other_idx_target= list(range(99))\n",
    "    for i in subset_idx_target:\n",
    "        other_idx_target.remove(i)\n",
    "    for i in range(len(all_x_target)):\n",
    "        cur = np.array(all_x_target[i], dtype=float)\n",
    "        cur_subset = cur[:, subset_idx_target]\n",
    "        cur_other = cur[:, other_idx_target]\n",
    "    #     tar_all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "        all_x_target[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    \n",
    "if target_dataset == 'PD':\n",
    "    all_x_target = all_x_target\n",
    "    all_y_target = all_time_target\n",
    "elif  target_dataset == 'HM' or target_dataset == 'TJ':\n",
    "    examples = []\n",
    "    for idx in range(len(all_x_target)):\n",
    "        examples.append((all_x_target[idx], all_y_target[idx], all_time_target[idx], all_x_len_target[idx]))\n",
    "    examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    all_x_target = [e[0] for e in examples]\n",
    "    all_y_target = [e[1] for e in examples]\n",
    "    all_time_target = [e[2] for e in examples]\n",
    "    all_x_len_target = [e[3] for e in examples]\n",
    "\n",
    "num_source = len(all_x_source)\n",
    "num_target = len(all_x_target)\n",
    "# print(target_dataset,len(all_x_target), len(all_x_target[0]),len(all_x_target[0][0]))\n",
    "all_x_target_confuse = []\n",
    "all_x_len_target_confuse = []\n",
    "all_y_target_confuse = []\n",
    "all_x_source_confuse = []\n",
    "all_x_len_source_confuse = []\n",
    "all_y_source_confuse = []\n",
    "repeat_times = 0\n",
    "\n",
    "if num_source < num_target:\n",
    "    all_x_target_confuse = all_x_target\n",
    "    all_y_target_confuse = all_y_target\n",
    "    all_x_len_target_confuse = all_x_len_target\n",
    "    while repeat_times * num_source < num_target:\n",
    "        all_x_source_confuse = all_x_source_confuse + all_x_source\n",
    "        all_x_len_source_confuse = all_x_len_source_confuse + all_x_len_source\n",
    "        all_y_source_confuse =  all_y_source_confuse + all_y_source\n",
    "        repeat_times = repeat_times + 1\n",
    "    all_x_source_confuse = all_x_source_confuse[:num_target]\n",
    "    all_x_len_source_confuse = all_x_len_source_confuse[:num_target]\n",
    "    all_y_source_confuse = all_y_source_confuse[:num_target]\n",
    "elif num_target < num_source:\n",
    "    all_x_source_confuse = all_x_source\n",
    "    all_x_len_source_confuse = all_x_len_source\n",
    "    all_y_source_confuse = all_y_source\n",
    "    while repeat_times * num_target < num_source:\n",
    "        all_x_target_confuse = all_x_target_confuse + all_x_target\n",
    "        all_x_len_target_confuse = all_x_len_target_confuse + all_x_len_target\n",
    "        all_y_target_confuse = all_y_target_confuse + all_y_target\n",
    "        repeat_times = repeat_times + 1\n",
    "    all_x_target_confuse = all_x_target_confuse[:num_source]\n",
    "    all_x_len_target_confuse = all_x_len_target_confuse[:num_source]\n",
    "    all_y_target_confuse = all_y_target_confuse[:num_source]\n",
    "\n",
    "# print(f\"len(all_x_source_confuse) is {len(all_x_source_confuse)}, len(all_x_target_confuse) is {len(all_x_target_confuse)}\")\n",
    "\n",
    "#todo 划分train、dev、test \n",
    "# all_x_source_confuse = pad_sents(all_x_source_confuse, pad_token_source)\n",
    "# all_x_target_confuse = pad_sents(all_x_target_confuse, pad_token_target)\n",
    "train_x_source_confuse, train_y_source_confuse, train_len_source_confuse, dev_x_source_confuse, dev_y_source_confuse, dev_len_source_confuse, test_x_source_confuse,\\\n",
    "test_y_source_confuse, test_len_source_confuse = getSplitData(all_x_source_confuse, all_x_len_source_confuse, all_y_source_confuse)\n",
    "\n",
    "train_x_target_confuse, train_y_target_confuse, train_len_target_confuse, dev_x_target_confuse, dev_y_target_confuse, dev_len_target_confuse, test_x_target_confuse,\\\n",
    "test_y_target_confuse, test_len_target_confuse = getSplitData(all_x_target_confuse, all_x_len_target_confuse, all_y_target_confuse)\n",
    "\n",
    "# long_x_source = all_x_source\n",
    "# long_y_source = [y[-1] for y in all_y_source]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_source = np.zeros(34)\n",
    "if target_dataset == 'PD':\n",
    "    pad_token_target = np.zeros(69)\n",
    "elif target_dataset == 'TJ':\n",
    "    pad_token_target = np.zeros(75)\n",
    "elif target_dataset == 'HM':\n",
    "    pad_token_target = np.zeros(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 256\n",
    "common_dim = subset_cnt \n",
    "hidden_dim = 64\n",
    "d_model = 64\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1\n",
    "model_student = codats(common_dim = common_dim, hidden_dim = hidden_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, output_dim = output_dim).to(device)\n",
    "optimizer_student = torch.optim.Adam(model_student.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskLoss(nn.Module):\n",
    "    def __init__(self, task_num=2):\n",
    "        super(MultitaskLoss, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.alpha = nn.Parameter(torch.ones((task_num)), requires_grad=True)\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.kl = nn.KLDivLoss(reduce=True, size_average=True)\n",
    "\n",
    "    def forward(self, opt_student, batch_y, emb_student, emb_teacher, tar_source, tar_tar):\n",
    "        BCE_Loss = self.bce(opt_student, batch_y)\n",
    "        emb_Loss = self.kl(emb_student, emb_teacher)\n",
    "        return BCE_Loss * self.alpha[0] + emb_Loss * self.alpha[1]\n",
    "\n",
    "def get_multitask_loss(opt_student, batch_y, emb_student, emb_teacher):\n",
    "    mtl = MultitaskLoss(task_num=3)\n",
    "    return mtl(opt_student, batch_y, emb_student, emb_teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_dataset == 'PD':\n",
    "    data_str = 'pd'\n",
    "elif target_dataset == 'TJ':\n",
    "    data_str = 'covid'\n",
    "elif target_dataset == 'HM':\n",
    "    data_str = 'spain'\n",
    "\n",
    "\n",
    "# if teacher_flag:\n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str\n",
    "# else: \n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher'\n",
    "\n",
    "file_name = './model/pretrained-codats'+ data_str;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training dann model\n",
    "# # If you don't want to train Student Model:\n",
    "# # - The pretrained student model is in direcrtory './model/', and can be directly loaded, \n",
    "# # - Simply skip this cell and load the model to validate on Dev Dataset.\n",
    "\n",
    "# logger.info('Training Student')\n",
    "# teacher_flag = False\n",
    "# epochs = 30\n",
    "# total_train_loss = []\n",
    "# total_valid_loss = []\n",
    "# global_best = 0\n",
    "# auroc = []\n",
    "# auprc = []\n",
    "# minpse = []\n",
    "# history = []\n",
    "# # begin_time = time.time()\n",
    "# best_auroc = 0\n",
    "# best_auprc = 0\n",
    "# best_minpse = 0\n",
    "# best_total_loss = 0x3f3f3f3f\n",
    "# loss_domain = torch.nn.NLLLoss()\n",
    "# loss_predict = torch.nn.MSELoss()\n",
    "# loss_embed = nn.KLDivLoss(reduce=True, size_average=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f'len(train_source_iter) is {len(train_x_source_confuse)}, len(train_target_iter) is {len(train_x_target_confuse)}, steps is {len(train_x_source_confuse) // batch_size + 1}')\n",
    "\n",
    "\n",
    "\n",
    "# for each_epoch in range(epochs):\n",
    "#     train_source_iter = batch_iter(train_x_source_confuse, train_y_source_confuse, train_len_source_confuse, batch_size=batch_size)\n",
    "#     dev_source_iter = batch_iter(dev_x_source_confuse, dev_y_source_confuse, dev_len_source_confuse, batch_size=batch_size)\n",
    "#     test_source_iter = batch_iter(test_x_source_confuse, test_y_source_confuse, test_len_source_confuse, batch_size=batch_size)\n",
    "#     train_target_iter = batch_iter(train_x_target_confuse, train_y_target_confuse, train_len_target_confuse, batch_size=batch_size)\n",
    "#     dev_target_iter = batch_iter(dev_x_target_confuse, dev_y_target_confuse, dev_len_target_confuse, batch_size=batch_size)\n",
    "#     test_target_iter = batch_iter(test_x_target_confuse, test_y_target_confuse, test_len_target_confuse, batch_size=batch_size)\n",
    "#     epoch_loss = []\n",
    "#     counter_batch = 0\n",
    "#     model_student.train()  \n",
    "#     steps = len(train_x_source_confuse) // batch_size + 1 if len(train_x_source_confuse) % batch_size != 0 else len(train_x_source_confuse) // batch_size\n",
    "#     for step in range(steps):\n",
    "#         # -----source_domain--------\n",
    "#         batch_x, batch_y, batch_lens= next(train_source_iter)\n",
    "#         p = float(step + each_epoch * steps) / epochs / steps\n",
    "#         alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "#         optimizer_student.zero_grad()\n",
    "#         batch_x = torch.tensor(pad_sents(batch_x, pad_token_source), dtype=torch.float32).to(device)\n",
    "#         batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "#         batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#         # batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "#         # opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "#         domain_label = torch.zeros(min(batch_size, batch_x.shape[0])).long().to(device)\n",
    "#         opt_student, opt_domain, emb_student = model_student(batch_x[:,:,:subset_cnt], batch_lens, alpha, True)\n",
    "#         emb_student = F.log_softmax(emb_student, dim=1)\n",
    "#         err_predict = loss_predict(opt_student, batch_y)\n",
    "#         err_domain1 = loss_domain(opt_domain, domain_label)\n",
    "#             # loss = get_multitask_loss(opt_student, batch_y.unsqueeze(-1), emb_student, emb_teacher)\n",
    "\n",
    "#         # -----target_domain--------\n",
    "#         batch_x, batch_y, batch_lens = next(train_target_iter)\n",
    "#         p = float(step + each_epoch * len(train_x_source)) / epochs / len(train_x_len_source)\n",
    "#         alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "#         optimizer_student.zero_grad()\n",
    "#         batch_x = torch.tensor(pad_sents(batch_x, pad_token_target), dtype=torch.float32).to(device)\n",
    "#         batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "#         batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#         # batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "#         # opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "#         domain_label = torch.ones(min(batch_size, batch_x.shape[0])).long().to(device)\n",
    "#         opt_domain = model_student(batch_x[:,:,:subset_cnt], batch_lens, alpha, False)\n",
    "#         err_domain2 = loss_domain(opt_domain, domain_label)\n",
    "\n",
    "#         # -----common--------\n",
    "#         loss = err_predict + err_domain1 + err_domain2\n",
    "#         epoch_loss.append(loss.cpu().detach().numpy())\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model_student.parameters(), 20)\n",
    "#         optimizer_student.step()\n",
    "\n",
    "#         if step % 20 == 0:\n",
    "#             print('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "#             logger.info('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, loss.cpu().detach().numpy()))\n",
    "\n",
    "#     epoch_loss = np.mean(epoch_loss)\n",
    "#     total_train_loss.append(epoch_loss)\n",
    "\n",
    "\n",
    "#     # dev_source_dataset = MyDataset(dev_x_source_confuse, dev_len_source_confuse, dev_y_source_confuse)\n",
    "#     # dev_target_dataset = MyDataset(dev_x_target_confuse, dev_len_target_confuse, dev_y_target_confuse)\n",
    "#     # dev_source_dataloader = DataLoader(dev_source_dataset, batch_size= batch_size)\n",
    "#     # dev_target_dataloader = DataLoader(dev_target_dataset, batch_size=batch_size)\n",
    "#     #Validation\n",
    "\n",
    "#     y_true = []\n",
    "#     y_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         steps = len(dev_x_source_confuse) // batch_size + 1 if len(dev_x_source_confuse) % batch_size != 0 else len(dev_x_source_confuse) // batch_size\n",
    "#         for step in range(steps):\n",
    "#             # -----source_domain--------\n",
    "#             batch_x, batch_y, batch_lens= next(dev_source_iter)\n",
    "#             p = float(step + each_epoch * steps) / epochs / steps\n",
    "#             alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "#             optimizer_student.zero_grad()\n",
    "#             batch_x = torch.tensor(pad_sents(batch_x, pad_token_source), dtype=torch.float32).to(device)\n",
    "#             batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "#             batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#             # batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "#             # opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "#             domain_label = torch.zeros(min(batch_size, batch_x.shape[0])).long().to(device)\n",
    "#             opt_student, opt_domain, emb_student = model_student(batch_x[:,:,:subset_cnt], batch_lens, alpha, True)\n",
    "#             # emb_teacher = torch.tensor(dev_teacher_emb[step], dtype=torch.float32).to(device)\n",
    "#             emb_student = F.log_softmax(emb_student, dim=1)\n",
    "#             # err_emb = loss_embed(emb_student, emb_teacher) #todo 是否考虑它\n",
    "#             err_predict = loss_predict(opt_student, batch_y)\n",
    "#             err_domain1 = loss_domain(opt_domain, domain_label)\n",
    "#                 # loss = get_multitask_loss(opt_student, batch_y.unsqueeze(-1), emb_student, emb_teacher)\n",
    "\n",
    "#             # -----target_domain--------\n",
    "#             batch_x, batch_y, batch_lens = next(dev_target_iter)\n",
    "#             optimizer_student.zero_grad()\n",
    "#             batch_x = torch.tensor(pad_sents(batch_x, pad_token_target), dtype=torch.float32).to(device)\n",
    "#             batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "#             batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "#             # batch_mask_x = torch.tensor(pad_sents(batch_mask_x, pad_token), dtype=torch.float32).to(device)\n",
    "#             # opt_student, decov_loss_student, emb_student, tar_result = model_student(batch_x[:,:,:subset_cnt], batch_x[:,:,subset_cnt:], batch_lens, [tar_all_x, tar_all_x_len], True)\n",
    "#             domain_label = torch.ones(min(batch_size, batch_x.shape[0])).long().to(device)\n",
    "#             opt_domain = model_student(batch_x[:,:,:subset_cnt], batch_lens, alpha, False)\n",
    "#             err_domain2 = loss_domain(opt_domain, domain_label)\n",
    "\n",
    "#             # -----common--------\n",
    "#             loss = err_domain1 + err_domain2\n",
    "#             if loss < best_total_loss:\n",
    "#                 best_total_loss = loss\n",
    "#                 state = {\n",
    "#                     'net': model_student.state_dict(),\n",
    "#                     'optimizer': optimizer_student.state_dict(),\n",
    "#                     'epoch': each_epoch\n",
    "#                 }\n",
    "#                 torch.save(state, file_name)\n",
    "#                 print('------------ Save best model - TOTAL_LOSS: %.4f ------------'%best_total_loss)\n",
    "#                 logger.info('------------ Save best model - TOTAL_LOSS: %.4f ------------'%best_total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last saved model is in epoch 27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "codats(\n",
       "  (layers): Sequential(\n",
       "    (0): Conv1d(1, 64, kernel_size=(8,), stride=(1,), padding=(4,), bias=False)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n",
       "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (demo_proj_main): Linear(in_features=12, out_features=64, bias=True)\n",
       "  (demo_proj): Linear(in_features=12, out_features=64, bias=True)\n",
       "  (Linear): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (output): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (domain_classifier): Sequential(\n",
       "    (d_fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (d_bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (d_relu1): ReLU(inplace=True)\n",
       "    (d_fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (d_softmax): LogSoftmax(dim=1)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (FC_embed): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (softmax): Softmax(dim=None)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       "  (to_MMD): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:3\" if torch.cuda.is_available() == True else 'cpu') )\n",
    "save_epoch = checkpoint['epoch']\n",
    "print(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "logger.info(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "model_student.load_state_dict(checkpoint['net'])\n",
    "optimizer_student.load_state_dict(checkpoint['optimizer'])\n",
    "model_student.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Test Loss = -2.8663\n",
      "\n",
      "==>Predicting on test\n",
      "Test Loss = -2.9028\n"
     ]
    }
   ],
   "source": [
    "#anchor\n",
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    model_student.eval()\n",
    "    test_target_iter = batch_iter(test_x_target_confuse, test_y_target_confuse, test_len_target_confuse, batch_size=batch_size, shuffle=True)\n",
    "    steps = len(test_x_target_confuse) // batch_size + 1 if len(test_x_target_confuse) % batch_size != 0 else len(test_x_target_confuse) // batch_size\n",
    "    for step in range(steps):\n",
    "        # -----target_domain--------\n",
    "        batch_x, batch_y, batch_lens= next(test_target_iter) \n",
    "        optimizer_student.zero_grad()\n",
    "        batch_x = torch.tensor(pad_sents(batch_x, pad_token_target), dtype=torch.float32).to(device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "        batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "        masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "        opt, _, _  = model_student(batch_x[:,:,:subset_cnt], batch_lens, 1, True)\n",
    "\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "#             REC_Loss = F.mse_loss(masks * recon, masks * batch_x, reduction='mean').to(device)\n",
    "\n",
    "        model_loss =  BCE_Loss \n",
    "\n",
    "        loss = model_loss\n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "            logger.info('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "        y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "        y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "logger.info(\"\\n==>Predicting on test\")\n",
    "logger.info('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "# test_res = metrics.print_metrics_binary(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_dataset == 'PD':\n",
    "    source_common_idx = [31, 29, 28, 33, 25, 18, 7, 21, 16, 15, 19, 17, 24, 3, 5, 0]\n",
    "    target_common_idx = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    source_data_path = './data/Challenge/'\n",
    "    source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "    target_data_path = './data/PD/'\n",
    "    target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "elif target_dataset == 'TJ':\n",
    "    source_common_idx = [27, 29, 18, 16, 26, 33, 28, 31, 32, 15, 11, 25, 21, 20, 9, 17, 30, 19]\n",
    "    target_common_idx = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "    source_data_path = './data/Challenge/'\n",
    "    source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "    target_data_path = './data/Tongji/'\n",
    "    target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "\n",
    "elif target_dataset == 'HM':\n",
    "    source_common_idx = [0, 1, 2, 3, 5, 9, 11, 12, 13, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
    "    target_common_idx = [5, 6, 4, 2, 3, 48, 79, 76, 87, 25, 30, 31, 18, 43, 58, 66, 40, 57, 23, 92, 50, 54, 91, 60, 39, 81]\n",
    "    source_data_path = './data/Challenge/'\n",
    "    source_x = pickle.load(open(source_data_path + 'new_x_front_fill.dat', 'rb'))\n",
    "    target_data_path = './data/CDSL/'\n",
    "    target_x = pickle.load(open(target_data_path + 'x.pkl', 'rb'))\n",
    "\n",
    "assert(len(source_common_idx) == len(target_common_idx))\n",
    "common_len = len(source_common_idx)\n",
    "source_x_diff = []\n",
    "target_x_diff = []\n",
    "\n",
    "source_total_len = 34\n",
    "source_other_idx = list(range(source_total_len))\n",
    "for i in source_common_idx:\n",
    "    source_other_idx.remove(i)\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    target_total_len = 69\n",
    "    target_other_idx = list(range(target_total_len))\n",
    "    for i in target_common_idx:\n",
    "        target_other_idx.remove(i)\n",
    "elif target_dataset == 'TJ':\n",
    "    target_other_idx = list(range(75))\n",
    "    target_total_len = 75\n",
    "    for i in target_common_idx:\n",
    "        target_other_idx.remove(i)\n",
    "elif target_dataset == 'HM':\n",
    "    target_other_idx = list(range(99))\n",
    "    target_total_len = 99\n",
    "    for i in target_common_idx:\n",
    "        target_other_idx.remove(i)\n",
    "\n",
    "for i in range(len(source_x)):\n",
    "    cur = np.array(source_x[i], dtype=float)\n",
    "    cur_subset = cur[:, source_common_idx]\n",
    "    cur_other = cur[:, source_other_idx]\n",
    "    source_x_diff.append(cur_other.tolist())\n",
    "    source_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "\n",
    "for i in range(len(target_x)):\n",
    "    cur = np.array(target_x[i], dtype=float)\n",
    "    cur_subset = cur[:, target_common_idx]\n",
    "    cur_other = cur[:, target_other_idx]\n",
    "    target_x_diff.append(cur_other.tolist())\n",
    "    target_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "\n",
    "\n",
    "source_max = 0\n",
    "for i in range(len(source_x_diff)):\n",
    "    if source_max < len(source_x_diff[i]):\n",
    "        source_max = len(source_x_diff[i])\n",
    "\n",
    "source_x_diff_longest = max(list(len(_) for _ in source_x_diff))\n",
    "source_x_longest = max(list(len(_) for _ in source_x))\n",
    "source_batch = len(source_x_diff)\n",
    "source_diff_features = source_total_len - common_len\n",
    "source_x_diff_ex = torch.zeros((source_batch, source_x_diff_longest, source_diff_features))\n",
    "source_x_ex = torch.zeros((source_batch, source_x_longest, source_total_len))\n",
    "\n",
    "for i in range(len(source_x_diff)):\n",
    "    for j in range(source_x_diff_longest):\n",
    "        cur_len = len(source_x_diff[i])\n",
    "        if j < cur_len:\n",
    "            source_x_diff_ex[i,j,:] = torch.Tensor(source_x_diff[i])[j,:]\n",
    "        else:\n",
    "            source_x_diff_ex[i,j,:] = torch.Tensor(source_x_diff[i])[cur_len - 1,:]\n",
    "\n",
    "for i in range(len(source_x)):\n",
    "    for j in range(source_x_longest):\n",
    "        cur_len = len(source_x[i])\n",
    "        if j < cur_len:\n",
    "            source_x_ex[i,j,:] = torch.Tensor(source_x[i])[j,:]\n",
    "        else:\n",
    "            source_x_ex[i,j,:] = torch.Tensor(source_x[i])[cur_len - 1,:]\n",
    "\n",
    "target_x_diff_longest = max(list(len(_) for _ in target_x_diff))\n",
    "target_batch = len(target_x_diff)\n",
    "target_features = target_total_len - common_len\n",
    "target_x_diff_ex = torch.zeros((target_batch, target_x_diff_longest, target_features))\n",
    "\n",
    "for i in range(len(target_x_diff)):\n",
    "    for j in range(target_x_diff_longest):\n",
    "        cur_len = len(target_x_diff[i])\n",
    "        if j < cur_len:\n",
    "            target_x_diff_ex[i,j,:] = torch.Tensor(target_x_diff[i])[j,:]\n",
    "        else:\n",
    "            target_x_diff_ex[i,j,:] = torch.Tensor(target_x_diff[i])[cur_len - 1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8285543981909917, 2.770245567717861, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8374745525934485, 2.8093811444689463, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5338330207864587, 0.431156978499758, 0.8964660630930379, -1.342421748214616, -0.6946051308187712, -0.2120300841305121, 0.8952797203562032, 0.0387041303378994, -0.8612693268611251, 0.0726334784031089, -0.5051102137388988, -0.2865727201409924, 0.3988826585206329, -1.3794377690564883, -0.8237089728907875, -0.9221119476695248, -1.6036614531597553, -0.2379932071009605, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -1.1501193786706505, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, 1.0826051686869729, -0.609009148503521, 0.9684393533852332, -0.3880554131475662, 0.8439788318452395, 2.837917502516613, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 0.278477698357045, 1.2505677424119097, -0.6305581644917595, -0.9062745442328174, -0.7536814885080627, -0.4374103947888615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, -0.7710128163592748, -0.6738408269117502, -2.438483340619628, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8608899578998963, 2.912112033440545, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3396023676711391, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8763143915541441, 2.979783968239297, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5125273793649132, 0.601421850901989, 0.5034187451388209, -1.455529079732558, -0.5254345623915906, -0.4163803392884947, 0.7885904984786939, 0.2816638966057675, -0.5890172833864098, 0.5902661209826593, -0.3430637446868887, -0.7482466726318323, -0.1864257310498265, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.2249939001501796, -0.0341172340589738, 0.3466058957088718, 0.3482841380580905, -0.2890718868318484, 0.1923438589112976, -0.2031244129114749, -1.100153472115602, -0.7598199909551685, -0.6868858002659636, 0.3602180776283341, -0.700061185363224, -0.501359972189453, -0.2332733777972843, -1.36588558517574, 0.3571411263970316, -0.7447833078367583, 0.8765002281041955, 2.9805992927549454, 0.0776143028335215, -0.2769313825285214, 0.0755136678180621, -0.1474826847594624, -0.3999358135056357, -0.0169688050451549, -0.0937841116273902, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, -0.0122781725754626, -0.3292808750219816, -0.2769313825285214, -0.107625012968948, -0.1474826847594624, -0.5354516373428909, -0.0002229391773086, -0.0862088042532133, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, 0.0766269712424727, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.3951350096437179, -0.5239726809327129, -0.8378690925056301, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.8988006141103361, 3.0784382346326584, 0.0776143028335215, -0.3487779453829513, 0.070457637721598, -0.1474826847594624, -0.3999358135056357, 0.0045338857876385, -0.0729488956980065, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.1275947723114954, -0.4255010572823463, -0.3487779453829513, -0.1777070069371955, -0.1474826847594624, -0.5354516373428909, 0.029608437271828, -0.0566564538358881, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.0232601678654147, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.9106941533136118, 3.1306190036341057, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9108799898636633, 3.1314343281497536, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 1.0542620417323658, -0.6738408269117502, -0.0796833960305785, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9222160194167848, 3.1811691236042576, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.5107528363891513, 1.4769583166408096, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-1.038066534429697, 0.260892106097527, 0.1645848503507034, -1.455529079732558, -1.6391408045371951, 0.196670426185453, 1.2420196914581063, 0.4706326037029981, -0.6616178283130005, -0.5004598044528213, -0.4609157221792596, 0.021209914852902, -1.1769476210921423, 0.0909225332951111, -1.348247483817418, -1.2590833683251033, -0.9252694826312332, 0.781386658108973, -0.0798138719333467, -0.2032969502372001, -0.5728175028694968, 0.103242147293012, -0.2031244129114749, -0.6504603131201653, -0.5868915885967425, -0.6218024077192129, -0.0284729839577739, -0.3925684926321655, -0.4454382040900255, -0.2791761177909213, -1.5149673075505703, 0.3413397023846657, -0.8352815289623093, 0.9329945393197532, 3.228457945511819, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8995798651896526, 0.4879119359671683, 0.1510314945591795, -1.4131138304133295, -0.6523124887119757, 0.196670426185453, 0.148455167213642, 0.4706326037029981, -0.752368509471239, -0.9441449266638648, -0.9507380036319262, 0.1751012323498492, -1.0418764542681906, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -1.3629417216818924, 0.1697587389830128, -0.364093717028159, -0.2645837378255657, -0.2890718868318484, 0.0364158635792983, -0.2031244129114749, -1.8246591171638051, -0.4061028043129334, -0.5350245509902118, 0.0492652283594477, 0.0357249008146661, -0.4929416415078188, -0.5851943844151671, -1.170932563608653, 0.051337096981241, -0.8763418268751098, 0.9447022419729768, 3.279823389997619, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.962914223877992, 3.359725192531085, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.973321070680858, 3.405383365407351, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9735069072309092, 3.4061986899229986, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9980373318376644, 3.5138215259884835, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0340896225475933, 3.671994482024121, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.2639920224449301, -0.6738408269117502, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0396647190491284, 3.696454217493548, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7418059691584676, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, -0.6693833821778409, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.057505027854041, 3.774725370995719, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5792130238763558, -0.9062745442328174, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, 0.0888023142972107, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0760886828591592, 3.856257822560481, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "69\n",
      "325\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Transfer Target Dataset & Model\")\n",
    "\n",
    "if target_dataset == 'PD':\n",
    "    data_path = './data/PD/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y_z.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    tar_subset_idx = [0, 2, 3, 4, 5, 7, 8, 9, 12, 16, 17, 19, 20, 56, 57, 58]\n",
    "    tar_other_idx = list(range(69))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "elif target_dataset == 'TJ':\n",
    "    data_path = './data/Tongji/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    for i in range(len(all_time)):\n",
    "        for j in range(len(all_time[i])):\n",
    "            all_time[i][j] = all_time[i][j][-1]\n",
    "            all_y[i][j] = all_y[i][j][0]\n",
    "\n",
    "    tar_subset_idx = [2, 3, 4, 9, 13, 14, 26, 27, 30, 32, 34, 38, 39, 41, 52, 53, 66, 74]\n",
    "    tar_other_idx = list(range(75))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "elif target_dataset == 'HM':\n",
    "    data_path = './data/CDSL/'\n",
    "    all_x = pickle.load(open(data_path + 'x.pkl', 'rb'))\n",
    "    all_y = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_time = pickle.load(open(data_path + 'y.pkl', 'rb'))\n",
    "    all_x_len = [len(i) for i in all_x]\n",
    "\n",
    "    for i in range(len(all_time)):\n",
    "        for j in range(len(all_time[i])):\n",
    "            all_time[i][j] = all_time[i][j][-1]\n",
    "            all_y[i][j] = all_y[i][j][0]\n",
    "\n",
    "    tar_subset_idx = [5, 6, 4, 2, 3, 48, 79, 76, 87, 25, 30, 31, 18, 43, 58, 66, 40, 57, 23, 92, 50, 54, 91, 60, 39, 81]\n",
    "    tar_other_idx = list(range(99))\n",
    "    for i in tar_subset_idx:\n",
    "        tar_other_idx.remove(i)\n",
    "    for i in range(len(all_x)):\n",
    "        cur = np.array(all_x[i], dtype=float)\n",
    "        cur_subset = cur[:, tar_subset_idx]\n",
    "        cur_other = cur[:, tar_other_idx]\n",
    "        all_x[i] = np.concatenate((cur_subset, cur_other), axis=1).tolist()\n",
    "    \n",
    "print(all_x[0])\n",
    "print(len(all_x[0][0]))\n",
    "print(len(all_x))\n",
    "logger.info(all_x[0])\n",
    "logger.info(len(all_x[0][0]))\n",
    "logger.info(len(all_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_x = all_x\n",
    "# long_y = all_y\n",
    "# long_y_kfold = [each[-1] for each in all_y]\n",
    "long_time = all_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_n2n_data(x, y, x_len):\n",
    "    length = len(x)\n",
    "    assert length == len(y)\n",
    "    assert length == len(x_len)\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    new_x_len = []\n",
    "    for i in range(length):\n",
    "        for j in range(len(x[i])):\n",
    "            new_x.append(x[i][:j+1])\n",
    "            new_y.append(y[i][j])\n",
    "            new_x_len.append(j+1)\n",
    "    return new_x, new_y, new_x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class target_model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, d_model,  MHD_num_head, d_ff, output_dim, keep_prob=0.5):\n",
    "        super(target_model, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.d_model = d_model\n",
    "        self.MHD_num_head = MHD_num_head\n",
    "        self.d_ff = d_ff\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        # layers\n",
    "        self.GRUs = clones(nn.GRU(1, self.hidden_dim, batch_first = True), self.input_dim)\n",
    "        \n",
    "\n",
    "        self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(12, self.hidden_dim)\n",
    "        self.Linear = nn.Linear(self.hidden_dim, 1)\n",
    "        self.output = nn.Linear(self.input_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.FC_embed = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(1, hidden_dim, kernel_size=8, padding=4, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(hidden_dim, 2*hidden_dim, kernel_size=5, padding=2, bias=False),\n",
    "            nn.BatchNorm1d(2*hidden_dim),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(2*hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        \n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, self.output_dim)\n",
    "        )\n",
    "        self.MLP_outcome = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, self.output_dim)\n",
    "        )\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, input, lens):\n",
    "        lens = lens.to('cpu')\n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "        assert(self.d_model % self.MHD_num_head == 0)\n",
    "        codats_embeded_input = self.layers(input[:,:,0].squeeze().unsqueeze(1)).unsqueeze(1)\n",
    "        # print(f'input[:,:,i].shape is {input[:,:,0].shape}')\n",
    "        for i in range(1, feature_dim):\n",
    "            embeded_input = self.layers(input[:,:,i].squeeze().unsqueeze(1)).unsqueeze(1)\n",
    "            codats_embeded_input = torch.cat((codats_embeded_input, embeded_input), 1)\n",
    "            # print(f'docats_embeded_input.shape is {codats_embeded_input.shape}')\n",
    "        codats_embeded_input = codats_embeded_input.squeeze()\n",
    "\n",
    "        codats_input = codats_embeded_input[:, 0, :]\n",
    "        for i in range(1, feature_dim):\n",
    "             codats_input =  codats_input + codats_embeded_input[:, i, :]  \n",
    "        # print(f\"common_input1.shape is {gru_input.shape}\")\n",
    "        codats_input = torch.squeeze(codats_input, 1) # batch * hidden\n",
    "        # print(f\"common_input2.shape is {gru_input.shape}\")\n",
    "\n",
    "        posi_input = self.dropout(codats_input) # batch_size * d_input + d_input_diff * hidden_dim\n",
    "        output = self.MLP(posi_input)\n",
    "        outcome = self.MLP_outcome(posi_input)\n",
    "        outcome = F.sigmoid(outcome)\n",
    "        if self.output_dim != 1:\n",
    "            output = F.softmax(output, dim=1)\n",
    "        return output, outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_gru_dict(pretrain_dict, model_dict):\n",
    "    state_dict = {}\n",
    "    \n",
    "    for k, v in pretrain_dict.items():\n",
    "        model_point_position1 = k.find('.')\n",
    "        model_module_name = k[:model_point_position1]\n",
    "        if \"layers\" == model_module_name:\n",
    "            model_point_position2 = k.find('.', model_point_position1+1)\n",
    "            model_module_idx = int(k[model_point_position1 + 1: model_point_position2])\n",
    "            print(f'model_module_idx is {model_module_idx}')\n",
    "            state_dict[k] = pretrain_dict[k]\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_dataset == 'PD':\n",
    "    input_dim = 69\n",
    "elif target_dataset == 'TJ':\n",
    "    input_dim = 75\n",
    "elif target_dataset == 'HM':\n",
    "    input_dim = 99\n",
    "    \n",
    "cell = 'GRU'\n",
    "hidden_dim = 64\n",
    "d_model = 64\n",
    "MHD_num_head = 4\n",
    "d_ff = 64\n",
    "output_dim = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ckd_batch_iter(x, y, lens, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(x) / batch_size) # 向下取整\n",
    "    index_array = list(range(len(x)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size] #  fetch out all the induces\n",
    "        \n",
    "        examples = []\n",
    "        for idx in indices:\n",
    "            examples.append((x[idx], y[idx],  lens[idx]))\n",
    "       \n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "    \n",
    "        batch_x = [e[0] for e in examples]\n",
    "        batch_y = [e[1] for e in examples]\n",
    "#         batch_name = [e[2] for e in examples]\n",
    "        batch_lens = [e[2] for e in examples]\n",
    "       \n",
    "\n",
    "        yield batch_x, batch_y, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetMultitaskLoss(nn.Module):\n",
    "    def __init__(self, task_num=2):\n",
    "        super(TargetMultitaskLoss, self).__init__()\n",
    "        self.task_num = task_num\n",
    "        self.alpha = nn.Parameter(torch.ones((task_num)), requires_grad=True)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "    def forward(self, opt_student, los, outcome, outcome_y):\n",
    "        MSE_Loss = self.mse(opt_student, los)\n",
    "        BCE_Loss = self.bce(outcome, outcome_y)\n",
    "        return MSE_Loss * self.alpha[0] + BCE_Loss * self.alpha[1]\n",
    "\n",
    "def get_target_multitask_loss(opt_student, los, outcome, outcome_y):\n",
    "    mtl = TargetMultitaskLoss(task_num=2)\n",
    "    return mtl(opt_student, los, outcome, outcome_y)\n",
    "\n",
    "def reverse_los(y, los_info):\n",
    "    return y * los_info[\"los_std\"] + los_info[\"los_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'los_mean': 1055.0307777880782, 'los_std': 799.0879849276147}\n"
     ]
    }
   ],
   "source": [
    "los_info = pickle.load(open(data_path + 'los_info.pkl', 'rb'))\n",
    "print(los_info)\n",
    "logger.info(los_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 0 Batch 0: Train Loss = 72.1071\n",
      "Fold 1, epoch 0: Loss = 7.8526 Valid loss = 1.1793 MSE = 814.2098\n",
      "------------ Save FOLD-BEST model - MSE: 814.2098 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0  60 206   0]\n",
      " [  0 202 757   0]\n",
      " [  0 131 377   0]\n",
      " [  0  99 243   0]]\n",
      "Mean absolute deviation (MAD) = 23.62084598421489\n",
      "Mean squared error (MSE) = 814.2098191072294\n",
      "Mean absolute percentage error (MAPE) = 302.500848144334\n",
      "Cohen kappa score = -0.03263354955477715\n",
      "------------ Save best model - MSE: 814.2098 ------------\n",
      "Fold 1, mse = 814.2098, mad = 23.6208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1 Batch 0: Train Loss = 1.3365\n",
      "------------ Save FOLD-BEST model - MSE: 750.5942 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0   0 266   0]\n",
      " [  0   0 959   0]\n",
      " [  0   0 508   0]\n",
      " [  0   0 342   0]]\n",
      "Mean absolute deviation (MAD) = 23.632402604685897\n",
      "Mean squared error (MSE) = 750.594190851123\n",
      "Mean absolute percentage error (MAPE) = 333.42080882346295\n",
      "Cohen kappa score = 0.0\n",
      "------------ Save best model - MSE: 750.5942 ------------\n",
      "Fold 1, mse = 750.5942, mad = 23.6324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 2 Batch 0: Train Loss = 1.0165\n",
      "------------ Save FOLD-BEST model - MSE: 749.3143 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0   0 266   0]\n",
      " [  0   0 959   0]\n",
      " [  0   0 508   0]\n",
      " [  0   0 342   0]]\n",
      "Mean absolute deviation (MAD) = 23.677345646159257\n",
      "Mean squared error (MSE) = 749.3143360443516\n",
      "Mean absolute percentage error (MAPE) = 337.80900797999436\n",
      "Cohen kappa score = 0.0\n",
      "------------ Save best model - MSE: 749.3143 ------------\n",
      "Fold 1, mse = 749.3143, mad = 23.6773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 3 Batch 0: Train Loss = 0.8943\n",
      "------------ Save FOLD-BEST model - MSE: 677.1827 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0  65 201   0]\n",
      " [  0 208 751   0]\n",
      " [  0  27 481   0]\n",
      " [  0  20 322   0]]\n",
      "Mean absolute deviation (MAD) = 21.932646845077787\n",
      "Mean squared error (MSE) = 677.1827282504215\n",
      "Mean absolute percentage error (MAPE) = 292.40350059492874\n",
      "Cohen kappa score = 0.09473391294904943\n",
      "------------ Save best model - MSE: 677.1827 ------------\n",
      "Fold 1, mse = 677.1827, mad = 21.9326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 4 Batch 0: Train Loss = 0.9905\n",
      "------------ Save FOLD-BEST model - MSE: 633.4753 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0 163 103   0]\n",
      " [  0 496 463   0]\n",
      " [  0 212 296   0]\n",
      " [  0  64 278   0]]\n",
      "Mean absolute deviation (MAD) = 20.594966195026462\n",
      "Mean squared error (MSE) = 633.475322366702\n",
      "Mean absolute percentage error (MAPE) = 248.41779477428375\n",
      "Cohen kappa score = 0.12861838770870437\n",
      "------------ Save best model - MSE: 633.4753 ------------\n",
      "Fold 1, mse = 633.4753, mad = 20.5950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 5 Batch 0: Train Loss = 1.1202\n",
      "Fold 1, mse = 659.8279, mad = 20.2928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 6 Batch 0: Train Loss = 0.8713\n",
      "Fold 1, mse = 650.0807, mad = 20.2310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 7 Batch 0: Train Loss = 1.0459\n",
      "Fold 1, mse = 635.1553, mad = 20.5666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 8 Batch 0: Train Loss = 1.0840\n",
      "Fold 1, mse = 662.8716, mad = 20.3004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 9 Batch 0: Train Loss = 1.0117\n",
      "Fold 1, mse = 680.9585, mad = 20.5112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 10 Batch 0: Train Loss = 0.9691\n",
      "Fold 1, epoch 10: Loss = 0.9452 Valid loss = 0.9515 MSE = 710.1398\n",
      "Fold 1, mse = 710.1398, mad = 20.8788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 11 Batch 0: Train Loss = 0.9234\n",
      "Fold 1, mse = 765.2659, mad = 21.4794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 12 Batch 0: Train Loss = 1.0027\n",
      "Fold 1, mse = 721.8142, mad = 21.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 13 Batch 0: Train Loss = 0.9516\n",
      "------------ Save FOLD-BEST model - MSE: 625.8400 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0 190  76   0]\n",
      " [  0 619 340   0]\n",
      " [  0 268 240   0]\n",
      " [  0 109 233   0]]\n",
      "Mean absolute deviation (MAD) = 20.19409781066168\n",
      "Mean squared error (MSE) = 625.8399505312944\n",
      "Mean absolute percentage error (MAPE) = 231.3948948087006\n",
      "Cohen kappa score = 0.1344706696737601\n",
      "------------ Save best model - MSE: 625.8400 ------------\n",
      "Fold 1, mse = 625.8400, mad = 20.1941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 14 Batch 0: Train Loss = 0.8635\n",
      "Fold 1, mse = 712.3837, mad = 20.8776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 15 Batch 0: Train Loss = 0.8531\n",
      "Fold 1, mse = 737.8831, mad = 21.1351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 16 Batch 0: Train Loss = 0.8397\n",
      "Fold 1, mse = 882.2655, mad = 23.1343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 17 Batch 0: Train Loss = 0.9895\n",
      "Fold 1, mse = 696.2643, mad = 20.6858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 18 Batch 0: Train Loss = 1.0658\n",
      "Fold 1, mse = 766.2148, mad = 21.4898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 19 Batch 0: Train Loss = 0.8657\n",
      "Fold 1, mse = 631.4930, mad = 20.8332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 20 Batch 0: Train Loss = 0.9311\n",
      "Fold 1, epoch 20: Loss = 0.9341 Valid loss = 1.0218 MSE = 751.7872\n",
      "Fold 1, mse = 751.7872, mad = 21.3301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 21 Batch 0: Train Loss = 0.9216\n",
      "Fold 1, mse = 854.9420, mad = 22.8062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 22 Batch 0: Train Loss = 0.8922\n",
      "------------ Save FOLD-BEST model - MSE: 613.5950 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0 199  67   0]\n",
      " [  0 640 319   0]\n",
      " [  0 262 246   0]\n",
      " [  0 112 230   0]]\n",
      "Mean absolute deviation (MAD) = 19.94442335857548\n",
      "Mean squared error (MSE) = 613.594980516164\n",
      "Mean absolute percentage error (MAPE) = 228.008702044516\n",
      "Cohen kappa score = 0.15230245475721194\n",
      "------------ Save best model - MSE: 613.5950 ------------\n",
      "Fold 1, mse = 613.5950, mad = 19.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 23 Batch 0: Train Loss = 1.1818\n",
      "Fold 1, mse = 832.0229, mad = 22.4699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 24 Batch 0: Train Loss = 0.9975\n",
      "Fold 1, mse = 933.6638, mad = 23.7635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 25 Batch 0: Train Loss = 0.9944\n",
      "Fold 1, mse = 779.7489, mad = 21.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 26 Batch 0: Train Loss = 1.0959\n",
      "Fold 1, mse = 748.5878, mad = 21.1958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 27 Batch 0: Train Loss = 0.9423\n",
      "Fold 1, mse = 807.3810, mad = 21.9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 28 Batch 0: Train Loss = 0.9169\n",
      "Fold 1, mse = 684.4929, mad = 20.5588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 29 Batch 0: Train Loss = 0.9781\n",
      "Fold 1, mse = 757.8957, mad = 21.4504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 0 Batch 0: Train Loss = 17.3569\n",
      "Fold 2, epoch 0: Loss = 5.9792 Valid loss = 0.7772 MSE = 569.8493\n",
      "------------ Save FOLD-BEST model - MSE: 569.8493 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0 155 116   0]\n",
      " [  0 548 425   0]\n",
      " [  0 241 330   0]\n",
      " [  0  45 225   0]]\n",
      "Mean absolute deviation (MAD) = 19.34140817836046\n",
      "Mean squared error (MSE) = 569.8492750891465\n",
      "Mean absolute percentage error (MAPE) = 259.50288556814775\n",
      "Cohen kappa score = 0.14169594102958694\n",
      "------------ Save best model - MSE: 569.8493 ------------\n",
      "Fold 2, mse = 569.8493, mad = 19.3414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 1 Batch 0: Train Loss = 1.0209\n",
      "------------ Save FOLD-BEST model - MSE: 545.6855 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0 178  93   0]\n",
      " [  0 616 357   0]\n",
      " [  0 296 275   0]\n",
      " [  0  76 194   0]]\n",
      "Mean absolute deviation (MAD) = 18.47127305523778\n",
      "Mean squared error (MSE) = 545.6855415317389\n",
      "Mean absolute percentage error (MAPE) = 232.9154776790005\n",
      "Cohen kappa score = 0.12607435758421337\n",
      "------------ Save best model - MSE: 545.6855 ------------\n",
      "Fold 2, mse = 545.6855, mad = 18.4713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 2 Batch 0: Train Loss = 0.9966\n",
      "Fold 2, mse = 549.1325, mad = 18.3344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 3 Batch 0: Train Loss = 0.9447\n",
      "------------ Save FOLD-BEST model - MSE: 542.6418 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[ 13 188  70   0]\n",
      " [  9 663 301   0]\n",
      " [  0 343 228   0]\n",
      " [  0 105 165   0]]\n",
      "Mean absolute deviation (MAD) = 18.242439507629108\n",
      "Mean squared error (MSE) = 542.6418033946734\n",
      "Mean absolute percentage error (MAPE) = 213.25146782073463\n",
      "Cohen kappa score = 0.12287004530749157\n",
      "------------ Save best model - MSE: 542.6418 ------------\n",
      "Fold 2, mse = 542.6418, mad = 18.2424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 4 Batch 0: Train Loss = 0.9414\n",
      "------------ Save FOLD-BEST model - MSE: 541.9739 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  6 164 101   0]\n",
      " [  3 593 377   0]\n",
      " [  0 274 297   0]\n",
      " [  0  61 209   0]]\n",
      "Mean absolute deviation (MAD) = 18.435064592206054\n",
      "Mean squared error (MSE) = 541.9739353425562\n",
      "Mean absolute percentage error (MAPE) = 234.95716899998666\n",
      "Cohen kappa score = 0.14431565392603496\n",
      "------------ Save best model - MSE: 541.9739 ------------\n",
      "Fold 2, mse = 541.9739, mad = 18.4351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 5 Batch 0: Train Loss = 1.0121\n",
      "Fold 2, mse = 551.1479, mad = 18.4690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 6 Batch 0: Train Loss = 0.8236\n",
      "Fold 2, mse = 543.4236, mad = 18.3083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 7 Batch 0: Train Loss = 1.0509\n",
      "------------ Save FOLD-BEST model - MSE: 538.3216 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[ 17 173  81   0]\n",
      " [ 13 625 335   0]\n",
      " [  0 316 255   0]\n",
      " [  0  89 181   0]]\n",
      "Mean absolute deviation (MAD) = 18.226069193796004\n",
      "Mean squared error (MSE) = 538.3215638147557\n",
      "Mean absolute percentage error (MAPE) = 218.74482677455367\n",
      "Cohen kappa score = 0.1344146768258906\n",
      "------------ Save best model - MSE: 538.3216 ------------\n",
      "Fold 2, mse = 538.3216, mad = 18.2261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 8 Batch 0: Train Loss = 0.9894\n",
      "Fold 2, mse = 557.2650, mad = 18.5608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 9 Batch 0: Train Loss = 0.9863\n",
      "Fold 2, mse = 660.2055, mad = 20.4778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 10 Batch 0: Train Loss = 0.9578\n",
      "Fold 2, epoch 10: Loss = 0.9835 Valid loss = 0.7344 MSE = 553.7648\n",
      "Fold 2, mse = 553.7648, mad = 18.5832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 11 Batch 0: Train Loss = 0.9607\n",
      "Fold 2, mse = 608.6864, mad = 19.6581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 12 Batch 0: Train Loss = 0.9533\n",
      "Fold 2, mse = 709.3873, mad = 21.2633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 13 Batch 0: Train Loss = 1.0500\n",
      "Fold 2, mse = 582.1337, mad = 19.0246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 14 Batch 0: Train Loss = 0.9214\n",
      "Fold 2, mse = 615.6527, mad = 19.7716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 15 Batch 0: Train Loss = 1.0217\n",
      "Fold 2, mse = 607.8264, mad = 19.5673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 16 Batch 0: Train Loss = 0.8610\n",
      "Fold 2, mse = 560.6755, mad = 18.7379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 17 Batch 0: Train Loss = 0.9867\n",
      "Fold 2, mse = 615.5615, mad = 19.8348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 18 Batch 0: Train Loss = 0.9148\n",
      "Fold 2, mse = 770.6163, mad = 22.1817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 19 Batch 0: Train Loss = 1.0319\n",
      "Fold 2, mse = 781.2954, mad = 22.3610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 20 Batch 0: Train Loss = 0.9752\n",
      "Fold 2, epoch 20: Loss = 0.9582 Valid loss = 1.7205 MSE = 775.3091\n",
      "Fold 2, mse = 775.3091, mad = 22.2714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 21 Batch 0: Train Loss = 1.1316\n",
      "Fold 2, mse = 605.1253, mad = 19.7284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 22 Batch 0: Train Loss = 0.8920\n",
      "Fold 2, mse = 594.7267, mad = 19.5065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 23 Batch 0: Train Loss = 0.9956\n",
      "Fold 2, mse = 797.3380, mad = 22.5208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 24 Batch 0: Train Loss = 1.0019\n",
      "Fold 2, mse = 820.7901, mad = 22.6070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 25 Batch 0: Train Loss = 1.0222\n",
      "Fold 2, mse = 745.8438, mad = 21.6058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 26 Batch 0: Train Loss = 0.9849\n",
      "Fold 2, mse = 707.3613, mad = 21.2887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 27 Batch 0: Train Loss = 0.9128\n",
      "Fold 2, mse = 806.3321, mad = 22.5521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 28 Batch 0: Train Loss = 0.9415\n",
      "Fold 2, mse = 999.0415, mad = 24.7160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 29 Batch 0: Train Loss = 1.0053\n",
      "Fold 2, mse = 703.0652, mad = 21.2001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 0 Batch 0: Train Loss = 19.6781\n",
      "Fold 3, epoch 0: Loss = 3.8732 Valid loss = 0.9116 MSE = 651.3079\n",
      "------------ Save FOLD-BEST model - MSE: 651.3079 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0 148 133   0]\n",
      " [  0 469 650   0]\n",
      " [  0 236 457   0]\n",
      " [  0  50 347   0]]\n",
      "Mean absolute deviation (MAD) = 20.84477064604104\n",
      "Mean squared error (MSE) = 651.3079292808779\n",
      "Mean absolute percentage error (MAPE) = 207.1259888751579\n",
      "Cohen kappa score = 0.11120305498856842\n",
      "Fold 3, mse = 651.3079, mad = 20.8448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 1 Batch 0: Train Loss = 1.1399\n",
      "------------ Save FOLD-BEST model - MSE: 624.2762 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0 261  20   0]\n",
      " [  0 886 233   0]\n",
      " [  0 450 243   0]\n",
      " [  0 215 182   0]]\n",
      "Mean absolute deviation (MAD) = 19.92716747754808\n",
      "Mean squared error (MSE) = 624.2762468365441\n",
      "Mean absolute percentage error (MAPE) = 176.03332305863378\n",
      "Cohen kappa score = 0.13841951219512194\n",
      "Fold 3, mse = 624.2762, mad = 19.9272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 2 Batch 0: Train Loss = 0.9251\n",
      "------------ Save FOLD-BEST model - MSE: 618.9336 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0 255  26   0]\n",
      " [  0 839 280   0]\n",
      " [  0 411 282   0]\n",
      " [  0 193 204   0]]\n",
      "Mean absolute deviation (MAD) = 19.92940166293779\n",
      "Mean squared error (MSE) = 618.9336492387221\n",
      "Mean absolute percentage error (MAPE) = 179.44295635695377\n",
      "Cohen kappa score = 0.1492564287927295\n",
      "Fold 3, mse = 618.9336, mad = 19.9294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 3 Batch 0: Train Loss = 0.9643\n",
      "Fold 3, mse = 631.4752, mad = 19.9394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 4 Batch 0: Train Loss = 1.0566\n",
      "Fold 3, mse = 690.8201, mad = 20.7467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 5 Batch 0: Train Loss = 0.9990\n",
      "Fold 3, mse = 656.2347, mad = 20.3240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 6 Batch 0: Train Loss = 0.9353\n",
      "Fold 3, mse = 640.4016, mad = 20.2078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 7 Batch 0: Train Loss = 1.0733\n",
      "Fold 3, mse = 627.8571, mad = 20.1888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 8 Batch 0: Train Loss = 0.9703\n",
      "Fold 3, mse = 629.4040, mad = 20.1573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 9 Batch 0: Train Loss = 1.1132\n",
      "Fold 3, mse = 730.8017, mad = 21.4309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 10 Batch 0: Train Loss = 1.0242\n",
      "Fold 3, epoch 10: Loss = 0.9710 Valid loss = 0.8968 MSE = 638.1349\n",
      "Fold 3, mse = 638.1349, mad = 20.2042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 11 Batch 0: Train Loss = 1.0593\n",
      "Fold 3, mse = 675.7176, mad = 20.7740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 12 Batch 0: Train Loss = 1.0377\n",
      "Fold 3, mse = 645.7640, mad = 20.3258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 13 Batch 0: Train Loss = 1.1186\n",
      "Fold 3, mse = 679.9009, mad = 20.6887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 14 Batch 0: Train Loss = 1.0065\n",
      "Fold 3, mse = 667.9975, mad = 20.5931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 15 Batch 0: Train Loss = 0.9175\n",
      "Fold 3, mse = 673.4206, mad = 20.5926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 16 Batch 0: Train Loss = 0.9251\n",
      "Fold 3, mse = 641.3312, mad = 20.2139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 17 Batch 0: Train Loss = 0.9262\n",
      "Fold 3, mse = 675.5153, mad = 20.7260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 18 Batch 0: Train Loss = 1.1502\n",
      "Fold 3, mse = 807.6460, mad = 22.2971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 19 Batch 0: Train Loss = 0.9321\n",
      "Fold 3, mse = 717.1849, mad = 21.0736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 20 Batch 0: Train Loss = 0.9346\n",
      "Fold 3, epoch 20: Loss = 0.9548 Valid loss = 1.3530 MSE = 820.0933\n",
      "Fold 3, mse = 820.0933, mad = 22.2767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 21 Batch 0: Train Loss = 0.8687\n",
      "Fold 3, mse = 739.5975, mad = 21.2385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 22 Batch 0: Train Loss = 0.8696\n",
      "Fold 3, mse = 843.2729, mad = 22.5037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 23 Batch 0: Train Loss = 1.0094\n",
      "Fold 3, mse = 858.5342, mad = 22.7345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 24 Batch 0: Train Loss = 0.8898\n",
      "Fold 3, mse = 770.6214, mad = 21.7128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 25 Batch 0: Train Loss = 1.1575\n",
      "Fold 3, mse = 785.6282, mad = 21.9452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 26 Batch 0: Train Loss = 0.9572\n",
      "Fold 3, mse = 817.1407, mad = 22.4509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 27 Batch 0: Train Loss = 0.9755\n",
      "Fold 3, mse = 825.2895, mad = 22.3501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 28 Batch 0: Train Loss = 1.0590\n",
      "Fold 3, mse = 779.2282, mad = 21.9237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 29 Batch 0: Train Loss = 0.9004\n",
      "Fold 3, mse = 947.0709, mad = 23.8943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 0 Batch 0: Train Loss = 18.1816\n",
      "Fold 4, epoch 0: Loss = 2.4800 Valid loss = 1.1380 MSE = 816.2749\n",
      "------------ Save FOLD-BEST model - MSE: 816.2749 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0 132 118   0]\n",
      " [  0 476 401   0]\n",
      " [  0 330 225   0]\n",
      " [  0 141 279   0]]\n",
      "Mean absolute deviation (MAD) = 23.113430187048532\n",
      "Mean squared error (MSE) = 816.274939521705\n",
      "Mean absolute percentage error (MAPE) = 227.18844621011664\n",
      "Cohen kappa score = 0.03430838300649319\n",
      "Fold 4, mse = 816.2749, mad = 23.1134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 1 Batch 0: Train Loss = 0.8474\n",
      "Fold 4, mse = 841.3248, mad = 23.3272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 2 Batch 0: Train Loss = 0.9318\n",
      "Fold 4, mse = 851.7866, mad = 23.2881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 3 Batch 0: Train Loss = 0.8632\n",
      "Fold 4, mse = 831.7455, mad = 23.2595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 4 Batch 0: Train Loss = 0.8539\n",
      "Fold 4, mse = 846.6688, mad = 23.2745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 5 Batch 0: Train Loss = 0.9331\n",
      "Fold 4, mse = 849.8091, mad = 23.3133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 6 Batch 0: Train Loss = 0.8918\n",
      "Fold 4, mse = 864.5068, mad = 23.2790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 7 Batch 0: Train Loss = 0.7644\n",
      "Fold 4, mse = 837.6782, mad = 23.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 8 Batch 0: Train Loss = 0.9790\n",
      "Fold 4, mse = 876.5536, mad = 23.2834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 9 Batch 0: Train Loss = 0.8811\n",
      "Fold 4, mse = 870.3937, mad = 23.2902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 10 Batch 0: Train Loss = 0.9348\n",
      "Fold 4, epoch 10: Loss = 0.9125 Valid loss = 1.1937 MSE = 875.0414\n",
      "Fold 4, mse = 875.0414, mad = 23.2987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 11 Batch 0: Train Loss = 1.0012\n",
      "Fold 4, mse = 892.8378, mad = 23.3228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 12 Batch 0: Train Loss = 0.9545\n",
      "Fold 4, mse = 901.5189, mad = 23.3594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 13 Batch 0: Train Loss = 0.9683\n",
      "Fold 4, mse = 879.5098, mad = 23.3446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 14 Batch 0: Train Loss = 0.8630\n",
      "Fold 4, mse = 869.7195, mad = 23.3537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 15 Batch 0: Train Loss = 1.0236\n",
      "Fold 4, mse = 916.1129, mad = 23.4351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 16 Batch 0: Train Loss = 1.0029\n",
      "Fold 4, mse = 899.6968, mad = 23.4019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 17 Batch 0: Train Loss = 0.8462\n",
      "Fold 4, mse = 878.7817, mad = 23.4739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 18 Batch 0: Train Loss = 0.9480\n",
      "Fold 4, mse = 897.3107, mad = 23.4621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 19 Batch 0: Train Loss = 0.7857\n",
      "Fold 4, mse = 907.6589, mad = 23.4667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 20 Batch 0: Train Loss = 0.8125\n",
      "Fold 4, epoch 20: Loss = 0.8852 Valid loss = 1.2483 MSE = 925.4428\n",
      "Fold 4, mse = 925.4428, mad = 23.5659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 21 Batch 0: Train Loss = 0.9953\n",
      "Fold 4, mse = 955.4614, mad = 23.6197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 22 Batch 0: Train Loss = 0.8781\n",
      "Fold 4, mse = 953.4800, mad = 23.7293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 23 Batch 0: Train Loss = 1.0178\n",
      "Fold 4, mse = 985.6396, mad = 23.7532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 24 Batch 0: Train Loss = 0.8773\n",
      "Fold 4, mse = 903.9036, mad = 23.6375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 25 Batch 0: Train Loss = 0.8402\n",
      "Fold 4, mse = 934.6306, mad = 23.6397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 26 Batch 0: Train Loss = 0.8374\n",
      "Fold 4, mse = 946.0298, mad = 23.6979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 27 Batch 0: Train Loss = 0.8029\n",
      "Fold 4, mse = 955.3393, mad = 23.7572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 28 Batch 0: Train Loss = 0.8884\n",
      "Fold 4, mse = 982.6415, mad = 23.7852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Epoch 29 Batch 0: Train Loss = 0.7636\n",
      "Fold 4, mse = 975.6768, mad = 23.7367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 0 Batch 0: Train Loss = 9.4128\n",
      "Fold 5, epoch 0: Loss = 1.8103 Valid loss = 1.0857 MSE = 770.1818\n",
      "------------ Save FOLD-BEST model - MSE: 770.1818 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0   0 293   0]\n",
      " [  0   0 910   0]\n",
      " [  0   0 521   0]\n",
      " [  0   0 311   0]]\n",
      "Mean absolute deviation (MAD) = 23.60025101142629\n",
      "Mean squared error (MSE) = 770.1818231650154\n",
      "Mean absolute percentage error (MAPE) = 329.28523532929063\n",
      "Cohen kappa score = 0.0\n",
      "Fold 5, mse = 770.1818, mad = 23.6003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 1 Batch 0: Train Loss = 0.9771\n",
      "------------ Save FOLD-BEST model - MSE: 759.9734 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0   0 293   0]\n",
      " [  0   0 910   0]\n",
      " [  0   0 521   0]\n",
      " [  0   0 311   0]]\n",
      "Mean absolute deviation (MAD) = 23.3655392802782\n",
      "Mean squared error (MSE) = 759.9733699417149\n",
      "Mean absolute percentage error (MAPE) = 322.4155760410827\n",
      "Cohen kappa score = 0.0\n",
      "Fold 5, mse = 759.9734, mad = 23.3655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 2 Batch 0: Train Loss = 0.9551\n",
      "------------ Save FOLD-BEST model - MSE: 751.1380 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0   3 290   0]\n",
      " [  0   0 910   0]\n",
      " [  0   0 521   0]\n",
      " [  0   0 311   0]]\n",
      "Mean absolute deviation (MAD) = 23.157001799869526\n",
      "Mean squared error (MSE) = 751.138007001242\n",
      "Mean absolute percentage error (MAPE) = 315.6428681843369\n",
      "Cohen kappa score = 0.0013579490616767442\n",
      "Fold 5, mse = 751.1380, mad = 23.1570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 3 Batch 0: Train Loss = 0.9572\n",
      "------------ Save FOLD-BEST model - MSE: 668.0501 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[ 11 103 179   0]\n",
      " [ 18 243 649   0]\n",
      " [  0 106 415   0]\n",
      " [  0   5 306   0]]\n",
      "Mean absolute deviation (MAD) = 21.21315295122685\n",
      "Mean squared error (MSE) = 668.0501440961275\n",
      "Mean absolute percentage error (MAPE) = 263.94654448625926\n",
      "Cohen kappa score = 0.1087070777855641\n",
      "Fold 5, mse = 668.0501, mad = 21.2132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 4 Batch 0: Train Loss = 0.9695\n",
      "Fold 5, mse = 705.6052, mad = 20.8135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 5 Batch 0: Train Loss = 0.9488\n",
      "Fold 5, mse = 684.6701, mad = 20.4734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 6 Batch 0: Train Loss = 0.8956\n",
      "Fold 5, mse = 766.3085, mad = 21.0966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 7 Batch 0: Train Loss = 1.0961\n",
      "Fold 5, mse = 885.9864, mad = 22.8099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 8 Batch 0: Train Loss = 0.9247\n",
      "Fold 5, mse = 853.0717, mad = 22.3373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 9 Batch 0: Train Loss = 0.8312\n",
      "------------ Save FOLD-BEST model - MSE: 655.1040 ------------\n",
      "Custom bins confusion matrix:\n",
      "[[  0 191 102   0]\n",
      " [  1 550 359   0]\n",
      " [  0 261 260   0]\n",
      " [  0 103 208   0]]\n",
      "Mean absolute deviation (MAD) = 20.625540224980906\n",
      "Mean squared error (MSE) = 655.1039651227211\n",
      "Mean absolute percentage error (MAPE) = 236.97014876538236\n",
      "Cohen kappa score = 0.10963208549343295\n",
      "Fold 5, mse = 655.1040, mad = 20.6255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 10 Batch 0: Train Loss = 0.9083\n",
      "Fold 5, epoch 10: Loss = 0.9483 Valid loss = 1.1313 MSE = 774.6718\n",
      "Fold 5, mse = 774.6718, mad = 21.1729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 11 Batch 0: Train Loss = 1.0426\n",
      "Fold 5, mse = 806.7620, mad = 21.6726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 12 Batch 0: Train Loss = 0.9108\n",
      "Fold 5, mse = 725.1805, mad = 20.6897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 13 Batch 0: Train Loss = 1.0442\n",
      "Fold 5, mse = 837.5602, mad = 21.9958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 14 Batch 0: Train Loss = 0.9121\n",
      "Fold 5, mse = 780.8209, mad = 21.2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 15 Batch 0: Train Loss = 0.9053\n",
      "Fold 5, mse = 889.8362, mad = 22.8526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 16 Batch 0: Train Loss = 0.9262\n",
      "Fold 5, mse = 1027.7036, mad = 24.7703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 17 Batch 0: Train Loss = 1.0287\n",
      "Fold 5, mse = 822.3946, mad = 21.6961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 18 Batch 0: Train Loss = 1.0090\n",
      "Fold 5, mse = 882.0251, mad = 22.4194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 19 Batch 0: Train Loss = 1.0251\n",
      "Fold 5, mse = 1034.6094, mad = 24.7585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 20 Batch 0: Train Loss = 0.8348\n",
      "Fold 5, epoch 20: Loss = 0.9143 Valid loss = 1.0042 MSE = 703.9792\n",
      "Fold 5, mse = 703.9792, mad = 20.1546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 21 Batch 0: Train Loss = 1.0813\n",
      "Fold 5, mse = 874.7063, mad = 22.5167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 22 Batch 0: Train Loss = 0.9057\n",
      "Fold 5, mse = 752.8681, mad = 20.9972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 23 Batch 0: Train Loss = 0.8785\n",
      "Fold 5, mse = 902.3720, mad = 23.0314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 24 Batch 0: Train Loss = 0.7134\n",
      "Fold 5, mse = 724.9596, mad = 20.5189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 25 Batch 0: Train Loss = 0.9268\n",
      "Fold 5, mse = 878.9907, mad = 22.6766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 26 Batch 0: Train Loss = 1.0435\n",
      "Fold 5, mse = 831.5833, mad = 21.9807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 27 Batch 0: Train Loss = 0.9216\n",
      "Fold 5, mse = 1065.6843, mad = 25.2957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 28 Batch 0: Train Loss = 0.8886\n",
      "Fold 5, mse = 904.7855, mad = 22.9090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzj/anaconda3/envs/distcare/lib/python3.7/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Epoch 29 Batch 0: Train Loss = 0.7771\n",
      "Fold 5, mse = 695.2265, mad = 20.3360\n",
      "mse 648.4458(92.1128)\n",
      "mad 20.3678(1.5851)\n"
     ]
    }
   ],
   "source": [
    "if target_dataset == 'PD':\n",
    "    n_splits = 5\n",
    "    epochs = 30\n",
    "elif target_dataset == 'TJ':\n",
    "    n_splits = 10\n",
    "    epochs = 150\n",
    "elif target_dataset == 'HM':\n",
    "    n_splits = 3\n",
    "    epochs = 20\n",
    "\n",
    "teacher_flag = True\n",
    "transfer_flag = True\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "if target_dataset == 'PD':    \n",
    "    data_str = 'pd'\n",
    "elif target_dataset == 'TJ':    \n",
    "    data_str = 'covid'\n",
    "elif target_dataset == 'HM':\n",
    "    data_str = 'spain'\n",
    "\n",
    "# if teacher_flag:\n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str\n",
    "# else: \n",
    "#     file_name = './model/pretrained-challenge-front-fill-2'+ data_str + '-noteacher'\n",
    "    \n",
    "file_name = './model/pretrained-dann'+ data_str;\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "fold_count = 0\n",
    "total_train_loss = []\n",
    "total_valid_loss = []\n",
    "\n",
    "global_best = 10000\n",
    "mse = []\n",
    "mad = []\n",
    "mape = []\n",
    "kappa = []\n",
    "history = []\n",
    "\n",
    "pad_token = np.zeros(input_dim)\n",
    "# begin_time = time.time()\n",
    "\n",
    "for train, test in kfold.split(long_x):\n",
    "        \n",
    "    train_x = [long_x[i] for i in train]\n",
    "    train_y = [long_time[i] for i in train]\n",
    "    train_x_len = [all_x_len[i] for i in train]\n",
    "    #train_static = [long_static[i] for i in train]\n",
    "    \n",
    "    train_x, train_y, train_x_len = get_n2n_data(train_x, train_y, train_x_len)\n",
    "    if len(train_x) % 256 == 1:\n",
    "        print(len(train_x))\n",
    "        print('wrong squeeze!')\n",
    "\n",
    "# for train, test in kfold.split(long_x):\n",
    "for train, test in kfold.split(long_x):\n",
    "    if reverse:\n",
    "        temp = train\n",
    "        train = test\n",
    "        test = temp\n",
    "    \n",
    "    model = target_model(input_dim = input_dim,output_dim=output_dim, d_model=d_model, MHD_num_head=MHD_num_head, d_ff=d_ff, hidden_dim=hidden_dim).to(device)\n",
    "    \n",
    "    if transfer_flag:\n",
    "        checkpoint = torch.load(file_name, \\\n",
    "                        map_location=torch.device(\"cuda:3\" if torch.cuda.is_available() == True else 'cpu'))\n",
    "        pretrain_dict = checkpoint['net']\n",
    "        model_dict = model.state_dict()\n",
    "        pretrain_dict = transfer_gru_dict(pretrain_dict, model_dict)\n",
    "        model_dict.update(pretrain_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    fold_count += 1\n",
    "#     print(train)\n",
    "\n",
    "    \n",
    "    train_x = [long_x[i] for i in train]\n",
    "    train_y = [long_time[i] for i in train]\n",
    "    train_x_len = [all_x_len[i] for i in train]\n",
    "    #train_static = [long_static[i] for i in train]\n",
    "    \n",
    "    train_x, train_y, train_x_len = get_n2n_data(train_x, train_y, train_x_len)\n",
    "    \n",
    "    test_x = [long_x[i] for i in test]\n",
    "    test_y = [long_time[i] for i in test]\n",
    "    test_x_len = [all_x_len[i] for i in test]\n",
    "    #test_static = [long_static[i] for i in test]\n",
    "    \n",
    "    test_x, test_y, test_x_len = get_n2n_data(test_x, test_y, test_x_len)\n",
    "    \n",
    "    if not os.path.exists('./model/'+data_str):\n",
    "        os.mkdir('./model/'+data_str)\n",
    "        \n",
    "    if transfer_flag:\n",
    "        target_file_name = './model/'+data_str+'/distcare-trans-'+str(n_splits)+'-fold-LOS-regression' + str(fold_count)#4114\n",
    "    else:\n",
    "        target_file_name = './model/'+data_str+'/distcare-no-trans-'+str(n_splits)+'-fold-LOS-regression' + str(fold_count)#4114\n",
    "    \n",
    "    fold_train_loss = []\n",
    "    fold_valid_loss = []\n",
    "    best_mse = 10000\n",
    "    best_mad = 0\n",
    "    best_mape = 0\n",
    "    best_kappa = 0\n",
    "    \n",
    "    for each_epoch in range(epochs):\n",
    "       \n",
    "        \n",
    "        epoch_loss = []\n",
    "        counter_batch = 0\n",
    "        model.train()  \n",
    "        \n",
    "        for step, (batch_x, batch_y, batch_lens) in enumerate(ckd_batch_iter(train_x, train_y, train_x_len, batch_size, shuffle=True)):  \n",
    "            optimizer.zero_grad()\n",
    "            batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "            batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "            batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "\n",
    "            masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "\n",
    "            opt, emb = model(batch_x, batch_lens)\n",
    "\n",
    "            MSE_Loss = get_re_loss(opt, batch_y.unsqueeze(-1))\n",
    "\n",
    "#             model_loss = pred_loss + 1e7*decov_loss\n",
    "            model_loss = MSE_Loss\n",
    "\n",
    "            loss = model_loss\n",
    "\n",
    "            epoch_loss.append(MSE_Loss.cpu().detach().numpy())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % 50 == 0:\n",
    "                print('Fold %d Epoch %d Batch %d: Train Loss = %.4f'%(fold_count,each_epoch, step, loss.cpu().detach().numpy()))\n",
    "                logger.info('Fold %d Epoch %d Batch %d: Train Loss = %.4f'%(fold_count,each_epoch, step, loss.cpu().detach().numpy()))\n",
    "            \n",
    "        epoch_loss = np.mean(epoch_loss)\n",
    "        fold_train_loss.append(epoch_loss)\n",
    "\n",
    "        #Validation\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_pred_flatten = []\n",
    "        y_true_flatten = []\n",
    "        outcome_pred_flatten = []\n",
    "        outcome_true_flatten = []\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            valid_loss = []\n",
    "            valid_true = []\n",
    "            valid_pred = []\n",
    "            for batch_x, batch_y, batch_lens in ckd_batch_iter(test_x, test_y, test_x_len, batch_size):\n",
    "                batch_x = torch.tensor(pad_sents(batch_x, pad_token), dtype=torch.float32).to(device)\n",
    "                batch_y = torch.tensor(batch_y, dtype=torch.float32).to(device)\n",
    "                batch_lens = torch.tensor(batch_lens, dtype=torch.float32).to(device).int()\n",
    "                masks = length_to_mask(batch_lens).unsqueeze(-1).float()\n",
    "               \n",
    "                opt, emb = model(batch_x, batch_lens)\n",
    "                \n",
    "                MSE_Loss = get_re_loss(opt, batch_y.unsqueeze(-1))\n",
    "                \n",
    "                valid_loss.append(MSE_Loss.cpu().detach().numpy())\n",
    "\n",
    "                y_pred_flatten += [reverse_los(x, los_info) / 30 for x in list(opt.cpu().detach().numpy().flatten())]\n",
    "                y_true_flatten += [reverse_los(x, los_info) / 30 for x in list(batch_y.cpu().numpy().flatten())]\n",
    "            \n",
    "\n",
    "            valid_loss = np.mean(valid_loss)\n",
    "            fold_valid_loss.append(valid_loss)\n",
    "            ret = metrics.print_metrics_regression(y_true_flatten, y_pred_flatten, verbose=0)\n",
    "            history.append(ret)\n",
    "            #print()\n",
    "\n",
    "            if each_epoch % 10 == 0:\n",
    "                print('Fold %d, epoch %d: Loss = %.4f Valid loss = %.4f MSE = %.4f' % (\n",
    "                    fold_count, each_epoch, fold_train_loss[-1], fold_valid_loss[-1], ret['mse']), flush=True)\n",
    "                logger.info('Fold %d, epoch %d: Loss = %.4f Valid loss = %.4f MSE = %.4f' % (\n",
    "                    fold_count, each_epoch, fold_train_loss[-1], fold_valid_loss[-1], ret['mse']))\n",
    "                # metrics.print_metrics_regression(y_true_flatten, y_pred_flatten)\n",
    "                \n",
    "            cur_mse = ret['mse']\n",
    "            if cur_mse < best_mse:\n",
    "                print('------------ Save FOLD-BEST model - MSE: %.4f ------------' % cur_mse, flush=True)\n",
    "                logger.info('------------ Save FOLD-BEST model - MSE: %.4f ------------' % cur_mse)\n",
    "                metrics.print_metrics_regression(y_true_flatten, y_pred_flatten)\n",
    "                best_mse = cur_mse\n",
    "                best_mad = ret['mad']\n",
    "                state = {\n",
    "                    'net': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': each_epoch\n",
    "                }\n",
    "                torch.save(state, target_file_name + '_' + str(fold_count))\n",
    "\n",
    "                if cur_mse < global_best:\n",
    "                    global_best = cur_mse\n",
    "                    state = {\n",
    "                        'net': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'epoch': each_epoch\n",
    "                    }\n",
    "                    torch.save(state, target_file_name)\n",
    "                    print('------------ Save best model - MSE: %.4f ------------' % cur_mse, flush=True)\n",
    "                    logger.info('------------ Save best model - MSE: %.4f ------------' % cur_mse)\n",
    "\n",
    "        print('Fold %d, mse = %.4f, mad = %.4f' % (fold_count, ret['mse'], ret['mad']), flush=True)\n",
    "        logger.info('Fold %d, mse = %.4f, mad = %.4f' % (fold_count, ret['mse'], ret['mad']))\n",
    "\n",
    "    mse.append(best_mse)\n",
    "    mad.append(best_mad)\n",
    "    total_train_loss.append(fold_train_loss)\n",
    "    total_valid_loss.append(fold_valid_loss)\n",
    "\n",
    "\n",
    "print('mse %.4f(%.4f)' % (np.mean(mse), np.std(mse)))\n",
    "print('mad %.4f(%.4f)' % (np.mean(mad), np.std(mad)))\n",
    "logger.info('mse %.4f(%.4f)' % (np.mean(mse), np.std(mse)))\n",
    "logger.info('mad %.4f(%.4f)' % (np.mean(mad), np.std(mad)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
