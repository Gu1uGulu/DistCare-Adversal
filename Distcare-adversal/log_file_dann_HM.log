2024-01-17 16:20:49,425 - __main__ - INFO - 这是希望输出的info内容
2024-01-17 16:20:49,426 - __main__ - WARNING - 这是希望输出的warning内容
2024-01-17 16:21:40,534 - __main__ - INFO - 32269
2024-01-17 16:21:40,534 - __main__ - INFO - 4034
2024-01-17 16:21:40,534 - __main__ - INFO - 4033
2024-01-17 16:21:48,467 - __main__ - INFO - load target data
2024-01-17 16:21:57,430 - __main__ - INFO - last saved model is in epoch 7
2024-01-17 16:21:58,321 - __main__ - INFO - Batch 0: Test Loss = 0.3133
2024-01-17 16:22:02,625 - __main__ - INFO - 
==>Predicting on test
2024-01-17 16:22:02,625 - __main__ - INFO - Test Loss = 0.4063
2024-01-17 16:22:02,765 - __main__ - INFO - Transfer Target Dataset & Model
2024-01-17 16:22:06,773 - __main__ - INFO - [[0.35672856748565834, 0.362193782095267, -0.0415507190085758, -0.39134565918228126, 0.7630646387621486, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [2.891396229598206, -0.1650564631881193, -0.7197426028629784, -0.5274138023823791, 0.7630646387621486, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.7295575734197193, 0.8894440273786534, 1.5408970099849981, -1.1397204467828197, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.7295575734197193, 0.362193782095267, 1.5408970099849981, -1.1397204467828197, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.09589065789158233, 1.4166942726620397, 0.6366411648458108, -0.9356182319826729, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [2.076681623919173, 1.4166942726620397, 0.6366411648458108, -1.7520270911832603, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.08515703225931394, 0.8894440273786534, 0.18451324227620902, -0.32331158758223233, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [1.0809193280892433, 1.4166942726620397, 0.41057720356100985, 0.9013017012186487, 1.1922324705555387, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.548509883268823, 1.943944517945426, 0.18451324227620902, 1.989846846819432, 1.0849405126071912, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.548509883268823, 1.4166942726620397, -1.6239984480021659, -1.0036523035827218, -1.1681906043081072, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.7253198692496488, 1.4166942726620397, 0.41057720356100985, 0.28899505681820825, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.17568087733476206, 1.4166942726620397, 0.18451324227620902, 0.561131343218404, 2.050568134142319, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.6390337283442711, 1.4166942726620397, -1.171870525432564, -1.0036523035827218, 0.5484807228654535, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 1.4166942726620397, -2.5282542931413534, 0.8332676296185998, -0.30985494072132685, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 0.8894440273786534, -2.5282542931413534, 0.8332676296185998, -0.30985494072132685, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.18217679879696, 1.943944517945426, -0.0415507190085758, 0.3570291284182572, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.09589065789158233, 1.4166942726620397, -0.49367864157817754, 0.4250632000183061, 0.11931289107206332, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 0.8894440273786534, -0.0415507190085758, 0.8332676296185998, 0.11931289107206332, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.17568087733476206, 1.943944517945426, 0.8627051261306116, -0.7995500887825749, 0.9776485546588437, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.9106052635706156, 1.943944517945426, -1.171870525432564, 0.15292691361811034, 1.1922324705555387, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.2769383480424786, 0.8894440273786534, -0.9458065641477793, -2.092197449183505, -1.3827745202048023, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.5442721790987524, 0.8894440273786534, 0.18451324227620902, 1.5816424172191383, -1.1681906043081072, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.2769383480424786, 1.4166942726620397, -0.9458065641477793, 0.22096098521815927, 0.44118876491710596, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-3.6263206158340595, 0.362193782095267, -1.3979344867173649, -1.8880952343833581, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.6390337283442711, 1.4166942726620397, -1.171870525432564, 0.01685877041801243, -0.8463147304630646, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.9106052635706156, 1.4166942726620397, -0.26761468029337665, 0.4930972716183551, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.0916529537215118, 0.8894440273786534, 0.41057720356100985, 1.5136083456190894, 0.22660484902041086, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-2.3589867847777857, 1.4166942726620397, 0.41057720356100985, 1.3775402024189913, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185]]
2024-01-17 16:22:06,802 - __main__ - INFO - 99
2024-01-17 16:22:06,802 - __main__ - INFO - 4255
2024-01-17 16:22:07,143 - __main__ - INFO - {'los_mean': 5.363315937659429, 'los_std': 4.099244607743503, 'los_median': 4.333333333333333, 'large_los': 21.291666666666668, 'threshold': 3.7605509886094994}
2024-01-17 16:22:09,851 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 8.7005
2024-01-17 16:23:00,184 - __main__ - INFO - Fold 1 Epoch 0 Batch 50: Train Loss = 2.3404
2024-01-17 16:23:48,486 - __main__ - INFO - Fold 1 Epoch 0 Batch 100: Train Loss = 2.9663
2024-01-17 16:24:36,872 - __main__ - INFO - Fold 1 Epoch 0 Batch 150: Train Loss = 2.0171
2024-01-17 16:25:21,925 - __main__ - INFO - Fold 1 Epoch 0 Batch 200: Train Loss = 2.0988
2024-01-17 16:26:05,989 - __main__ - INFO - Fold 1 Epoch 0 Batch 250: Train Loss = 2.2196
2024-01-17 16:26:49,327 - __main__ - INFO - Fold 1 Epoch 0 Batch 300: Train Loss = 1.5582
2024-01-17 16:28:09,380 - __main__ - INFO - Fold 1, epoch 0: Loss = 2.1934 Valid loss = 2.1958 MSE = 32.4247 AUROC = 0.8731
2024-01-17 16:28:09,381 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.4247 ------------
2024-01-17 16:28:10,059 - __main__ - INFO - ------------ Save best model - MSE: 32.4247 ------------
2024-01-17 16:28:10,062 - __main__ - INFO - Fold 1, mse = 32.4247, mad = 4.0262
2024-01-17 16:28:11,007 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 1.8678
2024-01-17 16:28:59,079 - __main__ - INFO - Fold 1 Epoch 1 Batch 50: Train Loss = 1.7375
2024-01-17 16:29:42,718 - __main__ - INFO - Fold 1 Epoch 1 Batch 100: Train Loss = 1.3250
2024-01-17 16:30:24,395 - __main__ - INFO - Fold 1 Epoch 1 Batch 150: Train Loss = 1.5767
2024-01-17 16:31:09,056 - __main__ - INFO - Fold 1 Epoch 1 Batch 200: Train Loss = 2.1442
2024-01-17 16:31:54,015 - __main__ - INFO - Fold 1 Epoch 1 Batch 250: Train Loss = 1.5476
2024-01-17 16:32:38,439 - __main__ - INFO - Fold 1 Epoch 1 Batch 300: Train Loss = 1.8564
2024-01-17 16:34:00,730 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.2017 ------------
2024-01-17 16:34:01,370 - __main__ - INFO - ------------ Save best model - MSE: 32.2017 ------------
2024-01-17 16:34:01,372 - __main__ - INFO - Fold 1, mse = 32.2017, mad = 3.8596
2024-01-17 16:34:02,334 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 1.5214
2024-01-17 16:34:50,206 - __main__ - INFO - Fold 1 Epoch 2 Batch 50: Train Loss = 1.3887
2024-01-17 16:35:34,027 - __main__ - INFO - Fold 1 Epoch 2 Batch 100: Train Loss = 1.6623
2024-01-17 16:36:19,284 - __main__ - INFO - Fold 1 Epoch 2 Batch 150: Train Loss = 1.3310
2024-01-17 16:37:03,545 - __main__ - INFO - Fold 1 Epoch 2 Batch 200: Train Loss = 1.6244
2024-01-17 16:37:48,118 - __main__ - INFO - Fold 1 Epoch 2 Batch 250: Train Loss = 1.6968
2024-01-17 16:38:32,112 - __main__ - INFO - Fold 1 Epoch 2 Batch 300: Train Loss = 1.6290
2024-01-17 16:39:52,954 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.8253 ------------
2024-01-17 16:39:53,587 - __main__ - INFO - ------------ Save best model - MSE: 31.8253 ------------
2024-01-17 16:39:53,588 - __main__ - INFO - Fold 1, mse = 31.8253, mad = 3.9102
2024-01-17 16:39:54,572 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 1.4222
2024-01-17 16:40:41,187 - __main__ - INFO - Fold 1 Epoch 3 Batch 50: Train Loss = 2.2203
2024-01-17 16:41:26,955 - __main__ - INFO - Fold 1 Epoch 3 Batch 100: Train Loss = 1.6857
2024-01-17 16:42:10,815 - __main__ - INFO - Fold 1 Epoch 3 Batch 150: Train Loss = 1.6745
2024-01-17 16:42:56,987 - __main__ - INFO - Fold 1 Epoch 3 Batch 200: Train Loss = 1.4534
2024-01-17 16:43:39,539 - __main__ - INFO - Fold 1 Epoch 3 Batch 250: Train Loss = 1.6217
2024-01-17 16:44:20,641 - __main__ - INFO - Fold 1 Epoch 3 Batch 300: Train Loss = 2.0541
2024-01-17 16:45:46,704 - __main__ - INFO - Fold 1, mse = 32.0900, mad = 3.8525
2024-01-17 16:45:47,587 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 1.5989
2024-01-17 16:46:32,345 - __main__ - INFO - Fold 1 Epoch 4 Batch 50: Train Loss = 1.3956
2024-01-17 16:47:11,224 - __main__ - INFO - Fold 1 Epoch 4 Batch 100: Train Loss = 2.1060
2024-01-17 16:47:55,363 - __main__ - INFO - Fold 1 Epoch 4 Batch 150: Train Loss = 1.4428
2024-01-17 16:48:42,660 - __main__ - INFO - Fold 1 Epoch 4 Batch 200: Train Loss = 1.5086
2024-01-17 16:49:24,144 - __main__ - INFO - Fold 1 Epoch 4 Batch 250: Train Loss = 1.2170
2024-01-17 16:50:06,997 - __main__ - INFO - Fold 1 Epoch 4 Batch 300: Train Loss = 1.6706
2024-01-17 16:51:27,703 - __main__ - INFO - Fold 1, mse = 32.3339, mad = 3.8789
2024-01-17 16:51:28,592 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 1.4096
2024-01-17 16:52:11,421 - __main__ - INFO - Fold 1 Epoch 5 Batch 50: Train Loss = 1.1482
2024-01-17 16:52:52,426 - __main__ - INFO - Fold 1 Epoch 5 Batch 100: Train Loss = 1.7105
2024-01-17 16:53:32,054 - __main__ - INFO - Fold 1 Epoch 5 Batch 150: Train Loss = 1.5622
2024-01-17 16:54:09,063 - __main__ - INFO - Fold 1 Epoch 5 Batch 200: Train Loss = 1.7306
2024-01-17 16:54:46,148 - __main__ - INFO - Fold 1 Epoch 5 Batch 250: Train Loss = 1.3275
2024-01-17 16:55:22,646 - __main__ - INFO - Fold 1 Epoch 5 Batch 300: Train Loss = 1.4576
2024-01-17 16:56:35,522 - __main__ - INFO - Fold 1, mse = 32.3635, mad = 4.0290
2024-01-17 16:56:36,412 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 1.3139
2024-01-17 16:57:13,174 - __main__ - INFO - Fold 1 Epoch 6 Batch 50: Train Loss = 1.5650
2024-01-17 16:57:51,178 - __main__ - INFO - Fold 1 Epoch 6 Batch 100: Train Loss = 1.2700
2024-01-17 16:58:29,559 - __main__ - INFO - Fold 1 Epoch 6 Batch 150: Train Loss = 1.2926
2024-01-17 16:59:07,060 - __main__ - INFO - Fold 1 Epoch 6 Batch 200: Train Loss = 1.6486
2024-01-17 16:59:45,510 - __main__ - INFO - Fold 1 Epoch 6 Batch 250: Train Loss = 1.5209
2024-01-17 17:00:23,083 - __main__ - INFO - Fold 1 Epoch 6 Batch 300: Train Loss = 1.2644
2024-01-17 17:01:37,598 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.7829 ------------
2024-01-17 17:01:38,268 - __main__ - INFO - ------------ Save best model - MSE: 31.7829 ------------
2024-01-17 17:01:38,269 - __main__ - INFO - Fold 1, mse = 31.7829, mad = 3.9109
2024-01-17 17:01:39,073 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 1.5264
2024-01-17 17:02:16,588 - __main__ - INFO - Fold 1 Epoch 7 Batch 50: Train Loss = 1.3044
2024-01-17 17:02:54,455 - __main__ - INFO - Fold 1 Epoch 7 Batch 100: Train Loss = 1.0213
2024-01-17 17:03:33,068 - __main__ - INFO - Fold 1 Epoch 7 Batch 150: Train Loss = 1.1983
2024-01-17 17:04:11,574 - __main__ - INFO - Fold 1 Epoch 7 Batch 200: Train Loss = 1.2626
2024-01-17 17:04:49,165 - __main__ - INFO - Fold 1 Epoch 7 Batch 250: Train Loss = 1.3456
2024-01-17 17:05:26,431 - __main__ - INFO - Fold 1 Epoch 7 Batch 300: Train Loss = 1.3368
2024-01-17 17:06:41,036 - __main__ - INFO - Fold 1, mse = 32.4161, mad = 3.8621
2024-01-17 17:06:41,832 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 1.0568
2024-01-17 17:07:18,904 - __main__ - INFO - Fold 1 Epoch 8 Batch 50: Train Loss = 1.0721
2024-01-17 17:07:56,789 - __main__ - INFO - Fold 1 Epoch 8 Batch 100: Train Loss = 1.2303
2024-01-17 17:08:33,761 - __main__ - INFO - Fold 1 Epoch 8 Batch 150: Train Loss = 1.4034
2024-01-17 17:09:10,404 - __main__ - INFO - Fold 1 Epoch 8 Batch 200: Train Loss = 1.1387
2024-01-17 17:09:49,698 - __main__ - INFO - Fold 1 Epoch 8 Batch 250: Train Loss = 1.5013
2024-01-17 17:10:26,502 - __main__ - INFO - Fold 1 Epoch 8 Batch 300: Train Loss = 1.4587
2024-01-17 17:11:40,061 - __main__ - INFO - Fold 1, mse = 32.9351, mad = 3.9166
2024-01-17 17:11:40,797 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 1.3479
2024-01-17 17:12:17,870 - __main__ - INFO - Fold 1 Epoch 9 Batch 50: Train Loss = 1.6948
2024-01-17 17:12:56,893 - __main__ - INFO - Fold 1 Epoch 9 Batch 100: Train Loss = 1.4461
2024-01-17 17:13:37,620 - __main__ - INFO - Fold 1 Epoch 9 Batch 150: Train Loss = 1.3008
2024-01-17 17:14:18,742 - __main__ - INFO - Fold 1 Epoch 9 Batch 200: Train Loss = 1.1171
2024-01-17 17:15:00,301 - __main__ - INFO - Fold 1 Epoch 9 Batch 250: Train Loss = 1.3999
2024-01-17 17:15:36,314 - __main__ - INFO - Fold 1 Epoch 9 Batch 300: Train Loss = 1.2163
2024-01-17 17:16:50,229 - __main__ - INFO - Fold 1, mse = 32.6437, mad = 3.8364
2024-01-17 17:16:51,009 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 1.3419
2024-01-17 17:17:27,376 - __main__ - INFO - Fold 1 Epoch 10 Batch 50: Train Loss = 1.3601
2024-01-17 17:18:04,043 - __main__ - INFO - Fold 1 Epoch 10 Batch 100: Train Loss = 1.5340
2024-01-17 17:18:39,364 - __main__ - INFO - Fold 1 Epoch 10 Batch 150: Train Loss = 1.1001
2024-01-17 17:19:15,096 - __main__ - INFO - Fold 1 Epoch 10 Batch 200: Train Loss = 1.3852
2024-01-17 17:19:51,648 - __main__ - INFO - Fold 1 Epoch 10 Batch 250: Train Loss = 1.0504
2024-01-17 17:20:29,364 - __main__ - INFO - Fold 1 Epoch 10 Batch 300: Train Loss = 1.2195
2024-01-17 17:21:44,497 - __main__ - INFO - Fold 1, epoch 10: Loss = 1.2823 Valid loss = 2.2621 MSE = 32.3741 AUROC = 0.8735
2024-01-17 17:21:44,498 - __main__ - INFO - Fold 1, mse = 32.3741, mad = 3.8597
2024-01-17 17:21:45,234 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 1.3147
2024-01-17 17:22:21,653 - __main__ - INFO - Fold 1 Epoch 11 Batch 50: Train Loss = 1.2767
2024-01-17 17:22:57,704 - __main__ - INFO - Fold 1 Epoch 11 Batch 100: Train Loss = 1.1439
2024-01-17 17:23:33,680 - __main__ - INFO - Fold 1 Epoch 11 Batch 150: Train Loss = 0.9328
2024-01-17 17:24:11,482 - __main__ - INFO - Fold 1 Epoch 11 Batch 200: Train Loss = 1.2243
2024-01-17 17:24:48,306 - __main__ - INFO - Fold 1 Epoch 11 Batch 250: Train Loss = 1.1217
2024-01-17 17:25:24,377 - __main__ - INFO - Fold 1 Epoch 11 Batch 300: Train Loss = 1.8149
2024-01-17 17:26:38,367 - __main__ - INFO - Fold 1, mse = 33.5047, mad = 3.9207
2024-01-17 17:26:39,141 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 1.0694
2024-01-17 17:27:17,679 - __main__ - INFO - Fold 1 Epoch 12 Batch 50: Train Loss = 0.9655
2024-01-17 17:27:56,144 - __main__ - INFO - Fold 1 Epoch 12 Batch 100: Train Loss = 1.0139
2024-01-17 17:28:33,901 - __main__ - INFO - Fold 1 Epoch 12 Batch 150: Train Loss = 1.2616
2024-01-17 17:29:08,848 - __main__ - INFO - Fold 1 Epoch 12 Batch 200: Train Loss = 1.0661
2024-01-17 17:29:44,824 - __main__ - INFO - Fold 1 Epoch 12 Batch 250: Train Loss = 1.0851
2024-01-17 17:30:21,804 - __main__ - INFO - Fold 1 Epoch 12 Batch 300: Train Loss = 1.0572
2024-01-17 17:31:34,380 - __main__ - INFO - Fold 1, mse = 33.0944, mad = 3.9784
2024-01-17 17:31:35,199 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 1.2932
2024-01-17 17:32:10,849 - __main__ - INFO - Fold 1 Epoch 13 Batch 50: Train Loss = 1.3385
2024-01-17 17:32:44,861 - __main__ - INFO - Fold 1 Epoch 13 Batch 100: Train Loss = 1.0359
2024-01-17 17:33:22,084 - __main__ - INFO - Fold 1 Epoch 13 Batch 150: Train Loss = 1.2898
2024-01-17 17:34:00,415 - __main__ - INFO - Fold 1 Epoch 13 Batch 200: Train Loss = 1.2492
2024-01-17 17:34:38,447 - __main__ - INFO - Fold 1 Epoch 13 Batch 250: Train Loss = 1.0615
2024-01-17 17:35:18,347 - __main__ - INFO - Fold 1 Epoch 13 Batch 300: Train Loss = 1.2413
2024-01-17 17:36:31,040 - __main__ - INFO - Fold 1, mse = 33.0131, mad = 3.9229
2024-01-17 17:36:31,831 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 1.3406
2024-01-17 17:37:09,093 - __main__ - INFO - Fold 1 Epoch 14 Batch 50: Train Loss = 1.1568
2024-01-17 17:37:47,764 - __main__ - INFO - Fold 1 Epoch 14 Batch 100: Train Loss = 1.1670
2024-01-17 17:38:25,457 - __main__ - INFO - Fold 1 Epoch 14 Batch 150: Train Loss = 1.0901
2024-01-17 17:39:05,084 - __main__ - INFO - Fold 1 Epoch 14 Batch 200: Train Loss = 1.0513
2024-01-17 17:39:44,300 - __main__ - INFO - Fold 1 Epoch 14 Batch 250: Train Loss = 1.2835
2024-01-17 17:40:22,243 - __main__ - INFO - Fold 1 Epoch 14 Batch 300: Train Loss = 1.1174
2024-01-17 17:41:37,053 - __main__ - INFO - Fold 1, mse = 33.1152, mad = 3.9369
2024-01-17 17:41:37,769 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 1.0659
2024-01-17 17:42:13,826 - __main__ - INFO - Fold 1 Epoch 15 Batch 50: Train Loss = 0.9252
2024-01-17 17:42:54,126 - __main__ - INFO - Fold 1 Epoch 15 Batch 100: Train Loss = 0.9798
2024-01-17 17:43:31,857 - __main__ - INFO - Fold 1 Epoch 15 Batch 150: Train Loss = 0.9988
2024-01-17 17:44:08,609 - __main__ - INFO - Fold 1 Epoch 15 Batch 200: Train Loss = 1.1144
2024-01-17 17:44:44,208 - __main__ - INFO - Fold 1 Epoch 15 Batch 250: Train Loss = 1.1524
2024-01-17 17:45:22,748 - __main__ - INFO - Fold 1 Epoch 15 Batch 300: Train Loss = 0.9017
2024-01-17 17:46:35,335 - __main__ - INFO - Fold 1, mse = 34.0060, mad = 3.9866
2024-01-17 17:46:36,085 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 1.1156
2024-01-17 17:47:16,147 - __main__ - INFO - Fold 1 Epoch 16 Batch 50: Train Loss = 1.3931
2024-01-17 17:47:52,953 - __main__ - INFO - Fold 1 Epoch 16 Batch 100: Train Loss = 0.7369
2024-01-17 17:48:30,772 - __main__ - INFO - Fold 1 Epoch 16 Batch 150: Train Loss = 0.7723
2024-01-17 17:49:07,030 - __main__ - INFO - Fold 1 Epoch 16 Batch 200: Train Loss = 1.0338
2024-01-17 17:49:44,751 - __main__ - INFO - Fold 1 Epoch 16 Batch 250: Train Loss = 1.1429
2024-01-17 17:50:24,188 - __main__ - INFO - Fold 1 Epoch 16 Batch 300: Train Loss = 0.9916
2024-01-17 17:51:35,864 - __main__ - INFO - Fold 1, mse = 33.7287, mad = 3.9472
2024-01-17 17:51:36,806 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 0.8800
2024-01-17 17:52:19,621 - __main__ - INFO - Fold 1 Epoch 17 Batch 50: Train Loss = 0.7773
2024-01-17 17:52:58,162 - __main__ - INFO - Fold 1 Epoch 17 Batch 100: Train Loss = 1.3070
2024-01-17 17:53:36,003 - __main__ - INFO - Fold 1 Epoch 17 Batch 150: Train Loss = 0.8971
2024-01-17 17:54:12,352 - __main__ - INFO - Fold 1 Epoch 17 Batch 200: Train Loss = 1.0630
2024-01-17 17:54:45,292 - __main__ - INFO - Fold 1 Epoch 17 Batch 250: Train Loss = 1.3030
2024-01-17 17:55:17,952 - __main__ - INFO - Fold 1 Epoch 17 Batch 300: Train Loss = 1.2060
2024-01-17 17:56:22,732 - __main__ - INFO - Fold 1, mse = 34.1614, mad = 3.9907
2024-01-17 17:56:23,391 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 0.7551
2024-01-17 17:56:58,605 - __main__ - INFO - Fold 1 Epoch 18 Batch 50: Train Loss = 0.9833
2024-01-17 17:57:34,989 - __main__ - INFO - Fold 1 Epoch 18 Batch 100: Train Loss = 1.2244
2024-01-17 17:58:08,301 - __main__ - INFO - Fold 1 Epoch 18 Batch 150: Train Loss = 0.7062
2024-01-17 17:58:41,695 - __main__ - INFO - Fold 1 Epoch 18 Batch 200: Train Loss = 1.3628
2024-01-17 17:59:13,515 - __main__ - INFO - Fold 1 Epoch 18 Batch 250: Train Loss = 1.3515
2024-01-17 17:59:46,576 - __main__ - INFO - Fold 1 Epoch 18 Batch 300: Train Loss = 0.9608
2024-01-17 18:00:52,097 - __main__ - INFO - Fold 1, mse = 34.2019, mad = 3.9836
2024-01-17 18:00:52,868 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 0.8760
2024-01-17 18:01:26,188 - __main__ - INFO - Fold 1 Epoch 19 Batch 50: Train Loss = 0.8997
2024-01-17 18:02:02,847 - __main__ - INFO - Fold 1 Epoch 19 Batch 100: Train Loss = 1.1496
2024-01-17 18:02:37,520 - __main__ - INFO - Fold 1 Epoch 19 Batch 150: Train Loss = 0.9897
2024-01-17 18:03:10,718 - __main__ - INFO - Fold 1 Epoch 19 Batch 200: Train Loss = 1.0137
2024-01-17 18:03:44,964 - __main__ - INFO - Fold 1 Epoch 19 Batch 250: Train Loss = 1.3927
2024-01-17 18:04:18,336 - __main__ - INFO - Fold 1 Epoch 19 Batch 300: Train Loss = 1.0750
2024-01-17 18:05:22,451 - __main__ - INFO - Fold 1, mse = 34.2113, mad = 4.0121
2024-01-17 18:05:26,806 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 7.1161
2024-01-17 18:05:59,672 - __main__ - INFO - Fold 2 Epoch 0 Batch 50: Train Loss = 3.8776
2024-01-17 18:06:34,565 - __main__ - INFO - Fold 2 Epoch 0 Batch 100: Train Loss = 2.2408
2024-01-17 18:07:12,346 - __main__ - INFO - Fold 2 Epoch 0 Batch 150: Train Loss = 1.6314
2024-01-17 18:07:45,718 - __main__ - INFO - Fold 2 Epoch 0 Batch 200: Train Loss = 3.9779
2024-01-17 18:08:18,813 - __main__ - INFO - Fold 2 Epoch 0 Batch 250: Train Loss = 1.5840
2024-01-17 18:08:51,855 - __main__ - INFO - Fold 2 Epoch 0 Batch 300: Train Loss = 3.2945
2024-01-17 18:09:57,519 - __main__ - INFO - Fold 2, epoch 0: Loss = 2.2749 Valid loss = 1.9252 MSE = 28.0596 AUROC = 0.8667
2024-01-17 18:09:57,520 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 28.0596 ------------
2024-01-17 18:09:57,977 - __main__ - INFO - ------------ Save best model - MSE: 28.0596 ------------
2024-01-17 18:09:57,979 - __main__ - INFO - Fold 2, mse = 28.0596, mad = 3.7796
2024-01-17 18:09:58,725 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 2.4049
2024-01-17 18:10:32,213 - __main__ - INFO - Fold 2 Epoch 1 Batch 50: Train Loss = 1.6692
2024-01-17 18:11:06,063 - __main__ - INFO - Fold 2 Epoch 1 Batch 100: Train Loss = 1.4869
2024-01-17 18:11:41,981 - __main__ - INFO - Fold 2 Epoch 1 Batch 150: Train Loss = 2.8938
2024-01-17 18:12:18,955 - __main__ - INFO - Fold 2 Epoch 1 Batch 200: Train Loss = 2.2197
2024-01-17 18:12:53,230 - __main__ - INFO - Fold 2 Epoch 1 Batch 250: Train Loss = 1.7942
2024-01-17 18:13:26,192 - __main__ - INFO - Fold 2 Epoch 1 Batch 300: Train Loss = 1.5117
2024-01-17 18:14:30,887 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.5802 ------------
2024-01-17 18:14:31,390 - __main__ - INFO - ------------ Save best model - MSE: 26.5802 ------------
2024-01-17 18:14:31,392 - __main__ - INFO - Fold 2, mse = 26.5802, mad = 3.7489
2024-01-17 18:14:32,105 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 1.7550
2024-01-17 18:15:05,294 - __main__ - INFO - Fold 2 Epoch 2 Batch 50: Train Loss = 1.9673
2024-01-17 18:15:40,075 - __main__ - INFO - Fold 2 Epoch 2 Batch 100: Train Loss = 1.5224
2024-01-17 18:16:13,560 - __main__ - INFO - Fold 2 Epoch 2 Batch 150: Train Loss = 1.5625
2024-01-17 18:16:50,348 - __main__ - INFO - Fold 2 Epoch 2 Batch 200: Train Loss = 1.8702
2024-01-17 18:17:25,261 - __main__ - INFO - Fold 2 Epoch 2 Batch 250: Train Loss = 1.3731
2024-01-17 18:17:58,184 - __main__ - INFO - Fold 2 Epoch 2 Batch 300: Train Loss = 1.6292
2024-01-17 18:19:04,497 - __main__ - INFO - Fold 2, mse = 26.7193, mad = 3.6667
2024-01-17 18:19:05,191 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 1.4927
2024-01-17 18:19:39,643 - __main__ - INFO - Fold 2 Epoch 3 Batch 50: Train Loss = 1.4681
2024-01-17 18:20:12,960 - __main__ - INFO - Fold 2 Epoch 3 Batch 100: Train Loss = 1.3862
2024-01-17 18:20:45,088 - __main__ - INFO - Fold 2 Epoch 3 Batch 150: Train Loss = 1.3970
2024-01-17 18:21:20,361 - __main__ - INFO - Fold 2 Epoch 3 Batch 200: Train Loss = 1.8136
2024-01-17 18:21:56,859 - __main__ - INFO - Fold 2 Epoch 3 Batch 250: Train Loss = 1.1984
2024-01-17 18:22:31,042 - __main__ - INFO - Fold 2 Epoch 3 Batch 300: Train Loss = 2.0695
2024-01-17 18:23:36,156 - __main__ - INFO - Fold 2, mse = 26.7238, mad = 3.6172
2024-01-17 18:23:36,901 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 2.1485
2024-01-17 18:24:09,711 - __main__ - INFO - Fold 2 Epoch 4 Batch 50: Train Loss = 1.5951
2024-01-17 18:24:42,371 - __main__ - INFO - Fold 2 Epoch 4 Batch 100: Train Loss = 1.2157
2024-01-17 18:25:15,007 - __main__ - INFO - Fold 2 Epoch 4 Batch 150: Train Loss = 1.9292
2024-01-17 18:25:49,596 - __main__ - INFO - Fold 2 Epoch 4 Batch 200: Train Loss = 1.6016
2024-01-17 18:26:25,051 - __main__ - INFO - Fold 2 Epoch 4 Batch 250: Train Loss = 1.5915
2024-01-17 18:27:01,063 - __main__ - INFO - Fold 2 Epoch 4 Batch 300: Train Loss = 1.7906
2024-01-17 18:28:04,373 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.4149 ------------
2024-01-17 18:28:04,770 - __main__ - INFO - ------------ Save best model - MSE: 26.4149 ------------
2024-01-17 18:28:04,772 - __main__ - INFO - Fold 2, mse = 26.4149, mad = 3.7038
2024-01-17 18:28:05,391 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 1.3698
2024-01-17 18:28:38,590 - __main__ - INFO - Fold 2 Epoch 5 Batch 50: Train Loss = 2.0560
2024-01-17 18:29:11,426 - __main__ - INFO - Fold 2 Epoch 5 Batch 100: Train Loss = 1.5257
2024-01-17 18:29:44,626 - __main__ - INFO - Fold 2 Epoch 5 Batch 150: Train Loss = 1.5291
2024-01-17 18:30:19,824 - __main__ - INFO - Fold 2 Epoch 5 Batch 200: Train Loss = 1.3072
2024-01-17 18:30:53,367 - __main__ - INFO - Fold 2 Epoch 5 Batch 250: Train Loss = 1.4852
2024-01-17 18:31:29,569 - __main__ - INFO - Fold 2 Epoch 5 Batch 300: Train Loss = 1.5157
2024-01-17 18:32:36,204 - __main__ - INFO - Fold 2, mse = 26.5284, mad = 3.7083
2024-01-17 18:32:36,853 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 1.1148
2024-01-17 18:33:11,388 - __main__ - INFO - Fold 2 Epoch 6 Batch 50: Train Loss = 2.1534
2024-01-17 18:33:45,109 - __main__ - INFO - Fold 2 Epoch 6 Batch 100: Train Loss = 1.3628
2024-01-17 18:34:19,313 - __main__ - INFO - Fold 2 Epoch 6 Batch 150: Train Loss = 1.3840
2024-01-17 18:34:53,359 - __main__ - INFO - Fold 2 Epoch 6 Batch 200: Train Loss = 1.1662
2024-01-17 18:35:27,308 - __main__ - INFO - Fold 2 Epoch 6 Batch 250: Train Loss = 1.5426
2024-01-17 18:35:59,779 - __main__ - INFO - Fold 2 Epoch 6 Batch 300: Train Loss = 2.2869
2024-01-17 18:37:10,077 - __main__ - INFO - Fold 2, mse = 27.0967, mad = 3.6757
2024-01-17 18:37:10,837 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 1.5744
2024-01-17 18:37:44,568 - __main__ - INFO - Fold 2 Epoch 7 Batch 50: Train Loss = 1.1484
2024-01-17 18:38:18,317 - __main__ - INFO - Fold 2 Epoch 7 Batch 100: Train Loss = 1.1563
2024-01-17 18:38:53,552 - __main__ - INFO - Fold 2 Epoch 7 Batch 150: Train Loss = 1.2597
2024-01-17 18:39:27,171 - __main__ - INFO - Fold 2 Epoch 7 Batch 200: Train Loss = 1.2792
2024-01-17 18:40:00,925 - __main__ - INFO - Fold 2 Epoch 7 Batch 250: Train Loss = 1.3134
2024-01-17 18:40:34,190 - __main__ - INFO - Fold 2 Epoch 7 Batch 300: Train Loss = 1.3965
2024-01-17 18:41:42,809 - __main__ - INFO - Fold 2, mse = 27.7599, mad = 3.6528
2024-01-17 18:41:43,618 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 1.4218
2024-01-17 18:42:18,082 - __main__ - INFO - Fold 2 Epoch 8 Batch 50: Train Loss = 1.4551
2024-01-17 18:42:52,183 - __main__ - INFO - Fold 2 Epoch 8 Batch 100: Train Loss = 1.4150
2024-01-17 18:43:26,351 - __main__ - INFO - Fold 2 Epoch 8 Batch 150: Train Loss = 1.5055
2024-01-17 18:43:59,741 - __main__ - INFO - Fold 2 Epoch 8 Batch 200: Train Loss = 1.8487
2024-01-17 18:44:34,535 - __main__ - INFO - Fold 2 Epoch 8 Batch 250: Train Loss = 2.6425
2024-01-17 18:45:09,573 - __main__ - INFO - Fold 2 Epoch 8 Batch 300: Train Loss = 1.2142
2024-01-17 18:46:16,536 - __main__ - INFO - Fold 2, mse = 27.5396, mad = 3.6865
2024-01-17 18:46:17,254 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 1.1791
2024-01-17 18:46:54,796 - __main__ - INFO - Fold 2 Epoch 9 Batch 50: Train Loss = 1.3159
2024-01-17 18:47:27,882 - __main__ - INFO - Fold 2 Epoch 9 Batch 100: Train Loss = 0.9833
2024-01-17 18:48:02,203 - __main__ - INFO - Fold 2 Epoch 9 Batch 150: Train Loss = 1.1954
2024-01-17 18:48:36,953 - __main__ - INFO - Fold 2 Epoch 9 Batch 200: Train Loss = 1.4946
2024-01-17 18:49:10,528 - __main__ - INFO - Fold 2 Epoch 9 Batch 250: Train Loss = 1.2164
2024-01-17 18:49:44,000 - __main__ - INFO - Fold 2 Epoch 9 Batch 300: Train Loss = 1.4028
2024-01-17 18:50:50,139 - __main__ - INFO - Fold 2, mse = 28.7464, mad = 3.7712
2024-01-17 18:50:50,856 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 1.4356
2024-01-17 18:51:28,284 - __main__ - INFO - Fold 2 Epoch 10 Batch 50: Train Loss = 1.3414
2024-01-17 18:52:04,291 - __main__ - INFO - Fold 2 Epoch 10 Batch 100: Train Loss = 1.0774
2024-01-17 18:52:37,893 - __main__ - INFO - Fold 2 Epoch 10 Batch 150: Train Loss = 1.7475
2024-01-17 18:53:11,049 - __main__ - INFO - Fold 2 Epoch 10 Batch 200: Train Loss = 1.0610
2024-01-17 18:53:44,037 - __main__ - INFO - Fold 2 Epoch 10 Batch 250: Train Loss = 1.2753
2024-01-17 18:54:16,603 - __main__ - INFO - Fold 2 Epoch 10 Batch 300: Train Loss = 0.9776
2024-01-17 18:55:21,844 - __main__ - INFO - Fold 2, epoch 10: Loss = 1.3393 Valid loss = 2.0504 MSE = 28.8552 AUROC = 0.8684
2024-01-17 18:55:21,845 - __main__ - INFO - Fold 2, mse = 28.8552, mad = 3.7463
2024-01-17 18:55:22,512 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 2.0225
2024-01-17 18:55:57,407 - __main__ - INFO - Fold 2 Epoch 11 Batch 50: Train Loss = 0.8783
2024-01-17 18:56:36,146 - __main__ - INFO - Fold 2 Epoch 11 Batch 100: Train Loss = 1.3261
2024-01-17 18:57:09,821 - __main__ - INFO - Fold 2 Epoch 11 Batch 150: Train Loss = 1.2967
2024-01-17 18:57:42,936 - __main__ - INFO - Fold 2 Epoch 11 Batch 200: Train Loss = 1.5681
2024-01-17 18:58:16,783 - __main__ - INFO - Fold 2 Epoch 11 Batch 250: Train Loss = 0.8412
2024-01-17 18:58:50,065 - __main__ - INFO - Fold 2 Epoch 11 Batch 300: Train Loss = 0.9815
2024-01-17 18:59:55,188 - __main__ - INFO - Fold 2, mse = 28.5555, mad = 3.7199
2024-01-17 18:59:55,924 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 1.1138
2024-01-17 19:00:29,726 - __main__ - INFO - Fold 2 Epoch 12 Batch 50: Train Loss = 1.1236
2024-01-17 19:01:06,790 - __main__ - INFO - Fold 2 Epoch 12 Batch 100: Train Loss = 1.2551
2024-01-17 19:01:43,189 - __main__ - INFO - Fold 2 Epoch 12 Batch 150: Train Loss = 1.2628
2024-01-17 19:02:17,305 - __main__ - INFO - Fold 2 Epoch 12 Batch 200: Train Loss = 0.9276
2024-01-17 19:02:50,063 - __main__ - INFO - Fold 2 Epoch 12 Batch 250: Train Loss = 0.8643
2024-01-17 19:03:23,972 - __main__ - INFO - Fold 2 Epoch 12 Batch 300: Train Loss = 1.0664
2024-01-17 19:04:29,960 - __main__ - INFO - Fold 2, mse = 27.9579, mad = 3.7451
2024-01-17 19:04:30,618 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 0.9291
2024-01-17 19:05:03,467 - __main__ - INFO - Fold 2 Epoch 13 Batch 50: Train Loss = 1.1411
2024-01-17 19:05:35,691 - __main__ - INFO - Fold 2 Epoch 13 Batch 100: Train Loss = 1.1488
2024-01-17 19:06:11,213 - __main__ - INFO - Fold 2 Epoch 13 Batch 150: Train Loss = 1.7653
2024-01-17 19:06:47,317 - __main__ - INFO - Fold 2 Epoch 13 Batch 200: Train Loss = 1.2726
2024-01-17 19:07:21,956 - __main__ - INFO - Fold 2 Epoch 13 Batch 250: Train Loss = 0.8687
2024-01-17 19:07:58,012 - __main__ - INFO - Fold 2 Epoch 13 Batch 300: Train Loss = 0.8999
2024-01-17 19:09:16,125 - __main__ - INFO - Fold 2, mse = 29.6917, mad = 3.7854
2024-01-17 19:09:16,903 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 0.9371
2024-01-17 19:09:51,802 - __main__ - INFO - Fold 2 Epoch 14 Batch 50: Train Loss = 1.1101
2024-01-17 19:10:26,284 - __main__ - INFO - Fold 2 Epoch 14 Batch 100: Train Loss = 1.0650
2024-01-17 19:11:10,543 - __main__ - INFO - Fold 2 Epoch 14 Batch 150: Train Loss = 1.0866
2024-01-17 19:11:54,841 - __main__ - INFO - Fold 2 Epoch 14 Batch 200: Train Loss = 1.0133
2024-01-17 19:12:31,423 - __main__ - INFO - Fold 2 Epoch 14 Batch 250: Train Loss = 1.1952
2024-01-17 19:13:06,793 - __main__ - INFO - Fold 2 Epoch 14 Batch 300: Train Loss = 0.9776
2024-01-17 19:14:29,352 - __main__ - INFO - Fold 2, mse = 28.5778, mad = 3.7380
2024-01-17 19:14:30,188 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 1.2106
2024-01-17 19:15:04,311 - __main__ - INFO - Fold 2 Epoch 15 Batch 50: Train Loss = 0.9336
2024-01-17 19:15:37,443 - __main__ - INFO - Fold 2 Epoch 15 Batch 100: Train Loss = 0.9406
2024-01-17 19:16:09,289 - __main__ - INFO - Fold 2 Epoch 15 Batch 150: Train Loss = 1.0127
2024-01-17 19:16:45,321 - __main__ - INFO - Fold 2 Epoch 15 Batch 200: Train Loss = 1.2571
2024-01-17 19:17:20,949 - __main__ - INFO - Fold 2 Epoch 15 Batch 250: Train Loss = 1.1584
2024-01-17 19:17:54,845 - __main__ - INFO - Fold 2 Epoch 15 Batch 300: Train Loss = 1.2049
2024-01-17 19:18:58,944 - __main__ - INFO - Fold 2, mse = 29.2725, mad = 3.7554
2024-01-17 19:18:59,575 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 1.0354
2024-01-17 19:19:32,648 - __main__ - INFO - Fold 2 Epoch 16 Batch 50: Train Loss = 0.8712
2024-01-17 19:20:06,751 - __main__ - INFO - Fold 2 Epoch 16 Batch 100: Train Loss = 0.8253
2024-01-17 19:20:39,749 - __main__ - INFO - Fold 2 Epoch 16 Batch 150: Train Loss = 1.2159
2024-01-17 19:21:24,624 - __main__ - INFO - Fold 2 Epoch 16 Batch 200: Train Loss = 1.7104
2024-01-17 19:22:14,443 - __main__ - INFO - Fold 2 Epoch 16 Batch 250: Train Loss = 0.9527
2024-01-17 19:22:51,252 - __main__ - INFO - Fold 2 Epoch 16 Batch 300: Train Loss = 0.9011
2024-01-17 19:24:05,836 - __main__ - INFO - Fold 2, mse = 28.9577, mad = 3.7740
2024-01-17 19:24:06,644 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 1.4162
2024-01-17 19:24:48,120 - __main__ - INFO - Fold 2 Epoch 17 Batch 50: Train Loss = 1.1740
2024-01-17 19:25:25,805 - __main__ - INFO - Fold 2 Epoch 17 Batch 100: Train Loss = 0.8973
2024-01-17 19:26:03,464 - __main__ - INFO - Fold 2 Epoch 17 Batch 150: Train Loss = 1.0490
2024-01-17 19:26:44,328 - __main__ - INFO - Fold 2 Epoch 17 Batch 200: Train Loss = 0.9460
2024-01-17 19:27:24,825 - __main__ - INFO - Fold 2 Epoch 17 Batch 250: Train Loss = 0.6747
2024-01-17 19:27:59,509 - __main__ - INFO - Fold 2 Epoch 17 Batch 300: Train Loss = 0.8842
2024-01-17 19:29:06,506 - __main__ - INFO - Fold 2, mse = 31.2103, mad = 3.9584
2024-01-17 19:29:07,191 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 1.3347
2024-01-17 19:29:41,456 - __main__ - INFO - Fold 2 Epoch 18 Batch 50: Train Loss = 1.0018
2024-01-17 19:30:15,162 - __main__ - INFO - Fold 2 Epoch 18 Batch 100: Train Loss = 1.0948
2024-01-17 19:30:47,147 - __main__ - INFO - Fold 2 Epoch 18 Batch 150: Train Loss = 0.9401
2024-01-17 19:31:19,801 - __main__ - INFO - Fold 2 Epoch 18 Batch 200: Train Loss = 1.1872
2024-01-17 19:31:53,877 - __main__ - INFO - Fold 2 Epoch 18 Batch 250: Train Loss = 1.0262
2024-01-17 19:32:30,417 - __main__ - INFO - Fold 2 Epoch 18 Batch 300: Train Loss = 1.3153
2024-01-17 19:33:36,966 - __main__ - INFO - Fold 2, mse = 29.3283, mad = 3.8101
2024-01-17 19:33:37,684 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 0.9586
2024-01-17 19:34:11,269 - __main__ - INFO - Fold 2 Epoch 19 Batch 50: Train Loss = 0.9123
2024-01-17 19:34:45,106 - __main__ - INFO - Fold 2 Epoch 19 Batch 100: Train Loss = 1.4714
2024-01-17 19:35:18,735 - __main__ - INFO - Fold 2 Epoch 19 Batch 150: Train Loss = 0.9988
2024-01-17 19:35:51,169 - __main__ - INFO - Fold 2 Epoch 19 Batch 200: Train Loss = 1.0300
2024-01-17 19:36:23,934 - __main__ - INFO - Fold 2 Epoch 19 Batch 250: Train Loss = 1.3275
2024-01-17 19:36:59,180 - __main__ - INFO - Fold 2 Epoch 19 Batch 300: Train Loss = 1.0706
2024-01-17 19:38:09,750 - __main__ - INFO - Fold 2, mse = 30.1249, mad = 3.7956
2024-01-17 19:38:10,724 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 3.1907
2024-01-17 19:38:44,253 - __main__ - INFO - Fold 3 Epoch 0 Batch 50: Train Loss = 2.6688
2024-01-17 19:39:17,238 - __main__ - INFO - Fold 3 Epoch 0 Batch 100: Train Loss = 1.8375
2024-01-17 19:39:50,881 - __main__ - INFO - Fold 3 Epoch 0 Batch 150: Train Loss = 2.1909
2024-01-17 19:40:24,773 - __main__ - INFO - Fold 3 Epoch 0 Batch 200: Train Loss = 2.0939
2024-01-17 19:40:58,598 - __main__ - INFO - Fold 3 Epoch 0 Batch 250: Train Loss = 1.8355
2024-01-17 19:41:32,230 - __main__ - INFO - Fold 3 Epoch 0 Batch 300: Train Loss = 2.1071
2024-01-17 19:42:42,528 - __main__ - INFO - Fold 3, epoch 0: Loss = 2.2220 Valid loss = 1.9509 MSE = 28.6347 AUROC = 0.8796
2024-01-17 19:42:42,529 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 28.6347 ------------
2024-01-17 19:42:42,800 - __main__ - INFO - Fold 3, mse = 28.6347, mad = 3.8012
2024-01-17 19:42:43,404 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 2.1421
2024-01-17 19:43:17,180 - __main__ - INFO - Fold 3 Epoch 1 Batch 50: Train Loss = 2.3421
2024-01-17 19:43:52,446 - __main__ - INFO - Fold 3 Epoch 1 Batch 100: Train Loss = 2.0924
2024-01-17 19:44:26,834 - __main__ - INFO - Fold 3 Epoch 1 Batch 150: Train Loss = 1.8828
2024-01-17 19:45:00,418 - __main__ - INFO - Fold 3 Epoch 1 Batch 200: Train Loss = 2.0094
2024-01-17 19:45:33,383 - __main__ - INFO - Fold 3 Epoch 1 Batch 250: Train Loss = 1.8860
2024-01-17 19:46:06,499 - __main__ - INFO - Fold 3 Epoch 1 Batch 300: Train Loss = 1.4536
2024-01-17 19:47:15,139 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.1125 ------------
2024-01-17 19:47:15,525 - __main__ - INFO - Fold 3, mse = 27.1125, mad = 3.7156
2024-01-17 19:47:16,298 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 1.8781
2024-01-17 19:47:52,247 - __main__ - INFO - Fold 3 Epoch 2 Batch 50: Train Loss = 1.5997
2024-01-17 19:48:25,382 - __main__ - INFO - Fold 3 Epoch 2 Batch 100: Train Loss = 1.4759
2024-01-17 19:48:59,494 - __main__ - INFO - Fold 3 Epoch 2 Batch 150: Train Loss = 1.7825
2024-01-17 19:49:32,789 - __main__ - INFO - Fold 3 Epoch 2 Batch 200: Train Loss = 2.2233
2024-01-17 19:50:05,780 - __main__ - INFO - Fold 3 Epoch 2 Batch 250: Train Loss = 1.5442
2024-01-17 19:50:40,211 - __main__ - INFO - Fold 3 Epoch 2 Batch 300: Train Loss = 1.7042
2024-01-17 19:51:46,442 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.7091 ------------
2024-01-17 19:51:46,837 - __main__ - INFO - Fold 3, mse = 26.7091, mad = 3.6861
2024-01-17 19:51:47,585 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 1.6072
2024-01-17 19:52:25,353 - __main__ - INFO - Fold 3 Epoch 3 Batch 50: Train Loss = 1.1758
2024-01-17 19:53:01,471 - __main__ - INFO - Fold 3 Epoch 3 Batch 100: Train Loss = 2.1422
2024-01-17 19:53:42,049 - __main__ - INFO - Fold 3 Epoch 3 Batch 150: Train Loss = 1.2133
2024-01-17 19:54:21,859 - __main__ - INFO - Fold 3 Epoch 3 Batch 200: Train Loss = 1.6986
2024-01-17 19:54:56,981 - __main__ - INFO - Fold 3 Epoch 3 Batch 250: Train Loss = 1.7283
2024-01-17 19:55:32,310 - __main__ - INFO - Fold 3 Epoch 3 Batch 300: Train Loss = 1.3471
2024-01-17 19:56:53,609 - __main__ - INFO - Fold 3, mse = 27.0145, mad = 3.6523
2024-01-17 19:56:54,425 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 1.2774
2024-01-17 19:57:33,226 - __main__ - INFO - Fold 3 Epoch 4 Batch 50: Train Loss = 1.1628
2024-01-17 19:58:10,039 - __main__ - INFO - Fold 3 Epoch 4 Batch 100: Train Loss = 1.7487
2024-01-17 19:58:44,304 - __main__ - INFO - Fold 3 Epoch 4 Batch 150: Train Loss = 1.4761
2024-01-17 19:59:18,775 - __main__ - INFO - Fold 3 Epoch 4 Batch 200: Train Loss = 1.3237
2024-01-17 19:59:54,743 - __main__ - INFO - Fold 3 Epoch 4 Batch 250: Train Loss = 2.0001
2024-01-17 20:00:29,183 - __main__ - INFO - Fold 3 Epoch 4 Batch 300: Train Loss = 1.3998
2024-01-17 20:01:49,505 - __main__ - INFO - Fold 3, mse = 26.8644, mad = 3.7430
2024-01-17 20:01:50,304 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 2.3702
2024-01-17 20:02:28,659 - __main__ - INFO - Fold 3 Epoch 5 Batch 50: Train Loss = 1.2322
2024-01-17 20:03:07,342 - __main__ - INFO - Fold 3 Epoch 5 Batch 100: Train Loss = 1.5564
2024-01-17 20:03:41,999 - __main__ - INFO - Fold 3 Epoch 5 Batch 150: Train Loss = 1.4106
2024-01-17 20:04:16,372 - __main__ - INFO - Fold 3 Epoch 5 Batch 200: Train Loss = 1.3649
2024-01-17 20:04:48,220 - __main__ - INFO - Fold 3 Epoch 5 Batch 250: Train Loss = 1.7379
2024-01-17 20:05:20,976 - __main__ - INFO - Fold 3 Epoch 5 Batch 300: Train Loss = 1.2617
2024-01-17 20:06:20,550 - __main__ - INFO - Fold 3, mse = 26.9143, mad = 3.6495
2024-01-17 20:06:21,254 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 1.3056
2024-01-17 20:06:52,839 - __main__ - INFO - Fold 3 Epoch 6 Batch 50: Train Loss = 1.1462
2024-01-17 20:07:25,530 - __main__ - INFO - Fold 3 Epoch 6 Batch 100: Train Loss = 1.3448
2024-01-17 20:08:00,159 - __main__ - INFO - Fold 3 Epoch 6 Batch 150: Train Loss = 1.2358
2024-01-17 20:08:31,819 - __main__ - INFO - Fold 3 Epoch 6 Batch 200: Train Loss = 1.1957
2024-01-17 20:09:04,072 - __main__ - INFO - Fold 3 Epoch 6 Batch 250: Train Loss = 1.5228
2024-01-17 20:09:35,782 - __main__ - INFO - Fold 3 Epoch 6 Batch 300: Train Loss = 1.0768
2024-01-17 20:10:37,116 - __main__ - INFO - Fold 3, mse = 27.0087, mad = 3.6085
2024-01-17 20:10:37,755 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 3.0040
2024-01-17 20:11:09,524 - __main__ - INFO - Fold 3 Epoch 7 Batch 50: Train Loss = 1.2001
2024-01-17 20:11:40,764 - __main__ - INFO - Fold 3 Epoch 7 Batch 100: Train Loss = 1.7084
2024-01-17 20:12:13,669 - __main__ - INFO - Fold 3 Epoch 7 Batch 150: Train Loss = 1.2413
2024-01-17 20:12:48,928 - __main__ - INFO - Fold 3 Epoch 7 Batch 200: Train Loss = 1.1211
2024-01-17 20:13:20,427 - __main__ - INFO - Fold 3 Epoch 7 Batch 250: Train Loss = 1.1635
2024-01-17 20:13:51,612 - __main__ - INFO - Fold 3 Epoch 7 Batch 300: Train Loss = 1.6444
2024-01-17 20:14:52,440 - __main__ - INFO - Fold 3, mse = 27.3411, mad = 3.7246
2024-01-17 20:14:53,059 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 1.4082
2024-01-17 20:15:25,199 - __main__ - INFO - Fold 3 Epoch 8 Batch 50: Train Loss = 1.2155
2024-01-17 20:15:57,000 - __main__ - INFO - Fold 3 Epoch 8 Batch 100: Train Loss = 1.5327
2024-01-17 20:16:28,511 - __main__ - INFO - Fold 3 Epoch 8 Batch 150: Train Loss = 1.6758
2024-01-17 20:17:02,433 - __main__ - INFO - Fold 3 Epoch 8 Batch 200: Train Loss = 1.5763
2024-01-17 20:17:35,860 - __main__ - INFO - Fold 3 Epoch 8 Batch 250: Train Loss = 2.7252
2024-01-17 20:18:07,015 - __main__ - INFO - Fold 3 Epoch 8 Batch 300: Train Loss = 1.0882
2024-01-17 20:19:06,375 - __main__ - INFO - Fold 3, mse = 27.3055, mad = 3.7775
2024-01-17 20:19:07,056 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 1.2460
2024-01-17 20:19:37,064 - __main__ - INFO - Fold 3 Epoch 9 Batch 50: Train Loss = 1.2030
2024-01-17 20:20:09,501 - __main__ - INFO - Fold 3 Epoch 9 Batch 100: Train Loss = 1.0272
2024-01-17 20:20:43,154 - __main__ - INFO - Fold 3 Epoch 9 Batch 150: Train Loss = 1.8312
2024-01-17 20:21:18,317 - __main__ - INFO - Fold 3 Epoch 9 Batch 200: Train Loss = 1.4282
2024-01-17 20:21:56,145 - __main__ - INFO - Fold 3 Epoch 9 Batch 250: Train Loss = 1.2339
2024-01-17 20:22:28,413 - __main__ - INFO - Fold 3 Epoch 9 Batch 300: Train Loss = 1.3826
2024-01-17 20:23:35,655 - __main__ - INFO - Fold 3, mse = 27.2720, mad = 3.6865
2024-01-17 20:23:36,372 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 1.2997
2024-01-17 20:24:09,395 - __main__ - INFO - Fold 3 Epoch 10 Batch 50: Train Loss = 1.2851
2024-01-17 20:24:42,718 - __main__ - INFO - Fold 3 Epoch 10 Batch 100: Train Loss = 1.1341
2024-01-17 20:25:17,602 - __main__ - INFO - Fold 3 Epoch 10 Batch 150: Train Loss = 1.0616
2024-01-17 20:25:55,192 - __main__ - INFO - Fold 3 Epoch 10 Batch 200: Train Loss = 1.0591
2024-01-17 20:26:29,358 - __main__ - INFO - Fold 3 Epoch 10 Batch 250: Train Loss = 1.1506
2024-01-17 20:27:05,177 - __main__ - INFO - Fold 3 Epoch 10 Batch 300: Train Loss = 2.1171
2024-01-17 20:28:06,246 - __main__ - INFO - Fold 3, epoch 10: Loss = 1.3648 Valid loss = 1.9633 MSE = 27.5470 AUROC = 0.8803
2024-01-17 20:28:06,248 - __main__ - INFO - Fold 3, mse = 27.5470, mad = 3.6988
2024-01-17 20:28:06,913 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 1.6983
2024-01-17 20:28:38,256 - __main__ - INFO - Fold 3 Epoch 11 Batch 50: Train Loss = 1.1716
2024-01-17 20:29:09,964 - __main__ - INFO - Fold 3 Epoch 11 Batch 100: Train Loss = 1.0304
2024-01-17 20:29:41,727 - __main__ - INFO - Fold 3 Epoch 11 Batch 150: Train Loss = 1.0380
2024-01-17 20:30:13,510 - __main__ - INFO - Fold 3 Epoch 11 Batch 200: Train Loss = 1.2630
2024-01-17 20:30:45,871 - __main__ - INFO - Fold 3 Epoch 11 Batch 250: Train Loss = 2.3662
2024-01-17 20:31:19,205 - __main__ - INFO - Fold 3 Epoch 11 Batch 300: Train Loss = 1.2875
2024-01-17 20:32:20,896 - __main__ - INFO - Fold 3, mse = 28.0939, mad = 3.7185
2024-01-17 20:32:21,564 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 0.7587
2024-01-17 20:32:53,822 - __main__ - INFO - Fold 3 Epoch 12 Batch 50: Train Loss = 1.1483
2024-01-17 20:33:25,721 - __main__ - INFO - Fold 3 Epoch 12 Batch 100: Train Loss = 1.1170
2024-01-17 20:33:59,966 - __main__ - INFO - Fold 3 Epoch 12 Batch 150: Train Loss = 1.0554
2024-01-17 20:34:31,011 - __main__ - INFO - Fold 3 Epoch 12 Batch 200: Train Loss = 1.4397
2024-01-17 20:35:02,038 - __main__ - INFO - Fold 3 Epoch 12 Batch 250: Train Loss = 1.4499
2024-01-17 20:35:40,763 - __main__ - INFO - Fold 3 Epoch 12 Batch 300: Train Loss = 1.0654
2024-01-17 20:36:54,873 - __main__ - INFO - Fold 3, mse = 27.9585, mad = 3.7556
2024-01-17 20:36:55,494 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 1.3978
2024-01-17 20:37:27,102 - __main__ - INFO - Fold 3 Epoch 13 Batch 50: Train Loss = 1.4511
2024-01-17 20:38:04,172 - __main__ - INFO - Fold 3 Epoch 13 Batch 100: Train Loss = 1.5147
2024-01-17 20:38:38,454 - __main__ - INFO - Fold 3 Epoch 13 Batch 150: Train Loss = 0.8976
2024-01-17 20:39:11,267 - __main__ - INFO - Fold 3 Epoch 13 Batch 200: Train Loss = 0.9753
2024-01-17 20:39:44,197 - __main__ - INFO - Fold 3 Epoch 13 Batch 250: Train Loss = 1.1966
2024-01-17 20:40:15,698 - __main__ - INFO - Fold 3 Epoch 13 Batch 300: Train Loss = 1.1828
2024-01-17 20:41:28,533 - __main__ - INFO - Fold 3, mse = 28.0353, mad = 3.7752
2024-01-17 20:41:29,151 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 0.9371
2024-01-17 20:42:02,118 - __main__ - INFO - Fold 3 Epoch 14 Batch 50: Train Loss = 1.1612
2024-01-17 20:42:35,090 - __main__ - INFO - Fold 3 Epoch 14 Batch 100: Train Loss = 0.7664
2024-01-17 20:43:04,873 - __main__ - INFO - Fold 3 Epoch 14 Batch 150: Train Loss = 1.2507
2024-01-17 20:43:36,088 - __main__ - INFO - Fold 3 Epoch 14 Batch 200: Train Loss = 1.1538
2024-01-17 20:44:08,509 - __main__ - INFO - Fold 3 Epoch 14 Batch 250: Train Loss = 1.0706
2024-01-17 20:44:39,919 - __main__ - INFO - Fold 3 Epoch 14 Batch 300: Train Loss = 0.8447
2024-01-17 20:45:40,631 - __main__ - INFO - Fold 3, mse = 28.2197, mad = 3.7644
2024-01-17 20:45:41,323 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 1.0412
2024-01-17 20:46:15,009 - __main__ - INFO - Fold 3 Epoch 15 Batch 50: Train Loss = 1.2700
2024-01-17 20:46:45,291 - __main__ - INFO - Fold 3 Epoch 15 Batch 100: Train Loss = 0.9692
2024-01-17 20:47:15,674 - __main__ - INFO - Fold 3 Epoch 15 Batch 150: Train Loss = 2.5635
2024-01-17 20:47:47,660 - __main__ - INFO - Fold 3 Epoch 15 Batch 200: Train Loss = 1.1539
2024-01-17 20:48:20,121 - __main__ - INFO - Fold 3 Epoch 15 Batch 250: Train Loss = 1.1344
2024-01-17 20:48:51,493 - __main__ - INFO - Fold 3 Epoch 15 Batch 300: Train Loss = 1.2461
2024-01-17 20:49:51,057 - __main__ - INFO - Fold 3, mse = 28.2373, mad = 3.7573
2024-01-17 20:49:51,689 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 1.1142
2024-01-17 20:50:25,440 - __main__ - INFO - Fold 3 Epoch 16 Batch 50: Train Loss = 0.9532
2024-01-17 20:50:58,577 - __main__ - INFO - Fold 3 Epoch 16 Batch 100: Train Loss = 1.0884
2024-01-17 20:51:30,954 - __main__ - INFO - Fold 3 Epoch 16 Batch 150: Train Loss = 1.3999
2024-01-17 20:52:04,958 - __main__ - INFO - Fold 3 Epoch 16 Batch 200: Train Loss = 1.2036
2024-01-17 20:52:39,182 - __main__ - INFO - Fold 3 Epoch 16 Batch 250: Train Loss = 0.9612
2024-01-17 20:53:11,915 - __main__ - INFO - Fold 3 Epoch 16 Batch 300: Train Loss = 1.0710
2024-01-17 20:54:11,165 - __main__ - INFO - Fold 3, mse = 28.4604, mad = 3.7786
2024-01-17 20:54:11,782 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 1.5739
2024-01-17 20:54:43,849 - __main__ - INFO - Fold 3 Epoch 17 Batch 50: Train Loss = 0.9981
2024-01-17 20:55:18,807 - __main__ - INFO - Fold 3 Epoch 17 Batch 100: Train Loss = 0.9885
2024-01-17 20:55:51,079 - __main__ - INFO - Fold 3 Epoch 17 Batch 150: Train Loss = 0.8097
2024-01-17 20:56:23,119 - __main__ - INFO - Fold 3 Epoch 17 Batch 200: Train Loss = 0.8698
2024-01-17 20:56:56,128 - __main__ - INFO - Fold 3 Epoch 17 Batch 250: Train Loss = 1.0677
2024-01-17 20:57:27,009 - __main__ - INFO - Fold 3 Epoch 17 Batch 300: Train Loss = 0.9102
2024-01-17 20:58:25,360 - __main__ - INFO - Fold 3, mse = 28.8135, mad = 3.8474
2024-01-17 20:58:25,904 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 1.1483
2024-01-17 20:58:57,943 - __main__ - INFO - Fold 3 Epoch 18 Batch 50: Train Loss = 0.8489
2024-01-17 20:59:31,657 - __main__ - INFO - Fold 3 Epoch 18 Batch 100: Train Loss = 1.1099
2024-01-17 21:00:06,375 - __main__ - INFO - Fold 3 Epoch 18 Batch 150: Train Loss = 1.0010
2024-01-17 21:00:38,992 - __main__ - INFO - Fold 3 Epoch 18 Batch 200: Train Loss = 0.9703
2024-01-17 21:01:11,592 - __main__ - INFO - Fold 3 Epoch 18 Batch 250: Train Loss = 1.2873
2024-01-17 21:01:42,740 - __main__ - INFO - Fold 3 Epoch 18 Batch 300: Train Loss = 1.6533
2024-01-17 21:02:40,795 - __main__ - INFO - Fold 3, mse = 28.8946, mad = 3.7698
2024-01-17 21:02:41,421 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 0.8372
2024-01-17 21:03:11,682 - __main__ - INFO - Fold 3 Epoch 19 Batch 50: Train Loss = 1.2197
2024-01-17 21:03:45,282 - __main__ - INFO - Fold 3 Epoch 19 Batch 100: Train Loss = 1.0353
2024-01-17 21:04:18,803 - __main__ - INFO - Fold 3 Epoch 19 Batch 150: Train Loss = 0.9826
2024-01-17 21:04:52,322 - __main__ - INFO - Fold 3 Epoch 19 Batch 200: Train Loss = 1.0721
2024-01-17 21:05:22,892 - __main__ - INFO - Fold 3 Epoch 19 Batch 250: Train Loss = 0.9808
2024-01-17 21:05:53,624 - __main__ - INFO - Fold 3 Epoch 19 Batch 300: Train Loss = 0.8579
2024-01-17 21:06:49,496 - __main__ - INFO - Fold 3, mse = 28.9055, mad = 3.8715
2024-01-17 21:06:49,498 - __main__ - INFO - mse 28.3023(2.4641)
2024-01-17 21:06:49,498 - __main__ - INFO - mad 3.7669(0.1021)
2024-01-17 21:06:49,499 - __main__ - INFO - auroc 0.8822(0.0065)
2024-01-17 21:06:49,499 - __main__ - INFO - auprc 0.5929(0.0025)
