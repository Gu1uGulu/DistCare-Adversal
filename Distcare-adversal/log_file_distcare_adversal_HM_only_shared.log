2024-03-21 11:07:55,245 - __main__ - INFO - 这是希望输出的info内容
2024-03-21 11:07:55,245 - __main__ - WARNING - 这是希望输出的warning内容
2024-03-21 11:09:11,209 - __main__ - INFO - 32269
2024-03-21 11:09:11,210 - __main__ - INFO - 4034
2024-03-21 11:09:11,210 - __main__ - INFO - 4033
2024-03-21 11:09:37,657 - __main__ - INFO - Batch 0: Test Loss = 0.0866
2024-03-21 11:09:42,582 - __main__ - INFO - 
==>Predicting on test
2024-03-21 11:09:42,582 - __main__ - INFO - Test Loss = 0.1194
2024-03-21 11:09:42,727 - __main__ - INFO - load target data
2024-03-21 11:09:50,122 - __main__ - INFO - Batch 0: Test Loss = 0.1309
2024-03-21 11:09:57,101 - __main__ - INFO - Batch 20: Test Loss = 0.1464
2024-03-21 11:10:03,461 - __main__ - INFO - Batch 40: Test Loss = 0.1320
2024-03-21 11:10:09,835 - __main__ - INFO - Batch 60: Test Loss = 0.1008
2024-03-21 11:10:16,669 - __main__ - INFO - Batch 80: Test Loss = 0.0734
2024-03-21 11:10:23,604 - __main__ - INFO - Batch 100: Test Loss = 0.0563
2024-03-21 11:10:30,375 - __main__ - INFO - Batch 120: Test Loss = 0.0909
2024-03-21 11:10:32,625 - __main__ - INFO - last saved model is in epoch 0
2024-03-21 11:10:33,262 - __main__ - INFO - Batch 0: Test Loss = 0.3345
2024-03-21 11:10:38,493 - __main__ - INFO - 
==>Predicting on test
2024-03-21 11:10:38,494 - __main__ - INFO - Test Loss = 0.2608
2024-03-21 11:33:51,092 - __main__ - INFO - Transfer Target Dataset & Model
2024-03-21 11:33:56,709 - __main__ - INFO - [[0.35672856748565834, 0.362193782095267, -0.0415507190085758, -0.39134565918228126, 0.7630646387621486, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [2.891396229598206, -0.1650564631881193, -0.7197426028629784, -0.5274138023823791, 0.7630646387621486, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.7295575734197193, 0.8894440273786534, 1.5408970099849981, -1.1397204467828197, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.7295575734197193, 0.362193782095267, 1.5408970099849981, -1.1397204467828197, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.09589065789158233, 1.4166942726620397, 0.6366411648458108, -0.9356182319826729, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [2.076681623919173, 1.4166942726620397, 0.6366411648458108, -1.7520270911832603, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.08515703225931394, 0.8894440273786534, 0.18451324227620902, -0.32331158758223233, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [1.0809193280892433, 1.4166942726620397, 0.41057720356100985, 0.9013017012186487, 1.1922324705555387, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.548509883268823, 1.943944517945426, 0.18451324227620902, 1.989846846819432, 1.0849405126071912, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.548509883268823, 1.4166942726620397, -1.6239984480021659, -1.0036523035827218, -1.1681906043081072, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.7253198692496488, 1.4166942726620397, 0.41057720356100985, 0.28899505681820825, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.17568087733476206, 1.4166942726620397, 0.18451324227620902, 0.561131343218404, 2.050568134142319, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.6390337283442711, 1.4166942726620397, -1.171870525432564, -1.0036523035827218, 0.5484807228654535, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 1.4166942726620397, -2.5282542931413534, 0.8332676296185998, -0.30985494072132685, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 0.8894440273786534, -2.5282542931413534, 0.8332676296185998, -0.30985494072132685, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.18217679879696, 1.943944517945426, -0.0415507190085758, 0.3570291284182572, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.09589065789158233, 1.4166942726620397, -0.49367864157817754, 0.4250632000183061, 0.11931289107206332, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 0.8894440273786534, -0.0415507190085758, 0.8332676296185998, 0.11931289107206332, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.17568087733476206, 1.943944517945426, 0.8627051261306116, -0.7995500887825749, 0.9776485546588437, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.9106052635706156, 1.943944517945426, -1.171870525432564, 0.15292691361811034, 1.1922324705555387, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.2769383480424786, 0.8894440273786534, -0.9458065641477793, -2.092197449183505, -1.3827745202048023, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.5442721790987524, 0.8894440273786534, 0.18451324227620902, 1.5816424172191383, -1.1681906043081072, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.2769383480424786, 1.4166942726620397, -0.9458065641477793, 0.22096098521815927, 0.44118876491710596, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-3.6263206158340595, 0.362193782095267, -1.3979344867173649, -1.8880952343833581, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.6390337283442711, 1.4166942726620397, -1.171870525432564, 0.01685877041801243, -0.8463147304630646, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.9106052635706156, 1.4166942726620397, -0.26761468029337665, 0.4930972716183551, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.0916529537215118, 0.8894440273786534, 0.41057720356100985, 1.5136083456190894, 0.22660484902041086, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-2.3589867847777857, 1.4166942726620397, 0.41057720356100985, 1.3775402024189913, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185]]
2024-03-21 11:33:56,712 - __main__ - INFO - 99
2024-03-21 11:33:56,712 - __main__ - INFO - 4255
2024-03-21 11:33:57,720 - __main__ - INFO - {'los_mean': 5.363315937659429, 'los_std': 4.099244607743503, 'los_median': 4.333333333333333, 'large_los': 21.291666666666668, 'threshold': 3.7605509886094994}
2024-03-21 11:34:00,859 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 2746.8147
2024-03-21 11:34:54,047 - __main__ - INFO - Fold 1 Epoch 0 Batch 50: Train Loss = 3.3132
2024-03-21 11:35:47,099 - __main__ - INFO - Fold 1 Epoch 0 Batch 100: Train Loss = 2.1902
2024-03-21 11:36:31,322 - __main__ - INFO - Fold 1 Epoch 0 Batch 150: Train Loss = 3.1137
2024-03-21 11:37:16,988 - __main__ - INFO - Fold 1 Epoch 0 Batch 200: Train Loss = 2.1779
2024-03-21 11:38:03,465 - __main__ - INFO - Fold 1 Epoch 0 Batch 250: Train Loss = 2.5505
2024-03-21 11:38:52,546 - __main__ - INFO - Fold 1 Epoch 0 Batch 300: Train Loss = 2.5438
2024-03-21 11:40:24,978 - __main__ - INFO - Fold 1, epoch 0: Loss = 2.3893 Valid loss = 2.5722 MSE = 37.4847 AUROC = 0.7205
2024-03-21 11:40:24,979 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.4847 ------------
2024-03-21 11:40:25,918 - __main__ - INFO - ------------ Save best model - MSE: 37.4847 ------------
2024-03-21 11:40:25,920 - __main__ - INFO - Fold 1, mse = 37.4847, mad = 4.3014
2024-03-21 11:40:27,050 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 2.7116
2024-03-21 11:41:20,376 - __main__ - INFO - Fold 1 Epoch 1 Batch 50: Train Loss = 2.7947
2024-03-21 11:42:11,766 - __main__ - INFO - Fold 1 Epoch 1 Batch 100: Train Loss = 2.5723
2024-03-21 11:43:01,784 - __main__ - INFO - Fold 1 Epoch 1 Batch 150: Train Loss = 1.7464
2024-03-21 11:43:51,250 - __main__ - INFO - Fold 1 Epoch 1 Batch 200: Train Loss = 1.9561
2024-03-21 11:44:40,441 - __main__ - INFO - Fold 1 Epoch 1 Batch 250: Train Loss = 1.6030
2024-03-21 11:45:31,067 - __main__ - INFO - Fold 1 Epoch 1 Batch 300: Train Loss = 2.0694
2024-03-21 11:47:01,982 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 36.0794 ------------
2024-03-21 11:47:02,854 - __main__ - INFO - ------------ Save best model - MSE: 36.0794 ------------
2024-03-21 11:47:02,855 - __main__ - INFO - Fold 1, mse = 36.0794, mad = 4.2531
2024-03-21 11:47:03,705 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 1.8303
2024-03-21 11:47:55,476 - __main__ - INFO - Fold 1 Epoch 2 Batch 50: Train Loss = 2.9396
2024-03-21 11:48:47,333 - __main__ - INFO - Fold 1 Epoch 2 Batch 100: Train Loss = 2.4170
2024-03-21 11:49:36,172 - __main__ - INFO - Fold 1 Epoch 2 Batch 150: Train Loss = 2.4238
2024-03-21 11:50:25,547 - __main__ - INFO - Fold 1 Epoch 2 Batch 200: Train Loss = 2.3663
2024-03-21 11:51:14,483 - __main__ - INFO - Fold 1 Epoch 2 Batch 250: Train Loss = 2.5461
2024-03-21 11:52:05,385 - __main__ - INFO - Fold 1 Epoch 2 Batch 300: Train Loss = 2.2102
2024-03-21 11:53:33,473 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.6596 ------------
2024-03-21 11:53:34,400 - __main__ - INFO - ------------ Save best model - MSE: 35.6596 ------------
2024-03-21 11:53:34,402 - __main__ - INFO - Fold 1, mse = 35.6596, mad = 4.0392
2024-03-21 11:53:35,476 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 1.7505
2024-03-21 11:54:26,136 - __main__ - INFO - Fold 1 Epoch 3 Batch 50: Train Loss = 2.1014
2024-03-21 11:55:16,423 - __main__ - INFO - Fold 1 Epoch 3 Batch 100: Train Loss = 2.1002
2024-03-21 11:56:06,620 - __main__ - INFO - Fold 1 Epoch 3 Batch 150: Train Loss = 2.0651
2024-03-21 11:56:54,507 - __main__ - INFO - Fold 1 Epoch 3 Batch 200: Train Loss = 1.9332
2024-03-21 11:57:43,105 - __main__ - INFO - Fold 1 Epoch 3 Batch 250: Train Loss = 2.4695
2024-03-21 11:58:30,877 - __main__ - INFO - Fold 1 Epoch 3 Batch 300: Train Loss = 2.6228
2024-03-21 11:59:58,194 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.6933 ------------
2024-03-21 11:59:59,122 - __main__ - INFO - ------------ Save best model - MSE: 33.6933 ------------
2024-03-21 11:59:59,124 - __main__ - INFO - Fold 1, mse = 33.6933, mad = 4.1383
2024-03-21 12:00:00,050 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 2.1208
2024-03-21 12:00:50,187 - __main__ - INFO - Fold 1 Epoch 4 Batch 50: Train Loss = 1.9935
2024-03-21 12:01:39,217 - __main__ - INFO - Fold 1 Epoch 4 Batch 100: Train Loss = 2.5288
2024-03-21 12:02:26,791 - __main__ - INFO - Fold 1 Epoch 4 Batch 150: Train Loss = 2.1520
2024-03-21 12:03:16,759 - __main__ - INFO - Fold 1 Epoch 4 Batch 200: Train Loss = 1.8668
2024-03-21 12:04:06,971 - __main__ - INFO - Fold 1 Epoch 4 Batch 250: Train Loss = 1.3623
2024-03-21 12:04:57,628 - __main__ - INFO - Fold 1 Epoch 4 Batch 300: Train Loss = 1.9278
2024-03-21 12:06:33,905 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.6393 ------------
2024-03-21 12:06:35,012 - __main__ - INFO - ------------ Save best model - MSE: 32.6393 ------------
2024-03-21 12:06:35,014 - __main__ - INFO - Fold 1, mse = 32.6393, mad = 3.9094
2024-03-21 12:06:36,095 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 1.9673
2024-03-21 12:07:33,121 - __main__ - INFO - Fold 1 Epoch 5 Batch 50: Train Loss = 1.8360
2024-03-21 12:08:28,887 - __main__ - INFO - Fold 1 Epoch 5 Batch 100: Train Loss = 2.0605
2024-03-21 12:09:27,457 - __main__ - INFO - Fold 1 Epoch 5 Batch 150: Train Loss = 1.9888
2024-03-21 12:10:23,511 - __main__ - INFO - Fold 1 Epoch 5 Batch 200: Train Loss = 2.4218
2024-03-21 12:11:19,922 - __main__ - INFO - Fold 1 Epoch 5 Batch 250: Train Loss = 2.2729
2024-03-21 12:12:16,840 - __main__ - INFO - Fold 1 Epoch 5 Batch 300: Train Loss = 1.6669
2024-03-21 12:13:55,673 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.1312 ------------
2024-03-21 12:13:56,705 - __main__ - INFO - ------------ Save best model - MSE: 32.1312 ------------
2024-03-21 12:13:56,706 - __main__ - INFO - Fold 1, mse = 32.1312, mad = 3.9456
2024-03-21 12:13:57,789 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 1.9721
2024-03-21 12:14:54,001 - __main__ - INFO - Fold 1 Epoch 6 Batch 50: Train Loss = 1.7202
2024-03-21 12:15:51,609 - __main__ - INFO - Fold 1 Epoch 6 Batch 100: Train Loss = 2.4786
2024-03-21 12:16:49,779 - __main__ - INFO - Fold 1 Epoch 6 Batch 150: Train Loss = 1.9198
2024-03-21 12:17:43,458 - __main__ - INFO - Fold 1 Epoch 6 Batch 200: Train Loss = 2.3223
2024-03-21 12:18:38,737 - __main__ - INFO - Fold 1 Epoch 6 Batch 250: Train Loss = 1.7565
2024-03-21 11:46:52,523 - __main__ - INFO - Fold 1 Epoch 6 Batch 300: Train Loss = 1.6834
2024-03-21 11:48:22,146 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.1057 ------------
2024-03-21 11:48:22,952 - __main__ - INFO - ------------ Save best model - MSE: 32.1057 ------------
2024-03-21 11:48:22,954 - __main__ - INFO - Fold 1, mse = 32.1057, mad = 3.9426
2024-03-21 11:48:23,801 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 1.7096
2024-03-21 11:49:13,816 - __main__ - INFO - Fold 1 Epoch 7 Batch 50: Train Loss = 2.0073
2024-03-21 11:50:03,849 - __main__ - INFO - Fold 1 Epoch 7 Batch 100: Train Loss = 2.1079
2024-03-21 11:50:52,002 - __main__ - INFO - Fold 1 Epoch 7 Batch 150: Train Loss = 1.7194
2024-03-21 11:51:42,531 - __main__ - INFO - Fold 1 Epoch 7 Batch 200: Train Loss = 2.1624
2024-03-21 11:52:32,629 - __main__ - INFO - Fold 1 Epoch 7 Batch 250: Train Loss = 1.9022
2024-03-21 11:53:24,525 - __main__ - INFO - Fold 1 Epoch 7 Batch 300: Train Loss = 2.0477
2024-03-21 11:54:51,964 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.6670 ------------
2024-03-21 11:54:52,894 - __main__ - INFO - ------------ Save best model - MSE: 31.6670 ------------
2024-03-21 11:54:52,896 - __main__ - INFO - Fold 1, mse = 31.6670, mad = 3.9733
2024-03-21 11:54:53,839 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 1.3127
2024-03-21 11:55:44,027 - __main__ - INFO - Fold 1 Epoch 8 Batch 50: Train Loss = 1.8746
2024-03-21 11:56:32,454 - __main__ - INFO - Fold 1 Epoch 8 Batch 100: Train Loss = 1.3792
2024-03-21 11:57:21,474 - __main__ - INFO - Fold 1 Epoch 8 Batch 150: Train Loss = 1.8831
2024-03-21 11:58:09,205 - __main__ - INFO - Fold 1 Epoch 8 Batch 200: Train Loss = 1.8245
2024-03-21 11:58:58,049 - __main__ - INFO - Fold 1 Epoch 8 Batch 250: Train Loss = 2.0103
2024-03-21 11:59:50,301 - __main__ - INFO - Fold 1 Epoch 8 Batch 300: Train Loss = 1.8443
2024-03-21 12:01:22,687 - __main__ - INFO - Fold 1, mse = 32.0488, mad = 3.8316
2024-03-21 12:01:23,658 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 1.4490
2024-03-21 12:02:14,267 - __main__ - INFO - Fold 1 Epoch 9 Batch 50: Train Loss = 1.8438
2024-03-21 12:03:03,781 - __main__ - INFO - Fold 1 Epoch 9 Batch 100: Train Loss = 1.8202
2024-03-21 12:03:55,093 - __main__ - INFO - Fold 1 Epoch 9 Batch 150: Train Loss = 2.0367
2024-03-21 12:04:47,318 - __main__ - INFO - Fold 1 Epoch 9 Batch 200: Train Loss = 1.8957
2024-03-21 12:05:38,119 - __main__ - INFO - Fold 1 Epoch 9 Batch 250: Train Loss = 2.3060
2024-03-21 12:06:33,695 - __main__ - INFO - Fold 1 Epoch 9 Batch 300: Train Loss = 2.0811
2024-03-21 12:08:12,329 - __main__ - INFO - Fold 1, mse = 31.8782, mad = 3.9599
2024-03-21 12:08:13,399 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 1.9420
2024-03-21 12:09:12,860 - __main__ - INFO - Fold 1 Epoch 10 Batch 50: Train Loss = 1.9470
2024-03-21 12:10:10,869 - __main__ - INFO - Fold 1 Epoch 10 Batch 100: Train Loss = 2.6968
2024-03-21 12:11:05,564 - __main__ - INFO - Fold 1 Epoch 10 Batch 150: Train Loss = 2.0204
2024-03-21 12:12:01,889 - __main__ - INFO - Fold 1 Epoch 10 Batch 200: Train Loss = 1.7149
2024-03-21 12:12:57,394 - __main__ - INFO - Fold 1 Epoch 10 Batch 250: Train Loss = 1.5832
2024-03-21 12:13:54,694 - __main__ - INFO - Fold 1 Epoch 10 Batch 300: Train Loss = 1.6458
2024-03-21 12:15:30,816 - __main__ - INFO - Fold 1, epoch 10: Loss = 1.8681 Valid loss = 2.1531 MSE = 31.7000 AUROC = 0.8611
2024-03-21 12:15:30,817 - __main__ - INFO - Fold 1, mse = 31.7000, mad = 3.9045
2024-03-21 12:15:32,020 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 1.6561
2024-03-21 12:16:30,528 - __main__ - INFO - Fold 1 Epoch 11 Batch 50: Train Loss = 1.7249
2024-03-21 12:17:25,212 - __main__ - INFO - Fold 1 Epoch 11 Batch 100: Train Loss = 1.6924
2024-03-21 12:18:18,149 - __main__ - INFO - Fold 1 Epoch 11 Batch 150: Train Loss = 1.8022
2024-03-21 12:19:14,265 - __main__ - INFO - Fold 1 Epoch 11 Batch 200: Train Loss = 1.7482
2024-03-21 12:19:58,588 - __main__ - INFO - Fold 1 Epoch 11 Batch 250: Train Loss = 1.6968
2024-03-21 12:20:41,219 - __main__ - INFO - Fold 1 Epoch 11 Batch 300: Train Loss = 1.8085
2024-03-21 12:21:58,636 - __main__ - INFO - Fold 1, mse = 31.6904, mad = 3.9711
2024-03-21 12:21:59,441 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 1.6761
2024-03-21 12:22:42,719 - __main__ - INFO - Fold 1 Epoch 12 Batch 50: Train Loss = 1.3612
2024-03-21 12:23:23,286 - __main__ - INFO - Fold 1 Epoch 12 Batch 100: Train Loss = 1.9131
2024-03-21 12:24:03,589 - __main__ - INFO - Fold 1 Epoch 12 Batch 150: Train Loss = 1.2961
2024-03-21 12:24:43,850 - __main__ - INFO - Fold 1 Epoch 12 Batch 200: Train Loss = 1.9710
2024-03-21 12:25:23,347 - __main__ - INFO - Fold 1 Epoch 12 Batch 250: Train Loss = 2.1064
2024-03-21 12:26:00,820 - __main__ - INFO - Fold 1 Epoch 12 Batch 300: Train Loss = 2.2481
2024-03-21 12:27:04,042 - __main__ - INFO - Fold 1, mse = 32.6069, mad = 3.8103
2024-03-21 12:27:04,829 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 2.4825
2024-03-21 12:27:43,424 - __main__ - INFO - Fold 1 Epoch 13 Batch 50: Train Loss = 1.7654
2024-03-21 12:28:19,979 - __main__ - INFO - Fold 1 Epoch 13 Batch 100: Train Loss = 1.5243
2024-03-21 12:28:56,537 - __main__ - INFO - Fold 1 Epoch 13 Batch 150: Train Loss = 1.2473
2024-03-21 12:29:33,259 - __main__ - INFO - Fold 1 Epoch 13 Batch 200: Train Loss = 1.5987
2024-03-21 12:30:10,423 - __main__ - INFO - Fold 1 Epoch 13 Batch 250: Train Loss = 1.8628
2024-03-21 12:30:47,682 - __main__ - INFO - Fold 1 Epoch 13 Batch 300: Train Loss = 1.8571
2024-03-21 12:31:57,198 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.3384 ------------
2024-03-21 12:31:57,926 - __main__ - INFO - ------------ Save best model - MSE: 31.3384 ------------
2024-03-21 12:31:57,928 - __main__ - INFO - Fold 1, mse = 31.3384, mad = 3.8210
2024-03-21 12:31:58,756 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 1.6565
2024-03-21 12:32:36,468 - __main__ - INFO - Fold 1 Epoch 14 Batch 50: Train Loss = 1.7547
2024-03-21 12:33:13,355 - __main__ - INFO - Fold 1 Epoch 14 Batch 100: Train Loss = 2.0872
2024-03-21 12:33:49,862 - __main__ - INFO - Fold 1 Epoch 14 Batch 150: Train Loss = 1.6046
2024-03-21 12:34:27,906 - __main__ - INFO - Fold 1 Epoch 14 Batch 200: Train Loss = 1.6113
2024-03-21 12:35:05,137 - __main__ - INFO - Fold 1 Epoch 14 Batch 250: Train Loss = 2.1123
2024-03-21 12:35:41,993 - __main__ - INFO - Fold 1 Epoch 14 Batch 300: Train Loss = 1.4690
2024-03-21 12:36:51,291 - __main__ - INFO - Fold 1, mse = 31.5240, mad = 3.8282
2024-03-21 12:36:51,902 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 1.8014
2024-03-21 12:37:27,496 - __main__ - INFO - Fold 1 Epoch 15 Batch 50: Train Loss = 1.3475
2024-03-21 12:38:05,398 - __main__ - INFO - Fold 1 Epoch 15 Batch 100: Train Loss = 1.7567
2024-03-21 12:38:41,429 - __main__ - INFO - Fold 1 Epoch 15 Batch 150: Train Loss = 1.3954
2024-03-21 12:39:17,947 - __main__ - INFO - Fold 1 Epoch 15 Batch 200: Train Loss = 1.3136
2024-03-21 12:39:53,762 - __main__ - INFO - Fold 1 Epoch 15 Batch 250: Train Loss = 1.9531
2024-03-21 12:40:31,458 - __main__ - INFO - Fold 1 Epoch 15 Batch 300: Train Loss = 1.4952
2024-03-21 12:41:39,889 - __main__ - INFO - Fold 1, mse = 31.6031, mad = 3.9224
2024-03-21 12:41:40,491 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 2.3145
2024-03-21 12:42:17,048 - __main__ - INFO - Fold 1 Epoch 16 Batch 50: Train Loss = 1.1847
2024-03-21 12:42:54,014 - __main__ - INFO - Fold 1 Epoch 16 Batch 100: Train Loss = 2.1824
2024-03-21 12:43:30,767 - __main__ - INFO - Fold 1 Epoch 16 Batch 150: Train Loss = 1.9486
2024-03-21 12:44:07,989 - __main__ - INFO - Fold 1 Epoch 16 Batch 200: Train Loss = 1.6790
2024-03-21 12:44:43,644 - __main__ - INFO - Fold 1 Epoch 16 Batch 250: Train Loss = 1.6777
2024-03-21 12:45:21,521 - __main__ - INFO - Fold 1 Epoch 16 Batch 300: Train Loss = 1.2343
2024-03-21 12:46:26,197 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.1398 ------------
2024-03-21 12:46:26,891 - __main__ - INFO - ------------ Save best model - MSE: 31.1398 ------------
2024-03-21 12:46:26,893 - __main__ - INFO - Fold 1, mse = 31.1398, mad = 3.8052
2024-03-21 12:46:27,757 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 1.5679
2024-03-21 12:47:05,475 - __main__ - INFO - Fold 1 Epoch 17 Batch 50: Train Loss = 1.1694
2024-03-21 12:47:41,728 - __main__ - INFO - Fold 1 Epoch 17 Batch 100: Train Loss = 1.5905
2024-03-21 12:48:17,634 - __main__ - INFO - Fold 1 Epoch 17 Batch 150: Train Loss = 1.3814
2024-03-21 12:48:53,427 - __main__ - INFO - Fold 1 Epoch 17 Batch 200: Train Loss = 1.5540
2024-03-21 12:49:29,916 - __main__ - INFO - Fold 1 Epoch 17 Batch 250: Train Loss = 1.3314
2024-03-21 12:50:08,151 - __main__ - INFO - Fold 1 Epoch 17 Batch 300: Train Loss = 1.6003
2024-03-21 12:51:11,086 - __main__ - INFO - Fold 1, mse = 31.2277, mad = 3.8047
2024-03-21 12:51:11,775 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 1.6303
2024-03-21 12:51:48,850 - __main__ - INFO - Fold 1 Epoch 18 Batch 50: Train Loss = 1.8235
2024-03-21 12:52:25,517 - __main__ - INFO - Fold 1 Epoch 18 Batch 100: Train Loss = 1.5656
2024-03-21 12:53:02,141 - __main__ - INFO - Fold 1 Epoch 18 Batch 150: Train Loss = 1.5859
2024-03-21 12:53:38,567 - __main__ - INFO - Fold 1 Epoch 18 Batch 200: Train Loss = 1.1949
2024-03-21 12:54:16,742 - __main__ - INFO - Fold 1 Epoch 18 Batch 250: Train Loss = 1.3569
2024-03-21 12:54:54,417 - __main__ - INFO - Fold 1 Epoch 18 Batch 300: Train Loss = 1.6483
2024-03-21 12:56:03,065 - __main__ - INFO - Fold 1, mse = 31.3277, mad = 3.8477
2024-03-21 12:56:03,820 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 1.6126
2024-03-21 12:56:40,160 - __main__ - INFO - Fold 1 Epoch 19 Batch 50: Train Loss = 1.5633
2024-03-21 12:57:16,706 - __main__ - INFO - Fold 1 Epoch 19 Batch 100: Train Loss = 1.5212
2024-03-21 12:57:53,765 - __main__ - INFO - Fold 1 Epoch 19 Batch 150: Train Loss = 1.1511
2024-03-21 12:58:30,466 - __main__ - INFO - Fold 1 Epoch 19 Batch 200: Train Loss = 1.7487
2024-03-21 12:59:09,002 - __main__ - INFO - Fold 1 Epoch 19 Batch 250: Train Loss = 1.5759
2024-03-21 12:59:45,454 - __main__ - INFO - Fold 1 Epoch 19 Batch 300: Train Loss = 1.8340
2024-03-21 13:00:51,050 - __main__ - INFO - Fold 1, mse = 31.1601, mad = 3.7793
2024-03-21 13:01:05,669 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 59871.6250
2024-03-21 13:01:42,802 - __main__ - INFO - Fold 2 Epoch 0 Batch 50: Train Loss = 3.1870
2024-03-21 13:02:18,829 - __main__ - INFO - Fold 2 Epoch 0 Batch 100: Train Loss = 2.3934
2024-03-21 13:02:55,910 - __main__ - INFO - Fold 2 Epoch 0 Batch 150: Train Loss = 2.6463
2024-03-21 13:03:33,924 - __main__ - INFO - Fold 2 Epoch 0 Batch 200: Train Loss = 2.0936
2024-03-21 13:04:10,276 - __main__ - INFO - Fold 2 Epoch 0 Batch 250: Train Loss = 3.0583
2024-03-21 13:04:45,244 - __main__ - INFO - Fold 2 Epoch 0 Batch 300: Train Loss = 2.4523
2024-03-21 13:05:47,467 - __main__ - INFO - Fold 2, epoch 0: Loss = 2.5314 Valid loss = 2.3277 MSE = 33.7552 AUROC = 0.7770
2024-03-21 13:05:47,468 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 33.7552 ------------
2024-03-21 13:05:47,815 - __main__ - INFO - Fold 2, mse = 33.7552, mad = 4.2414
2024-03-21 13:05:48,551 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 2.5224
2024-03-21 13:06:24,555 - __main__ - INFO - Fold 2 Epoch 1 Batch 50: Train Loss = 3.5028
2024-03-21 13:07:00,947 - __main__ - INFO - Fold 2 Epoch 1 Batch 100: Train Loss = 1.8958
2024-03-21 13:07:36,181 - __main__ - INFO - Fold 2 Epoch 1 Batch 150: Train Loss = 2.2420
2024-03-21 13:08:14,269 - __main__ - INFO - Fold 2 Epoch 1 Batch 200: Train Loss = 2.4199
2024-03-21 13:08:54,577 - __main__ - INFO - Fold 2 Epoch 1 Batch 250: Train Loss = 3.3632
2024-03-21 13:09:30,250 - __main__ - INFO - Fold 2 Epoch 1 Batch 300: Train Loss = 2.3338
2024-03-21 13:10:34,586 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 31.5877 ------------
2024-03-21 13:10:35,088 - __main__ - INFO - Fold 2, mse = 31.5877, mad = 4.0924
2024-03-21 13:10:35,790 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 1.8501
2024-03-21 13:11:10,728 - __main__ - INFO - Fold 2 Epoch 2 Batch 50: Train Loss = 2.5764
2024-03-21 13:11:47,497 - __main__ - INFO - Fold 2 Epoch 2 Batch 100: Train Loss = 2.4014
2024-03-21 13:12:25,024 - __main__ - INFO - Fold 2 Epoch 2 Batch 150: Train Loss = 1.9814
2024-03-21 13:13:01,968 - __main__ - INFO - Fold 2 Epoch 2 Batch 200: Train Loss = 1.8834
2024-03-21 13:13:39,815 - __main__ - INFO - Fold 2 Epoch 2 Batch 250: Train Loss = 1.5482
2024-03-21 13:14:16,257 - __main__ - INFO - Fold 2 Epoch 2 Batch 300: Train Loss = 1.9936
2024-03-21 13:15:19,335 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.7144 ------------
2024-03-21 13:15:19,949 - __main__ - INFO - ------------ Save best model - MSE: 30.7144 ------------
2024-03-21 13:15:19,951 - __main__ - INFO - Fold 2, mse = 30.7144, mad = 3.8557
2024-03-21 13:15:20,541 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 1.7996
2024-03-21 13:15:49,823 - __main__ - INFO - Fold 2 Epoch 3 Batch 50: Train Loss = 2.2077
2024-03-21 13:16:21,811 - __main__ - INFO - Fold 2 Epoch 3 Batch 100: Train Loss = 2.8178
2024-03-21 13:16:58,318 - __main__ - INFO - Fold 2 Epoch 3 Batch 150: Train Loss = 2.0982
2024-03-21 13:17:29,641 - __main__ - INFO - Fold 2 Epoch 3 Batch 200: Train Loss = 2.5019
2024-03-21 13:18:00,256 - __main__ - INFO - Fold 2 Epoch 3 Batch 250: Train Loss = 2.1617
2024-03-21 13:18:32,073 - __main__ - INFO - Fold 2 Epoch 3 Batch 300: Train Loss = 1.9159
2024-03-21 13:19:26,169 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 28.8528 ------------
2024-03-21 13:19:26,748 - __main__ - INFO - ------------ Save best model - MSE: 28.8528 ------------
2024-03-21 13:19:26,750 - __main__ - INFO - Fold 2, mse = 28.8528, mad = 3.8971
2024-03-21 13:19:27,344 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 2.0400
2024-03-21 13:19:57,638 - __main__ - INFO - Fold 2 Epoch 4 Batch 50: Train Loss = 2.6179
2024-03-21 13:20:27,612 - __main__ - INFO - Fold 2 Epoch 4 Batch 100: Train Loss = 1.7303
2024-03-21 13:21:00,533 - __main__ - INFO - Fold 2 Epoch 4 Batch 150: Train Loss = 2.5793
2024-03-21 13:21:32,633 - __main__ - INFO - Fold 2 Epoch 4 Batch 200: Train Loss = 2.6123
2024-03-21 13:22:04,107 - __main__ - INFO - Fold 2 Epoch 4 Batch 250: Train Loss = 1.9622
2024-03-21 13:22:37,223 - __main__ - INFO - Fold 2 Epoch 4 Batch 300: Train Loss = 1.8005
2024-03-21 13:23:33,915 - __main__ - INFO - Fold 2, mse = 29.6897, mad = 3.7192
2024-03-21 13:23:34,608 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 2.7372
2024-03-21 13:24:04,521 - __main__ - INFO - Fold 2 Epoch 5 Batch 50: Train Loss = 2.0591
2024-03-21 13:24:36,083 - __main__ - INFO - Fold 2 Epoch 5 Batch 100: Train Loss = 1.9777
2024-03-21 13:25:12,231 - __main__ - INFO - Fold 2 Epoch 5 Batch 150: Train Loss = 2.3213
2024-03-21 13:25:42,457 - __main__ - INFO - Fold 2 Epoch 5 Batch 200: Train Loss = 2.1363
2024-03-21 13:26:14,549 - __main__ - INFO - Fold 2 Epoch 5 Batch 250: Train Loss = 1.8523
2024-03-21 13:26:44,837 - __main__ - INFO - Fold 2 Epoch 5 Batch 300: Train Loss = 1.4957
2024-03-21 13:27:42,249 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.8523 ------------
2024-03-21 13:27:42,867 - __main__ - INFO - ------------ Save best model - MSE: 27.8523 ------------
2024-03-21 13:27:42,869 - __main__ - INFO - Fold 2, mse = 27.8523, mad = 3.7647
2024-03-21 13:27:43,459 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 1.6512
2024-03-21 13:28:12,592 - __main__ - INFO - Fold 2 Epoch 6 Batch 50: Train Loss = 1.9755
2024-03-21 13:28:44,676 - __main__ - INFO - Fold 2 Epoch 6 Batch 100: Train Loss = 1.3834
2024-03-21 13:29:17,062 - __main__ - INFO - Fold 2 Epoch 6 Batch 150: Train Loss = 1.7557
2024-03-21 13:29:48,451 - __main__ - INFO - Fold 2 Epoch 6 Batch 200: Train Loss = 2.2531
2024-03-21 13:30:18,855 - __main__ - INFO - Fold 2 Epoch 6 Batch 250: Train Loss = 1.9003
2024-03-21 13:30:50,312 - __main__ - INFO - Fold 2 Epoch 6 Batch 300: Train Loss = 2.1339
2024-03-21 13:31:44,738 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.7713 ------------
2024-03-21 13:31:45,297 - __main__ - INFO - ------------ Save best model - MSE: 27.7713 ------------
2024-03-21 13:31:45,298 - __main__ - INFO - Fold 2, mse = 27.7713, mad = 3.7610
2024-03-21 13:31:45,892 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 1.3271
2024-03-21 13:32:16,840 - __main__ - INFO - Fold 2 Epoch 7 Batch 50: Train Loss = 2.0635
2024-03-21 13:32:48,599 - __main__ - INFO - Fold 2 Epoch 7 Batch 100: Train Loss = 1.9032
2024-03-21 13:33:22,428 - __main__ - INFO - Fold 2 Epoch 7 Batch 150: Train Loss = 2.4342
2024-03-21 13:33:54,414 - __main__ - INFO - Fold 2 Epoch 7 Batch 200: Train Loss = 2.7442
2024-03-21 13:34:23,434 - __main__ - INFO - Fold 2 Epoch 7 Batch 250: Train Loss = 1.5659
2024-03-21 13:34:53,021 - __main__ - INFO - Fold 2 Epoch 7 Batch 300: Train Loss = 1.8859
2024-03-21 13:35:49,953 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.5948 ------------
2024-03-21 13:35:50,567 - __main__ - INFO - ------------ Save best model - MSE: 27.5948 ------------
2024-03-21 13:35:50,569 - __main__ - INFO - Fold 2, mse = 27.5948, mad = 3.7114
2024-03-21 13:35:51,186 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 1.9405
2024-03-21 13:36:21,130 - __main__ - INFO - Fold 2 Epoch 8 Batch 50: Train Loss = 1.7306
2024-03-21 13:36:50,723 - __main__ - INFO - Fold 2 Epoch 8 Batch 100: Train Loss = 2.3449
2024-03-21 13:37:22,520 - __main__ - INFO - Fold 2 Epoch 8 Batch 150: Train Loss = 1.9948
2024-03-21 13:37:53,389 - __main__ - INFO - Fold 2 Epoch 8 Batch 200: Train Loss = 1.7890
2024-03-21 13:38:23,797 - __main__ - INFO - Fold 2 Epoch 8 Batch 250: Train Loss = 1.9241
2024-03-21 13:38:58,069 - __main__ - INFO - Fold 2 Epoch 8 Batch 300: Train Loss = 1.9957
2024-03-21 13:39:53,202 - __main__ - INFO - Fold 2, mse = 27.7605, mad = 3.6450
2024-03-21 13:39:53,849 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 2.0600
2024-03-21 13:40:23,949 - __main__ - INFO - Fold 2 Epoch 9 Batch 50: Train Loss = 2.0680
2024-03-21 13:40:55,157 - __main__ - INFO - Fold 2 Epoch 9 Batch 100: Train Loss = 1.9376
2024-03-21 13:41:27,201 - __main__ - INFO - Fold 2 Epoch 9 Batch 150: Train Loss = 1.8967
2024-03-21 13:41:58,515 - __main__ - INFO - Fold 2 Epoch 9 Batch 200: Train Loss = 2.0555
2024-03-21 13:42:29,257 - __main__ - INFO - Fold 2 Epoch 9 Batch 250: Train Loss = 3.0527
2024-03-21 13:43:00,409 - __main__ - INFO - Fold 2 Epoch 9 Batch 300: Train Loss = 2.2525
2024-03-21 13:44:00,823 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.6885 ------------
2024-03-21 13:44:01,474 - __main__ - INFO - ------------ Save best model - MSE: 26.6885 ------------
2024-03-21 13:44:01,476 - __main__ - INFO - Fold 2, mse = 26.6885, mad = 3.6583
2024-03-21 13:44:02,132 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 2.0345
2024-03-21 13:44:32,910 - __main__ - INFO - Fold 2 Epoch 10 Batch 50: Train Loss = 2.1103
2024-03-21 13:45:04,651 - __main__ - INFO - Fold 2 Epoch 10 Batch 100: Train Loss = 1.8790
2024-03-21 13:45:36,490 - __main__ - INFO - Fold 2 Epoch 10 Batch 150: Train Loss = 1.5487
2024-03-21 13:46:10,610 - __main__ - INFO - Fold 2 Epoch 10 Batch 200: Train Loss = 1.9619
2024-03-21 13:46:42,125 - __main__ - INFO - Fold 2 Epoch 10 Batch 250: Train Loss = 1.6539
2024-03-21 13:47:14,504 - __main__ - INFO - Fold 2 Epoch 10 Batch 300: Train Loss = 2.6698
2024-03-21 13:48:11,427 - __main__ - INFO - Fold 2, epoch 10: Loss = 1.9299 Valid loss = 1.8678 MSE = 26.9355 AUROC = 0.8673
2024-03-21 13:48:11,428 - __main__ - INFO - Fold 2, mse = 26.9355, mad = 3.8972
2024-03-21 13:48:12,041 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 1.9874
2024-03-21 13:48:43,839 - __main__ - INFO - Fold 2 Epoch 11 Batch 50: Train Loss = 1.7181
2024-03-21 13:49:14,556 - __main__ - INFO - Fold 2 Epoch 11 Batch 100: Train Loss = 1.3916
2024-03-21 13:49:45,475 - __main__ - INFO - Fold 2 Epoch 11 Batch 150: Train Loss = 1.5437
2024-03-21 13:50:19,660 - __main__ - INFO - Fold 2 Epoch 11 Batch 200: Train Loss = 1.6425
2024-03-21 13:50:51,348 - __main__ - INFO - Fold 2 Epoch 11 Batch 250: Train Loss = 1.5441
2024-03-21 13:51:21,792 - __main__ - INFO - Fold 2 Epoch 11 Batch 300: Train Loss = 1.5191
2024-03-21 13:52:23,666 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.2440 ------------
2024-03-21 13:52:24,268 - __main__ - INFO - ------------ Save best model - MSE: 26.2440 ------------
2024-03-21 13:52:24,269 - __main__ - INFO - Fold 2, mse = 26.2440, mad = 3.6213
2024-03-21 13:52:24,982 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 1.7707
2024-03-21 13:52:56,062 - __main__ - INFO - Fold 2 Epoch 12 Batch 50: Train Loss = 1.6710
2024-03-21 13:53:26,888 - __main__ - INFO - Fold 2 Epoch 12 Batch 100: Train Loss = 1.6323
2024-03-21 13:53:58,259 - __main__ - INFO - Fold 2 Epoch 12 Batch 150: Train Loss = 2.0375
2024-03-21 13:54:33,730 - __main__ - INFO - Fold 2 Epoch 12 Batch 200: Train Loss = 1.8456
2024-03-21 13:55:08,202 - __main__ - INFO - Fold 2 Epoch 12 Batch 250: Train Loss = 1.8488
2024-03-21 13:55:41,515 - __main__ - INFO - Fold 2 Epoch 12 Batch 300: Train Loss = 1.4394
2024-03-21 13:56:39,368 - __main__ - INFO - Fold 2, mse = 26.8958, mad = 3.8747
2024-03-21 13:56:39,996 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 1.8750
2024-03-21 13:57:11,570 - __main__ - INFO - Fold 2 Epoch 13 Batch 50: Train Loss = 2.3291
2024-03-21 13:57:43,147 - __main__ - INFO - Fold 2 Epoch 13 Batch 100: Train Loss = 1.5849
2024-03-21 13:58:15,425 - __main__ - INFO - Fold 2 Epoch 13 Batch 150: Train Loss = 3.4590
2024-03-21 13:58:51,481 - __main__ - INFO - Fold 2 Epoch 13 Batch 200: Train Loss = 1.6021
2024-03-21 13:59:24,102 - __main__ - INFO - Fold 2 Epoch 13 Batch 250: Train Loss = 1.5498
2024-03-21 13:59:57,762 - __main__ - INFO - Fold 2 Epoch 13 Batch 300: Train Loss = 1.6877
2024-03-21 14:00:55,623 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.1448 ------------
2024-03-21 14:00:56,160 - __main__ - INFO - ------------ Save best model - MSE: 26.1448 ------------
2024-03-21 14:00:56,162 - __main__ - INFO - Fold 2, mse = 26.1448, mad = 3.6966
2024-03-21 14:00:56,815 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 1.6343
2024-03-21 14:01:26,899 - __main__ - INFO - Fold 2 Epoch 14 Batch 50: Train Loss = 3.3073
2024-03-21 14:01:59,646 - __main__ - INFO - Fold 2 Epoch 14 Batch 100: Train Loss = 2.1145
2024-03-21 14:02:31,175 - __main__ - INFO - Fold 2 Epoch 14 Batch 150: Train Loss = 1.4929
2024-03-21 14:03:04,835 - __main__ - INFO - Fold 2 Epoch 14 Batch 200: Train Loss = 1.4572
2024-03-21 14:03:36,694 - __main__ - INFO - Fold 2 Epoch 14 Batch 250: Train Loss = 1.9728
2024-03-21 14:04:07,576 - __main__ - INFO - Fold 2 Epoch 14 Batch 300: Train Loss = 1.8843
2024-03-21 14:05:07,822 - __main__ - INFO - Fold 2, mse = 26.1568, mad = 3.7412
2024-03-21 14:05:08,466 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 1.7607
2024-03-21 14:05:40,595 - __main__ - INFO - Fold 2 Epoch 15 Batch 50: Train Loss = 1.7126
2024-03-21 14:06:13,265 - __main__ - INFO - Fold 2 Epoch 15 Batch 100: Train Loss = 1.6046
2024-03-21 14:06:44,418 - __main__ - INFO - Fold 2 Epoch 15 Batch 150: Train Loss = 1.7784
2024-03-21 14:07:17,079 - __main__ - INFO - Fold 2 Epoch 15 Batch 200: Train Loss = 1.4548
2024-03-21 14:07:49,977 - __main__ - INFO - Fold 2 Epoch 15 Batch 250: Train Loss = 1.5161
2024-03-21 14:08:20,418 - __main__ - INFO - Fold 2 Epoch 15 Batch 300: Train Loss = 1.7671
2024-03-21 14:09:20,744 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.0365 ------------
2024-03-21 14:09:21,371 - __main__ - INFO - ------------ Save best model - MSE: 26.0365 ------------
2024-03-21 14:09:21,373 - __main__ - INFO - Fold 2, mse = 26.0365, mad = 3.6199
2024-03-21 14:09:22,012 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 1.9782
2024-03-21 14:09:52,770 - __main__ - INFO - Fold 2 Epoch 16 Batch 50: Train Loss = 1.8552
2024-03-21 14:10:24,335 - __main__ - INFO - Fold 2 Epoch 16 Batch 100: Train Loss = 1.6042
2024-03-21 14:10:56,252 - __main__ - INFO - Fold 2 Epoch 16 Batch 150: Train Loss = 2.2016
2024-03-21 14:11:27,273 - __main__ - INFO - Fold 2 Epoch 16 Batch 200: Train Loss = 2.1355
2024-03-21 14:12:01,732 - __main__ - INFO - Fold 2 Epoch 16 Batch 250: Train Loss = 1.3937
2024-03-21 14:12:32,359 - __main__ - INFO - Fold 2 Epoch 16 Batch 300: Train Loss = 1.5214
2024-03-21 14:13:35,490 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 25.9028 ------------
2024-03-21 14:13:36,095 - __main__ - INFO - ------------ Save best model - MSE: 25.9028 ------------
2024-03-21 14:13:36,097 - __main__ - INFO - Fold 2, mse = 25.9028, mad = 3.6268
2024-03-21 14:13:36,705 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 2.1484
2024-03-21 14:14:07,489 - __main__ - INFO - Fold 2 Epoch 17 Batch 50: Train Loss = 1.7854
2024-03-21 14:14:38,937 - __main__ - INFO - Fold 2 Epoch 17 Batch 100: Train Loss = 1.5658
2024-03-21 14:15:11,268 - __main__ - INFO - Fold 2 Epoch 17 Batch 150: Train Loss = 1.5327
2024-03-21 14:15:44,345 - __main__ - INFO - Fold 2 Epoch 17 Batch 200: Train Loss = 1.4318
2024-03-21 14:16:19,300 - __main__ - INFO - Fold 2 Epoch 17 Batch 250: Train Loss = 1.3711
2024-03-21 14:16:50,425 - __main__ - INFO - Fold 2 Epoch 17 Batch 300: Train Loss = 2.1635
2024-03-21 14:17:50,287 - __main__ - INFO - Fold 2, mse = 26.9216, mad = 3.6066
2024-03-21 14:17:50,947 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 1.8721
2024-03-21 14:18:23,099 - __main__ - INFO - Fold 2 Epoch 18 Batch 50: Train Loss = 1.3531
2024-03-21 14:18:53,611 - __main__ - INFO - Fold 2 Epoch 18 Batch 100: Train Loss = 2.5612
2024-03-21 14:19:24,523 - __main__ - INFO - Fold 2 Epoch 18 Batch 150: Train Loss = 1.3966
2024-03-21 14:19:56,657 - __main__ - INFO - Fold 2 Epoch 18 Batch 200: Train Loss = 1.5722
2024-03-21 14:20:31,731 - __main__ - INFO - Fold 2 Epoch 18 Batch 250: Train Loss = 1.6452
2024-03-21 14:21:02,634 - __main__ - INFO - Fold 2 Epoch 18 Batch 300: Train Loss = 1.5900
2024-03-21 14:22:01,050 - __main__ - INFO - Fold 2, mse = 26.7364, mad = 3.6868
2024-03-21 14:22:01,625 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 1.3705
2024-03-21 14:22:31,735 - __main__ - INFO - Fold 2 Epoch 19 Batch 50: Train Loss = 1.3716
2024-03-21 14:23:03,140 - __main__ - INFO - Fold 2 Epoch 19 Batch 100: Train Loss = 1.4597
2024-03-21 14:23:34,031 - __main__ - INFO - Fold 2 Epoch 19 Batch 150: Train Loss = 1.5376
2024-03-21 14:24:06,964 - __main__ - INFO - Fold 2 Epoch 19 Batch 200: Train Loss = 1.7974
2024-03-21 14:24:39,480 - __main__ - INFO - Fold 2 Epoch 19 Batch 250: Train Loss = 2.1778
2024-03-21 14:25:11,601 - __main__ - INFO - Fold 2 Epoch 19 Batch 300: Train Loss = 1.5709
2024-03-21 14:26:11,277 - __main__ - INFO - Fold 2, mse = 27.0040, mad = 3.7356
2024-03-21 14:26:12,605 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 15452.5674
2024-03-21 14:26:44,356 - __main__ - INFO - Fold 3 Epoch 0 Batch 50: Train Loss = 5.0164
2024-03-21 14:27:16,692 - __main__ - INFO - Fold 3 Epoch 0 Batch 100: Train Loss = 2.4966
2024-03-21 14:27:48,496 - __main__ - INFO - Fold 3 Epoch 0 Batch 150: Train Loss = 2.1813
2024-03-21 14:28:21,673 - __main__ - INFO - Fold 3 Epoch 0 Batch 200: Train Loss = 2.8669
2024-03-21 14:28:54,295 - __main__ - INFO - Fold 3 Epoch 0 Batch 250: Train Loss = 2.1127
2024-03-21 14:29:27,300 - __main__ - INFO - Fold 3 Epoch 0 Batch 300: Train Loss = 2.8600
2024-03-21 14:30:26,279 - __main__ - INFO - Fold 3, epoch 0: Loss = 2.5170 Valid loss = 2.2426 MSE = 32.8824 AUROC = 0.8410
2024-03-21 14:30:26,281 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.8824 ------------
2024-03-21 14:30:26,675 - __main__ - INFO - Fold 3, mse = 32.8824, mad = 4.2467
2024-03-21 14:30:27,394 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 3.0210
2024-03-21 14:31:00,088 - __main__ - INFO - Fold 3 Epoch 1 Batch 50: Train Loss = 2.5197
2024-03-21 14:31:31,619 - __main__ - INFO - Fold 3 Epoch 1 Batch 100: Train Loss = 3.1873
2024-03-21 14:32:04,946 - __main__ - INFO - Fold 3 Epoch 1 Batch 150: Train Loss = 2.1981
2024-03-21 14:32:36,352 - __main__ - INFO - Fold 3 Epoch 1 Batch 200: Train Loss = 2.7502
2024-03-21 14:33:09,139 - __main__ - INFO - Fold 3 Epoch 1 Batch 250: Train Loss = 1.8503
2024-03-21 14:33:39,997 - __main__ - INFO - Fold 3 Epoch 1 Batch 300: Train Loss = 2.0413
2024-03-21 14:34:40,903 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.8178 ------------
2024-03-21 14:34:41,265 - __main__ - INFO - Fold 3, mse = 30.8178, mad = 3.9607
2024-03-21 14:34:42,016 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 2.1258
2024-03-21 14:35:16,030 - __main__ - INFO - Fold 3 Epoch 2 Batch 50: Train Loss = 2.1679
2024-03-21 14:35:47,645 - __main__ - INFO - Fold 3 Epoch 2 Batch 100: Train Loss = 2.6001
2024-03-21 14:36:19,280 - __main__ - INFO - Fold 3 Epoch 2 Batch 150: Train Loss = 3.1306
2024-03-21 14:36:50,956 - __main__ - INFO - Fold 3 Epoch 2 Batch 200: Train Loss = 3.7138
2024-03-21 14:37:24,594 - __main__ - INFO - Fold 3 Epoch 2 Batch 250: Train Loss = 3.1377
2024-03-21 14:37:57,186 - __main__ - INFO - Fold 3 Epoch 2 Batch 300: Train Loss = 3.9010
2024-03-21 14:38:56,429 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.4763 ------------
2024-03-21 14:38:56,766 - __main__ - INFO - Fold 3, mse = 30.4763, mad = 4.1311
2024-03-21 14:38:57,354 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 2.5995
2024-03-21 14:39:28,863 - __main__ - INFO - Fold 3 Epoch 3 Batch 50: Train Loss = 1.5989
2024-03-21 14:40:02,927 - __main__ - INFO - Fold 3 Epoch 3 Batch 100: Train Loss = 1.7165
2024-03-21 14:40:38,376 - __main__ - INFO - Fold 3 Epoch 3 Batch 150: Train Loss = 2.0432
2024-03-21 14:41:12,407 - __main__ - INFO - Fold 3 Epoch 3 Batch 200: Train Loss = 2.3983
2024-03-21 14:41:45,302 - __main__ - INFO - Fold 3 Epoch 3 Batch 250: Train Loss = 2.4447
2024-03-21 14:42:18,481 - __main__ - INFO - Fold 3 Epoch 3 Batch 300: Train Loss = 1.5739
2024-03-21 14:43:16,612 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 29.0402 ------------
2024-03-21 14:43:16,966 - __main__ - INFO - Fold 3, mse = 29.0402, mad = 3.8889
2024-03-21 14:43:17,559 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 1.8899
2024-03-21 14:43:50,177 - __main__ - INFO - Fold 3 Epoch 4 Batch 50: Train Loss = 2.3428
2024-03-21 14:44:21,609 - __main__ - INFO - Fold 3 Epoch 4 Batch 100: Train Loss = 2.0740
2024-03-21 14:44:52,249 - __main__ - INFO - Fold 3 Epoch 4 Batch 150: Train Loss = 3.3464
2024-03-21 14:45:24,877 - __main__ - INFO - Fold 3 Epoch 4 Batch 200: Train Loss = 2.6169
2024-03-21 14:45:58,663 - __main__ - INFO - Fold 3 Epoch 4 Batch 250: Train Loss = 2.3048
2024-03-21 14:46:30,595 - __main__ - INFO - Fold 3 Epoch 4 Batch 300: Train Loss = 1.7237
2024-03-21 14:47:34,420 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 28.6752 ------------
2024-03-21 14:47:34,759 - __main__ - INFO - Fold 3, mse = 28.6752, mad = 3.8739
2024-03-21 14:47:35,397 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 2.1401
2024-03-21 14:48:06,640 - __main__ - INFO - Fold 3 Epoch 5 Batch 50: Train Loss = 1.8509
2024-03-21 14:48:38,623 - __main__ - INFO - Fold 3 Epoch 5 Batch 100: Train Loss = 2.1074
2024-03-21 14:49:11,032 - __main__ - INFO - Fold 3 Epoch 5 Batch 150: Train Loss = 1.5203
2024-03-21 14:49:42,424 - __main__ - INFO - Fold 3 Epoch 5 Batch 200: Train Loss = 1.9552
2024-03-21 14:50:16,699 - __main__ - INFO - Fold 3 Epoch 5 Batch 250: Train Loss = 2.1932
2024-03-21 14:50:50,873 - __main__ - INFO - Fold 3 Epoch 5 Batch 300: Train Loss = 2.0499
2024-03-21 14:51:51,854 - __main__ - INFO - Fold 3, mse = 28.9167, mad = 3.7660
2024-03-21 14:51:52,494 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 1.9765
2024-03-21 14:52:24,641 - __main__ - INFO - Fold 3 Epoch 6 Batch 50: Train Loss = 1.7691
2024-03-21 14:52:56,776 - __main__ - INFO - Fold 3 Epoch 6 Batch 100: Train Loss = 2.1695
2024-03-21 14:53:28,947 - __main__ - INFO - Fold 3 Epoch 6 Batch 150: Train Loss = 1.5298
2024-03-21 14:54:00,809 - __main__ - INFO - Fold 3 Epoch 6 Batch 200: Train Loss = 1.8782
2024-03-21 14:54:34,891 - __main__ - INFO - Fold 3 Epoch 6 Batch 250: Train Loss = 1.7782
2024-03-21 14:55:07,875 - __main__ - INFO - Fold 3 Epoch 6 Batch 300: Train Loss = 2.1069
2024-03-21 14:56:11,249 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.9939 ------------
2024-03-21 14:56:11,595 - __main__ - INFO - Fold 3, mse = 27.9939, mad = 3.7994
2024-03-21 14:56:12,256 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 1.6595
2024-03-21 14:56:44,330 - __main__ - INFO - Fold 3 Epoch 7 Batch 50: Train Loss = 1.8604
2024-03-21 14:57:15,572 - __main__ - INFO - Fold 3 Epoch 7 Batch 100: Train Loss = 2.0718
2024-03-21 14:57:46,721 - __main__ - INFO - Fold 3 Epoch 7 Batch 150: Train Loss = 2.1375
2024-03-21 14:58:21,741 - __main__ - INFO - Fold 3 Epoch 7 Batch 200: Train Loss = 1.8983
2024-03-21 14:58:54,990 - __main__ - INFO - Fold 3 Epoch 7 Batch 250: Train Loss = 2.1773
2024-03-21 14:59:26,949 - __main__ - INFO - Fold 3 Epoch 7 Batch 300: Train Loss = 1.9020
2024-03-21 15:00:24,932 - __main__ - INFO - Fold 3, mse = 28.1246, mad = 3.8706
2024-03-21 15:00:25,624 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 1.5177
2024-03-21 15:00:58,196 - __main__ - INFO - Fold 3 Epoch 8 Batch 50: Train Loss = 2.0955
2024-03-21 15:01:29,875 - __main__ - INFO - Fold 3 Epoch 8 Batch 100: Train Loss = 1.6376
2024-03-21 15:02:00,553 - __main__ - INFO - Fold 3 Epoch 8 Batch 150: Train Loss = 1.4440
2024-03-21 15:02:33,119 - __main__ - INFO - Fold 3 Epoch 8 Batch 200: Train Loss = 2.0392
2024-03-21 15:03:08,978 - __main__ - INFO - Fold 3 Epoch 8 Batch 250: Train Loss = 1.9178
2024-03-21 15:03:41,255 - __main__ - INFO - Fold 3 Epoch 8 Batch 300: Train Loss = 2.0240
2024-03-21 15:04:40,292 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.9170 ------------
2024-03-21 15:04:40,725 - __main__ - INFO - Fold 3, mse = 27.9170, mad = 3.9156
2024-03-21 15:04:41,405 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 2.2356
2024-03-21 15:05:12,444 - __main__ - INFO - Fold 3 Epoch 9 Batch 50: Train Loss = 1.4829
2024-03-21 15:05:42,411 - __main__ - INFO - Fold 3 Epoch 9 Batch 100: Train Loss = 1.9666
2024-03-21 15:06:13,383 - __main__ - INFO - Fold 3 Epoch 9 Batch 150: Train Loss = 1.7721
2024-03-21 15:06:47,366 - __main__ - INFO - Fold 3 Epoch 9 Batch 200: Train Loss = 1.5119
2024-03-21 15:07:21,467 - __main__ - INFO - Fold 3 Epoch 9 Batch 250: Train Loss = 3.6376
2024-03-21 15:07:54,148 - __main__ - INFO - Fold 3 Epoch 9 Batch 300: Train Loss = 1.3636
2024-03-21 15:08:53,228 - __main__ - INFO - Fold 3, mse = 28.0023, mad = 3.9383
2024-03-21 15:08:53,861 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 1.5965
2024-03-21 15:09:27,011 - __main__ - INFO - Fold 3 Epoch 10 Batch 50: Train Loss = 1.7170
2024-03-21 15:09:58,924 - __main__ - INFO - Fold 3 Epoch 10 Batch 100: Train Loss = 1.5887
2024-03-21 15:10:30,800 - __main__ - INFO - Fold 3 Epoch 10 Batch 150: Train Loss = 1.5728
2024-03-21 15:11:02,202 - __main__ - INFO - Fold 3 Epoch 10 Batch 200: Train Loss = 1.6563
2024-03-21 15:11:37,188 - __main__ - INFO - Fold 3 Epoch 10 Batch 250: Train Loss = 1.9106
2024-03-21 15:12:09,302 - __main__ - INFO - Fold 3 Epoch 10 Batch 300: Train Loss = 3.1992
2024-03-21 15:13:11,746 - __main__ - INFO - Fold 3, epoch 10: Loss = 1.9402 Valid loss = 1.9734 MSE = 28.7854 AUROC = 0.8674
2024-03-21 15:13:11,747 - __main__ - INFO - Fold 3, mse = 28.7854, mad = 3.9261
2024-03-21 15:13:12,421 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 1.8169
2024-03-21 15:13:43,800 - __main__ - INFO - Fold 3 Epoch 11 Batch 50: Train Loss = 1.8605
2024-03-21 15:14:15,614 - __main__ - INFO - Fold 3 Epoch 11 Batch 100: Train Loss = 1.9620
2024-03-21 15:14:47,259 - __main__ - INFO - Fold 3 Epoch 11 Batch 150: Train Loss = 2.0256
2024-03-21 15:15:21,083 - __main__ - INFO - Fold 3 Epoch 11 Batch 200: Train Loss = 1.3949
2024-03-21 15:15:55,239 - __main__ - INFO - Fold 3 Epoch 11 Batch 250: Train Loss = 2.1678
2024-03-21 15:16:28,102 - __main__ - INFO - Fold 3 Epoch 11 Batch 300: Train Loss = 1.7458
2024-03-21 15:17:24,802 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 27.2080 ------------
2024-03-21 15:17:25,147 - __main__ - INFO - Fold 3, mse = 27.2080, mad = 3.7811
2024-03-21 15:17:25,820 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 1.4735
2024-03-21 15:17:57,632 - __main__ - INFO - Fold 3 Epoch 12 Batch 50: Train Loss = 2.4069
2024-03-21 15:18:29,491 - __main__ - INFO - Fold 3 Epoch 12 Batch 100: Train Loss = 1.8969
2024-03-21 15:18:59,397 - __main__ - INFO - Fold 3 Epoch 12 Batch 150: Train Loss = 2.4678
2024-03-21 15:19:30,900 - __main__ - INFO - Fold 3 Epoch 12 Batch 200: Train Loss = 2.5340
2024-03-21 15:20:04,060 - __main__ - INFO - Fold 3 Epoch 12 Batch 250: Train Loss = 1.5674
2024-03-21 15:20:36,264 - __main__ - INFO - Fold 3 Epoch 12 Batch 300: Train Loss = 1.6596
2024-03-21 15:21:40,587 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.8490 ------------
2024-03-21 15:21:40,934 - __main__ - INFO - Fold 3, mse = 26.8490, mad = 3.7191
2024-03-21 15:21:41,587 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 2.0119
2024-03-21 15:22:12,184 - __main__ - INFO - Fold 3 Epoch 13 Batch 50: Train Loss = 1.6518
2024-03-21 15:22:42,966 - __main__ - INFO - Fold 3 Epoch 13 Batch 100: Train Loss = 1.4548
2024-03-21 15:23:13,831 - __main__ - INFO - Fold 3 Epoch 13 Batch 150: Train Loss = 1.8956
2024-03-21 15:23:46,183 - __main__ - INFO - Fold 3 Epoch 13 Batch 200: Train Loss = 1.7577
2024-03-21 15:24:19,564 - __main__ - INFO - Fold 3 Epoch 13 Batch 250: Train Loss = 1.5299
2024-03-21 15:24:53,289 - __main__ - INFO - Fold 3 Epoch 13 Batch 300: Train Loss = 1.3759
2024-03-21 15:25:51,950 - __main__ - INFO - Fold 3, mse = 27.1098, mad = 3.8431
2024-03-21 15:25:52,566 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 2.3372
2024-03-21 15:26:24,276 - __main__ - INFO - Fold 3 Epoch 14 Batch 50: Train Loss = 1.9316
2024-03-21 15:26:54,853 - __main__ - INFO - Fold 3 Epoch 14 Batch 100: Train Loss = 1.7504
2024-03-21 15:27:26,627 - __main__ - INFO - Fold 3 Epoch 14 Batch 150: Train Loss = 1.6621
2024-03-21 15:27:56,777 - __main__ - INFO - Fold 3 Epoch 14 Batch 200: Train Loss = 1.7928
2024-03-21 15:28:27,701 - __main__ - INFO - Fold 3 Epoch 14 Batch 250: Train Loss = 2.1307
2024-03-21 15:28:59,838 - __main__ - INFO - Fold 3 Epoch 14 Batch 300: Train Loss = 1.7932
2024-03-21 15:29:57,751 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.7646 ------------
2024-03-21 15:29:58,122 - __main__ - INFO - Fold 3, mse = 26.7646, mad = 3.7473
2024-03-21 15:29:58,822 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 1.8352
2024-03-21 15:30:27,997 - __main__ - INFO - Fold 3 Epoch 15 Batch 50: Train Loss = 1.4176
2024-03-21 15:30:58,505 - __main__ - INFO - Fold 3 Epoch 15 Batch 100: Train Loss = 1.4484
2024-03-21 15:31:29,120 - __main__ - INFO - Fold 3 Epoch 15 Batch 150: Train Loss = 1.7356
2024-03-21 15:32:01,564 - __main__ - INFO - Fold 3 Epoch 15 Batch 200: Train Loss = 1.4649
2024-03-21 15:32:35,110 - __main__ - INFO - Fold 3 Epoch 15 Batch 250: Train Loss = 1.5862
2024-03-21 15:33:06,566 - __main__ - INFO - Fold 3 Epoch 15 Batch 300: Train Loss = 1.5497
2024-03-21 15:34:00,010 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.6595 ------------
2024-03-21 15:34:00,334 - __main__ - INFO - Fold 3, mse = 26.6595, mad = 3.6837
2024-03-21 15:34:00,950 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 2.4146
2024-03-21 15:34:30,978 - __main__ - INFO - Fold 3 Epoch 16 Batch 50: Train Loss = 1.1391
2024-03-21 15:35:02,255 - __main__ - INFO - Fold 3 Epoch 16 Batch 100: Train Loss = 2.2577
2024-03-21 15:35:33,330 - __main__ - INFO - Fold 3 Epoch 16 Batch 150: Train Loss = 1.9044
2024-03-21 15:36:03,547 - __main__ - INFO - Fold 3 Epoch 16 Batch 200: Train Loss = 1.9783
2024-03-21 15:36:34,840 - __main__ - INFO - Fold 3 Epoch 16 Batch 250: Train Loss = 1.5043
2024-03-21 15:37:07,338 - __main__ - INFO - Fold 3 Epoch 16 Batch 300: Train Loss = 1.3765
2024-03-21 15:38:00,343 - __main__ - INFO - Fold 3, mse = 26.9020, mad = 3.7085
2024-03-21 15:38:00,991 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 1.2974
2024-03-21 15:38:32,394 - __main__ - INFO - Fold 3 Epoch 17 Batch 50: Train Loss = 2.0306
2024-03-21 15:39:01,654 - __main__ - INFO - Fold 3 Epoch 17 Batch 100: Train Loss = 1.5666
2024-03-21 15:39:32,661 - __main__ - INFO - Fold 3 Epoch 17 Batch 150: Train Loss = 1.1934
2024-03-21 15:40:02,833 - __main__ - INFO - Fold 3 Epoch 17 Batch 200: Train Loss = 1.6606
2024-03-21 15:40:31,952 - __main__ - INFO - Fold 3 Epoch 17 Batch 250: Train Loss = 2.1568
2024-03-21 15:41:05,280 - __main__ - INFO - Fold 3 Epoch 17 Batch 300: Train Loss = 1.5702
2024-03-21 15:42:05,065 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 26.5581 ------------
2024-03-21 15:42:05,410 - __main__ - INFO - Fold 3, mse = 26.5581, mad = 3.7554
2024-03-21 15:42:06,105 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 1.8461
2024-03-21 15:42:37,480 - __main__ - INFO - Fold 3 Epoch 18 Batch 50: Train Loss = 1.9102
2024-03-21 15:43:06,586 - __main__ - INFO - Fold 3 Epoch 18 Batch 100: Train Loss = 1.8275
2024-03-21 15:43:36,021 - __main__ - INFO - Fold 3 Epoch 18 Batch 150: Train Loss = 1.7100
2024-03-21 15:44:05,612 - __main__ - INFO - Fold 3 Epoch 18 Batch 200: Train Loss = 1.8916
2024-03-21 15:44:35,394 - __main__ - INFO - Fold 3 Epoch 18 Batch 250: Train Loss = 1.7795
2024-03-21 15:45:06,097 - __main__ - INFO - Fold 3 Epoch 18 Batch 300: Train Loss = 1.7302
2024-03-21 15:45:59,934 - __main__ - INFO - Fold 3, mse = 26.5899, mad = 3.7530
2024-03-21 15:46:00,542 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 2.0291
2024-03-21 15:46:30,857 - __main__ - INFO - Fold 3 Epoch 19 Batch 50: Train Loss = 2.0898
2024-03-21 15:47:01,262 - __main__ - INFO - Fold 3 Epoch 19 Batch 100: Train Loss = 1.2903
2024-03-21 15:47:30,788 - __main__ - INFO - Fold 3 Epoch 19 Batch 150: Train Loss = 1.9773
2024-03-21 15:48:00,963 - __main__ - INFO - Fold 3 Epoch 19 Batch 200: Train Loss = 1.0941
2024-03-21 15:48:33,554 - __main__ - INFO - Fold 3 Epoch 19 Batch 250: Train Loss = 1.4825
2024-03-21 15:49:06,637 - __main__ - INFO - Fold 3 Epoch 19 Batch 300: Train Loss = 1.5699
2024-03-21 15:50:02,840 - __main__ - INFO - Fold 3, mse = 27.0727, mad = 3.6921
2024-03-21 15:50:02,841 - __main__ - INFO - mse 27.8669(2.3297)
2024-03-21 15:50:02,841 - __main__ - INFO - mad 3.7291(0.0752)
2024-03-21 15:50:02,842 - __main__ - INFO - auroc 0.8821(0.0007)
2024-03-21 15:50:02,842 - __main__ - INFO - auprc 0.5763(0.0177)
