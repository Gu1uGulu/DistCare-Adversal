2024-01-18 01:06:38,070 - __main__ - INFO - 这是希望输出的info内容
2024-01-18 01:06:38,070 - __main__ - WARNING - 这是希望输出的warning内容
2024-01-18 01:07:24,240 - __main__ - INFO - 32269
2024-01-18 01:07:24,242 - __main__ - INFO - 4034
2024-01-18 01:07:24,242 - __main__ - INFO - 4033
2024-01-18 01:07:35,087 - __main__ - INFO - load target data
2024-01-18 01:07:44,099 - __main__ - INFO - Training Student
2024-01-18 01:07:47,024 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 1.7358
2024-01-18 01:08:05,411 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 1.4934
2024-01-18 01:08:23,086 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 1.5245
2024-01-18 01:08:40,910 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 1.4853
2024-01-18 01:08:58,389 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 1.4114
2024-01-18 01:09:16,335 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 1.4137
2024-01-18 01:09:34,371 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 1.4054
2024-01-18 01:09:39,623 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.3356 ------------
2024-01-18 01:09:48,542 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 1.4418
2024-01-18 01:10:06,791 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 1.5487
2024-01-18 01:10:25,114 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 1.5269
2024-01-18 01:10:44,206 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 1.4772
2024-01-18 01:11:02,649 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 1.4600
2024-01-18 01:11:21,998 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 1.4518
2024-01-18 01:11:41,168 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 1.4583
2024-01-18 01:11:56,066 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 1.4705
2024-01-18 01:12:16,714 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 1.4746
2024-01-18 01:12:34,568 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 1.4797
2024-01-18 01:12:52,404 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 1.5061
2024-01-18 01:13:10,997 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 1.4791
2024-01-18 01:13:31,106 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 1.4442
2024-01-18 01:13:50,142 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 1.4700
2024-01-18 01:14:05,481 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 1.4714
2024-01-18 01:14:23,609 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 1.4947
2024-01-18 01:14:41,314 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 1.4851
2024-01-18 01:14:58,511 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 1.4916
2024-01-18 01:15:16,517 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 1.4579
2024-01-18 01:15:34,648 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 1.4472
2024-01-18 01:15:51,801 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 1.4583
2024-01-18 01:16:06,256 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 1.4694
2024-01-18 01:16:24,205 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 1.4698
2024-01-18 01:16:42,972 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 1.4791
2024-01-18 01:16:59,632 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 1.4867
2024-01-18 01:17:17,747 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 1.4520
2024-01-18 01:17:36,130 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 1.4430
2024-01-18 01:17:52,400 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 1.4564
2024-01-18 01:18:07,427 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 1.4649
2024-01-18 01:18:25,166 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 1.4656
2024-01-18 01:18:42,268 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 1.4694
2024-01-18 01:18:59,131 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 1.4810
2024-01-18 01:19:15,477 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 1.4498
2024-01-18 01:19:33,627 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 1.4405
2024-01-18 01:19:50,732 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 1.4562
2024-01-18 01:20:05,734 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 1.4621
2024-01-18 01:20:23,912 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 1.4672
2024-01-18 01:20:41,078 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 1.4683
2024-01-18 01:20:57,822 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 1.4798
2024-01-18 01:21:15,363 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 1.4483
2024-01-18 01:21:32,778 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 1.4406
2024-01-18 01:21:50,657 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 1.4540
2024-01-18 01:22:05,161 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 1.4619
2024-01-18 01:22:22,645 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 1.4674
2024-01-18 01:22:39,437 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 1.4706
2024-01-18 01:22:56,423 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 1.4789
2024-01-18 01:23:15,440 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 1.4486
2024-01-18 01:23:34,137 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 1.4413
2024-01-18 01:23:53,425 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 1.4550
2024-01-18 01:24:08,084 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 1.4608
2024-01-18 01:24:26,208 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 1.4634
2024-01-18 01:24:43,680 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 1.4703
2024-01-18 01:25:01,503 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 1.4794
2024-01-18 01:25:18,674 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 1.4486
2024-01-18 01:25:36,227 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 1.4439
2024-01-18 01:25:53,898 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 1.4544
2024-01-18 01:26:08,004 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 1.4610
2024-01-18 01:26:25,323 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 1.4654
2024-01-18 01:26:42,621 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 1.4715
2024-01-18 01:27:00,332 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 1.4796
2024-01-18 01:27:18,123 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 1.4472
2024-01-18 01:27:35,923 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 1.4416
2024-01-18 01:27:54,276 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 1.4556
2024-01-18 01:28:08,982 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 1.4617
2024-01-18 01:28:27,078 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 1.4660
2024-01-18 01:28:45,075 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 1.4691
2024-01-18 01:29:03,913 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 1.4807
2024-01-18 01:29:22,092 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 1.4489
2024-01-18 01:29:41,081 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 1.4405
2024-01-18 01:29:58,710 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 1.4562
2024-01-18 01:30:13,119 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 1.4629
2024-01-18 01:30:32,176 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 1.4654
2024-01-18 01:30:49,417 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 1.4685
2024-01-18 01:31:06,674 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 1.4803
2024-01-18 01:31:24,282 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 1.4501
2024-01-18 01:31:41,441 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 1.4421
2024-01-18 01:31:59,805 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 1.4567
2024-01-18 01:32:13,710 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 1.4627
2024-01-18 01:32:31,082 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 1.4663
2024-01-18 01:32:48,778 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 1.4687
2024-01-18 01:33:06,490 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 1.4787
2024-01-18 01:33:25,086 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 1.4469
2024-01-18 01:33:44,318 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 1.4408
2024-01-18 01:34:01,633 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 1.4543
2024-01-18 01:34:15,628 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 1.4628
2024-01-18 01:34:35,213 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 1.4672
2024-01-18 01:34:52,649 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 1.4679
2024-01-18 01:35:10,150 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 1.4814
2024-01-18 01:35:29,088 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 1.4482
2024-01-18 01:35:46,680 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 1.4402
2024-01-18 01:36:04,625 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 1.4575
2024-01-18 01:36:19,152 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 1.4619
2024-01-18 01:36:37,171 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 1.4643
2024-01-18 01:36:55,445 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 1.4703
2024-01-18 01:37:12,485 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 1.4793
2024-01-18 01:37:30,032 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 1.4485
2024-01-18 01:37:47,712 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 1.4420
2024-01-18 01:38:05,852 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 1.4541
2024-01-18 01:38:20,637 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 1.4598
2024-01-18 01:38:39,848 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 1.4645
2024-01-18 01:38:57,300 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 1.4688
2024-01-18 01:39:15,583 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 1.4780
2024-01-18 01:39:34,013 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 1.4489
2024-01-18 01:39:51,414 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 1.4432
2024-01-18 01:40:10,214 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 1.4575
2024-01-18 01:40:24,403 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 1.4636
2024-01-18 01:40:43,824 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 1.4657
2024-01-18 01:41:02,404 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 1.4675
2024-01-18 01:41:20,281 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 1.4783
2024-01-18 01:41:37,058 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 1.4487
2024-01-18 01:41:55,545 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 1.4420
2024-01-18 01:42:13,042 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 1.4555
2024-01-18 01:42:26,575 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 1.4614
2024-01-18 01:42:43,671 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 1.4653
2024-01-18 01:43:01,014 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 1.4687
2024-01-18 01:43:18,295 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 1.4819
2024-01-18 01:43:36,459 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 1.4486
2024-01-18 01:43:53,692 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 1.4401
2024-01-18 01:44:10,990 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 1.4549
2024-01-18 01:44:25,004 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 1.4602
2024-01-18 01:44:43,333 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 1.4653
2024-01-18 01:45:00,713 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 1.4703
2024-01-18 01:45:18,601 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 1.4771
2024-01-18 01:45:37,665 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 1.4490
2024-01-18 01:45:56,212 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 1.4420
2024-01-18 01:46:14,332 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 1.4564
2024-01-18 01:46:28,800 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 1.4622
2024-01-18 01:46:48,413 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 1.4643
2024-01-18 01:47:06,023 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 1.4690
2024-01-18 01:47:23,231 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 1.4798
2024-01-18 01:47:41,434 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 1.4480
2024-01-18 01:47:59,789 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 1.4406
2024-01-18 01:48:17,725 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 1.4533
2024-01-18 01:48:31,768 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 1.4624
2024-01-18 01:48:49,373 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 1.4646
2024-01-18 01:49:05,987 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 1.4695
2024-01-18 01:49:24,710 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 1.4789
2024-01-18 01:49:42,577 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 1.4476
2024-01-18 01:50:11,330 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 1.4403
2024-01-18 01:50:29,990 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 1.4530
2024-01-18 01:50:43,250 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 1.4627
2024-01-18 01:51:00,169 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 1.4664
2024-01-18 01:51:16,684 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 1.4672
2024-01-18 01:51:34,468 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 1.4754
2024-01-18 01:51:52,211 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 1.4475
2024-01-18 01:52:08,167 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 1.4394
2024-01-18 01:52:23,910 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 1.4550
2024-01-18 01:52:35,920 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 1.4615
2024-01-18 01:52:51,004 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 1.4642
2024-01-18 01:53:05,542 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 1.4661
2024-01-18 01:53:20,525 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 1.4788
2024-01-18 01:53:36,371 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 1.4445
2024-01-18 01:53:51,652 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 1.4402
2024-01-18 01:54:06,788 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 1.4524
2024-01-18 01:54:18,550 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 1.4621
2024-01-18 01:54:33,387 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 1.4638
2024-01-18 01:54:46,906 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 1.4693
2024-01-18 01:55:00,978 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 1.4774
2024-01-18 01:55:16,403 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 1.4489
2024-01-18 01:55:31,063 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 1.4400
2024-01-18 01:55:46,221 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 1.4552
2024-01-18 01:55:58,930 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 1.4613
2024-01-18 01:56:15,106 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 1.4632
2024-01-18 01:56:30,459 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 1.4658
2024-01-18 01:56:45,917 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 1.4760
2024-01-18 01:57:01,815 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 1.4470
2024-01-18 01:57:17,807 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 1.4376
2024-01-18 01:57:33,560 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 1.4527
2024-01-18 01:57:46,082 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 1.4613
2024-01-18 01:58:01,293 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 1.4681
2024-01-18 01:58:15,955 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 1.4678
2024-01-18 01:58:30,187 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 1.4786
2024-01-18 01:58:44,487 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 1.4474
2024-01-18 01:58:58,963 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 1.4389
2024-01-18 01:59:13,898 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 1.4524
2024-01-18 01:59:26,511 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 1.4620
2024-01-18 01:59:41,327 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 1.4654
2024-01-18 01:59:55,938 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 1.4683
2024-01-18 02:00:11,639 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 1.4798
2024-01-18 02:00:27,137 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 1.4457
2024-01-18 02:00:43,796 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 1.4384
2024-01-18 02:00:59,630 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 1.4499
2024-01-18 02:01:12,910 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 1.4587
2024-01-18 02:01:28,290 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 1.4642
2024-01-18 02:01:43,442 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 1.4653
2024-01-18 02:01:58,193 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 1.4757
2024-01-18 02:02:13,057 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 1.4449
2024-01-18 02:02:26,758 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 1.4366
2024-01-18 02:02:40,908 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 1.4535
2024-01-18 02:02:53,068 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 1.4607
2024-01-18 02:03:06,467 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 1.4620
2024-01-18 02:03:19,693 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 1.4642
2024-01-18 02:03:32,776 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 1.4769
2024-01-18 02:03:47,081 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 1.4486
2024-01-18 02:04:01,098 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 1.4409
2024-01-18 02:04:14,840 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 1.4573
2024-01-18 02:04:26,321 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 1.4621
2024-01-18 02:04:39,773 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 1.4639
2024-01-18 02:04:52,627 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 1.4668
2024-01-18 02:05:05,517 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 1.4761
2024-01-18 02:05:18,708 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 1.4468
2024-01-18 02:05:32,629 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 1.4415
2024-01-18 02:05:46,108 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 1.4530
2024-01-18 02:05:56,552 - __main__ - INFO - last saved model is in epoch 27
2024-01-18 02:35:51,396 - __main__ - INFO - last saved model is in epoch 27
2024-01-18 02:38:37,042 - __main__ - INFO - Training Student
2024-01-18 02:38:37,731 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 1.5256
2024-01-18 02:38:49,509 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 1.4485
2024-01-18 02:39:00,514 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 1.4229
2024-01-18 02:39:11,834 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 1.4197
2024-01-18 02:39:23,072 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 1.3795
2024-01-18 02:39:34,748 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 1.3363
2024-01-18 02:39:46,704 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 1.3573
2024-01-18 02:39:50,366 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.2985 ------------
2024-01-18 02:39:50,770 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.2973 ------------
2024-01-18 02:39:51,273 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.2721 ------------
2024-01-18 02:39:52,067 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.2684 ------------
2024-01-18 02:39:57,035 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 1.3570
2024-01-18 02:40:09,136 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 1.4905
2024-01-18 02:40:21,158 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 1.4807
2024-01-18 02:40:33,110 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 1.4751
2024-01-18 02:40:45,190 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 1.4631
2024-01-18 02:40:57,424 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 1.4609
2024-01-18 02:41:09,680 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 1.4808
2024-01-18 02:41:19,404 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 1.4451
2024-01-18 02:41:31,730 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 1.4634
2024-01-18 02:41:42,552 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 1.4584
2024-01-18 02:41:53,778 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 1.4863
2024-01-18 02:42:04,915 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 1.4485
2024-01-18 02:42:16,483 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 1.4450
2024-01-18 02:42:28,040 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 1.4461
2024-01-18 02:42:37,810 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 1.4697
2024-01-18 02:42:49,679 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 1.4693
2024-01-18 02:43:01,232 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 1.4735
2024-01-18 02:43:13,158 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 1.4821
2024-01-18 02:43:25,155 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 1.4383
2024-01-18 02:43:37,141 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 1.4374
2024-01-18 02:43:49,306 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 1.4563
2024-01-18 02:43:59,865 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 1.4704
2024-01-18 02:44:11,374 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 1.4621
2024-01-18 02:44:22,518 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 1.4687
2024-01-18 02:44:33,921 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 1.4777
2024-01-18 02:44:45,544 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 1.4533
2024-01-18 02:44:57,107 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 1.4463
2024-01-18 02:45:08,420 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 1.4556
2024-01-18 02:45:18,228 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 1.4609
2024-01-18 02:45:29,685 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 1.4637
2024-01-18 02:45:41,009 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 1.4721
2024-01-18 02:45:52,864 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 1.4806
2024-01-18 02:46:04,159 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 1.4484
2024-01-18 02:46:16,339 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 1.4425
2024-01-18 02:46:28,120 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 1.4481
2024-01-18 02:46:38,898 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 1.4585
2024-01-18 02:46:51,819 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 1.4600
2024-01-18 02:47:03,588 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 1.4655
2024-01-18 02:47:14,233 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 1.4803
2024-01-18 02:47:25,585 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 1.4521
2024-01-18 02:47:36,634 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 1.4431
2024-01-18 02:47:47,969 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 1.4547
2024-01-18 02:47:57,282 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 1.4645
2024-01-18 02:48:07,270 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 1.4620
2024-01-18 02:48:15,902 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 1.4659
2024-01-18 02:48:25,142 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 1.4769
2024-01-18 02:48:35,458 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 1.4474
2024-01-18 02:48:46,474 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 1.4410
2024-01-18 02:48:57,168 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 1.4531
2024-01-18 02:49:05,767 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 1.4602
2024-01-18 02:49:17,084 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 1.4612
2024-01-18 02:49:28,783 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 1.4666
2024-01-18 02:49:39,868 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 1.4738
2024-01-18 02:49:51,216 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 1.4448
2024-01-18 02:50:01,042 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 1.4398
2024-01-18 02:50:11,212 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 1.4527
2024-01-18 02:50:18,992 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 1.4590
2024-01-18 02:50:28,847 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 1.4622
2024-01-18 02:50:38,575 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 1.4664
2024-01-18 02:50:48,275 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 1.4753
2024-01-18 02:50:58,026 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 1.4459
2024-01-18 02:51:07,701 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 1.4406
2024-01-18 02:51:17,835 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 1.4512
2024-01-18 02:51:26,998 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 1.4606
2024-01-18 02:51:37,962 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 1.4637
2024-01-18 02:51:48,739 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 1.4676
2024-01-18 02:51:59,306 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 1.4749
2024-01-18 02:52:10,310 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 1.4463
2024-01-18 02:52:22,467 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 1.4411
2024-01-18 02:52:33,181 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 1.4530
2024-01-18 02:52:41,965 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 1.4577
2024-01-18 02:52:51,829 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 1.4635
2024-01-18 02:53:01,310 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 1.4640
2024-01-18 02:53:11,032 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 1.4736
2024-01-18 02:53:20,514 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 1.4457
2024-01-18 02:53:30,225 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 1.4391
2024-01-18 02:53:39,552 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 1.4533
2024-01-18 02:53:48,133 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 1.4579
2024-01-18 02:53:58,383 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 1.4636
2024-01-18 02:54:08,668 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 1.4643
2024-01-18 02:54:19,156 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 1.4793
2024-01-18 02:54:29,707 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 1.4477
2024-01-18 02:54:40,344 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 1.4400
2024-01-18 02:54:51,307 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 1.4519
2024-01-18 02:55:00,629 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 1.4594
2024-01-18 02:55:12,381 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 1.4610
2024-01-18 02:55:24,297 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 1.4678
2024-01-18 02:55:35,613 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 1.4778
2024-01-18 02:55:46,503 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 1.4495
2024-01-18 02:55:57,273 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 1.4370
2024-01-18 02:56:08,114 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 1.4553
2024-01-18 02:56:17,452 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 1.4599
2024-01-18 02:56:28,471 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 1.4650
2024-01-18 02:56:39,241 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 1.4687
2024-01-18 02:56:50,037 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 1.4769
2024-01-18 02:57:01,695 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 1.4458
2024-01-18 02:57:13,360 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 1.4399
2024-01-18 02:57:25,177 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 1.4528
2024-01-18 02:57:35,934 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 1.4597
2024-01-18 02:57:47,965 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 1.4623
2024-01-18 02:57:59,959 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 1.4657
2024-01-18 02:58:11,467 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 1.4755
2024-01-18 02:58:23,222 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 1.4465
2024-01-18 02:58:34,325 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 1.4391
2024-01-18 02:58:45,320 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 1.4540
2024-01-18 02:58:54,958 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 1.4605
2024-01-18 02:59:06,227 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 1.4617
2024-01-18 02:59:16,990 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 1.4627
2024-01-18 02:59:26,163 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 1.4731
2024-01-18 02:59:36,084 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 1.4459
2024-01-18 02:59:46,083 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 1.4398
2024-01-18 02:59:56,137 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 1.4528
2024-01-18 03:00:05,832 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 1.4595
2024-01-18 03:00:16,781 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 1.4619
2024-01-18 03:00:27,713 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 1.4666
2024-01-18 03:00:38,917 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 1.4754
2024-01-18 03:00:50,338 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 1.4443
2024-01-18 03:01:01,353 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 1.4387
2024-01-18 03:01:12,064 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 1.4518
2024-01-18 03:01:21,261 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 1.4580
2024-01-18 03:01:31,046 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 1.4625
2024-01-18 03:01:40,641 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 1.4658
2024-01-18 03:01:50,036 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 1.4744
2024-01-18 03:01:59,321 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 1.4459
2024-01-18 03:02:09,398 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 1.4362
2024-01-18 03:02:19,079 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 1.4516
2024-01-18 03:02:26,770 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 1.4556
2024-01-18 03:02:36,842 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 1.4629
2024-01-18 03:02:48,298 - __main__ - INFO - last saved model is in epoch 0
2024-01-18 03:02:51,318 - __main__ - INFO - Batch 0: Test Loss = -2.5771
2024-01-18 03:02:54,197 - __main__ - INFO - 
==>Predicting on test
2024-01-18 03:02:54,198 - __main__ - INFO - Test Loss = -2.5198
2024-01-18 03:26:07,641 - __main__ - INFO - Transfer Target Dataset & Model
2024-01-18 03:26:07,696 - __main__ - INFO - [[-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8285543981909917, 2.770245567717861, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8374745525934485, 2.8093811444689463, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5338330207864587, 0.431156978499758, 0.8964660630930379, -1.342421748214616, -0.6946051308187712, -0.2120300841305121, 0.8952797203562032, 0.0387041303378994, -0.8612693268611251, 0.0726334784031089, -0.5051102137388988, -0.2865727201409924, 0.3988826585206329, -1.3794377690564883, -0.8237089728907875, -0.9221119476695248, -1.6036614531597553, -0.2379932071009605, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -1.1501193786706505, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, 1.0826051686869729, -0.609009148503521, 0.9684393533852332, -0.3880554131475662, 0.8439788318452395, 2.837917502516613, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 0.278477698357045, 1.2505677424119097, -0.6305581644917595, -0.9062745442328174, -0.7536814885080627, -0.4374103947888615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, -0.7710128163592748, -0.6738408269117502, -2.438483340619628, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8608899578998963, 2.912112033440545, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3396023676711391, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8763143915541441, 2.979783968239297, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5125273793649132, 0.601421850901989, 0.5034187451388209, -1.455529079732558, -0.5254345623915906, -0.4163803392884947, 0.7885904984786939, 0.2816638966057675, -0.5890172833864098, 0.5902661209826593, -0.3430637446868887, -0.7482466726318323, -0.1864257310498265, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.2249939001501796, -0.0341172340589738, 0.3466058957088718, 0.3482841380580905, -0.2890718868318484, 0.1923438589112976, -0.2031244129114749, -1.100153472115602, -0.7598199909551685, -0.6868858002659636, 0.3602180776283341, -0.700061185363224, -0.501359972189453, -0.2332733777972843, -1.36588558517574, 0.3571411263970316, -0.7447833078367583, 0.8765002281041955, 2.9805992927549454, 0.0776143028335215, -0.2769313825285214, 0.0755136678180621, -0.1474826847594624, -0.3999358135056357, -0.0169688050451549, -0.0937841116273902, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, -0.0122781725754626, -0.3292808750219816, -0.2769313825285214, -0.107625012968948, -0.1474826847594624, -0.5354516373428909, -0.0002229391773086, -0.0862088042532133, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, 0.0766269712424727, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.3951350096437179, -0.5239726809327129, -0.8378690925056301, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.8988006141103361, 3.0784382346326584, 0.0776143028335215, -0.3487779453829513, 0.070457637721598, -0.1474826847594624, -0.3999358135056357, 0.0045338857876385, -0.0729488956980065, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.1275947723114954, -0.4255010572823463, -0.3487779453829513, -0.1777070069371955, -0.1474826847594624, -0.5354516373428909, 0.029608437271828, -0.0566564538358881, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.0232601678654147, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.9106941533136118, 3.1306190036341057, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9108799898636633, 3.1314343281497536, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 1.0542620417323658, -0.6738408269117502, -0.0796833960305785, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9222160194167848, 3.1811691236042576, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.5107528363891513, 1.4769583166408096, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-1.038066534429697, 0.260892106097527, 0.1645848503507034, -1.455529079732558, -1.6391408045371951, 0.196670426185453, 1.2420196914581063, 0.4706326037029981, -0.6616178283130005, -0.5004598044528213, -0.4609157221792596, 0.021209914852902, -1.1769476210921423, 0.0909225332951111, -1.348247483817418, -1.2590833683251033, -0.9252694826312332, 0.781386658108973, -0.0798138719333467, -0.2032969502372001, -0.5728175028694968, 0.103242147293012, -0.2031244129114749, -0.6504603131201653, -0.5868915885967425, -0.6218024077192129, -0.0284729839577739, -0.3925684926321655, -0.4454382040900255, -0.2791761177909213, -1.5149673075505703, 0.3413397023846657, -0.8352815289623093, 0.9329945393197532, 3.228457945511819, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8995798651896526, 0.4879119359671683, 0.1510314945591795, -1.4131138304133295, -0.6523124887119757, 0.196670426185453, 0.148455167213642, 0.4706326037029981, -0.752368509471239, -0.9441449266638648, -0.9507380036319262, 0.1751012323498492, -1.0418764542681906, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -1.3629417216818924, 0.1697587389830128, -0.364093717028159, -0.2645837378255657, -0.2890718868318484, 0.0364158635792983, -0.2031244129114749, -1.8246591171638051, -0.4061028043129334, -0.5350245509902118, 0.0492652283594477, 0.0357249008146661, -0.4929416415078188, -0.5851943844151671, -1.170932563608653, 0.051337096981241, -0.8763418268751098, 0.9447022419729768, 3.279823389997619, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.962914223877992, 3.359725192531085, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.973321070680858, 3.405383365407351, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9735069072309092, 3.4061986899229986, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9980373318376644, 3.5138215259884835, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0340896225475933, 3.671994482024121, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.2639920224449301, -0.6738408269117502, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0396647190491284, 3.696454217493548, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7418059691584676, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, -0.6693833821778409, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.057505027854041, 3.774725370995719, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5792130238763558, -0.9062745442328174, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, 0.0888023142972107, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0760886828591592, 3.856257822560481, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]
2024-01-18 03:26:07,697 - __main__ - INFO - 69
2024-01-18 03:26:07,698 - __main__ - INFO - 325
2024-01-18 03:26:29,939 - __main__ - INFO - {'los_mean': 1055.0307777880782, 'los_std': 799.0879849276147}
2024-01-18 03:26:35,152 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 1.9969
2024-01-18 03:26:49,150 - __main__ - INFO - Fold 1, epoch 0: Loss = 1.6106 Valid loss = 1.0472 MSE = 733.3216
2024-01-18 03:26:49,151 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 733.3216 ------------
2024-01-18 03:26:49,381 - __main__ - INFO - ------------ Save best model - MSE: 733.3216 ------------
2024-01-18 03:26:49,383 - __main__ - INFO - Fold 1, mse = 733.3216, mad = 22.6979
2024-01-18 03:26:49,892 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 1.1314
2024-01-18 03:27:03,432 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 712.1826 ------------
2024-01-18 03:27:03,687 - __main__ - INFO - ------------ Save best model - MSE: 712.1826 ------------
2024-01-18 03:27:03,687 - __main__ - INFO - Fold 1, mse = 712.1826, mad = 21.4896
2024-01-18 03:27:04,222 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 1.0596
2024-01-18 03:27:16,709 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 706.8989 ------------
2024-01-18 03:27:16,969 - __main__ - INFO - ------------ Save best model - MSE: 706.8989 ------------
2024-01-18 03:27:16,971 - __main__ - INFO - Fold 1, mse = 706.8989, mad = 21.6374
2024-01-18 03:27:17,549 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 0.9721
2024-01-18 03:27:31,782 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 692.8919 ------------
2024-01-18 03:27:32,011 - __main__ - INFO - ------------ Save best model - MSE: 692.8919 ------------
2024-01-18 03:27:32,012 - __main__ - INFO - Fold 1, mse = 692.8919, mad = 21.6473
2024-01-18 03:27:32,536 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 0.9522
2024-01-18 03:27:46,090 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 666.9043 ------------
2024-01-18 03:27:46,311 - __main__ - INFO - ------------ Save best model - MSE: 666.9043 ------------
2024-01-18 03:27:46,313 - __main__ - INFO - Fold 1, mse = 666.9043, mad = 20.9665
2024-01-18 03:27:46,925 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 1.0202
2024-01-18 03:28:01,636 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 651.9625 ------------
2024-01-18 03:28:01,933 - __main__ - INFO - ------------ Save best model - MSE: 651.9625 ------------
2024-01-18 03:28:01,934 - __main__ - INFO - Fold 1, mse = 651.9625, mad = 20.6777
2024-01-18 03:28:02,642 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 0.7553
2024-01-18 03:28:17,951 - __main__ - INFO - Fold 1, mse = 660.8637, mad = 20.4310
2024-01-18 03:28:18,493 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 0.7328
2024-01-18 03:28:32,599 - __main__ - INFO - Fold 1, mse = 654.7805, mad = 20.5451
2024-01-18 03:28:33,328 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 0.8768
2024-01-18 03:28:48,260 - __main__ - INFO - Fold 1, mse = 663.9837, mad = 20.4719
2024-01-18 03:28:48,827 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 0.8454
2024-01-18 03:29:01,789 - __main__ - INFO - Fold 1, mse = 665.2875, mad = 20.6106
2024-01-18 03:29:02,328 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 0.6998
2024-01-18 03:29:15,125 - __main__ - INFO - Fold 1, epoch 10: Loss = 0.7103 Valid loss = 0.9471 MSE = 668.8218
2024-01-18 03:29:15,126 - __main__ - INFO - Fold 1, mse = 668.8218, mad = 20.7751
2024-01-18 03:29:15,767 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 0.7054
2024-01-18 03:29:28,735 - __main__ - INFO - Fold 1, mse = 687.6098, mad = 21.1811
2024-01-18 03:29:29,296 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 0.6115
2024-01-18 03:29:43,726 - __main__ - INFO - Fold 1, mse = 703.4314, mad = 21.1764
2024-01-18 03:29:44,317 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 0.5727
2024-01-18 03:29:57,751 - __main__ - INFO - Fold 1, mse = 768.6074, mad = 22.0746
2024-01-18 03:29:58,322 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 0.4440
2024-01-18 03:30:12,257 - __main__ - INFO - Fold 1, mse = 834.9606, mad = 23.3840
2024-01-18 03:30:12,783 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 0.5606
2024-01-18 03:30:27,808 - __main__ - INFO - Fold 1, mse = 791.5300, mad = 22.6049
2024-01-18 03:30:28,389 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 0.3966
2024-01-18 03:30:42,975 - __main__ - INFO - Fold 1, mse = 802.1485, mad = 22.7967
2024-01-18 03:30:43,532 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 0.4795
2024-01-18 03:30:58,197 - __main__ - INFO - Fold 1, mse = 805.8884, mad = 22.8537
2024-01-18 03:30:58,738 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 0.3678
2024-01-18 03:31:14,740 - __main__ - INFO - Fold 1, mse = 801.0290, mad = 22.6624
2024-01-18 03:31:15,319 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 0.2493
2024-01-18 03:31:29,460 - __main__ - INFO - Fold 1, mse = 817.8216, mad = 22.9393
2024-01-18 03:31:30,019 - __main__ - INFO - Fold 1 Epoch 20 Batch 0: Train Loss = 0.3220
2024-01-18 03:31:43,071 - __main__ - INFO - Fold 1, epoch 20: Loss = 0.3316 Valid loss = 1.1733 MSE = 838.7015
2024-01-18 03:31:43,072 - __main__ - INFO - Fold 1, mse = 838.7015, mad = 23.1818
2024-01-18 03:31:43,622 - __main__ - INFO - Fold 1 Epoch 21 Batch 0: Train Loss = 0.3343
2024-01-18 03:31:56,562 - __main__ - INFO - Fold 1, mse = 857.6031, mad = 23.3794
2024-01-18 03:31:57,203 - __main__ - INFO - Fold 1 Epoch 22 Batch 0: Train Loss = 0.4058
2024-01-18 03:32:10,297 - __main__ - INFO - Fold 1, mse = 872.9593, mad = 23.6040
2024-01-18 03:32:10,832 - __main__ - INFO - Fold 1 Epoch 23 Batch 0: Train Loss = 0.3344
2024-01-18 03:32:25,755 - __main__ - INFO - Fold 1, mse = 861.6811, mad = 23.4344
2024-01-18 03:32:26,344 - __main__ - INFO - Fold 1 Epoch 24 Batch 0: Train Loss = 0.2982
2024-01-18 03:32:40,985 - __main__ - INFO - Fold 1, mse = 833.0036, mad = 22.9129
2024-01-18 03:32:41,556 - __main__ - INFO - Fold 1 Epoch 25 Batch 0: Train Loss = 0.2969
2024-01-18 03:32:55,486 - __main__ - INFO - Fold 1, mse = 831.0172, mad = 22.9113
2024-01-18 03:32:56,088 - __main__ - INFO - Fold 1 Epoch 26 Batch 0: Train Loss = 0.2768
2024-01-18 03:33:10,418 - __main__ - INFO - Fold 1, mse = 855.0684, mad = 23.3043
2024-01-18 03:33:11,003 - __main__ - INFO - Fold 1 Epoch 27 Batch 0: Train Loss = 0.2418
2024-01-18 03:33:25,175 - __main__ - INFO - Fold 1, mse = 864.3920, mad = 23.3731
2024-01-18 03:33:25,837 - __main__ - INFO - Fold 1 Epoch 28 Batch 0: Train Loss = 0.2079
2024-01-18 03:33:41,151 - __main__ - INFO - Fold 1, mse = 860.4450, mad = 23.3459
2024-01-18 03:33:41,804 - __main__ - INFO - Fold 1 Epoch 29 Batch 0: Train Loss = 0.2469
2024-01-18 03:33:56,975 - __main__ - INFO - Fold 1, mse = 873.9788, mad = 23.4116
2024-01-18 03:33:57,867 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 2.6445
2024-01-18 03:34:14,508 - __main__ - INFO - Fold 2, epoch 0: Loss = 1.4270 Valid loss = 1.0392 MSE = 737.2733
2024-01-18 03:34:14,509 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 737.2733 ------------
2024-01-18 03:34:14,656 - __main__ - INFO - Fold 2, mse = 737.2733, mad = 22.6556
2024-01-18 03:34:15,219 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 0.9878
2024-01-18 03:34:32,913 - __main__ - INFO - Fold 2, mse = 769.6189, mad = 21.6456
2024-01-18 03:34:33,564 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 0.8441
2024-01-18 03:34:48,977 - __main__ - INFO - Fold 2, mse = 783.3700, mad = 21.6879
2024-01-18 03:34:49,621 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 0.7608
2024-01-18 03:35:04,604 - __main__ - INFO - Fold 2, mse = 779.8354, mad = 21.6787
2024-01-18 03:35:05,198 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 0.8530
2024-01-18 03:35:21,395 - __main__ - INFO - Fold 2, mse = 764.7247, mad = 21.6379
2024-01-18 03:35:22,121 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 0.9305
2024-01-18 03:35:38,559 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 734.9472 ------------
2024-01-18 03:35:38,733 - __main__ - INFO - Fold 2, mse = 734.9472, mad = 21.5428
2024-01-18 03:35:39,382 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 0.6909
2024-01-18 03:35:55,532 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 719.5872 ------------
2024-01-18 03:35:55,740 - __main__ - INFO - Fold 2, mse = 719.5872, mad = 21.4634
2024-01-18 03:35:56,432 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 0.8138
2024-01-18 03:36:12,308 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 717.7732 ------------
2024-01-18 03:36:12,452 - __main__ - INFO - Fold 2, mse = 717.7732, mad = 21.3553
2024-01-18 03:36:13,088 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 0.7135
2024-01-18 03:36:29,471 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 702.1456 ------------
2024-01-18 03:36:29,683 - __main__ - INFO - Fold 2, mse = 702.1456, mad = 21.2052
2024-01-18 03:36:30,317 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 0.7819
2024-01-18 03:36:46,062 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 690.7182 ------------
2024-01-18 03:36:46,231 - __main__ - INFO - Fold 2, mse = 690.7182, mad = 21.1215
2024-01-18 03:36:46,807 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 0.6917
2024-01-18 03:37:01,447 - __main__ - INFO - Fold 2, epoch 10: Loss = 0.7243 Valid loss = 0.9703 MSE = 688.4260
2024-01-18 03:37:01,448 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 688.4260 ------------
2024-01-18 03:37:01,579 - __main__ - INFO - Fold 2, mse = 688.4260, mad = 21.0907
2024-01-18 03:37:02,194 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 0.6333
2024-01-18 03:37:18,103 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 670.8719 ------------
2024-01-18 03:37:18,231 - __main__ - INFO - Fold 2, mse = 670.8719, mad = 20.8313
2024-01-18 03:37:18,937 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 0.7514
2024-01-18 03:37:33,963 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 670.8063 ------------
2024-01-18 03:37:34,124 - __main__ - INFO - Fold 2, mse = 670.8063, mad = 20.6298
2024-01-18 03:37:34,787 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 0.6565
2024-01-18 03:37:49,907 - __main__ - INFO - Fold 2, mse = 683.9297, mad = 20.6232
2024-01-18 03:37:50,523 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 0.3913
2024-01-18 03:38:05,313 - __main__ - INFO - Fold 2, mse = 700.0893, mad = 20.9088
2024-01-18 03:38:05,851 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 0.5092
2024-01-18 03:38:21,590 - __main__ - INFO - Fold 2, mse = 709.2501, mad = 20.9092
2024-01-18 03:38:22,191 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 0.4356
2024-01-18 03:38:37,545 - __main__ - INFO - Fold 2, mse = 710.3388, mad = 20.7865
2024-01-18 03:38:38,120 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 0.4662
2024-01-18 03:38:54,096 - __main__ - INFO - Fold 2, mse = 707.7830, mad = 20.6780
2024-01-18 03:38:54,813 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 0.4778
2024-01-18 03:39:11,769 - __main__ - INFO - Fold 2, mse = 726.6997, mad = 20.8823
2024-01-18 03:39:12,422 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 0.3651
2024-01-18 03:39:28,242 - __main__ - INFO - Fold 2, mse = 721.2229, mad = 20.6962
2024-01-18 03:39:28,924 - __main__ - INFO - Fold 2 Epoch 20 Batch 0: Train Loss = 0.3552
2024-01-18 03:39:44,950 - __main__ - INFO - Fold 2, epoch 20: Loss = 0.3754 Valid loss = 1.0518 MSE = 746.2702
2024-01-18 03:39:44,951 - __main__ - INFO - Fold 2, mse = 746.2702, mad = 20.9172
2024-01-18 03:39:45,474 - __main__ - INFO - Fold 2 Epoch 21 Batch 0: Train Loss = 0.4103
2024-01-18 03:40:01,671 - __main__ - INFO - Fold 2, mse = 778.7477, mad = 21.3697
2024-01-18 03:40:02,223 - __main__ - INFO - Fold 2 Epoch 22 Batch 0: Train Loss = 0.3462
2024-01-18 03:40:17,607 - __main__ - INFO - Fold 2, mse = 798.7776, mad = 21.6681
2024-01-18 03:40:18,229 - __main__ - INFO - Fold 2 Epoch 23 Batch 0: Train Loss = 0.3346
2024-01-18 03:40:33,174 - __main__ - INFO - Fold 2, mse = 794.3725, mad = 21.5618
2024-01-18 03:40:33,801 - __main__ - INFO - Fold 2 Epoch 24 Batch 0: Train Loss = 0.3181
2024-01-18 03:40:48,452 - __main__ - INFO - Fold 2, mse = 792.5855, mad = 21.6886
2024-01-18 03:40:49,048 - __main__ - INFO - Fold 2 Epoch 25 Batch 0: Train Loss = 0.2844
2024-01-18 03:41:03,763 - __main__ - INFO - Fold 2, mse = 819.3910, mad = 21.9756
2024-01-18 03:41:04,350 - __main__ - INFO - Fold 2 Epoch 26 Batch 0: Train Loss = 0.2517
2024-01-18 03:41:20,654 - __main__ - INFO - Fold 2, mse = 859.0794, mad = 22.5626
2024-01-18 03:41:21,217 - __main__ - INFO - Fold 2 Epoch 27 Batch 0: Train Loss = 0.2634
2024-01-18 03:41:37,244 - __main__ - INFO - Fold 2, mse = 852.6183, mad = 22.5004
2024-01-18 03:41:37,928 - __main__ - INFO - Fold 2 Epoch 28 Batch 0: Train Loss = 0.2496
2024-01-18 03:41:56,544 - __main__ - INFO - Fold 2, mse = 876.9011, mad = 22.7516
2024-01-18 03:41:57,335 - __main__ - INFO - Fold 2 Epoch 29 Batch 0: Train Loss = 0.3164
2024-01-18 03:42:14,631 - __main__ - INFO - Fold 2, mse = 850.1002, mad = 22.4512
2024-01-18 03:42:15,545 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 2.6640
2024-01-18 03:42:33,494 - __main__ - INFO - Fold 3, epoch 0: Loss = 1.3494 Valid loss = 1.0228 MSE = 724.6376
2024-01-18 03:42:33,495 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 724.6376 ------------
2024-01-18 03:42:33,656 - __main__ - INFO - Fold 3, mse = 724.6376, mad = 21.8075
2024-01-18 03:42:34,494 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 0.9661
2024-01-18 03:42:52,124 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 721.8136 ------------
2024-01-18 03:42:52,292 - __main__ - INFO - Fold 3, mse = 721.8136, mad = 21.9030
2024-01-18 03:42:53,050 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 0.8726
2024-01-18 03:43:09,660 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 720.9435 ------------
2024-01-18 03:43:09,850 - __main__ - INFO - Fold 3, mse = 720.9435, mad = 22.1278
2024-01-18 03:43:10,555 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 0.9505
2024-01-18 03:43:27,063 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 719.6456 ------------
2024-01-18 03:43:27,266 - __main__ - INFO - Fold 3, mse = 719.6456, mad = 22.1343
2024-01-18 03:43:27,939 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 1.0717
2024-01-18 03:43:45,873 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 716.4781 ------------
2024-01-18 03:43:46,042 - __main__ - INFO - Fold 3, mse = 716.4781, mad = 21.9964
2024-01-18 03:43:46,736 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 0.8654
2024-01-18 03:44:03,974 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 714.0709 ------------
2024-01-18 03:44:04,180 - __main__ - INFO - Fold 3, mse = 714.0709, mad = 21.8901
2024-01-18 03:44:04,891 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 1.0559
2024-01-18 03:44:22,893 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 711.7580 ------------
2024-01-18 03:44:23,134 - __main__ - INFO - Fold 3, mse = 711.7580, mad = 21.8063
2024-01-18 03:44:23,976 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 0.9486
2024-01-18 03:44:41,378 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 708.8670 ------------
2024-01-18 03:44:41,597 - __main__ - INFO - Fold 3, mse = 708.8670, mad = 21.7254
2024-01-18 03:44:42,400 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 0.7991
2024-01-18 03:45:01,110 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 705.5631 ------------
2024-01-18 03:45:01,327 - __main__ - INFO - Fold 3, mse = 705.5631, mad = 21.5009
2024-01-18 03:45:02,092 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 0.8828
2024-01-18 03:45:19,679 - __main__ - INFO - Fold 3, mse = 710.8039, mad = 21.3152
2024-01-18 03:45:20,567 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 0.8726
2024-01-18 03:45:38,546 - __main__ - INFO - Fold 3, epoch 10: Loss = 0.8159 Valid loss = 1.0273 MSE = 727.5251
2024-01-18 03:45:38,549 - __main__ - INFO - Fold 3, mse = 727.5251, mad = 21.4077
2024-01-18 03:45:39,381 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 0.9238
2024-01-18 03:45:55,934 - __main__ - INFO - Fold 3, mse = 726.3713, mad = 21.4669
2024-01-18 03:45:56,641 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 0.6123
2024-01-18 03:46:14,765 - __main__ - INFO - Fold 3, mse = 760.2475, mad = 22.0157
2024-01-18 03:46:15,471 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 0.5960
2024-01-18 03:46:31,968 - __main__ - INFO - Fold 3, mse = 818.5775, mad = 22.8582
2024-01-18 03:46:32,839 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 0.5106
2024-01-18 03:46:50,908 - __main__ - INFO - Fold 3, mse = 788.2917, mad = 22.3087
2024-01-18 03:46:51,568 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 0.4455
2024-01-18 03:47:08,981 - __main__ - INFO - Fold 3, mse = 823.9479, mad = 22.7401
2024-01-18 03:47:09,739 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 0.4393
2024-01-18 03:47:26,630 - __main__ - INFO - Fold 3, mse = 862.2367, mad = 23.1083
2024-01-18 03:47:27,432 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 0.4131
2024-01-18 03:47:45,153 - __main__ - INFO - Fold 3, mse = 892.6720, mad = 23.4604
2024-01-18 03:47:45,938 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 0.3795
2024-01-18 03:48:03,272 - __main__ - INFO - Fold 3, mse = 921.5254, mad = 23.7654
2024-01-18 03:48:03,929 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 0.3117
2024-01-18 03:48:21,710 - __main__ - INFO - Fold 3, mse = 935.4032, mad = 23.7726
2024-01-18 03:48:22,520 - __main__ - INFO - Fold 3 Epoch 20 Batch 0: Train Loss = 0.3195
2024-01-18 03:48:41,164 - __main__ - INFO - Fold 3, epoch 20: Loss = 0.3059 Valid loss = 1.3202 MSE = 931.2289
2024-01-18 03:48:41,165 - __main__ - INFO - Fold 3, mse = 931.2289, mad = 23.7626
2024-01-18 03:48:42,112 - __main__ - INFO - Fold 3 Epoch 21 Batch 0: Train Loss = 0.2935
2024-01-18 03:48:59,434 - __main__ - INFO - Fold 3, mse = 944.6989, mad = 23.7364
2024-01-18 03:49:00,099 - __main__ - INFO - Fold 3 Epoch 22 Batch 0: Train Loss = 0.2917
2024-01-18 03:49:17,308 - __main__ - INFO - Fold 3, mse = 978.9594, mad = 24.0490
2024-01-18 03:49:18,053 - __main__ - INFO - Fold 3 Epoch 23 Batch 0: Train Loss = 0.2570
2024-01-18 03:49:34,547 - __main__ - INFO - Fold 3, mse = 987.8535, mad = 24.1416
2024-01-18 03:49:35,358 - __main__ - INFO - Fold 3 Epoch 24 Batch 0: Train Loss = 0.2677
2024-01-18 03:49:54,337 - __main__ - INFO - Fold 3, mse = 977.7295, mad = 24.1109
2024-01-18 03:49:55,044 - __main__ - INFO - Fold 3 Epoch 25 Batch 0: Train Loss = 0.2588
2024-01-18 03:50:12,926 - __main__ - INFO - Fold 3, mse = 975.9726, mad = 24.2607
2024-01-18 03:50:13,596 - __main__ - INFO - Fold 3 Epoch 26 Batch 0: Train Loss = 0.2732
2024-01-18 03:50:31,502 - __main__ - INFO - Fold 3, mse = 1008.9216, mad = 24.6090
2024-01-18 03:50:32,282 - __main__ - INFO - Fold 3 Epoch 27 Batch 0: Train Loss = 0.2482
2024-01-18 03:50:49,938 - __main__ - INFO - Fold 3, mse = 1016.4365, mad = 24.4728
2024-01-18 03:50:50,648 - __main__ - INFO - Fold 3 Epoch 28 Batch 0: Train Loss = 0.2561
2024-01-18 03:51:09,756 - __main__ - INFO - Fold 3, mse = 990.5908, mad = 24.3249
2024-01-18 03:51:10,420 - __main__ - INFO - Fold 3 Epoch 29 Batch 0: Train Loss = 0.2552
2024-01-18 03:51:29,923 - __main__ - INFO - Fold 3, mse = 1004.1059, mad = 24.4932
2024-01-18 03:51:30,917 - __main__ - INFO - Fold 4 Epoch 0 Batch 0: Train Loss = 2.8481
2024-01-18 03:51:48,283 - __main__ - INFO - Fold 4, epoch 0: Loss = 1.8876 Valid loss = 1.0460 MSE = 741.5236
2024-01-18 03:51:48,284 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 741.5236 ------------
2024-01-18 03:51:48,499 - __main__ - INFO - Fold 4, mse = 741.5236, mad = 23.1289
2024-01-18 03:51:49,205 - __main__ - INFO - Fold 4 Epoch 1 Batch 0: Train Loss = 1.2613
2024-01-18 03:52:06,023 - __main__ - INFO - Fold 4, mse = 770.9723, mad = 23.8464
2024-01-18 03:52:06,714 - __main__ - INFO - Fold 4 Epoch 2 Batch 0: Train Loss = 1.2348
2024-01-18 03:52:23,466 - __main__ - INFO - Fold 4, mse = 765.9230, mad = 23.7506
2024-01-18 03:52:24,177 - __main__ - INFO - Fold 4 Epoch 3 Batch 0: Train Loss = 1.1531
2024-01-18 03:52:40,443 - __main__ - INFO - Fold 4, mse = 749.1766, mad = 23.4224
2024-01-18 03:52:41,066 - __main__ - INFO - Fold 4 Epoch 4 Batch 0: Train Loss = 1.1900
2024-01-18 03:52:57,774 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 729.8496 ------------
2024-01-18 03:52:58,033 - __main__ - INFO - Fold 4, mse = 729.8496, mad = 23.0587
2024-01-18 03:52:58,836 - __main__ - INFO - Fold 4 Epoch 5 Batch 0: Train Loss = 1.0868
2024-01-18 03:53:15,911 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 683.6904 ------------
2024-01-18 03:53:16,093 - __main__ - INFO - Fold 4, mse = 683.6904, mad = 22.0865
2024-01-18 03:53:16,912 - __main__ - INFO - Fold 4 Epoch 6 Batch 0: Train Loss = 1.0522
2024-01-18 03:53:34,584 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 643.4372 ------------
2024-01-18 03:53:34,858 - __main__ - INFO - ------------ Save best model - MSE: 643.4372 ------------
2024-01-18 03:53:34,859 - __main__ - INFO - Fold 4, mse = 643.4372, mad = 21.1200
2024-01-18 03:53:35,645 - __main__ - INFO - Fold 4 Epoch 7 Batch 0: Train Loss = 1.0793
2024-01-18 03:53:53,216 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 637.2630 ------------
2024-01-18 03:53:53,490 - __main__ - INFO - ------------ Save best model - MSE: 637.2630 ------------
2024-01-18 03:53:53,492 - __main__ - INFO - Fold 4, mse = 637.2630, mad = 20.9177
2024-01-18 03:53:54,382 - __main__ - INFO - Fold 4 Epoch 8 Batch 0: Train Loss = 0.8313
2024-01-18 03:54:14,338 - __main__ - INFO - Fold 4, mse = 642.1958, mad = 20.7418
2024-01-18 03:54:15,108 - __main__ - INFO - Fold 4 Epoch 9 Batch 0: Train Loss = 0.9104
2024-01-18 03:54:32,633 - __main__ - INFO - Fold 4, mse = 666.5009, mad = 20.7978
2024-01-18 03:54:33,454 - __main__ - INFO - Fold 4 Epoch 10 Batch 0: Train Loss = 0.6782
2024-01-18 03:54:50,571 - __main__ - INFO - Fold 4, epoch 10: Loss = 0.7078 Valid loss = 0.9887 MSE = 701.2426
2024-01-18 03:54:50,572 - __main__ - INFO - Fold 4, mse = 701.2426, mad = 21.3428
2024-01-18 03:54:51,295 - __main__ - INFO - Fold 4 Epoch 11 Batch 0: Train Loss = 0.6081
2024-01-18 03:55:09,673 - __main__ - INFO - Fold 4, mse = 688.9318, mad = 21.0896
2024-01-18 03:55:10,358 - __main__ - INFO - Fold 4 Epoch 12 Batch 0: Train Loss = 0.5635
2024-01-18 03:55:27,439 - __main__ - INFO - Fold 4, mse = 700.8806, mad = 20.9620
2024-01-18 03:55:28,201 - __main__ - INFO - Fold 4 Epoch 13 Batch 0: Train Loss = 0.5279
2024-01-18 03:55:44,517 - __main__ - INFO - Fold 4, mse = 710.3409, mad = 21.1476
2024-01-18 03:55:45,344 - __main__ - INFO - Fold 4 Epoch 14 Batch 0: Train Loss = 0.4321
2024-01-18 03:56:02,955 - __main__ - INFO - Fold 4, mse = 727.2381, mad = 21.3203
2024-01-18 03:56:03,749 - __main__ - INFO - Fold 4 Epoch 15 Batch 0: Train Loss = 0.4377
2024-01-18 03:56:20,035 - __main__ - INFO - Fold 4, mse = 740.9418, mad = 21.5374
2024-01-18 03:56:20,913 - __main__ - INFO - Fold 4 Epoch 16 Batch 0: Train Loss = 0.4204
2024-01-18 03:56:38,290 - __main__ - INFO - Fold 4, mse = 725.0348, mad = 21.3816
2024-01-18 03:56:38,970 - __main__ - INFO - Fold 4 Epoch 17 Batch 0: Train Loss = 0.3774
2024-01-18 03:56:57,647 - __main__ - INFO - Fold 4, mse = 706.6315, mad = 21.0327
2024-01-18 03:56:58,589 - __main__ - INFO - Fold 4 Epoch 18 Batch 0: Train Loss = 0.3533
2024-01-18 03:57:16,809 - __main__ - INFO - Fold 4, mse = 724.0982, mad = 21.4003
2024-01-18 03:57:17,576 - __main__ - INFO - Fold 4 Epoch 19 Batch 0: Train Loss = 0.3890
2024-01-18 03:57:35,119 - __main__ - INFO - Fold 4, mse = 766.1439, mad = 21.8446
2024-01-18 03:57:35,906 - __main__ - INFO - Fold 4 Epoch 20 Batch 0: Train Loss = 0.3982
2024-01-18 03:57:54,970 - __main__ - INFO - Fold 4, epoch 20: Loss = 0.3704 Valid loss = 1.0758 MSE = 761.6488
2024-01-18 03:57:54,971 - __main__ - INFO - Fold 4, mse = 761.6488, mad = 21.7244
2024-01-18 03:57:55,775 - __main__ - INFO - Fold 4 Epoch 21 Batch 0: Train Loss = 0.3362
2024-01-18 03:58:14,130 - __main__ - INFO - Fold 4, mse = 780.7475, mad = 21.9418
2024-01-18 03:58:14,832 - __main__ - INFO - Fold 4 Epoch 22 Batch 0: Train Loss = 0.2844
2024-01-18 03:58:31,634 - __main__ - INFO - Fold 4, mse = 764.7619, mad = 21.7556
2024-01-18 03:58:32,457 - __main__ - INFO - Fold 4 Epoch 23 Batch 0: Train Loss = 0.2815
2024-01-18 03:58:49,480 - __main__ - INFO - Fold 4, mse = 762.6568, mad = 21.7811
2024-01-18 03:58:50,246 - __main__ - INFO - Fold 4 Epoch 24 Batch 0: Train Loss = 0.2727
2024-01-18 03:59:08,299 - __main__ - INFO - Fold 4, mse = 761.3436, mad = 21.7778
2024-01-18 03:59:08,973 - __main__ - INFO - Fold 4 Epoch 25 Batch 0: Train Loss = 0.2529
2024-01-18 03:59:26,589 - __main__ - INFO - Fold 4, mse = 795.9733, mad = 22.2245
2024-01-18 03:59:27,298 - __main__ - INFO - Fold 4 Epoch 26 Batch 0: Train Loss = 0.3340
2024-01-18 03:59:45,215 - __main__ - INFO - Fold 4, mse = 820.4604, mad = 22.4192
2024-01-18 03:59:46,019 - __main__ - INFO - Fold 4 Epoch 27 Batch 0: Train Loss = 0.2765
2024-01-18 04:00:05,038 - __main__ - INFO - Fold 4, mse = 808.4759, mad = 22.1803
2024-01-18 04:00:05,927 - __main__ - INFO - Fold 4 Epoch 28 Batch 0: Train Loss = 0.2913
2024-01-18 04:00:24,366 - __main__ - INFO - Fold 4, mse = 828.6896, mad = 22.4564
2024-01-18 04:00:25,092 - __main__ - INFO - Fold 4 Epoch 29 Batch 0: Train Loss = 0.2077
2024-01-18 04:00:42,606 - __main__ - INFO - Fold 4, mse = 800.0361, mad = 22.1152
2024-01-18 04:00:43,618 - __main__ - INFO - Fold 5 Epoch 0 Batch 0: Train Loss = 11.3561
2024-01-18 04:01:02,407 - __main__ - INFO - Fold 5, epoch 0: Loss = 4.3324 Valid loss = 1.6638 MSE = 1195.8692
2024-01-18 04:01:02,408 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 1195.8692 ------------
2024-01-18 04:01:02,587 - __main__ - INFO - Fold 5, mse = 1195.8692, mad = 25.8908
2024-01-18 04:01:03,278 - __main__ - INFO - Fold 5 Epoch 1 Batch 0: Train Loss = 1.8537
2024-01-18 04:01:20,938 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 754.8935 ------------
2024-01-18 04:01:21,087 - __main__ - INFO - Fold 5, mse = 754.8935, mad = 23.2653
2024-01-18 04:01:21,850 - __main__ - INFO - Fold 5 Epoch 2 Batch 0: Train Loss = 1.4380
2024-01-18 04:01:38,443 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 697.9567 ------------
2024-01-18 04:01:38,617 - __main__ - INFO - Fold 5, mse = 697.9567, mad = 21.7714
2024-01-18 04:01:39,424 - __main__ - INFO - Fold 5 Epoch 3 Batch 0: Train Loss = 0.9988
2024-01-18 04:01:56,481 - __main__ - INFO - Fold 5, mse = 700.8994, mad = 21.8699
2024-01-18 04:01:57,099 - __main__ - INFO - Fold 5 Epoch 4 Batch 0: Train Loss = 1.0900
2024-01-18 04:02:13,743 - __main__ - INFO - Fold 5, mse = 702.1467, mad = 21.8981
2024-01-18 04:02:14,523 - __main__ - INFO - Fold 5 Epoch 5 Batch 0: Train Loss = 0.9397
2024-01-18 04:02:31,083 - __main__ - INFO - Fold 5, mse = 698.6875, mad = 21.7680
2024-01-18 04:02:31,877 - __main__ - INFO - Fold 5 Epoch 6 Batch 0: Train Loss = 0.9267
2024-01-18 04:02:48,654 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 693.3619 ------------
2024-01-18 04:02:48,884 - __main__ - INFO - Fold 5, mse = 693.3619, mad = 21.5670
2024-01-18 04:02:49,561 - __main__ - INFO - Fold 5 Epoch 7 Batch 0: Train Loss = 0.7954
2024-01-18 04:03:08,468 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 689.2944 ------------
2024-01-18 04:03:08,660 - __main__ - INFO - Fold 5, mse = 689.2944, mad = 21.3731
2024-01-18 04:03:09,411 - __main__ - INFO - Fold 5 Epoch 8 Batch 0: Train Loss = 0.9037
2024-01-18 04:03:27,436 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 684.1130 ------------
2024-01-18 04:03:27,661 - __main__ - INFO - Fold 5, mse = 684.1130, mad = 20.9975
2024-01-18 04:03:28,324 - __main__ - INFO - Fold 5 Epoch 9 Batch 0: Train Loss = 0.9205
2024-01-18 04:03:46,850 - __main__ - INFO - Fold 5, mse = 693.9452, mad = 20.8677
2024-01-18 04:03:47,633 - __main__ - INFO - Fold 5 Epoch 10 Batch 0: Train Loss = 0.7915
2024-01-18 04:04:05,483 - __main__ - INFO - Fold 5, epoch 10: Loss = 0.8543 Valid loss = 0.9932 MSE = 700.0370
2024-01-18 04:04:05,485 - __main__ - INFO - Fold 5, mse = 700.0370, mad = 20.9004
2024-01-18 04:04:06,251 - __main__ - INFO - Fold 5 Epoch 11 Batch 0: Train Loss = 0.9121
2024-01-18 04:04:23,840 - __main__ - INFO - Fold 5, mse = 690.1399, mad = 20.9925
2024-01-18 04:04:24,599 - __main__ - INFO - Fold 5 Epoch 12 Batch 0: Train Loss = 0.7213
2024-01-18 04:04:41,549 - __main__ - INFO - Fold 5, mse = 716.7672, mad = 21.0846
2024-01-18 04:04:42,257 - __main__ - INFO - Fold 5 Epoch 13 Batch 0: Train Loss = 0.6761
2024-01-18 04:04:58,457 - __main__ - INFO - Fold 5, mse = 729.4101, mad = 21.0784
2024-01-18 04:04:59,153 - __main__ - INFO - Fold 5 Epoch 14 Batch 0: Train Loss = 0.6201
2024-01-18 04:05:17,107 - __main__ - INFO - Fold 5, mse = 741.8029, mad = 21.3412
2024-01-18 04:05:17,885 - __main__ - INFO - Fold 5 Epoch 15 Batch 0: Train Loss = 0.5648
2024-01-18 04:05:34,308 - __main__ - INFO - Fold 5, mse = 761.2857, mad = 21.6739
2024-01-18 04:05:34,999 - __main__ - INFO - Fold 5 Epoch 16 Batch 0: Train Loss = 0.5203
2024-01-18 04:05:51,222 - __main__ - INFO - Fold 5, mse = 764.1130, mad = 21.6571
2024-01-18 04:05:51,892 - __main__ - INFO - Fold 5 Epoch 17 Batch 0: Train Loss = 0.5386
2024-01-18 04:06:09,234 - __main__ - INFO - Fold 5, mse = 775.1615, mad = 21.7558
2024-01-18 04:06:09,944 - __main__ - INFO - Fold 5 Epoch 18 Batch 0: Train Loss = 0.4424
2024-01-18 04:06:28,233 - __main__ - INFO - Fold 5, mse = 798.7573, mad = 21.9508
2024-01-18 04:06:29,062 - __main__ - INFO - Fold 5 Epoch 19 Batch 0: Train Loss = 0.4241
2024-01-18 04:06:46,792 - __main__ - INFO - Fold 5, mse = 799.2299, mad = 21.9380
2024-01-18 04:06:47,465 - __main__ - INFO - Fold 5 Epoch 20 Batch 0: Train Loss = 0.4088
2024-01-18 04:07:06,245 - __main__ - INFO - Fold 5, epoch 20: Loss = 0.4260 Valid loss = 1.1265 MSE = 799.6188
2024-01-18 04:07:06,248 - __main__ - INFO - Fold 5, mse = 799.6188, mad = 22.0401
2024-01-18 04:07:07,026 - __main__ - INFO - Fold 5 Epoch 21 Batch 0: Train Loss = 0.4254
2024-01-18 04:07:25,115 - __main__ - INFO - Fold 5, mse = 816.7109, mad = 22.2302
2024-01-18 04:07:25,814 - __main__ - INFO - Fold 5 Epoch 22 Batch 0: Train Loss = 0.4364
2024-01-18 04:07:44,027 - __main__ - INFO - Fold 5, mse = 809.1118, mad = 22.1246
2024-01-18 04:07:44,735 - __main__ - INFO - Fold 5 Epoch 23 Batch 0: Train Loss = 0.4139
2024-01-18 04:08:01,593 - __main__ - INFO - Fold 5, mse = 842.9683, mad = 22.5006
2024-01-18 04:08:02,292 - __main__ - INFO - Fold 5 Epoch 24 Batch 0: Train Loss = 0.3950
2024-01-18 04:08:18,766 - __main__ - INFO - Fold 5, mse = 858.6816, mad = 22.7302
2024-01-18 04:08:19,595 - __main__ - INFO - Fold 5 Epoch 25 Batch 0: Train Loss = 0.3776
2024-01-18 04:08:36,909 - __main__ - INFO - Fold 5, mse = 880.3399, mad = 22.9806
2024-01-18 04:08:37,675 - __main__ - INFO - Fold 5 Epoch 26 Batch 0: Train Loss = 0.3071
2024-01-18 04:08:55,630 - __main__ - INFO - Fold 5, mse = 896.8445, mad = 23.2178
2024-01-18 04:08:56,291 - __main__ - INFO - Fold 5 Epoch 27 Batch 0: Train Loss = 0.3289
2024-01-18 04:09:14,584 - __main__ - INFO - Fold 5, mse = 906.6051, mad = 23.3868
2024-01-18 04:09:15,333 - __main__ - INFO - Fold 5 Epoch 28 Batch 0: Train Loss = 0.3481
2024-01-18 04:09:32,146 - __main__ - INFO - Fold 5, mse = 926.1284, mad = 23.5724
2024-01-18 04:09:32,887 - __main__ - INFO - Fold 5 Epoch 29 Batch 0: Train Loss = 0.3005
2024-01-18 04:09:50,394 - __main__ - INFO - Fold 5, mse = 939.5606, mad = 23.6715
2024-01-18 04:09:50,396 - __main__ - INFO - mse 669.9416(23.9232)
2024-01-18 04:09:50,397 - __main__ - INFO - mad 20.9447(0.3109)
