2023-10-23 15:20:16,383 - __main__ - INFO - 这是希望输出的info内容
2023-10-23 15:20:16,385 - __main__ - WARNING - 这是希望输出的warning内容
2023-10-23 15:20:39,723 - __main__ - INFO - 32269
2023-10-23 15:20:39,725 - __main__ - INFO - 4034
2023-10-23 15:20:39,726 - __main__ - INFO - 4033
2023-10-23 15:20:45,875 - __main__ - INFO - last saved model is in epoch 115
2023-10-23 15:20:46,323 - __main__ - INFO - Batch 0: Test Loss = 0.0931
2023-10-23 15:20:49,283 - __main__ - INFO - 
==>Predicting on test
2023-10-23 15:20:49,285 - __main__ - INFO - Test Loss = 0.1227
2023-10-23 15:20:49,401 - __main__ - INFO - load target data
2023-10-23 15:20:49,817 - __main__ - INFO - Batch 0: Test Loss = 0.1131
2023-10-23 15:20:52,954 - __main__ - INFO - Batch 20: Test Loss = 0.1478
2023-10-23 15:20:56,080 - __main__ - INFO - Batch 40: Test Loss = 0.1526
2023-10-23 15:20:59,169 - __main__ - INFO - Batch 60: Test Loss = 0.1207
2023-10-23 15:21:02,196 - __main__ - INFO - Batch 80: Test Loss = 0.0851
2023-10-23 15:21:06,780 - __main__ - INFO - Batch 100: Test Loss = 0.0724
2023-10-23 15:21:10,606 - __main__ - INFO - Batch 120: Test Loss = 0.1097
2023-10-23 15:21:11,448 - __main__ - INFO - Training Student
2023-10-23 15:21:12,150 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 1.6849
2023-10-23 15:21:23,514 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 1.4762
2023-10-23 15:21:34,493 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 1.5211
2023-10-23 15:21:43,290 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 1.4911
2023-10-23 15:21:52,242 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 1.5139
2023-10-23 15:22:01,249 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 1.4713
2023-10-23 15:22:10,194 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 1.4593
2023-10-23 15:22:13,259 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4509 ------------
2023-10-23 15:22:18,928 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 1.4813
2023-10-23 15:22:29,336 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 1.5291
2023-10-23 15:22:39,481 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 1.5144
2023-10-23 15:22:48,295 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 1.4965
2023-10-23 15:22:57,592 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 1.4626
2023-10-23 15:23:09,495 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 1.4665
2023-10-23 15:23:20,881 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 1.4742
2023-10-23 15:23:24,385 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4491 ------------
2023-10-23 15:23:30,279 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 1.4789
2023-10-23 15:23:42,181 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 1.4836
2023-10-23 15:23:54,239 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 1.4741
2023-10-23 15:24:05,847 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 1.4933
2023-10-23 15:24:18,728 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 1.4612
2023-10-23 15:24:31,958 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 1.4548
2023-10-23 15:24:42,841 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 1.4679
2023-10-23 15:24:45,571 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4430 ------------
2023-10-23 15:24:50,107 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 1.4724
2023-10-23 15:24:59,586 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 1.4734
2023-10-23 15:25:09,416 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 1.4751
2023-10-23 15:25:19,036 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 1.4873
2023-10-23 15:25:29,489 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 1.4573
2023-10-23 15:25:40,553 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 1.4500
2023-10-23 15:25:51,192 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 1.4631
2023-10-23 15:25:54,762 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4429 ------------
2023-10-23 15:25:57,686 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4410 ------------
2023-10-23 15:25:59,585 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 1.4692
2023-10-23 15:26:11,526 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 1.4723
2023-10-23 15:26:22,334 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 1.4744
2023-10-23 15:26:34,211 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 1.4869
2023-10-23 15:26:46,495 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 1.4571
2023-10-23 15:26:58,347 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 1.4494
2023-10-23 15:27:10,134 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 1.4605
2023-10-23 15:27:13,722 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4394 ------------
2023-10-23 15:27:18,989 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 1.4669
2023-10-23 15:27:31,249 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 1.4728
2023-10-23 15:27:43,276 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 1.4754
2023-10-23 15:27:55,438 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 1.4866
2023-10-23 15:28:07,294 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 1.4561
2023-10-23 15:28:19,136 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 1.4484
2023-10-23 15:28:31,042 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 1.4603
2023-10-23 15:28:41,278 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 1.4684
2023-10-23 15:28:53,753 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 1.4727
2023-10-23 15:29:05,861 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 1.4744
2023-10-23 15:29:18,318 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 1.4845
2023-10-23 15:29:29,976 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 1.4557
2023-10-23 15:29:41,681 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 1.4484
2023-10-23 15:29:53,684 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 1.4624
2023-10-23 15:30:02,576 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 1.4673
2023-10-23 15:30:15,526 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 1.4713
2023-10-23 15:30:27,407 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 1.4756
2023-10-23 15:30:39,114 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 1.4858
2023-10-23 15:30:50,698 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 1.4532
2023-10-23 15:31:02,517 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 1.4464
2023-10-23 15:31:14,340 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 1.4600
2023-10-23 15:31:17,917 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4389 ------------
2023-10-23 15:31:24,516 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 1.4670
2023-10-23 15:31:36,519 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 1.4705
2023-10-23 15:31:48,675 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 1.4748
2023-10-23 15:32:00,439 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 1.4847
2023-10-23 15:32:12,300 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 1.4551
2023-10-23 15:32:24,167 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 1.4479
2023-10-23 15:32:36,163 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 1.4616
2023-10-23 15:32:45,224 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 1.4677
2023-10-23 15:32:58,396 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 1.4701
2023-10-23 15:33:11,021 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 1.4738
2023-10-23 15:33:22,412 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 1.4852
2023-10-23 15:33:33,229 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 1.4551
2023-10-23 15:33:44,098 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 1.4466
2023-10-23 15:33:56,235 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 1.4596
2023-10-23 15:34:06,279 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 1.4672
2023-10-23 15:34:19,361 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 1.4721
2023-10-23 15:34:31,680 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 1.4748
2023-10-23 15:34:43,344 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 1.4837
2023-10-23 15:34:55,935 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 1.4549
2023-10-23 15:35:07,740 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 1.4475
2023-10-23 15:35:20,234 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 1.4594
2023-10-23 15:35:24,151 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4387 ------------
2023-10-23 15:35:29,027 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 1.4658
2023-10-23 15:35:40,049 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 1.4707
2023-10-23 15:35:51,579 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 1.4743
2023-10-23 15:36:03,802 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 1.4842
2023-10-23 15:36:15,824 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 1.4542
2023-10-23 15:36:27,749 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 1.4464
2023-10-23 15:36:40,386 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 1.4603
2023-10-23 15:36:49,727 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 1.4672
2023-10-23 15:37:01,296 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 1.4708
2023-10-23 15:37:13,920 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 1.4726
2023-10-23 15:37:26,077 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 1.4834
2023-10-23 15:37:38,472 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 1.4529
2023-10-23 15:37:50,966 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 1.4456
2023-10-23 15:38:03,653 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 1.4590
2023-10-23 15:38:12,909 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 1.4661
2023-10-23 15:38:23,501 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 1.4702
2023-10-23 15:38:35,112 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 1.4728
2023-10-23 15:38:46,777 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 1.4831
2023-10-23 15:38:57,617 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 1.4524
2023-10-23 15:39:09,175 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 1.4459
2023-10-23 15:39:20,876 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 1.4600
2023-10-23 15:39:29,497 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 1.4657
2023-10-23 15:39:41,519 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 1.4688
2023-10-23 15:39:53,181 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 1.4739
2023-10-23 15:40:05,044 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 1.4838
2023-10-23 15:40:17,416 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 1.4528
2023-10-23 15:40:29,484 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 1.4467
2023-10-23 15:40:41,861 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 1.4614
2023-10-23 15:40:45,673 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4376 ------------
2023-10-23 15:40:49,158 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4370 ------------
2023-10-23 15:40:51,203 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 1.4662
2023-10-23 15:41:04,077 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 1.4697
2023-10-23 15:41:16,458 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 1.4730
2023-10-23 15:41:29,056 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 1.4850
2023-10-23 15:41:40,675 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 1.4535
2023-10-23 15:41:53,159 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 1.4466
2023-10-23 15:42:05,617 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 1.4598
2023-10-23 15:42:16,559 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 1.4666
2023-10-23 15:42:29,107 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 1.4699
2023-10-23 15:42:39,121 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 1.4730
2023-10-23 15:42:50,797 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 1.4831
2023-10-23 15:43:03,502 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 1.4549
2023-10-23 15:43:16,274 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 1.4456
2023-10-23 15:43:28,948 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 1.4588
2023-10-23 15:43:38,517 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 1.4666
2023-10-23 15:43:49,018 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 1.4697
2023-10-23 15:44:00,183 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 1.4727
2023-10-23 15:44:11,861 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 1.4843
2023-10-23 15:44:24,595 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 1.4532
2023-10-23 15:44:37,361 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 1.4482
2023-10-23 15:44:49,787 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 1.4604
2023-10-23 15:44:58,703 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 1.4650
2023-10-23 15:45:10,707 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 1.4693
2023-10-23 15:45:22,491 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 1.4728
2023-10-23 15:45:35,065 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 1.4864
2023-10-23 15:45:47,980 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 1.4533
2023-10-23 15:46:00,802 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 1.4472
2023-10-23 15:46:13,530 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 1.4592
2023-10-23 15:46:17,440 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4366 ------------
2023-10-23 15:46:24,011 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 1.4649
2023-10-23 15:46:35,735 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 1.4690
2023-10-23 15:46:48,225 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 1.4717
2023-10-23 15:46:59,325 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 1.4838
2023-10-23 15:47:11,274 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 1.4510
2023-10-23 15:47:24,052 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 1.4419
2023-10-23 15:47:36,517 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 1.4563
2023-10-23 15:47:39,894 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4361 ------------
2023-10-23 15:47:45,907 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 1.4633
2023-10-23 15:47:59,018 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 1.4703
2023-10-23 15:48:11,579 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 1.4727
2023-10-23 15:48:24,201 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 1.4818
2023-10-23 15:48:35,640 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 1.4502
2023-10-23 15:48:46,939 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 1.4486
2023-10-23 15:48:57,559 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 1.4596
2023-10-23 15:49:05,800 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 1.4642
2023-10-23 15:49:16,369 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 1.4697
2023-10-23 15:49:26,980 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 1.4725
2023-10-23 15:49:38,442 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 1.4831
2023-10-23 15:49:49,063 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 1.4518
2023-10-23 15:49:59,820 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 1.4432
2023-10-23 15:50:11,928 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 1.4592
2023-10-23 15:50:15,392 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4356 ------------
2023-10-23 15:50:21,229 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 1.4630
2023-10-23 15:50:33,751 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 1.4704
2023-10-23 15:50:46,326 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 1.4700
2023-10-23 15:50:58,402 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 1.4836
2023-10-23 15:51:09,318 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 1.4550
2023-10-23 15:51:20,039 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 1.4446
2023-10-23 15:51:32,428 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 1.4610
2023-10-23 15:51:41,615 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 1.4658
2023-10-23 15:51:54,470 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 1.4695
2023-10-23 15:52:07,025 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 1.4730
2023-10-23 15:52:19,749 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 1.4821
2023-10-23 15:52:32,520 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 1.4531
2023-10-23 15:52:45,272 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 1.4469
2023-10-23 15:52:58,280 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 1.4614
2023-10-23 15:53:07,193 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 1.4663
2023-10-23 15:53:19,861 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 1.4675
2023-10-23 15:53:32,416 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 1.4691
2023-10-23 15:53:45,300 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 1.4826
2023-10-23 15:53:58,226 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 1.4499
2023-10-23 15:54:10,265 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 1.4456
2023-10-23 15:54:22,660 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 1.4573
2023-10-23 15:54:31,814 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 1.4622
2023-10-23 15:54:44,826 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 1.4686
2023-10-23 15:54:55,426 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 1.4688
2023-10-23 15:55:07,451 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 1.4816
2023-10-23 15:55:20,200 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 1.4463
2023-10-23 15:55:32,986 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 1.4402
2023-10-23 15:55:45,682 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 1.4509
2023-10-23 15:55:49,586 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4238 ------------
2023-10-23 15:55:54,590 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 1.4515
2023-10-23 15:56:07,542 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 1.4514
2023-10-23 15:56:20,193 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 1.4618
2023-10-23 15:56:32,562 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 1.4775
2023-10-23 15:56:45,156 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 1.4474
2023-10-23 15:56:57,349 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 1.4262
2023-10-23 15:57:09,741 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 1.4384
2023-10-23 15:57:13,364 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4168 ------------
2023-10-23 15:57:16,342 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.4155 ------------
2023-10-23 15:57:19,111 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 1.4461
2023-10-23 15:57:32,249 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 1.4550
2023-10-23 15:57:44,221 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 1.4650
2023-10-23 15:57:56,503 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 1.4759
2023-10-23 15:58:10,044 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 1.4381
2023-10-23 15:58:23,056 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 1.4381
2023-10-23 15:58:35,751 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 1.4504
2023-10-23 15:58:45,419 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 1.4595
2023-10-23 15:58:58,450 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 1.4543
2023-10-23 15:59:11,320 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 1.4515
2023-10-23 15:59:23,964 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 1.4588
2023-10-23 15:59:36,373 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 1.4507
2023-10-23 15:59:48,001 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 1.4476
2023-10-23 15:59:59,996 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 1.4532
2023-10-23 16:00:10,449 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 1.4715
2023-10-23 16:00:22,699 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 1.4778
2023-10-23 16:00:35,003 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 1.4836
2023-10-23 16:00:47,609 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 1.4764
2023-10-23 16:01:00,378 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 1.4531
2023-10-23 16:01:13,163 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 1.4473
2023-10-23 16:01:25,990 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 1.4451
2023-10-23 16:01:35,782 - __main__ - INFO - last saved model is in epoch 26
2023-10-23 16:01:36,158 - __main__ - INFO - Batch 0: Test Loss = 0.3375
2023-10-23 16:01:39,587 - __main__ - INFO - 
==>Predicting on test
2023-10-23 16:01:39,589 - __main__ - INFO - Test Loss = 0.2609
2023-10-23 16:16:34,343 - __main__ - INFO - Transfer Target Dataset & Model
2023-10-23 16:16:34,398 - __main__ - INFO - [[-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8285543981909917, 2.770245567717861, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8374745525934485, 2.8093811444689463, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5338330207864587, 0.431156978499758, 0.8964660630930379, -1.342421748214616, -0.6946051308187712, -0.2120300841305121, 0.8952797203562032, 0.0387041303378994, -0.8612693268611251, 0.0726334784031089, -0.5051102137388988, -0.2865727201409924, 0.3988826585206329, -1.3794377690564883, -0.8237089728907875, -0.9221119476695248, -1.6036614531597553, -0.2379932071009605, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -1.1501193786706505, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, 1.0826051686869729, -0.609009148503521, 0.9684393533852332, -0.3880554131475662, 0.8439788318452395, 2.837917502516613, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 0.278477698357045, 1.2505677424119097, -0.6305581644917595, -0.9062745442328174, -0.7536814885080627, -0.4374103947888615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, -0.7710128163592748, -0.6738408269117502, -2.438483340619628, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8608899578998963, 2.912112033440545, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3396023676711391, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8763143915541441, 2.979783968239297, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5125273793649132, 0.601421850901989, 0.5034187451388209, -1.455529079732558, -0.5254345623915906, -0.4163803392884947, 0.7885904984786939, 0.2816638966057675, -0.5890172833864098, 0.5902661209826593, -0.3430637446868887, -0.7482466726318323, -0.1864257310498265, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.2249939001501796, -0.0341172340589738, 0.3466058957088718, 0.3482841380580905, -0.2890718868318484, 0.1923438589112976, -0.2031244129114749, -1.100153472115602, -0.7598199909551685, -0.6868858002659636, 0.3602180776283341, -0.700061185363224, -0.501359972189453, -0.2332733777972843, -1.36588558517574, 0.3571411263970316, -0.7447833078367583, 0.8765002281041955, 2.9805992927549454, 0.0776143028335215, -0.2769313825285214, 0.0755136678180621, -0.1474826847594624, -0.3999358135056357, -0.0169688050451549, -0.0937841116273902, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, -0.0122781725754626, -0.3292808750219816, -0.2769313825285214, -0.107625012968948, -0.1474826847594624, -0.5354516373428909, -0.0002229391773086, -0.0862088042532133, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, 0.0766269712424727, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.3951350096437179, -0.5239726809327129, -0.8378690925056301, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.8988006141103361, 3.0784382346326584, 0.0776143028335215, -0.3487779453829513, 0.070457637721598, -0.1474826847594624, -0.3999358135056357, 0.0045338857876385, -0.0729488956980065, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.1275947723114954, -0.4255010572823463, -0.3487779453829513, -0.1777070069371955, -0.1474826847594624, -0.5354516373428909, 0.029608437271828, -0.0566564538358881, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.0232601678654147, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.9106941533136118, 3.1306190036341057, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9108799898636633, 3.1314343281497536, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 1.0542620417323658, -0.6738408269117502, -0.0796833960305785, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9222160194167848, 3.1811691236042576, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.5107528363891513, 1.4769583166408096, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-1.038066534429697, 0.260892106097527, 0.1645848503507034, -1.455529079732558, -1.6391408045371951, 0.196670426185453, 1.2420196914581063, 0.4706326037029981, -0.6616178283130005, -0.5004598044528213, -0.4609157221792596, 0.021209914852902, -1.1769476210921423, 0.0909225332951111, -1.348247483817418, -1.2590833683251033, -0.9252694826312332, 0.781386658108973, -0.0798138719333467, -0.2032969502372001, -0.5728175028694968, 0.103242147293012, -0.2031244129114749, -0.6504603131201653, -0.5868915885967425, -0.6218024077192129, -0.0284729839577739, -0.3925684926321655, -0.4454382040900255, -0.2791761177909213, -1.5149673075505703, 0.3413397023846657, -0.8352815289623093, 0.9329945393197532, 3.228457945511819, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8995798651896526, 0.4879119359671683, 0.1510314945591795, -1.4131138304133295, -0.6523124887119757, 0.196670426185453, 0.148455167213642, 0.4706326037029981, -0.752368509471239, -0.9441449266638648, -0.9507380036319262, 0.1751012323498492, -1.0418764542681906, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -1.3629417216818924, 0.1697587389830128, -0.364093717028159, -0.2645837378255657, -0.2890718868318484, 0.0364158635792983, -0.2031244129114749, -1.8246591171638051, -0.4061028043129334, -0.5350245509902118, 0.0492652283594477, 0.0357249008146661, -0.4929416415078188, -0.5851943844151671, -1.170932563608653, 0.051337096981241, -0.8763418268751098, 0.9447022419729768, 3.279823389997619, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.962914223877992, 3.359725192531085, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.973321070680858, 3.405383365407351, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9735069072309092, 3.4061986899229986, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9980373318376644, 3.5138215259884835, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0340896225475933, 3.671994482024121, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.2639920224449301, -0.6738408269117502, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0396647190491284, 3.696454217493548, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7418059691584676, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, -0.6693833821778409, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.057505027854041, 3.774725370995719, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5792130238763558, -0.9062745442328174, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, 0.0888023142972107, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0760886828591592, 3.856257822560481, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]
2023-10-23 16:16:34,401 - __main__ - INFO - 69
2023-10-23 16:16:34,401 - __main__ - INFO - 325
2023-10-23 16:16:34,509 - __main__ - INFO - {'los_mean': 1055.0307777880782, 'los_std': 799.0879849276147}
2023-10-23 16:16:35,512 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 1.0611
2023-10-23 16:16:46,838 - __main__ - INFO - Fold 1, epoch 0: Loss = 1.0319 Valid loss = 1.0056 MSE = 703.0809
2023-10-23 16:16:46,839 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 703.0809 ------------
2023-10-23 16:16:47,204 - __main__ - INFO - ------------ Save best model - MSE: 703.0809 ------------
2023-10-23 16:16:47,205 - __main__ - INFO - Fold 1, mse = 703.0809, mad = 21.7183
2023-10-23 16:16:47,652 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 0.9571
2023-10-23 16:17:00,743 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 691.9372 ------------
2023-10-23 16:17:01,034 - __main__ - INFO - ------------ Save best model - MSE: 691.9372 ------------
2023-10-23 16:17:01,035 - __main__ - INFO - Fold 1, mse = 691.9372, mad = 21.2845
2023-10-23 16:17:01,801 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 0.8372
2023-10-23 16:17:14,847 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 687.4102 ------------
2023-10-23 16:17:15,104 - __main__ - INFO - ------------ Save best model - MSE: 687.4102 ------------
2023-10-23 16:17:15,105 - __main__ - INFO - Fold 1, mse = 687.4102, mad = 21.0609
2023-10-23 16:17:15,679 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 1.0354
2023-10-23 16:17:29,774 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 674.2789 ------------
2023-10-23 16:17:30,060 - __main__ - INFO - ------------ Save best model - MSE: 674.2789 ------------
2023-10-23 16:17:30,062 - __main__ - INFO - Fold 1, mse = 674.2789, mad = 21.0591
2023-10-23 16:17:30,618 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 0.6882
2023-10-23 16:17:43,759 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 666.2853 ------------
2023-10-23 16:17:44,086 - __main__ - INFO - ------------ Save best model - MSE: 666.2853 ------------
2023-10-23 16:17:44,089 - __main__ - INFO - Fold 1, mse = 666.2853, mad = 20.7691
2023-10-23 16:17:44,789 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 0.7129
2023-10-23 16:17:58,058 - __main__ - INFO - Fold 1, mse = 668.4437, mad = 20.9047
2023-10-23 16:17:58,652 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 0.7793
2023-10-23 16:18:13,065 - __main__ - INFO - Fold 1, mse = 670.1167, mad = 20.9031
2023-10-23 16:18:13,631 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 0.6941
2023-10-23 16:18:28,453 - __main__ - INFO - Fold 1, mse = 670.5960, mad = 20.8828
2023-10-23 16:18:29,135 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 0.8704
2023-10-23 16:18:42,854 - __main__ - INFO - Fold 1, mse = 685.2779, mad = 21.0866
2023-10-23 16:18:43,430 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 0.6940
2023-10-23 16:18:58,582 - __main__ - INFO - Fold 1, mse = 680.5184, mad = 21.1292
2023-10-23 16:18:59,214 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 0.7325
2023-10-23 16:19:13,433 - __main__ - INFO - Fold 1, epoch 10: Loss = 0.6905 Valid loss = 0.9795 MSE = 689.2506
2023-10-23 16:19:13,435 - __main__ - INFO - Fold 1, mse = 689.2506, mad = 21.0549
2023-10-23 16:19:14,135 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 0.6690
2023-10-23 16:19:27,907 - __main__ - INFO - Fold 1, mse = 691.1181, mad = 21.1254
2023-10-23 16:19:28,570 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 0.5297
2023-10-23 16:19:43,520 - __main__ - INFO - Fold 1, mse = 683.5336, mad = 21.1130
2023-10-23 16:19:44,108 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 0.6151
2023-10-23 16:19:57,471 - __main__ - INFO - Fold 1, mse = 715.0560, mad = 21.4478
2023-10-23 16:19:58,132 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 0.6513
2023-10-23 16:20:11,784 - __main__ - INFO - Fold 1, mse = 721.6930, mad = 21.5949
2023-10-23 16:20:12,480 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 0.6574
2023-10-23 16:20:26,455 - __main__ - INFO - Fold 1, mse = 686.9280, mad = 21.2926
2023-10-23 16:20:27,232 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 0.5637
2023-10-23 16:20:41,004 - __main__ - INFO - Fold 1, mse = 711.4383, mad = 21.4084
2023-10-23 16:20:41,749 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 0.6083
2023-10-23 16:20:56,861 - __main__ - INFO - Fold 1, mse = 698.8621, mad = 21.2560
2023-10-23 16:20:57,486 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 0.4493
2023-10-23 16:21:11,626 - __main__ - INFO - Fold 1, mse = 700.2648, mad = 21.5133
2023-10-23 16:21:12,186 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 0.5810
2023-10-23 16:21:25,528 - __main__ - INFO - Fold 1, mse = 738.7568, mad = 21.9997
2023-10-23 16:21:26,177 - __main__ - INFO - Fold 1 Epoch 20 Batch 0: Train Loss = 0.5916
2023-10-23 16:21:39,765 - __main__ - INFO - Fold 1, epoch 20: Loss = 0.4829 Valid loss = 1.0504 MSE = 732.5930
2023-10-23 16:21:39,767 - __main__ - INFO - Fold 1, mse = 732.5930, mad = 21.7758
2023-10-23 16:21:40,391 - __main__ - INFO - Fold 1 Epoch 21 Batch 0: Train Loss = 0.4793
2023-10-23 16:21:53,652 - __main__ - INFO - Fold 1, mse = 728.4355, mad = 21.6903
2023-10-23 16:21:54,387 - __main__ - INFO - Fold 1 Epoch 22 Batch 0: Train Loss = 0.4662
2023-10-23 16:22:08,973 - __main__ - INFO - Fold 1, mse = 732.7169, mad = 21.7440
2023-10-23 16:22:09,596 - __main__ - INFO - Fold 1 Epoch 23 Batch 0: Train Loss = 0.4907
2023-10-23 16:22:23,983 - __main__ - INFO - Fold 1, mse = 730.5012, mad = 21.7652
2023-10-23 16:22:24,546 - __main__ - INFO - Fold 1 Epoch 24 Batch 0: Train Loss = 0.4704
2023-10-23 16:22:38,063 - __main__ - INFO - Fold 1, mse = 766.3992, mad = 22.2410
2023-10-23 16:22:38,689 - __main__ - INFO - Fold 1 Epoch 25 Batch 0: Train Loss = 0.4473
2023-10-23 16:22:52,279 - __main__ - INFO - Fold 1, mse = 756.4798, mad = 22.2576
2023-10-23 16:22:52,937 - __main__ - INFO - Fold 1 Epoch 26 Batch 0: Train Loss = 0.4122
2023-10-23 16:23:07,154 - __main__ - INFO - Fold 1, mse = 755.8769, mad = 22.0292
2023-10-23 16:23:07,800 - __main__ - INFO - Fold 1 Epoch 27 Batch 0: Train Loss = 0.4634
2023-10-23 16:23:20,286 - __main__ - INFO - Fold 1, mse = 744.0913, mad = 21.9049
2023-10-23 16:23:20,712 - __main__ - INFO - Fold 1 Epoch 28 Batch 0: Train Loss = 0.4930
2023-10-23 16:23:31,870 - __main__ - INFO - Fold 1, mse = 777.3058, mad = 22.2437
2023-10-23 16:23:32,484 - __main__ - INFO - Fold 1 Epoch 29 Batch 0: Train Loss = 0.4153
2023-10-23 16:23:43,444 - __main__ - INFO - Fold 1, mse = 732.9905, mad = 21.9367
2023-10-23 16:23:44,033 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 1.2373
2023-10-23 16:23:55,146 - __main__ - INFO - Fold 2, epoch 0: Loss = 1.1005 Valid loss = 1.0246 MSE = 726.9417
2023-10-23 16:23:55,148 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 726.9417 ------------
2023-10-23 16:23:55,396 - __main__ - INFO - Fold 2, mse = 726.9417, mad = 22.2721
2023-10-23 16:23:55,942 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 0.9092
2023-10-23 16:24:08,861 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 723.3960 ------------
2023-10-23 16:24:09,029 - __main__ - INFO - Fold 2, mse = 723.3960, mad = 21.6832
2023-10-23 16:24:09,515 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 0.9304
2023-10-23 16:24:23,269 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 712.8263 ------------
2023-10-23 16:24:23,433 - __main__ - INFO - Fold 2, mse = 712.8263, mad = 21.6365
2023-10-23 16:24:24,030 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 0.8876
2023-10-23 16:24:35,840 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 706.6598 ------------
2023-10-23 16:24:36,036 - __main__ - INFO - Fold 2, mse = 706.6598, mad = 21.6283
2023-10-23 16:24:36,512 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 0.7230
2023-10-23 16:24:49,520 - __main__ - INFO - Fold 2, mse = 710.2478, mad = 21.5515
2023-10-23 16:24:49,983 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 0.8347
2023-10-23 16:25:02,421 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 697.6824 ------------
2023-10-23 16:25:02,594 - __main__ - INFO - Fold 2, mse = 697.6824, mad = 21.5265
2023-10-23 16:25:03,262 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 0.6693
2023-10-23 16:25:15,469 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 696.1523 ------------
2023-10-23 16:25:15,651 - __main__ - INFO - Fold 2, mse = 696.1523, mad = 21.4306
2023-10-23 16:25:16,164 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 0.6307
2023-10-23 16:25:28,114 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 691.7538 ------------
2023-10-23 16:25:28,257 - __main__ - INFO - Fold 2, mse = 691.7538, mad = 21.4745
2023-10-23 16:25:28,715 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 0.6314
2023-10-23 16:25:40,409 - __main__ - INFO - Fold 2, mse = 695.2687, mad = 21.5095
2023-10-23 16:25:40,820 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 0.6592
2023-10-23 16:25:53,247 - __main__ - INFO - Fold 2, mse = 711.1838, mad = 21.5599
2023-10-23 16:25:53,716 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 0.6167
2023-10-23 16:26:05,946 - __main__ - INFO - Fold 2, epoch 10: Loss = 0.6542 Valid loss = 1.0004 MSE = 709.5931
2023-10-23 16:26:05,949 - __main__ - INFO - Fold 2, mse = 709.5931, mad = 21.6053
2023-10-23 16:26:06,403 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 0.5973
2023-10-23 16:26:19,585 - __main__ - INFO - Fold 2, mse = 720.2664, mad = 21.9623
2023-10-23 16:26:20,125 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 0.5497
2023-10-23 16:26:31,347 - __main__ - INFO - Fold 2, mse = 731.6115, mad = 22.0019
2023-10-23 16:26:31,817 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 0.6976
2023-10-23 16:26:44,387 - __main__ - INFO - Fold 2, mse = 735.1507, mad = 22.0491
2023-10-23 16:26:44,966 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 0.5165
2023-10-23 16:26:56,878 - __main__ - INFO - Fold 2, mse = 734.8229, mad = 21.9966
2023-10-23 16:26:57,424 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 0.5430
2023-10-23 16:27:11,032 - __main__ - INFO - Fold 2, mse = 720.2294, mad = 21.8619
2023-10-23 16:27:11,503 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 0.5728
2023-10-23 16:27:23,600 - __main__ - INFO - Fold 2, mse = 767.9172, mad = 22.3614
2023-10-23 16:27:24,001 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 0.4653
2023-10-23 16:27:36,270 - __main__ - INFO - Fold 2, mse = 771.6602, mad = 22.5405
2023-10-23 16:27:36,745 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 0.4800
2023-10-23 16:27:50,416 - __main__ - INFO - Fold 2, mse = 753.4935, mad = 22.2407
2023-10-23 16:27:50,982 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 0.5666
2023-10-23 16:28:02,817 - __main__ - INFO - Fold 2, mse = 768.6409, mad = 22.3285
2023-10-23 16:28:03,243 - __main__ - INFO - Fold 2 Epoch 20 Batch 0: Train Loss = 0.5405
2023-10-23 16:28:16,615 - __main__ - INFO - Fold 2, epoch 20: Loss = 0.4575 Valid loss = 1.1070 MSE = 773.6713
2023-10-23 16:28:16,618 - __main__ - INFO - Fold 2, mse = 773.6713, mad = 22.5034
2023-10-23 16:28:17,237 - __main__ - INFO - Fold 2 Epoch 21 Batch 0: Train Loss = 0.5371
2023-10-23 16:28:30,609 - __main__ - INFO - Fold 2, mse = 753.1738, mad = 22.2112
2023-10-23 16:28:31,144 - __main__ - INFO - Fold 2 Epoch 22 Batch 0: Train Loss = 0.4361
2023-10-23 16:28:45,101 - __main__ - INFO - Fold 2, mse = 752.1015, mad = 22.1687
2023-10-23 16:28:45,565 - __main__ - INFO - Fold 2 Epoch 23 Batch 0: Train Loss = 0.4185
2023-10-23 16:28:56,656 - __main__ - INFO - Fold 2, mse = 788.1212, mad = 22.4716
2023-10-23 16:28:57,181 - __main__ - INFO - Fold 2 Epoch 24 Batch 0: Train Loss = 0.4350
2023-10-23 16:29:08,899 - __main__ - INFO - Fold 2, mse = 788.4786, mad = 22.6178
2023-10-23 16:29:09,522 - __main__ - INFO - Fold 2 Epoch 25 Batch 0: Train Loss = 0.4757
2023-10-23 16:29:21,612 - __main__ - INFO - Fold 2, mse = 771.3108, mad = 22.4009
2023-10-23 16:29:22,029 - __main__ - INFO - Fold 2 Epoch 26 Batch 0: Train Loss = 0.4598
2023-10-23 16:29:33,941 - __main__ - INFO - Fold 2, mse = 758.6881, mad = 21.9911
2023-10-23 16:29:34,369 - __main__ - INFO - Fold 2 Epoch 27 Batch 0: Train Loss = 0.5631
2023-10-23 16:29:47,360 - __main__ - INFO - Fold 2, mse = 760.3871, mad = 22.1199
2023-10-23 16:29:47,847 - __main__ - INFO - Fold 2 Epoch 28 Batch 0: Train Loss = 0.4045
2023-10-23 16:29:59,874 - __main__ - INFO - Fold 2, mse = 739.1905, mad = 21.8902
2023-10-23 16:30:00,299 - __main__ - INFO - Fold 2 Epoch 29 Batch 0: Train Loss = 0.4819
2023-10-23 16:30:13,153 - __main__ - INFO - Fold 2, mse = 767.9412, mad = 22.1524
2023-10-23 16:30:14,138 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 1.2266
2023-10-23 16:30:26,037 - __main__ - INFO - Fold 3, epoch 0: Loss = 1.0695 Valid loss = 1.0182 MSE = 718.0731
2023-10-23 16:30:26,039 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 718.0731 ------------
2023-10-23 16:30:26,267 - __main__ - INFO - Fold 3, mse = 718.0731, mad = 22.0435
2023-10-23 16:30:26,971 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 0.8454
2023-10-23 16:30:41,838 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 708.7006 ------------
2023-10-23 16:30:42,023 - __main__ - INFO - Fold 3, mse = 708.7006, mad = 21.9622
2023-10-23 16:30:42,600 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 0.8651
2023-10-23 16:30:57,006 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 703.7107 ------------
2023-10-23 16:30:57,224 - __main__ - INFO - Fold 3, mse = 703.7107, mad = 21.7318
2023-10-23 16:30:57,705 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 0.8409
2023-10-23 16:31:10,198 - __main__ - INFO - Fold 3, mse = 705.0126, mad = 21.7677
2023-10-23 16:31:10,895 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 0.8963
2023-10-23 16:31:23,218 - __main__ - INFO - Fold 3, mse = 714.0947, mad = 21.7384
2023-10-23 16:31:23,766 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 0.8414
2023-10-23 16:31:37,330 - __main__ - INFO - Fold 3, mse = 732.6003, mad = 22.1363
2023-10-23 16:31:37,994 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 0.7788
2023-10-23 16:31:52,505 - __main__ - INFO - Fold 3, mse = 748.9895, mad = 22.4596
2023-10-23 16:31:53,212 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 0.6243
2023-10-23 16:32:06,427 - __main__ - INFO - Fold 3, mse = 770.4573, mad = 22.7488
2023-10-23 16:32:07,176 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 0.6720
2023-10-23 16:32:21,549 - __main__ - INFO - Fold 3, mse = 764.7576, mad = 22.7620
2023-10-23 16:32:22,269 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 0.6966
2023-10-23 16:32:36,279 - __main__ - INFO - Fold 3, mse = 774.2924, mad = 22.7805
2023-10-23 16:32:36,952 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 0.6212
2023-10-23 16:32:51,732 - __main__ - INFO - Fold 3, epoch 10: Loss = 0.6133 Valid loss = 1.0899 MSE = 766.9304
2023-10-23 16:32:51,734 - __main__ - INFO - Fold 3, mse = 766.9304, mad = 22.8655
2023-10-23 16:32:52,335 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 0.6736
2023-10-23 16:33:07,031 - __main__ - INFO - Fold 3, mse = 788.2581, mad = 23.0325
2023-10-23 16:33:07,663 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 0.6210
2023-10-23 16:33:23,296 - __main__ - INFO - Fold 3, mse = 797.4051, mad = 23.0802
2023-10-23 16:33:23,979 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 0.5900
2023-10-23 16:33:38,671 - __main__ - INFO - Fold 3, mse = 831.3800, mad = 23.3699
2023-10-23 16:33:39,373 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 0.6304
2023-10-23 16:33:54,519 - __main__ - INFO - Fold 3, mse = 820.4564, mad = 23.0907
2023-10-23 16:33:55,219 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 0.5163
2023-10-23 16:34:10,699 - __main__ - INFO - Fold 3, mse = 803.7919, mad = 23.0216
2023-10-23 16:34:11,330 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 0.5277
2023-10-23 16:34:25,506 - __main__ - INFO - Fold 3, mse = 792.3839, mad = 22.6959
2023-10-23 16:34:26,091 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 0.4862
2023-10-23 16:34:40,071 - __main__ - INFO - Fold 3, mse = 820.2079, mad = 22.8874
2023-10-23 16:34:40,766 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 0.4354
2023-10-23 16:34:55,809 - __main__ - INFO - Fold 3, mse = 810.1563, mad = 22.8067
2023-10-23 16:34:56,370 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 0.4041
2023-10-23 16:35:10,211 - __main__ - INFO - Fold 3, mse = 803.6174, mad = 22.7280
2023-10-23 16:35:11,003 - __main__ - INFO - Fold 3 Epoch 20 Batch 0: Train Loss = 0.4818
2023-10-23 16:35:25,034 - __main__ - INFO - Fold 3, epoch 20: Loss = 0.4453 Valid loss = 1.1829 MSE = 815.2968
2023-10-23 16:35:25,036 - __main__ - INFO - Fold 3, mse = 815.2968, mad = 22.9045
2023-10-23 16:35:25,745 - __main__ - INFO - Fold 3 Epoch 21 Batch 0: Train Loss = 0.4902
2023-10-23 16:35:41,010 - __main__ - INFO - Fold 3, mse = 813.6929, mad = 22.8042
2023-10-23 16:35:41,705 - __main__ - INFO - Fold 3 Epoch 22 Batch 0: Train Loss = 0.3775
2023-10-23 16:35:55,689 - __main__ - INFO - Fold 3, mse = 818.1824, mad = 22.8033
2023-10-23 16:35:56,460 - __main__ - INFO - Fold 3 Epoch 23 Batch 0: Train Loss = 0.4279
2023-10-23 16:36:09,927 - __main__ - INFO - Fold 3, mse = 808.3017, mad = 22.7428
2023-10-23 16:36:10,620 - __main__ - INFO - Fold 3 Epoch 24 Batch 0: Train Loss = 0.3689
2023-10-23 16:36:25,318 - __main__ - INFO - Fold 3, mse = 811.0575, mad = 22.7575
2023-10-23 16:36:26,000 - __main__ - INFO - Fold 3 Epoch 25 Batch 0: Train Loss = 0.4235
2023-10-23 16:36:40,367 - __main__ - INFO - Fold 3, mse = 802.9375, mad = 22.5245
2023-10-23 16:36:40,984 - __main__ - INFO - Fold 3 Epoch 26 Batch 0: Train Loss = 0.4039
2023-10-23 16:36:55,134 - __main__ - INFO - Fold 3, mse = 816.8197, mad = 22.8226
2023-10-23 16:36:55,733 - __main__ - INFO - Fold 3 Epoch 27 Batch 0: Train Loss = 0.4211
2023-10-23 16:37:10,292 - __main__ - INFO - Fold 3, mse = 807.0252, mad = 22.6187
2023-10-23 16:37:10,963 - __main__ - INFO - Fold 3 Epoch 28 Batch 0: Train Loss = 0.4504
2023-10-23 16:37:24,872 - __main__ - INFO - Fold 3, mse = 787.3443, mad = 22.4602
2023-10-23 16:37:25,459 - __main__ - INFO - Fold 3 Epoch 29 Batch 0: Train Loss = 0.3767
2023-10-23 16:37:40,058 - __main__ - INFO - Fold 3, mse = 804.4231, mad = 22.6784
2023-10-23 16:37:40,875 - __main__ - INFO - Fold 4 Epoch 0 Batch 0: Train Loss = 1.5697
2023-10-23 16:37:53,342 - __main__ - INFO - Fold 4, epoch 0: Loss = 1.3381 Valid loss = 1.0086 MSE = 715.3669
2023-10-23 16:37:53,345 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 715.3669 ------------
2023-10-23 16:37:53,601 - __main__ - INFO - Fold 4, mse = 715.3669, mad = 22.4362
2023-10-23 16:37:54,058 - __main__ - INFO - Fold 4 Epoch 1 Batch 0: Train Loss = 1.2958
2023-10-23 16:38:05,908 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 671.6149 ------------
2023-10-23 16:38:06,189 - __main__ - INFO - Fold 4, mse = 671.6149, mad = 21.4743
2023-10-23 16:38:06,685 - __main__ - INFO - Fold 4 Epoch 2 Batch 0: Train Loss = 1.3043
2023-10-23 16:38:18,129 - __main__ - INFO - Fold 4, mse = 672.3301, mad = 21.6333
2023-10-23 16:38:18,577 - __main__ - INFO - Fold 4 Epoch 3 Batch 0: Train Loss = 1.2020
2023-10-23 16:38:30,802 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 660.2424 ------------
2023-10-23 16:38:31,186 - __main__ - INFO - ------------ Save best model - MSE: 660.2424 ------------
2023-10-23 16:38:31,188 - __main__ - INFO - Fold 4, mse = 660.2424, mad = 21.3754
2023-10-23 16:38:31,776 - __main__ - INFO - Fold 4 Epoch 4 Batch 0: Train Loss = 1.1388
2023-10-23 16:38:45,415 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 648.9993 ------------
2023-10-23 16:38:45,701 - __main__ - INFO - ------------ Save best model - MSE: 648.9993 ------------
2023-10-23 16:38:45,703 - __main__ - INFO - Fold 4, mse = 648.9993, mad = 21.1414
2023-10-23 16:38:46,325 - __main__ - INFO - Fold 4 Epoch 5 Batch 0: Train Loss = 1.0295
2023-10-23 16:38:58,387 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 636.6401 ------------
2023-10-23 16:38:58,702 - __main__ - INFO - ------------ Save best model - MSE: 636.6401 ------------
2023-10-23 16:38:58,705 - __main__ - INFO - Fold 4, mse = 636.6401, mad = 20.8310
2023-10-23 16:38:59,381 - __main__ - INFO - Fold 4 Epoch 6 Batch 0: Train Loss = 1.0099
2023-10-23 16:39:11,158 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 632.3076 ------------
2023-10-23 16:39:11,450 - __main__ - INFO - ------------ Save best model - MSE: 632.3076 ------------
2023-10-23 16:39:11,452 - __main__ - INFO - Fold 4, mse = 632.3076, mad = 20.6768
2023-10-23 16:39:12,048 - __main__ - INFO - Fold 4 Epoch 7 Batch 0: Train Loss = 0.8110
2023-10-23 16:39:25,833 - __main__ - INFO - Fold 4, mse = 645.4034, mad = 20.9132
2023-10-23 16:39:26,458 - __main__ - INFO - Fold 4 Epoch 8 Batch 0: Train Loss = 0.7846
2023-10-23 16:39:39,557 - __main__ - INFO - Fold 4, mse = 655.0443, mad = 20.9835
2023-10-23 16:39:40,041 - __main__ - INFO - Fold 4 Epoch 9 Batch 0: Train Loss = 0.6967
2023-10-23 16:39:53,041 - __main__ - INFO - Fold 4, mse = 649.7168, mad = 20.7098
2023-10-23 16:39:53,635 - __main__ - INFO - Fold 4 Epoch 10 Batch 0: Train Loss = 0.7270
2023-10-23 16:40:06,351 - __main__ - INFO - Fold 4, epoch 10: Loss = 0.7238 Valid loss = 0.9382 MSE = 665.2771
2023-10-23 16:40:06,352 - __main__ - INFO - Fold 4, mse = 665.2771, mad = 20.9902
2023-10-23 16:40:07,128 - __main__ - INFO - Fold 4 Epoch 11 Batch 0: Train Loss = 0.5911
2023-10-23 16:40:21,574 - __main__ - INFO - Fold 4, mse = 665.8847, mad = 20.8977
2023-10-23 16:40:22,048 - __main__ - INFO - Fold 4 Epoch 12 Batch 0: Train Loss = 0.6917
2023-10-23 16:40:36,279 - __main__ - INFO - Fold 4, mse = 670.8296, mad = 20.9996
2023-10-23 16:40:36,731 - __main__ - INFO - Fold 4 Epoch 13 Batch 0: Train Loss = 0.6764
2023-10-23 16:40:47,720 - __main__ - INFO - Fold 4, mse = 669.5463, mad = 20.8764
2023-10-23 16:40:48,177 - __main__ - INFO - Fold 4 Epoch 14 Batch 0: Train Loss = 0.7143
2023-10-23 16:41:00,394 - __main__ - INFO - Fold 4, mse = 690.8517, mad = 21.0457
2023-10-23 16:41:00,830 - __main__ - INFO - Fold 4 Epoch 15 Batch 0: Train Loss = 0.6123
2023-10-23 16:41:12,931 - __main__ - INFO - Fold 4, mse = 695.9310, mad = 21.2288
2023-10-23 16:41:13,695 - __main__ - INFO - Fold 4 Epoch 16 Batch 0: Train Loss = 0.6312
2023-10-23 16:41:25,531 - __main__ - INFO - Fold 4, mse = 700.7143, mad = 21.1957
2023-10-23 16:41:26,037 - __main__ - INFO - Fold 4 Epoch 17 Batch 0: Train Loss = 0.5786
2023-10-23 16:41:39,692 - __main__ - INFO - Fold 4, mse = 687.1907, mad = 21.1286
2023-10-23 16:41:40,273 - __main__ - INFO - Fold 4 Epoch 18 Batch 0: Train Loss = 0.5558
2023-10-23 16:41:52,250 - __main__ - INFO - Fold 4, mse = 689.2702, mad = 21.3482
2023-10-23 16:41:52,842 - __main__ - INFO - Fold 4 Epoch 19 Batch 0: Train Loss = 0.5257
2023-10-23 16:42:04,931 - __main__ - INFO - Fold 4, mse = 691.3767, mad = 21.1929
2023-10-23 16:42:05,490 - __main__ - INFO - Fold 4 Epoch 20 Batch 0: Train Loss = 0.4746
2023-10-23 16:42:17,597 - __main__ - INFO - Fold 4, epoch 20: Loss = 0.5422 Valid loss = 0.9786 MSE = 690.2420
2023-10-23 16:42:17,599 - __main__ - INFO - Fold 4, mse = 690.2420, mad = 21.1563
2023-10-23 16:42:18,061 - __main__ - INFO - Fold 4 Epoch 21 Batch 0: Train Loss = 0.5490
2023-10-23 16:42:30,098 - __main__ - INFO - Fold 4, mse = 667.2096, mad = 20.7848
2023-10-23 16:42:30,607 - __main__ - INFO - Fold 4 Epoch 22 Batch 0: Train Loss = 0.4955
2023-10-23 16:42:44,387 - __main__ - INFO - Fold 4, mse = 690.9374, mad = 21.2904
2023-10-23 16:42:44,916 - __main__ - INFO - Fold 4 Epoch 23 Batch 0: Train Loss = 0.5748
2023-10-23 16:42:58,729 - __main__ - INFO - Fold 4, mse = 674.7797, mad = 20.9496
2023-10-23 16:42:59,279 - __main__ - INFO - Fold 4 Epoch 24 Batch 0: Train Loss = 0.5539
2023-10-23 16:43:10,996 - __main__ - INFO - Fold 4, mse = 668.1885, mad = 20.7750
2023-10-23 16:43:11,693 - __main__ - INFO - Fold 4 Epoch 25 Batch 0: Train Loss = 0.5240
2023-10-23 16:43:24,790 - __main__ - INFO - Fold 4, mse = 678.9822, mad = 21.0799
2023-10-23 16:43:25,468 - __main__ - INFO - Fold 4 Epoch 26 Batch 0: Train Loss = 0.5048
2023-10-23 16:43:38,728 - __main__ - INFO - Fold 4, mse = 713.3369, mad = 21.3629
2023-10-23 16:43:39,286 - __main__ - INFO - Fold 4 Epoch 27 Batch 0: Train Loss = 0.4343
2023-10-23 16:43:53,067 - __main__ - INFO - Fold 4, mse = 679.6951, mad = 20.9240
2023-10-23 16:43:53,627 - __main__ - INFO - Fold 4 Epoch 28 Batch 0: Train Loss = 0.5060
2023-10-23 16:44:08,120 - __main__ - INFO - Fold 4, mse = 662.9323, mad = 20.7364
2023-10-23 16:44:08,724 - __main__ - INFO - Fold 4 Epoch 29 Batch 0: Train Loss = 0.4720
2023-10-23 16:44:21,974 - __main__ - INFO - Fold 4, mse = 673.1460, mad = 20.7807
2023-10-23 16:44:22,974 - __main__ - INFO - Fold 5 Epoch 0 Batch 0: Train Loss = 1.7541
2023-10-23 16:44:35,684 - __main__ - INFO - Fold 5, epoch 0: Loss = 1.3358 Valid loss = 1.0437 MSE = 738.5178
2023-10-23 16:44:35,685 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 738.5178 ------------
2023-10-23 16:44:35,834 - __main__ - INFO - Fold 5, mse = 738.5178, mad = 21.3466
2023-10-23 16:44:36,363 - __main__ - INFO - Fold 5 Epoch 1 Batch 0: Train Loss = 1.0118
2023-10-23 16:44:49,575 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 719.2118 ------------
2023-10-23 16:44:49,883 - __main__ - INFO - Fold 5, mse = 719.2118, mad = 21.2968
2023-10-23 16:44:50,605 - __main__ - INFO - Fold 5 Epoch 2 Batch 0: Train Loss = 1.1178
2023-10-23 16:45:04,845 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 696.4456 ------------
2023-10-23 16:45:05,016 - __main__ - INFO - Fold 5, mse = 696.4456, mad = 21.5167
2023-10-23 16:45:05,622 - __main__ - INFO - Fold 5 Epoch 3 Batch 0: Train Loss = 0.9419
2023-10-23 16:45:18,871 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 692.4835 ------------
2023-10-23 16:45:19,037 - __main__ - INFO - Fold 5, mse = 692.4835, mad = 21.3880
2023-10-23 16:45:19,641 - __main__ - INFO - Fold 5 Epoch 4 Batch 0: Train Loss = 0.9174
2023-10-23 16:45:32,679 - __main__ - INFO - Fold 5, mse = 699.6867, mad = 21.1547
2023-10-23 16:45:33,283 - __main__ - INFO - Fold 5 Epoch 5 Batch 0: Train Loss = 0.9653
2023-10-23 16:45:47,588 - __main__ - INFO - Fold 5, mse = 701.6915, mad = 21.1372
2023-10-23 16:45:48,176 - __main__ - INFO - Fold 5 Epoch 6 Batch 0: Train Loss = 0.7901
2023-10-23 16:46:01,695 - __main__ - INFO - Fold 5, mse = 695.8579, mad = 21.1058
2023-10-23 16:46:02,279 - __main__ - INFO - Fold 5 Epoch 7 Batch 0: Train Loss = 0.8176
2023-10-23 16:46:14,998 - __main__ - INFO - Fold 5, mse = 697.6361, mad = 21.1156
2023-10-23 16:46:15,583 - __main__ - INFO - Fold 5 Epoch 8 Batch 0: Train Loss = 0.9196
2023-10-23 16:46:28,880 - __main__ - INFO - Fold 5, mse = 706.4075, mad = 21.1968
2023-10-23 16:46:29,506 - __main__ - INFO - Fold 5 Epoch 9 Batch 0: Train Loss = 1.0278
2023-10-23 16:46:44,529 - __main__ - INFO - Fold 5, mse = 706.1995, mad = 21.2831
2023-10-23 16:46:45,165 - __main__ - INFO - Fold 5 Epoch 10 Batch 0: Train Loss = 0.7251
2023-10-23 16:47:00,453 - __main__ - INFO - Fold 5, epoch 10: Loss = 0.7468 Valid loss = 1.0225 MSE = 719.6267
2023-10-23 16:47:00,455 - __main__ - INFO - Fold 5, mse = 719.6267, mad = 21.4118
2023-10-23 16:47:01,072 - __main__ - INFO - Fold 5 Epoch 11 Batch 0: Train Loss = 0.6730
2023-10-23 16:47:14,315 - __main__ - INFO - Fold 5, mse = 709.6317, mad = 21.2518
2023-10-23 16:47:14,848 - __main__ - INFO - Fold 5 Epoch 12 Batch 0: Train Loss = 0.7116
2023-10-23 16:47:28,332 - __main__ - INFO - Fold 5, mse = 708.7332, mad = 21.2171
2023-10-23 16:47:29,024 - __main__ - INFO - Fold 5 Epoch 13 Batch 0: Train Loss = 0.7289
2023-10-23 16:47:42,013 - __main__ - INFO - Fold 5, mse = 709.6000, mad = 21.2692
2023-10-23 16:47:42,557 - __main__ - INFO - Fold 5 Epoch 14 Batch 0: Train Loss = 0.6408
2023-10-23 16:47:55,521 - __main__ - INFO - Fold 5, mse = 712.5862, mad = 21.2209
2023-10-23 16:47:56,017 - __main__ - INFO - Fold 5 Epoch 15 Batch 0: Train Loss = 0.7421
2023-10-23 16:48:09,235 - __main__ - INFO - Fold 5, mse = 720.0857, mad = 21.3670
2023-10-23 16:48:09,835 - __main__ - INFO - Fold 5 Epoch 16 Batch 0: Train Loss = 0.5575
2023-10-23 16:48:24,246 - __main__ - INFO - Fold 5, mse = 749.7937, mad = 21.7351
2023-10-23 16:48:24,876 - __main__ - INFO - Fold 5 Epoch 17 Batch 0: Train Loss = 0.6386
2023-10-23 16:48:39,280 - __main__ - INFO - Fold 5, mse = 723.6621, mad = 21.4121
2023-10-23 16:48:39,891 - __main__ - INFO - Fold 5 Epoch 18 Batch 0: Train Loss = 0.5683
2023-10-23 16:48:54,276 - __main__ - INFO - Fold 5, mse = 746.4870, mad = 21.5719
2023-10-23 16:48:54,878 - __main__ - INFO - Fold 5 Epoch 19 Batch 0: Train Loss = 0.4858
2023-10-23 16:49:10,275 - __main__ - INFO - Fold 5, mse = 730.0838, mad = 21.4773
2023-10-23 16:49:10,871 - __main__ - INFO - Fold 5 Epoch 20 Batch 0: Train Loss = 0.5365
2023-10-23 16:49:26,099 - __main__ - INFO - Fold 5, epoch 20: Loss = 0.5585 Valid loss = 1.0575 MSE = 738.2134
2023-10-23 16:49:26,101 - __main__ - INFO - Fold 5, mse = 738.2134, mad = 21.5830
2023-10-23 16:49:26,757 - __main__ - INFO - Fold 5 Epoch 21 Batch 0: Train Loss = 0.5575
2023-10-23 16:49:41,497 - __main__ - INFO - Fold 5, mse = 714.7995, mad = 21.1330
2023-10-23 16:49:42,153 - __main__ - INFO - Fold 5 Epoch 22 Batch 0: Train Loss = 0.5050
2023-10-23 16:49:55,612 - __main__ - INFO - Fold 5, mse = 757.3343, mad = 21.8017
2023-10-23 16:49:56,282 - __main__ - INFO - Fold 5 Epoch 23 Batch 0: Train Loss = 0.4508
2023-10-23 16:50:09,294 - __main__ - INFO - Fold 5, mse = 736.4571, mad = 21.6020
2023-10-23 16:50:09,848 - __main__ - INFO - Fold 5 Epoch 24 Batch 0: Train Loss = 0.5863
2023-10-23 16:50:23,757 - __main__ - INFO - Fold 5, mse = 752.3629, mad = 21.7407
2023-10-23 16:50:24,226 - __main__ - INFO - Fold 5 Epoch 25 Batch 0: Train Loss = 0.5236
2023-10-23 16:50:38,454 - __main__ - INFO - Fold 5, mse = 729.6920, mad = 21.3773
2023-10-23 16:50:38,989 - __main__ - INFO - Fold 5 Epoch 26 Batch 0: Train Loss = 0.5145
2023-10-23 16:50:50,739 - __main__ - INFO - Fold 5, mse = 763.9601, mad = 21.8254
2023-10-23 16:50:51,180 - __main__ - INFO - Fold 5 Epoch 27 Batch 0: Train Loss = 0.4861
2023-10-23 16:51:04,465 - __main__ - INFO - Fold 5, mse = 736.0367, mad = 21.4959
2023-10-23 16:51:04,882 - __main__ - INFO - Fold 5 Epoch 28 Batch 0: Train Loss = 0.5952
2023-10-23 16:51:18,064 - __main__ - INFO - Fold 5, mse = 775.2321, mad = 22.1259
2023-10-23 16:51:18,512 - __main__ - INFO - Fold 5 Epoch 29 Batch 0: Train Loss = 0.5148
2023-10-23 16:51:31,154 - __main__ - INFO - Fold 5, mse = 742.0806, mad = 21.5799
2023-10-23 16:51:31,156 - __main__ - INFO - mse 677.3082(25.6227)
2023-10-23 16:51:31,158 - __main__ - INFO - mad 21.2080(0.4129)
