2024-01-17 16:01:35,754 - __main__ - INFO - 这是希望输出的info内容
2024-01-17 16:01:35,755 - __main__ - WARNING - 这是希望输出的warning内容
2024-01-17 16:02:17,986 - __main__ - INFO - 32269
2024-01-17 16:02:17,986 - __main__ - INFO - 4034
2024-01-17 16:02:17,987 - __main__ - INFO - 4033
2024-01-17 16:02:24,199 - __main__ - INFO - load target data
2024-01-17 16:02:29,791 - __main__ - INFO - Training Student
2024-01-17 16:02:34,670 - __main__ - INFO - Epoch 0 Batch 0: Train Loss = 1.6844
2024-01-17 16:02:45,854 - __main__ - INFO - Epoch 0 Batch 20: Train Loss = 1.5455
2024-01-17 16:02:56,536 - __main__ - INFO - Epoch 0 Batch 40: Train Loss = 1.4743
2024-01-17 16:03:07,428 - __main__ - INFO - Epoch 0 Batch 60: Train Loss = 1.4730
2024-01-17 16:03:18,273 - __main__ - INFO - Epoch 0 Batch 80: Train Loss = 1.4033
2024-01-17 16:03:28,925 - __main__ - INFO - Epoch 0 Batch 100: Train Loss = 1.4385
2024-01-17 16:03:39,983 - __main__ - INFO - Epoch 0 Batch 120: Train Loss = 1.3897
2024-01-17 16:03:43,249 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.3554 ------------
2024-01-17 16:03:43,592 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.3062 ------------
2024-01-17 16:03:47,730 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.2924 ------------
2024-01-17 16:03:49,777 - __main__ - INFO - Epoch 1 Batch 0: Train Loss = 1.4618
2024-01-17 16:04:00,960 - __main__ - INFO - Epoch 1 Batch 20: Train Loss = 1.4727
2024-01-17 16:04:11,419 - __main__ - INFO - Epoch 1 Batch 40: Train Loss = 1.4716
2024-01-17 16:04:21,618 - __main__ - INFO - Epoch 1 Batch 60: Train Loss = 1.5175
2024-01-17 16:04:33,088 - __main__ - INFO - Epoch 1 Batch 80: Train Loss = 1.5116
2024-01-17 16:04:43,712 - __main__ - INFO - Epoch 1 Batch 100: Train Loss = 1.4163
2024-01-17 16:04:55,192 - __main__ - INFO - Epoch 1 Batch 120: Train Loss = 1.4104
2024-01-17 16:05:05,101 - __main__ - INFO - Epoch 2 Batch 0: Train Loss = 1.4560
2024-01-17 16:05:16,308 - __main__ - INFO - Epoch 2 Batch 20: Train Loss = 1.4562
2024-01-17 16:05:27,526 - __main__ - INFO - Epoch 2 Batch 40: Train Loss = 1.4903
2024-01-17 16:05:38,348 - __main__ - INFO - Epoch 2 Batch 60: Train Loss = 1.4995
2024-01-17 16:05:49,174 - __main__ - INFO - Epoch 2 Batch 80: Train Loss = 1.4782
2024-01-17 16:06:00,152 - __main__ - INFO - Epoch 2 Batch 100: Train Loss = 1.4308
2024-01-17 16:06:11,562 - __main__ - INFO - Epoch 2 Batch 120: Train Loss = 1.3739
2024-01-17 16:06:19,465 - __main__ - INFO - ------------ Save best model - TOTAL_LOSS: 1.2872 ------------
2024-01-17 16:06:21,678 - __main__ - INFO - Epoch 3 Batch 0: Train Loss = 1.4430
2024-01-17 16:06:34,157 - __main__ - INFO - Epoch 3 Batch 20: Train Loss = 1.4286
2024-01-17 16:06:47,137 - __main__ - INFO - Epoch 3 Batch 40: Train Loss = 1.4488
2024-01-17 16:06:59,418 - __main__ - INFO - Epoch 3 Batch 60: Train Loss = 1.4622
2024-01-17 16:07:12,848 - __main__ - INFO - Epoch 3 Batch 80: Train Loss = 1.4456
2024-01-17 16:07:25,105 - __main__ - INFO - Epoch 3 Batch 100: Train Loss = 1.4296
2024-01-17 16:07:38,134 - __main__ - INFO - Epoch 3 Batch 120: Train Loss = 1.4469
2024-01-17 16:07:48,627 - __main__ - INFO - Epoch 4 Batch 0: Train Loss = 1.4448
2024-01-17 16:08:01,068 - __main__ - INFO - Epoch 4 Batch 20: Train Loss = 1.4917
2024-01-17 16:08:12,132 - __main__ - INFO - Epoch 4 Batch 40: Train Loss = 1.4682
2024-01-17 16:08:22,978 - __main__ - INFO - Epoch 4 Batch 60: Train Loss = 1.4838
2024-01-17 16:08:33,843 - __main__ - INFO - Epoch 4 Batch 80: Train Loss = 1.4679
2024-01-17 16:08:44,362 - __main__ - INFO - Epoch 4 Batch 100: Train Loss = 1.4412
2024-01-17 16:08:56,010 - __main__ - INFO - Epoch 4 Batch 120: Train Loss = 1.4621
2024-01-17 16:09:06,013 - __main__ - INFO - Epoch 5 Batch 0: Train Loss = 1.4552
2024-01-17 16:09:17,518 - __main__ - INFO - Epoch 5 Batch 20: Train Loss = 1.4657
2024-01-17 16:09:28,395 - __main__ - INFO - Epoch 5 Batch 40: Train Loss = 1.4687
2024-01-17 16:09:39,460 - __main__ - INFO - Epoch 5 Batch 60: Train Loss = 1.4995
2024-01-17 16:09:51,169 - __main__ - INFO - Epoch 5 Batch 80: Train Loss = 1.4675
2024-01-17 16:10:04,360 - __main__ - INFO - Epoch 5 Batch 100: Train Loss = 1.4482
2024-01-17 16:10:17,879 - __main__ - INFO - Epoch 5 Batch 120: Train Loss = 1.4597
2024-01-17 16:10:29,097 - __main__ - INFO - Epoch 6 Batch 0: Train Loss = 1.4563
2024-01-17 16:10:41,545 - __main__ - INFO - Epoch 6 Batch 20: Train Loss = 1.4733
2024-01-17 16:10:54,334 - __main__ - INFO - Epoch 6 Batch 40: Train Loss = 1.4676
2024-01-17 16:11:06,277 - __main__ - INFO - Epoch 6 Batch 60: Train Loss = 1.4861
2024-01-17 16:11:18,104 - __main__ - INFO - Epoch 6 Batch 80: Train Loss = 1.4562
2024-01-17 16:11:30,712 - __main__ - INFO - Epoch 6 Batch 100: Train Loss = 1.4377
2024-01-17 16:11:43,543 - __main__ - INFO - Epoch 6 Batch 120: Train Loss = 1.4619
2024-01-17 16:11:54,619 - __main__ - INFO - Epoch 7 Batch 0: Train Loss = 1.4617
2024-01-17 16:12:07,031 - __main__ - INFO - Epoch 7 Batch 20: Train Loss = 1.4818
2024-01-17 16:12:18,715 - __main__ - INFO - Epoch 7 Batch 40: Train Loss = 1.4592
2024-01-17 16:12:31,238 - __main__ - INFO - Epoch 7 Batch 60: Train Loss = 1.4836
2024-01-17 16:12:42,079 - __main__ - INFO - Epoch 7 Batch 80: Train Loss = 1.4800
2024-01-17 16:12:53,187 - __main__ - INFO - Epoch 7 Batch 100: Train Loss = 1.4252
2024-01-17 16:13:04,049 - __main__ - INFO - Epoch 7 Batch 120: Train Loss = 1.4372
2024-01-17 16:13:14,367 - __main__ - INFO - Epoch 8 Batch 0: Train Loss = 1.4349
2024-01-17 16:13:27,864 - __main__ - INFO - Epoch 8 Batch 20: Train Loss = 1.4655
2024-01-17 16:13:40,248 - __main__ - INFO - Epoch 8 Batch 40: Train Loss = 1.4879
2024-01-17 16:13:52,717 - __main__ - INFO - Epoch 8 Batch 60: Train Loss = 1.4976
2024-01-17 16:14:04,909 - __main__ - INFO - Epoch 8 Batch 80: Train Loss = 1.4689
2024-01-17 16:14:17,079 - __main__ - INFO - Epoch 8 Batch 100: Train Loss = 1.4364
2024-01-17 16:14:30,372 - __main__ - INFO - Epoch 8 Batch 120: Train Loss = 1.4055
2024-01-17 16:14:41,762 - __main__ - INFO - Epoch 9 Batch 0: Train Loss = 1.4345
2024-01-17 16:14:51,975 - __main__ - INFO - Epoch 9 Batch 20: Train Loss = 1.4700
2024-01-17 16:15:02,714 - __main__ - INFO - Epoch 9 Batch 40: Train Loss = 1.4563
2024-01-17 16:15:13,321 - __main__ - INFO - Epoch 9 Batch 60: Train Loss = 1.4807
2024-01-17 16:15:23,862 - __main__ - INFO - Epoch 9 Batch 80: Train Loss = 1.4953
2024-01-17 16:15:34,412 - __main__ - INFO - Epoch 9 Batch 100: Train Loss = 1.4384
2024-01-17 16:15:45,221 - __main__ - INFO - Epoch 9 Batch 120: Train Loss = 1.4669
2024-01-17 16:15:55,330 - __main__ - INFO - Epoch 10 Batch 0: Train Loss = 1.4594
2024-01-17 16:16:07,719 - __main__ - INFO - Epoch 10 Batch 20: Train Loss = 1.4614
2024-01-17 16:16:20,298 - __main__ - INFO - Epoch 10 Batch 40: Train Loss = 1.4775
2024-01-17 16:16:32,485 - __main__ - INFO - Epoch 10 Batch 60: Train Loss = 1.4866
2024-01-17 16:16:44,401 - __main__ - INFO - Epoch 10 Batch 80: Train Loss = 1.4509
2024-01-17 16:16:56,853 - __main__ - INFO - Epoch 10 Batch 100: Train Loss = 1.4403
2024-01-17 16:17:09,323 - __main__ - INFO - Epoch 10 Batch 120: Train Loss = 1.4584
2024-01-17 16:17:20,567 - __main__ - INFO - Epoch 11 Batch 0: Train Loss = 1.4629
2024-01-17 16:17:32,569 - __main__ - INFO - Epoch 11 Batch 20: Train Loss = 1.4541
2024-01-17 16:17:45,128 - __main__ - INFO - Epoch 11 Batch 40: Train Loss = 1.4730
2024-01-17 16:17:57,796 - __main__ - INFO - Epoch 11 Batch 60: Train Loss = 1.4841
2024-01-17 16:18:10,370 - __main__ - INFO - Epoch 11 Batch 80: Train Loss = 1.4537
2024-01-17 16:18:22,779 - __main__ - INFO - Epoch 11 Batch 100: Train Loss = 1.4332
2024-01-17 16:18:34,547 - __main__ - INFO - Epoch 11 Batch 120: Train Loss = 1.4475
2024-01-17 16:18:44,331 - __main__ - INFO - Epoch 12 Batch 0: Train Loss = 1.4549
2024-01-17 16:18:55,531 - __main__ - INFO - Epoch 12 Batch 20: Train Loss = 1.4541
2024-01-17 16:19:06,891 - __main__ - INFO - Epoch 12 Batch 40: Train Loss = 1.4762
2024-01-17 16:19:17,756 - __main__ - INFO - Epoch 12 Batch 60: Train Loss = 1.4792
2024-01-17 16:19:29,002 - __main__ - INFO - Epoch 12 Batch 80: Train Loss = 1.4486
2024-01-17 16:19:41,281 - __main__ - INFO - Epoch 12 Batch 100: Train Loss = 1.4296
2024-01-17 16:19:54,407 - __main__ - INFO - Epoch 12 Batch 120: Train Loss = 1.4373
2024-01-17 16:20:04,907 - __main__ - INFO - Epoch 13 Batch 0: Train Loss = 1.4427
2024-01-17 16:20:17,543 - __main__ - INFO - Epoch 13 Batch 20: Train Loss = 1.4455
2024-01-17 16:20:30,659 - __main__ - INFO - Epoch 13 Batch 40: Train Loss = 1.4559
2024-01-17 16:20:43,114 - __main__ - INFO - Epoch 13 Batch 60: Train Loss = 1.4649
2024-01-17 16:20:55,352 - __main__ - INFO - Epoch 13 Batch 80: Train Loss = 1.4414
2024-01-17 16:21:07,666 - __main__ - INFO - Epoch 13 Batch 100: Train Loss = 1.4358
2024-01-17 16:21:20,443 - __main__ - INFO - Epoch 13 Batch 120: Train Loss = 1.4638
2024-01-17 16:21:30,734 - __main__ - INFO - Epoch 14 Batch 0: Train Loss = 1.4618
2024-01-17 16:21:43,665 - __main__ - INFO - Epoch 14 Batch 20: Train Loss = 1.4675
2024-01-17 16:21:55,170 - __main__ - INFO - Epoch 14 Batch 40: Train Loss = 1.4773
2024-01-17 16:22:08,294 - __main__ - INFO - Epoch 14 Batch 60: Train Loss = 1.4858
2024-01-17 16:22:21,362 - __main__ - INFO - Epoch 14 Batch 80: Train Loss = 1.4534
2024-01-17 16:22:34,635 - __main__ - INFO - Epoch 14 Batch 100: Train Loss = 1.4490
2024-01-17 16:22:46,921 - __main__ - INFO - Epoch 14 Batch 120: Train Loss = 1.4611
2024-01-17 16:22:57,646 - __main__ - INFO - Epoch 15 Batch 0: Train Loss = 1.4541
2024-01-17 16:23:09,966 - __main__ - INFO - Epoch 15 Batch 20: Train Loss = 1.4464
2024-01-17 16:23:20,819 - __main__ - INFO - Epoch 15 Batch 40: Train Loss = 1.4660
2024-01-17 16:23:31,928 - __main__ - INFO - Epoch 15 Batch 60: Train Loss = 1.4898
2024-01-17 16:23:43,019 - __main__ - INFO - Epoch 15 Batch 80: Train Loss = 1.4675
2024-01-17 16:23:54,066 - __main__ - INFO - Epoch 15 Batch 100: Train Loss = 1.4336
2024-01-17 16:24:05,120 - __main__ - INFO - Epoch 15 Batch 120: Train Loss = 1.4462
2024-01-17 16:24:15,059 - __main__ - INFO - Epoch 16 Batch 0: Train Loss = 1.4372
2024-01-17 16:24:26,268 - __main__ - INFO - Epoch 16 Batch 20: Train Loss = 1.4513
2024-01-17 16:24:37,704 - __main__ - INFO - Epoch 16 Batch 40: Train Loss = 1.4661
2024-01-17 16:24:49,281 - __main__ - INFO - Epoch 16 Batch 60: Train Loss = 1.4854
2024-01-17 16:24:59,894 - __main__ - INFO - Epoch 16 Batch 80: Train Loss = 1.4568
2024-01-17 16:25:10,363 - __main__ - INFO - Epoch 16 Batch 100: Train Loss = 1.4267
2024-01-17 16:25:21,784 - __main__ - INFO - Epoch 16 Batch 120: Train Loss = 1.4399
2024-01-17 16:25:31,531 - __main__ - INFO - Epoch 17 Batch 0: Train Loss = 1.4572
2024-01-17 16:25:41,951 - __main__ - INFO - Epoch 17 Batch 20: Train Loss = 1.4863
2024-01-17 16:25:52,673 - __main__ - INFO - Epoch 17 Batch 40: Train Loss = 1.4742
2024-01-17 16:26:04,083 - __main__ - INFO - Epoch 17 Batch 60: Train Loss = 1.4833
2024-01-17 16:26:14,233 - __main__ - INFO - Epoch 17 Batch 80: Train Loss = 1.4536
2024-01-17 16:26:25,077 - __main__ - INFO - Epoch 17 Batch 100: Train Loss = 1.4257
2024-01-17 16:26:35,396 - __main__ - INFO - Epoch 17 Batch 120: Train Loss = 1.4347
2024-01-17 16:26:44,847 - __main__ - INFO - Epoch 18 Batch 0: Train Loss = 1.4388
2024-01-17 16:26:55,208 - __main__ - INFO - Epoch 18 Batch 20: Train Loss = 1.4451
2024-01-17 16:27:05,909 - __main__ - INFO - Epoch 18 Batch 40: Train Loss = 1.4744
2024-01-17 16:27:16,449 - __main__ - INFO - Epoch 18 Batch 60: Train Loss = 1.4792
2024-01-17 16:27:27,336 - __main__ - INFO - Epoch 18 Batch 80: Train Loss = 1.4719
2024-01-17 16:27:38,814 - __main__ - INFO - Epoch 18 Batch 100: Train Loss = 1.4316
2024-01-17 16:27:50,247 - __main__ - INFO - Epoch 18 Batch 120: Train Loss = 1.4357
2024-01-17 16:28:00,781 - __main__ - INFO - Epoch 19 Batch 0: Train Loss = 1.4543
2024-01-17 16:28:12,334 - __main__ - INFO - Epoch 19 Batch 20: Train Loss = 1.4614
2024-01-17 16:28:23,403 - __main__ - INFO - Epoch 19 Batch 40: Train Loss = 1.4574
2024-01-17 16:28:34,234 - __main__ - INFO - Epoch 19 Batch 60: Train Loss = 1.4786
2024-01-17 16:28:44,609 - __main__ - INFO - Epoch 19 Batch 80: Train Loss = 1.4478
2024-01-17 16:28:55,327 - __main__ - INFO - Epoch 19 Batch 100: Train Loss = 1.4251
2024-01-17 16:29:06,713 - __main__ - INFO - Epoch 19 Batch 120: Train Loss = 1.4454
2024-01-17 16:29:16,094 - __main__ - INFO - Epoch 20 Batch 0: Train Loss = 1.4501
2024-01-17 16:29:26,698 - __main__ - INFO - Epoch 20 Batch 20: Train Loss = 1.4486
2024-01-17 16:29:37,771 - __main__ - INFO - Epoch 20 Batch 40: Train Loss = 1.4624
2024-01-17 16:29:48,497 - __main__ - INFO - Epoch 20 Batch 60: Train Loss = 1.4877
2024-01-17 16:29:58,943 - __main__ - INFO - Epoch 20 Batch 80: Train Loss = 1.4461
2024-01-17 16:30:08,941 - __main__ - INFO - Epoch 20 Batch 100: Train Loss = 1.4263
2024-01-17 16:30:20,295 - __main__ - INFO - Epoch 20 Batch 120: Train Loss = 1.4272
2024-01-17 16:30:29,973 - __main__ - INFO - Epoch 21 Batch 0: Train Loss = 1.4568
2024-01-17 16:30:40,794 - __main__ - INFO - Epoch 21 Batch 20: Train Loss = 1.4610
2024-01-17 16:30:51,083 - __main__ - INFO - Epoch 21 Batch 40: Train Loss = 1.4577
2024-01-17 16:31:00,797 - __main__ - INFO - Epoch 21 Batch 60: Train Loss = 1.4657
2024-01-17 16:31:11,049 - __main__ - INFO - Epoch 21 Batch 80: Train Loss = 1.4343
2024-01-17 16:31:21,928 - __main__ - INFO - Epoch 21 Batch 100: Train Loss = 1.4358
2024-01-17 16:31:32,651 - __main__ - INFO - Epoch 21 Batch 120: Train Loss = 1.4538
2024-01-17 16:31:42,269 - __main__ - INFO - Epoch 22 Batch 0: Train Loss = 1.4711
2024-01-17 16:31:52,883 - __main__ - INFO - Epoch 22 Batch 20: Train Loss = 1.4680
2024-01-17 16:32:03,291 - __main__ - INFO - Epoch 22 Batch 40: Train Loss = 1.4868
2024-01-17 16:32:13,575 - __main__ - INFO - Epoch 22 Batch 60: Train Loss = 1.4868
2024-01-17 16:32:24,184 - __main__ - INFO - Epoch 22 Batch 80: Train Loss = 1.4530
2024-01-17 16:32:34,829 - __main__ - INFO - Epoch 22 Batch 100: Train Loss = 1.4401
2024-01-17 16:32:45,730 - __main__ - INFO - Epoch 22 Batch 120: Train Loss = 1.4401
2024-01-17 16:32:55,081 - __main__ - INFO - Epoch 23 Batch 0: Train Loss = 1.4529
2024-01-17 16:33:06,097 - __main__ - INFO - Epoch 23 Batch 20: Train Loss = 1.4593
2024-01-17 16:33:17,149 - __main__ - INFO - Epoch 23 Batch 40: Train Loss = 1.4566
2024-01-17 16:33:27,881 - __main__ - INFO - Epoch 23 Batch 60: Train Loss = 1.5025
2024-01-17 16:33:38,942 - __main__ - INFO - Epoch 23 Batch 80: Train Loss = 1.4897
2024-01-17 16:33:49,976 - __main__ - INFO - Epoch 23 Batch 100: Train Loss = 1.4251
2024-01-17 16:34:01,688 - __main__ - INFO - Epoch 23 Batch 120: Train Loss = 1.4326
2024-01-17 16:34:11,453 - __main__ - INFO - Epoch 24 Batch 0: Train Loss = 1.4350
2024-01-17 16:34:22,796 - __main__ - INFO - Epoch 24 Batch 20: Train Loss = 1.4528
2024-01-17 16:34:33,810 - __main__ - INFO - Epoch 24 Batch 40: Train Loss = 1.4754
2024-01-17 16:34:44,012 - __main__ - INFO - Epoch 24 Batch 60: Train Loss = 1.4793
2024-01-17 16:34:54,230 - __main__ - INFO - Epoch 24 Batch 80: Train Loss = 1.4685
2024-01-17 16:35:04,644 - __main__ - INFO - Epoch 24 Batch 100: Train Loss = 1.4336
2024-01-17 16:35:15,276 - __main__ - INFO - Epoch 24 Batch 120: Train Loss = 1.4281
2024-01-17 16:35:24,919 - __main__ - INFO - Epoch 25 Batch 0: Train Loss = 1.4530
2024-01-17 16:35:35,454 - __main__ - INFO - Epoch 25 Batch 20: Train Loss = 1.4552
2024-01-17 16:35:45,761 - __main__ - INFO - Epoch 25 Batch 40: Train Loss = 1.4652
2024-01-17 16:35:55,945 - __main__ - INFO - Epoch 25 Batch 60: Train Loss = 1.4710
2024-01-17 16:36:06,261 - __main__ - INFO - Epoch 25 Batch 80: Train Loss = 1.4301
2024-01-17 16:36:16,488 - __main__ - INFO - Epoch 25 Batch 100: Train Loss = 1.4520
2024-01-17 16:36:26,736 - __main__ - INFO - Epoch 25 Batch 120: Train Loss = 1.4275
2024-01-17 16:36:35,756 - __main__ - INFO - Epoch 26 Batch 0: Train Loss = 1.4650
2024-01-17 16:36:46,125 - __main__ - INFO - Epoch 26 Batch 20: Train Loss = 1.4971
2024-01-17 16:36:56,692 - __main__ - INFO - Epoch 26 Batch 40: Train Loss = 1.4701
2024-01-17 16:37:06,509 - __main__ - INFO - Epoch 26 Batch 60: Train Loss = 1.4779
2024-01-17 16:37:16,805 - __main__ - INFO - Epoch 26 Batch 80: Train Loss = 1.4297
2024-01-17 16:37:26,825 - __main__ - INFO - Epoch 26 Batch 100: Train Loss = 1.4279
2024-01-17 16:37:37,381 - __main__ - INFO - Epoch 26 Batch 120: Train Loss = 1.4351
2024-01-17 16:37:46,459 - __main__ - INFO - Epoch 27 Batch 0: Train Loss = 1.4490
2024-01-17 16:37:56,936 - __main__ - INFO - Epoch 27 Batch 20: Train Loss = 1.4626
2024-01-17 16:38:07,686 - __main__ - INFO - Epoch 27 Batch 40: Train Loss = 1.4876
2024-01-17 16:38:17,757 - __main__ - INFO - Epoch 27 Batch 60: Train Loss = 1.5090
2024-01-17 16:38:27,408 - __main__ - INFO - Epoch 27 Batch 80: Train Loss = 1.4878
2024-01-17 16:38:38,133 - __main__ - INFO - Epoch 27 Batch 100: Train Loss = 1.4262
2024-01-17 16:38:48,580 - __main__ - INFO - Epoch 27 Batch 120: Train Loss = 1.4279
2024-01-17 16:38:58,533 - __main__ - INFO - Epoch 28 Batch 0: Train Loss = 1.4448
2024-01-17 16:39:09,961 - __main__ - INFO - Epoch 28 Batch 20: Train Loss = 1.4590
2024-01-17 16:39:20,916 - __main__ - INFO - Epoch 28 Batch 40: Train Loss = 1.4675
2024-01-17 16:39:32,004 - __main__ - INFO - Epoch 28 Batch 60: Train Loss = 1.4923
2024-01-17 16:39:42,587 - __main__ - INFO - Epoch 28 Batch 80: Train Loss = 1.4852
2024-01-17 16:39:53,947 - __main__ - INFO - Epoch 28 Batch 100: Train Loss = 1.4385
2024-01-17 16:40:05,282 - __main__ - INFO - Epoch 28 Batch 120: Train Loss = 1.4529
2024-01-17 16:40:14,505 - __main__ - INFO - Epoch 29 Batch 0: Train Loss = 1.4555
2024-01-17 16:40:25,166 - __main__ - INFO - Epoch 29 Batch 20: Train Loss = 1.4671
2024-01-17 16:40:35,769 - __main__ - INFO - Epoch 29 Batch 40: Train Loss = 1.4688
2024-01-17 16:40:46,353 - __main__ - INFO - Epoch 29 Batch 60: Train Loss = 1.4786
2024-01-17 16:40:56,706 - __main__ - INFO - Epoch 29 Batch 80: Train Loss = 1.4557
2024-01-17 16:41:07,119 - __main__ - INFO - Epoch 29 Batch 100: Train Loss = 1.4424
2024-01-17 16:41:17,702 - __main__ - INFO - Epoch 29 Batch 120: Train Loss = 1.4565
2024-01-17 16:41:26,228 - __main__ - INFO - last saved model is in epoch 2
2024-01-17 16:41:26,458 - __main__ - INFO - Batch 0: Test Loss = 0.3913
2024-01-17 16:41:29,625 - __main__ - INFO - 
==>Predicting on test
2024-01-17 16:41:29,626 - __main__ - INFO - Test Loss = 0.5304
2024-01-17 16:41:29,687 - __main__ - INFO - Transfer Target Dataset & Model
2024-01-17 16:41:33,089 - __main__ - INFO - [[0.35672856748565834, 0.362193782095267, -0.0415507190085758, -0.39134565918228126, 0.7630646387621486, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [2.891396229598206, -0.1650564631881193, -0.7197426028629784, -0.5274138023823791, 0.7630646387621486, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.7295575734197193, 0.8894440273786534, 1.5408970099849981, -1.1397204467828197, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.7295575734197193, 0.362193782095267, 1.5408970099849981, -1.1397204467828197, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.09589065789158233, 1.4166942726620397, 0.6366411648458108, -0.9356182319826729, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [2.076681623919173, 1.4166942726620397, 0.6366411648458108, -1.7520270911832603, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.08515703225931394, 0.8894440273786534, 0.18451324227620902, -0.32331158758223233, -0.09527102482463178, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [1.0809193280892433, 1.4166942726620397, 0.41057720356100985, 0.9013017012186487, 1.1922324705555387, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.548509883268823, 1.943944517945426, 0.18451324227620902, 1.989846846819432, 1.0849405126071912, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.548509883268823, 1.4166942726620397, -1.6239984480021659, -1.0036523035827218, -1.1681906043081072, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.7253198692496488, 1.4166942726620397, 0.41057720356100985, 0.28899505681820825, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.17568087733476206, 1.4166942726620397, 0.18451324227620902, 0.561131343218404, 2.050568134142319, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.6390337283442711, 1.4166942726620397, -1.171870525432564, -1.0036523035827218, 0.5484807228654535, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 1.4166942726620397, -2.5282542931413534, 0.8332676296185998, -0.30985494072132685, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 0.8894440273786534, -2.5282542931413534, 0.8332676296185998, -0.30985494072132685, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.18217679879696, 1.943944517945426, -0.0415507190085758, 0.3570291284182572, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.09589065789158233, 1.4166942726620397, -0.49367864157817754, 0.4250632000183061, 0.11931289107206332, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.272700643872408, 0.8894440273786534, -0.0415507190085758, 0.8332676296185998, 0.11931289107206332, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [0.17568087733476206, 1.943944517945426, 0.8627051261306116, -0.7995500887825749, 0.9776485546588437, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.9106052635706156, 1.943944517945426, -1.171870525432564, 0.15292691361811034, 1.1922324705555387, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.2769383480424786, 0.8894440273786534, -0.9458065641477793, -2.092197449183505, -1.3827745202048023, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.5442721790987524, 0.8894440273786534, 0.18451324227620902, 1.5816424172191383, -1.1681906043081072, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.2769383480424786, 1.4166942726620397, -0.9458065641477793, 0.22096098521815927, 0.44118876491710596, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-3.6263206158340595, 0.362193782095267, -1.3979344867173649, -1.8880952343833581, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.6390337283442711, 1.4166942726620397, -1.171870525432564, 0.01685877041801243, -0.8463147304630646, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-0.9106052635706156, 1.4166942726620397, -0.26761468029337665, 0.4930972716183551, 0.012020933123715766, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-1.0916529537215118, 0.8894440273786534, 0.41057720356100985, 1.5136083456190894, 0.22660484902041086, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185], [-2.3589867847777857, 1.4166942726620397, 0.41057720356100985, 1.3775402024189913, 1.5141083444005814, -0.053944463391532395, 0.23950892438003255, -0.09317341164862601, 0.27449751326076044, -0.08003116599596992, -0.11276048206854025, -0.19232823522339773, -0.32689733132001597, -0.30210296834341416, -0.1920394961218317, -0.09367250499452183, -0.058661728080839165, -0.02507608834202364, -0.2539704877679013, -0.38426823024859896, 0.1113217036858432, 0.10224702120257448, -0.17622868834829972, -0.2221088487413897, -0.04331355167616206, -0.14866187746753454, 0.0, 0.3582866239721009, -0.05840471544695842, 0.0020796716856398088, -0.017707468132584787, -0.30938285094998835, -0.008241366021706666, -0.07376437314271661, -0.23777159425714214, -0.28390371567955686, -0.4230107155991255, -0.11543298040046182, -0.23061455647984533, -0.029142770807359483, 0.02602235552654817, -0.03418385058726334, 0.021391782427413616, -0.2519677833718266, 0.03767890116355937, 0.03550519429120874, 0.08887182857332589, -0.38744626413333827, -0.38495703276651483, -0.2707642924848631, -0.26052899279358305, -0.2263766405394266, -0.2422285208743585, -0.29707783041491825, -0.26333562825630913, 0.0, -0.3727664369523662, -0.31562514828001637, -0.3346075120295629, 0.04983364667252367, 0.09273923010941068, 0.0765960596752501, -0.01692869722389257, 0.11911147790936279, -0.018478781976574567, -0.043339905359219597, -0.17666370226220254, -0.21174962206992068, -0.23941718638772186, -0.14974245322065022, -0.11094803706756479, -0.16794039705842378, -0.1742551529001042, -0.11586425080367756, -0.0956260785757183, -0.05244482101870695, -0.02391782279598911, -0.06722256636268456, -0.24163571293807515, -0.25770909874261017, 0.08342141991702888, 0.06062716895647873, -0.012735874702375725, -0.36747688045251553, 0.031031613305826433, -0.10904781143122072, -0.13556093368453873, -0.1260573083539185, -0.40190766182886617, -0.058525287409528205, 0.359273006722474, -0.05127922079610631, 0.041823574127139364, -0.25099604352722615, -0.03678804515820732, -0.031038832563236623, -0.09512129567975375, -0.04235088088534124, -0.021495300497735185]]
2024-01-17 16:41:33,093 - __main__ - INFO - 99
2024-01-17 16:41:33,094 - __main__ - INFO - 4255
2024-01-17 16:41:33,445 - __main__ - INFO - {'los_mean': 5.363315937659429, 'los_std': 4.099244607743503, 'los_median': 4.333333333333333, 'large_los': 21.291666666666668, 'threshold': 3.7605509886094994}
2024-01-17 16:41:34,702 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 166.8659
2024-01-17 16:42:04,000 - __main__ - INFO - Fold 1 Epoch 0 Batch 50: Train Loss = 2.5376
2024-01-17 16:42:35,854 - __main__ - INFO - Fold 1 Epoch 0 Batch 100: Train Loss = 3.1778
2024-01-17 16:43:06,242 - __main__ - INFO - Fold 1 Epoch 0 Batch 150: Train Loss = 2.2433
2024-01-17 16:43:35,147 - __main__ - INFO - Fold 1 Epoch 0 Batch 200: Train Loss = 2.3050
2024-01-17 16:44:00,385 - __main__ - INFO - Fold 1 Epoch 0 Batch 250: Train Loss = 2.4051
2024-01-17 16:44:25,625 - __main__ - INFO - Fold 1 Epoch 0 Batch 300: Train Loss = 1.9661
2024-01-17 16:45:29,277 - __main__ - INFO - Fold 1, epoch 0: Loss = 3.7511 Valid loss = 2.7889 MSE = 40.4189 AUROC = 0.6443
2024-01-17 16:45:29,279 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 40.4189 ------------
2024-01-17 16:45:29,695 - __main__ - INFO - ------------ Save best model - MSE: 40.4189 ------------
2024-01-17 16:45:29,697 - __main__ - INFO - Fold 1, mse = 40.4189, mad = 4.8253
2024-01-17 16:45:30,337 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 2.2136
2024-01-17 16:46:00,132 - __main__ - INFO - Fold 1 Epoch 1 Batch 50: Train Loss = 2.2322
2024-01-17 16:46:28,967 - __main__ - INFO - Fold 1 Epoch 1 Batch 100: Train Loss = 1.8347
2024-01-17 16:46:54,221 - __main__ - INFO - Fold 1 Epoch 1 Batch 150: Train Loss = 1.9391
2024-01-17 16:47:19,218 - __main__ - INFO - Fold 1 Epoch 1 Batch 200: Train Loss = 2.7366
2024-01-17 16:47:48,463 - __main__ - INFO - Fold 1 Epoch 1 Batch 250: Train Loss = 1.9388
2024-01-17 16:48:16,692 - __main__ - INFO - Fold 1 Epoch 1 Batch 300: Train Loss = 2.3512
2024-01-17 16:49:14,380 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 37.4816 ------------
2024-01-17 16:49:14,646 - __main__ - INFO - ------------ Save best model - MSE: 37.4816 ------------
2024-01-17 16:49:14,648 - __main__ - INFO - Fold 1, mse = 37.4816, mad = 4.5572
2024-01-17 16:49:15,154 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 2.0953
2024-01-17 16:49:42,803 - __main__ - INFO - Fold 1 Epoch 2 Batch 50: Train Loss = 2.3899
2024-01-17 16:50:14,065 - __main__ - INFO - Fold 1 Epoch 2 Batch 100: Train Loss = 2.2745
2024-01-17 16:50:46,261 - __main__ - INFO - Fold 1 Epoch 2 Batch 150: Train Loss = 1.9031
2024-01-17 16:51:13,758 - __main__ - INFO - Fold 1 Epoch 2 Batch 200: Train Loss = 2.1769
2024-01-17 16:51:48,151 - __main__ - INFO - Fold 1 Epoch 2 Batch 250: Train Loss = 2.2124
2024-01-17 16:52:19,504 - __main__ - INFO - Fold 1 Epoch 2 Batch 300: Train Loss = 2.2500
2024-01-17 16:53:14,097 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.8154 ------------
2024-01-17 16:53:14,314 - __main__ - INFO - ------------ Save best model - MSE: 35.8154 ------------
2024-01-17 16:53:14,315 - __main__ - INFO - Fold 1, mse = 35.8154, mad = 4.3210
2024-01-17 16:53:14,832 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 2.0156
2024-01-17 16:53:38,964 - __main__ - INFO - Fold 1 Epoch 3 Batch 50: Train Loss = 2.5915
2024-01-17 16:54:02,956 - __main__ - INFO - Fold 1 Epoch 3 Batch 100: Train Loss = 2.3718
2024-01-17 16:54:26,746 - __main__ - INFO - Fold 1 Epoch 3 Batch 150: Train Loss = 2.3710
2024-01-17 16:54:50,954 - __main__ - INFO - Fold 1 Epoch 3 Batch 200: Train Loss = 1.9563
2024-01-17 16:55:14,848 - __main__ - INFO - Fold 1 Epoch 3 Batch 250: Train Loss = 2.3717
2024-01-17 16:55:39,221 - __main__ - INFO - Fold 1 Epoch 3 Batch 300: Train Loss = 2.7868
2024-01-17 16:56:34,037 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.3237 ------------
2024-01-17 16:56:34,338 - __main__ - INFO - ------------ Save best model - MSE: 35.3237 ------------
2024-01-17 16:56:34,339 - __main__ - INFO - Fold 1, mse = 35.3237, mad = 4.1401
2024-01-17 16:56:34,867 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 2.1292
2024-01-17 16:56:59,501 - __main__ - INFO - Fold 1 Epoch 4 Batch 50: Train Loss = 2.0109
2024-01-17 16:57:23,378 - __main__ - INFO - Fold 1 Epoch 4 Batch 100: Train Loss = 2.8028
2024-01-17 16:57:47,934 - __main__ - INFO - Fold 1 Epoch 4 Batch 150: Train Loss = 2.1777
2024-01-17 16:58:11,803 - __main__ - INFO - Fold 1 Epoch 4 Batch 200: Train Loss = 2.0804
2024-01-17 16:58:35,551 - __main__ - INFO - Fold 1 Epoch 4 Batch 250: Train Loss = 1.7926
2024-01-17 16:58:59,586 - __main__ - INFO - Fold 1 Epoch 4 Batch 300: Train Loss = 2.2625
2024-01-17 16:59:50,709 - __main__ - INFO - Fold 1, mse = 37.7478, mad = 4.5813
2024-01-17 16:59:51,219 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 2.1408
2024-01-17 17:00:15,654 - __main__ - INFO - Fold 1 Epoch 5 Batch 50: Train Loss = 1.8051
2024-01-17 17:00:40,326 - __main__ - INFO - Fold 1 Epoch 5 Batch 100: Train Loss = 2.9698
2024-01-17 17:01:06,640 - __main__ - INFO - Fold 1 Epoch 5 Batch 150: Train Loss = 3.1940
2024-01-17 17:01:33,859 - __main__ - INFO - Fold 1 Epoch 5 Batch 200: Train Loss = 2.4788
2024-01-17 17:01:58,724 - __main__ - INFO - Fold 1 Epoch 5 Batch 250: Train Loss = 1.6994
2024-01-17 17:02:22,827 - __main__ - INFO - Fold 1 Epoch 5 Batch 300: Train Loss = 2.1938
2024-01-17 17:03:13,785 - __main__ - INFO - Fold 1, mse = 36.1185, mad = 4.3744
2024-01-17 17:03:14,330 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 1.8599
2024-01-17 17:03:38,211 - __main__ - INFO - Fold 1 Epoch 6 Batch 50: Train Loss = 2.3844
2024-01-17 17:04:02,197 - __main__ - INFO - Fold 1 Epoch 6 Batch 100: Train Loss = 2.0651
2024-01-17 17:04:26,852 - __main__ - INFO - Fold 1 Epoch 6 Batch 150: Train Loss = 1.9118
2024-01-17 17:04:50,608 - __main__ - INFO - Fold 1 Epoch 6 Batch 200: Train Loss = 2.3169
2024-01-17 17:05:14,867 - __main__ - INFO - Fold 1 Epoch 6 Batch 250: Train Loss = 3.0364
2024-01-17 17:05:39,018 - __main__ - INFO - Fold 1 Epoch 6 Batch 300: Train Loss = 2.1971
2024-01-17 17:06:34,339 - __main__ - INFO - Fold 1, mse = 36.4289, mad = 4.3654
2024-01-17 17:06:34,775 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 2.0514
2024-01-17 17:06:59,422 - __main__ - INFO - Fold 1 Epoch 7 Batch 50: Train Loss = 2.0389
2024-01-17 17:07:24,023 - __main__ - INFO - Fold 1 Epoch 7 Batch 100: Train Loss = 1.7419
2024-01-17 17:07:47,840 - __main__ - INFO - Fold 1 Epoch 7 Batch 150: Train Loss = 2.0003
2024-01-17 17:08:11,658 - __main__ - INFO - Fold 1 Epoch 7 Batch 200: Train Loss = 2.0726
2024-01-17 17:08:35,619 - __main__ - INFO - Fold 1 Epoch 7 Batch 250: Train Loss = 2.2809
2024-01-17 17:08:59,715 - __main__ - INFO - Fold 1 Epoch 7 Batch 300: Train Loss = 2.7428
2024-01-17 17:09:50,373 - __main__ - INFO - Fold 1, mse = 38.0434, mad = 4.5271
2024-01-17 17:09:50,857 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 2.0200
2024-01-17 17:10:15,333 - __main__ - INFO - Fold 1 Epoch 8 Batch 50: Train Loss = 2.0244
2024-01-17 17:10:39,495 - __main__ - INFO - Fold 1 Epoch 8 Batch 100: Train Loss = 2.0446
2024-01-17 17:11:05,269 - __main__ - INFO - Fold 1 Epoch 8 Batch 150: Train Loss = 2.0073
2024-01-17 17:11:31,842 - __main__ - INFO - Fold 1 Epoch 8 Batch 200: Train Loss = 2.0833
2024-01-17 17:11:56,561 - __main__ - INFO - Fold 1 Epoch 8 Batch 250: Train Loss = 2.4678
2024-01-17 17:12:21,147 - __main__ - INFO - Fold 1 Epoch 8 Batch 300: Train Loss = 2.1689
2024-01-17 17:13:12,877 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.2299 ------------
2024-01-17 17:13:13,143 - __main__ - INFO - ------------ Save best model - MSE: 35.2299 ------------
2024-01-17 17:13:13,144 - __main__ - INFO - Fold 1, mse = 35.2299, mad = 4.2674
2024-01-17 17:13:13,741 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 2.5032
2024-01-17 17:13:42,244 - __main__ - INFO - Fold 1 Epoch 9 Batch 50: Train Loss = 2.7155
2024-01-17 17:14:11,338 - __main__ - INFO - Fold 1 Epoch 9 Batch 100: Train Loss = 2.2365
2024-01-17 17:14:40,848 - __main__ - INFO - Fold 1 Epoch 9 Batch 150: Train Loss = 2.0540
2024-01-17 17:15:09,905 - __main__ - INFO - Fold 1 Epoch 9 Batch 200: Train Loss = 1.8996
2024-01-17 17:15:36,969 - __main__ - INFO - Fold 1 Epoch 9 Batch 250: Train Loss = 2.4482
2024-01-17 17:16:06,523 - __main__ - INFO - Fold 1 Epoch 9 Batch 300: Train Loss = 2.0111
2024-01-17 17:17:04,656 - __main__ - INFO - Fold 1, mse = 36.8003, mad = 4.4886
2024-01-17 17:17:05,218 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 2.5937
2024-01-17 17:17:32,148 - __main__ - INFO - Fold 1 Epoch 10 Batch 50: Train Loss = 2.6372
2024-01-17 17:17:58,969 - __main__ - INFO - Fold 1 Epoch 10 Batch 100: Train Loss = 2.1335
2024-01-17 17:18:26,514 - __main__ - INFO - Fold 1 Epoch 10 Batch 150: Train Loss = 1.8461
2024-01-17 17:18:53,475 - __main__ - INFO - Fold 1 Epoch 10 Batch 200: Train Loss = 2.7642
2024-01-17 17:19:20,587 - __main__ - INFO - Fold 1 Epoch 10 Batch 250: Train Loss = 1.9891
2024-01-17 17:19:47,675 - __main__ - INFO - Fold 1 Epoch 10 Batch 300: Train Loss = 2.2281
2024-01-17 17:20:43,577 - __main__ - INFO - Fold 1, epoch 10: Loss = 2.2003 Valid loss = 2.4394 MSE = 35.2288 AUROC = 0.7121
2024-01-17 17:20:43,578 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.2288 ------------
2024-01-17 17:20:43,948 - __main__ - INFO - ------------ Save best model - MSE: 35.2288 ------------
2024-01-17 17:20:43,949 - __main__ - INFO - Fold 1, mse = 35.2288, mad = 4.2441
2024-01-17 17:20:44,586 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 2.4079
2024-01-17 17:21:13,789 - __main__ - INFO - Fold 1 Epoch 11 Batch 50: Train Loss = 2.5366
2024-01-17 17:21:41,485 - __main__ - INFO - Fold 1 Epoch 11 Batch 100: Train Loss = 1.8885
2024-01-17 17:22:08,394 - __main__ - INFO - Fold 1 Epoch 11 Batch 150: Train Loss = 1.6742
2024-01-17 17:22:35,673 - __main__ - INFO - Fold 1 Epoch 11 Batch 200: Train Loss = 1.8339
2024-01-17 17:23:02,456 - __main__ - INFO - Fold 1 Epoch 11 Batch 250: Train Loss = 1.9163
2024-01-17 17:23:29,391 - __main__ - INFO - Fold 1 Epoch 11 Batch 300: Train Loss = 2.6907
2024-01-17 17:24:24,664 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 35.1029 ------------
2024-01-17 17:24:24,901 - __main__ - INFO - ------------ Save best model - MSE: 35.1029 ------------
2024-01-17 17:24:24,901 - __main__ - INFO - Fold 1, mse = 35.1029, mad = 4.2511
2024-01-17 17:24:25,451 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 1.9923
2024-01-17 17:24:52,300 - __main__ - INFO - Fold 1 Epoch 12 Batch 50: Train Loss = 2.0765
2024-01-17 17:25:20,213 - __main__ - INFO - Fold 1 Epoch 12 Batch 100: Train Loss = 2.1721
2024-01-17 17:25:49,085 - __main__ - INFO - Fold 1 Epoch 12 Batch 150: Train Loss = 2.1645
2024-01-17 17:26:18,013 - __main__ - INFO - Fold 1 Epoch 12 Batch 200: Train Loss = 1.6522
2024-01-17 17:26:47,620 - __main__ - INFO - Fold 1 Epoch 12 Batch 250: Train Loss = 2.0170
2024-01-17 17:27:16,170 - __main__ - INFO - Fold 1 Epoch 12 Batch 300: Train Loss = 1.7600
2024-01-17 17:28:12,855 - __main__ - INFO - Fold 1, mse = 37.4844, mad = 4.5464
2024-01-17 17:28:13,411 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 2.0715
2024-01-17 17:28:40,377 - __main__ - INFO - Fold 1 Epoch 13 Batch 50: Train Loss = 2.9301
2024-01-17 17:29:06,255 - __main__ - INFO - Fold 1 Epoch 13 Batch 100: Train Loss = 1.9183
2024-01-17 17:29:32,303 - __main__ - INFO - Fold 1 Epoch 13 Batch 150: Train Loss = 1.9737
2024-01-17 17:29:58,364 - __main__ - INFO - Fold 1 Epoch 13 Batch 200: Train Loss = 2.2252
2024-01-17 17:30:25,495 - __main__ - INFO - Fold 1 Epoch 13 Batch 250: Train Loss = 2.2256
2024-01-17 17:30:54,075 - __main__ - INFO - Fold 1 Epoch 13 Batch 300: Train Loss = 3.0518
2024-01-17 17:31:50,650 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 34.9175 ------------
2024-01-17 17:31:50,967 - __main__ - INFO - ------------ Save best model - MSE: 34.9175 ------------
2024-01-17 17:31:50,968 - __main__ - INFO - Fold 1, mse = 34.9175, mad = 4.1647
2024-01-17 17:31:51,519 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 2.3177
2024-01-17 17:32:18,965 - __main__ - INFO - Fold 1 Epoch 14 Batch 50: Train Loss = 2.0772
2024-01-17 17:32:45,294 - __main__ - INFO - Fold 1 Epoch 14 Batch 100: Train Loss = 2.2484
2024-01-17 17:33:13,208 - __main__ - INFO - Fold 1 Epoch 14 Batch 150: Train Loss = 2.0613
2024-01-17 17:33:40,610 - __main__ - INFO - Fold 1 Epoch 14 Batch 200: Train Loss = 2.3810
2024-01-17 17:34:08,826 - __main__ - INFO - Fold 1 Epoch 14 Batch 250: Train Loss = 1.9982
2024-01-17 17:34:37,150 - __main__ - INFO - Fold 1 Epoch 14 Batch 300: Train Loss = 2.1742
2024-01-17 17:35:36,058 - __main__ - INFO - Fold 1, mse = 34.9838, mad = 4.1988
2024-01-17 17:35:36,663 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 1.9806
2024-01-17 17:36:07,957 - __main__ - INFO - Fold 1 Epoch 15 Batch 50: Train Loss = 1.8081
2024-01-17 17:36:36,999 - __main__ - INFO - Fold 1 Epoch 15 Batch 100: Train Loss = 1.9737
2024-01-17 17:37:04,151 - __main__ - INFO - Fold 1 Epoch 15 Batch 150: Train Loss = 2.5688
2024-01-17 17:37:32,817 - __main__ - INFO - Fold 1 Epoch 15 Batch 200: Train Loss = 1.9817
2024-01-17 17:38:00,882 - __main__ - INFO - Fold 1 Epoch 15 Batch 250: Train Loss = 1.9654
2024-01-17 17:38:28,435 - __main__ - INFO - Fold 1 Epoch 15 Batch 300: Train Loss = 1.9474
2024-01-17 17:39:25,198 - __main__ - INFO - Fold 1, mse = 35.2936, mad = 4.3332
2024-01-17 17:39:25,802 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 2.1598
2024-01-17 17:39:53,587 - __main__ - INFO - Fold 1 Epoch 16 Batch 50: Train Loss = 2.3206
2024-01-17 17:40:21,878 - __main__ - INFO - Fold 1 Epoch 16 Batch 100: Train Loss = 1.8806
2024-01-17 17:40:50,434 - __main__ - INFO - Fold 1 Epoch 16 Batch 150: Train Loss = 2.6706
2024-01-17 17:41:22,460 - __main__ - INFO - Fold 1 Epoch 16 Batch 200: Train Loss = 1.8704
2024-01-17 17:41:51,512 - __main__ - INFO - Fold 1 Epoch 16 Batch 250: Train Loss = 2.4079
2024-01-17 17:42:19,867 - __main__ - INFO - Fold 1 Epoch 16 Batch 300: Train Loss = 1.9593
2024-01-17 17:43:17,393 - __main__ - INFO - Fold 1, mse = 36.8827, mad = 4.4882
2024-01-17 17:43:17,992 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 1.8313
2024-01-17 17:43:45,804 - __main__ - INFO - Fold 1 Epoch 17 Batch 50: Train Loss = 1.8021
2024-01-17 17:44:14,299 - __main__ - INFO - Fold 1 Epoch 17 Batch 100: Train Loss = 2.0435
2024-01-17 17:44:41,910 - __main__ - INFO - Fold 1 Epoch 17 Batch 150: Train Loss = 1.8801
2024-01-17 17:45:09,415 - __main__ - INFO - Fold 1 Epoch 17 Batch 200: Train Loss = 2.1230
2024-01-17 17:45:37,116 - __main__ - INFO - Fold 1 Epoch 17 Batch 250: Train Loss = 2.6836
2024-01-17 17:46:07,225 - __main__ - INFO - Fold 1 Epoch 17 Batch 300: Train Loss = 2.2405
2024-01-17 17:47:05,558 - __main__ - INFO - Fold 1, mse = 35.0520, mad = 4.1142
2024-01-17 17:47:06,119 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 2.4301
2024-01-17 17:47:33,102 - __main__ - INFO - Fold 1 Epoch 18 Batch 50: Train Loss = 1.8996
2024-01-17 17:48:00,531 - __main__ - INFO - Fold 1 Epoch 18 Batch 100: Train Loss = 2.9131
2024-01-17 17:48:28,541 - __main__ - INFO - Fold 1 Epoch 18 Batch 150: Train Loss = 1.6298
2024-01-17 17:48:56,836 - __main__ - INFO - Fold 1 Epoch 18 Batch 200: Train Loss = 2.3613
2024-01-17 17:49:24,540 - __main__ - INFO - Fold 1 Epoch 18 Batch 250: Train Loss = 2.5101
2024-01-17 17:49:51,953 - __main__ - INFO - Fold 1 Epoch 18 Batch 300: Train Loss = 1.8399
2024-01-17 17:50:48,742 - __main__ - INFO - Fold 1, mse = 37.8293, mad = 4.5255
2024-01-17 17:50:49,333 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 2.0199
2024-01-17 17:51:17,522 - __main__ - INFO - Fold 1 Epoch 19 Batch 50: Train Loss = 1.8090
2024-01-17 17:51:46,314 - __main__ - INFO - Fold 1 Epoch 19 Batch 100: Train Loss = 2.1818
2024-01-17 17:52:16,129 - __main__ - INFO - Fold 1 Epoch 19 Batch 150: Train Loss = 2.0705
2024-01-17 17:52:44,907 - __main__ - INFO - Fold 1 Epoch 19 Batch 200: Train Loss = 2.2663
2024-01-17 17:53:13,764 - __main__ - INFO - Fold 1 Epoch 19 Batch 250: Train Loss = 2.4147
2024-01-17 17:53:39,539 - __main__ - INFO - Fold 1 Epoch 19 Batch 300: Train Loss = 2.1172
2024-01-17 17:54:29,958 - __main__ - INFO - Fold 1, mse = 35.3563, mad = 4.2818
2024-01-17 17:54:33,874 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 65.8105
2024-01-17 17:54:56,728 - __main__ - INFO - Fold 2 Epoch 0 Batch 50: Train Loss = 3.9071
2024-01-17 17:55:19,754 - __main__ - INFO - Fold 2 Epoch 0 Batch 100: Train Loss = 2.5395
2024-01-17 17:55:43,174 - __main__ - INFO - Fold 2 Epoch 0 Batch 150: Train Loss = 2.0212
2024-01-17 17:56:08,594 - __main__ - INFO - Fold 2 Epoch 0 Batch 200: Train Loss = 4.4139
2024-01-17 17:56:32,267 - __main__ - INFO - Fold 2 Epoch 0 Batch 250: Train Loss = 1.7304
2024-01-17 17:56:55,677 - __main__ - INFO - Fold 2 Epoch 0 Batch 300: Train Loss = 3.6624
2024-01-17 17:57:45,034 - __main__ - INFO - Fold 2, epoch 0: Loss = 3.4213 Valid loss = 2.2957 MSE = 32.2715 AUROC = 0.5875
2024-01-17 17:57:45,034 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.2715 ------------
2024-01-17 17:57:45,253 - __main__ - INFO - ------------ Save best model - MSE: 32.2715 ------------
2024-01-17 17:57:45,254 - __main__ - INFO - Fold 2, mse = 32.2715, mad = 4.2372
2024-01-17 17:57:45,723 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 3.0139
2024-01-17 17:58:08,974 - __main__ - INFO - Fold 2 Epoch 1 Batch 50: Train Loss = 2.1145
2024-01-17 17:58:31,348 - __main__ - INFO - Fold 2 Epoch 1 Batch 100: Train Loss = 1.7620
2024-01-17 17:58:54,007 - __main__ - INFO - Fold 2 Epoch 1 Batch 150: Train Loss = 3.3524
2024-01-17 17:59:16,467 - __main__ - INFO - Fold 2 Epoch 1 Batch 200: Train Loss = 2.6847
2024-01-17 17:59:39,273 - __main__ - INFO - Fold 2 Epoch 1 Batch 250: Train Loss = 3.2416
2024-01-17 18:00:01,899 - __main__ - INFO - Fold 2 Epoch 1 Batch 300: Train Loss = 1.7985
2024-01-17 18:00:52,350 - __main__ - INFO - Fold 2, mse = 33.4103, mad = 4.4341
2024-01-17 18:00:52,818 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 2.3214
2024-01-17 18:01:15,435 - __main__ - INFO - Fold 2 Epoch 2 Batch 50: Train Loss = 2.7334
2024-01-17 18:01:38,190 - __main__ - INFO - Fold 2 Epoch 2 Batch 100: Train Loss = 2.0583
2024-01-17 18:02:01,505 - __main__ - INFO - Fold 2 Epoch 2 Batch 150: Train Loss = 2.0508
2024-01-17 18:02:24,932 - __main__ - INFO - Fold 2 Epoch 2 Batch 200: Train Loss = 2.5166
2024-01-17 18:02:47,278 - __main__ - INFO - Fold 2 Epoch 2 Batch 250: Train Loss = 1.9847
2024-01-17 18:03:09,440 - __main__ - INFO - Fold 2 Epoch 2 Batch 300: Train Loss = 2.5122
2024-01-17 18:03:58,491 - __main__ - INFO - Fold 2, mse = 41.4516, mad = 4.9935
2024-01-17 18:03:58,934 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 2.1332
2024-01-17 18:04:21,807 - __main__ - INFO - Fold 2 Epoch 3 Batch 50: Train Loss = 2.0320
2024-01-17 18:04:44,426 - __main__ - INFO - Fold 2 Epoch 3 Batch 100: Train Loss = 1.7714
2024-01-17 18:05:07,873 - __main__ - INFO - Fold 2 Epoch 3 Batch 150: Train Loss = 1.9435
2024-01-17 18:05:30,509 - __main__ - INFO - Fold 2 Epoch 3 Batch 200: Train Loss = 2.6327
2024-01-17 18:05:53,544 - __main__ - INFO - Fold 2 Epoch 3 Batch 250: Train Loss = 2.1327
2024-01-17 18:06:16,683 - __main__ - INFO - Fold 2 Epoch 3 Batch 300: Train Loss = 2.8340
2024-01-17 18:07:07,327 - __main__ - INFO - Fold 2, mse = 34.7106, mad = 4.2769
2024-01-17 18:07:07,916 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 2.7028
2024-01-17 18:07:31,173 - __main__ - INFO - Fold 2 Epoch 4 Batch 50: Train Loss = 1.9703
2024-01-17 18:07:53,974 - __main__ - INFO - Fold 2 Epoch 4 Batch 100: Train Loss = 1.7745
2024-01-17 18:08:16,279 - __main__ - INFO - Fold 2 Epoch 4 Batch 150: Train Loss = 2.9966
2024-01-17 18:08:38,952 - __main__ - INFO - Fold 2 Epoch 4 Batch 200: Train Loss = 2.3209
2024-01-17 18:09:02,059 - __main__ - INFO - Fold 2 Epoch 4 Batch 250: Train Loss = 2.2841
2024-01-17 18:09:25,473 - __main__ - INFO - Fold 2 Epoch 4 Batch 300: Train Loss = 2.2644
2024-01-17 18:10:14,160 - __main__ - INFO - Fold 2, mse = 33.4120, mad = 4.4730
2024-01-17 18:10:14,636 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 1.8920
2024-01-17 18:10:37,558 - __main__ - INFO - Fold 2 Epoch 5 Batch 50: Train Loss = 3.2246
2024-01-17 18:11:00,756 - __main__ - INFO - Fold 2 Epoch 5 Batch 100: Train Loss = 1.9513
2024-01-17 18:11:23,388 - __main__ - INFO - Fold 2 Epoch 5 Batch 150: Train Loss = 2.2189
2024-01-17 18:11:47,278 - __main__ - INFO - Fold 2 Epoch 5 Batch 200: Train Loss = 2.0179
2024-01-17 18:12:11,342 - __main__ - INFO - Fold 2 Epoch 5 Batch 250: Train Loss = 2.0193
2024-01-17 18:12:34,097 - __main__ - INFO - Fold 2 Epoch 5 Batch 300: Train Loss = 2.2559
2024-01-17 18:13:22,637 - __main__ - INFO - Fold 2, mse = 32.3961, mad = 4.2270
2024-01-17 18:13:23,068 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 2.0981
2024-01-17 18:13:45,762 - __main__ - INFO - Fold 2 Epoch 6 Batch 50: Train Loss = 2.8030
2024-01-17 18:14:10,519 - __main__ - INFO - Fold 2 Epoch 6 Batch 100: Train Loss = 2.2227
2024-01-17 18:14:35,613 - __main__ - INFO - Fold 2 Epoch 6 Batch 150: Train Loss = 2.1644
2024-01-17 18:14:58,543 - __main__ - INFO - Fold 2 Epoch 6 Batch 200: Train Loss = 1.7545
2024-01-17 18:15:21,749 - __main__ - INFO - Fold 2 Epoch 6 Batch 250: Train Loss = 2.3058
2024-01-17 18:15:44,958 - __main__ - INFO - Fold 2 Epoch 6 Batch 300: Train Loss = 3.5720
2024-01-17 18:16:34,466 - __main__ - INFO - Fold 2, mse = 32.5846, mad = 4.3430
2024-01-17 18:16:34,906 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 2.5164
2024-01-17 18:16:59,056 - __main__ - INFO - Fold 2 Epoch 7 Batch 50: Train Loss = 1.9335
2024-01-17 18:17:22,231 - __main__ - INFO - Fold 2 Epoch 7 Batch 100: Train Loss = 1.8511
2024-01-17 18:17:45,022 - __main__ - INFO - Fold 2 Epoch 7 Batch 150: Train Loss = 2.1263
2024-01-17 18:18:07,950 - __main__ - INFO - Fold 2 Epoch 7 Batch 200: Train Loss = 1.7503
2024-01-17 18:18:32,438 - __main__ - INFO - Fold 2 Epoch 7 Batch 250: Train Loss = 2.0021
2024-01-17 18:18:56,739 - __main__ - INFO - Fold 2 Epoch 7 Batch 300: Train Loss = 2.4524
2024-01-17 18:19:45,546 - __main__ - INFO - Fold 2, mse = 33.1932, mad = 4.2619
2024-01-17 18:19:46,000 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 2.1614
2024-01-17 18:20:08,973 - __main__ - INFO - Fold 2 Epoch 8 Batch 50: Train Loss = 2.4746
2024-01-17 18:20:31,345 - __main__ - INFO - Fold 2 Epoch 8 Batch 100: Train Loss = 2.0497
2024-01-17 18:20:54,022 - __main__ - INFO - Fold 2 Epoch 8 Batch 150: Train Loss = 2.4393
2024-01-17 18:21:17,551 - __main__ - INFO - Fold 2 Epoch 8 Batch 200: Train Loss = 3.0980
2024-01-17 18:21:41,268 - __main__ - INFO - Fold 2 Epoch 8 Batch 250: Train Loss = 3.6533
2024-01-17 18:22:05,860 - __main__ - INFO - Fold 2 Epoch 8 Batch 300: Train Loss = 1.8943
2024-01-17 18:22:54,942 - __main__ - INFO - Fold 2, mse = 33.5621, mad = 4.3002
2024-01-17 18:22:55,536 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 2.1299
2024-01-17 18:23:19,590 - __main__ - INFO - Fold 2 Epoch 9 Batch 50: Train Loss = 2.0047
2024-01-17 18:23:44,037 - __main__ - INFO - Fold 2 Epoch 9 Batch 100: Train Loss = 1.8221
2024-01-17 18:24:07,054 - __main__ - INFO - Fold 2 Epoch 9 Batch 150: Train Loss = 1.9666
2024-01-17 18:24:30,295 - __main__ - INFO - Fold 2 Epoch 9 Batch 200: Train Loss = 2.2971
2024-01-17 18:24:53,119 - __main__ - INFO - Fold 2 Epoch 9 Batch 250: Train Loss = 2.0991
2024-01-17 18:25:15,836 - __main__ - INFO - Fold 2 Epoch 9 Batch 300: Train Loss = 2.4379
2024-01-17 18:26:05,161 - __main__ - INFO - Fold 2, mse = 32.8278, mad = 4.1686
2024-01-17 18:26:05,595 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 2.4574
2024-01-17 18:26:29,615 - __main__ - INFO - Fold 2 Epoch 10 Batch 50: Train Loss = 2.7366
2024-01-17 18:26:53,979 - __main__ - INFO - Fold 2 Epoch 10 Batch 100: Train Loss = 1.7639
2024-01-17 18:27:17,210 - __main__ - INFO - Fold 2 Epoch 10 Batch 150: Train Loss = 3.4428
2024-01-17 18:27:40,520 - __main__ - INFO - Fold 2 Epoch 10 Batch 200: Train Loss = 2.0224
2024-01-17 18:28:04,511 - __main__ - INFO - Fold 2 Epoch 10 Batch 250: Train Loss = 2.4108
2024-01-17 18:28:27,282 - __main__ - INFO - Fold 2 Epoch 10 Batch 300: Train Loss = 1.9197
2024-01-17 18:29:15,536 - __main__ - INFO - Fold 2, epoch 10: Loss = 2.2902 Valid loss = 2.6564 MSE = 38.2103 AUROC = 0.6573
2024-01-17 18:29:15,537 - __main__ - INFO - Fold 2, mse = 38.2103, mad = 4.8011
2024-01-17 18:29:15,992 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 3.4107
2024-01-17 18:29:39,831 - __main__ - INFO - Fold 2 Epoch 11 Batch 50: Train Loss = 1.6994
2024-01-17 18:30:02,710 - __main__ - INFO - Fold 2 Epoch 11 Batch 100: Train Loss = 2.1686
2024-01-17 18:30:25,486 - __main__ - INFO - Fold 2 Epoch 11 Batch 150: Train Loss = 2.2174
2024-01-17 18:30:47,622 - __main__ - INFO - Fold 2 Epoch 11 Batch 200: Train Loss = 2.7870
2024-01-17 18:31:10,587 - __main__ - INFO - Fold 2 Epoch 11 Batch 250: Train Loss = 1.6298
2024-01-17 18:31:34,859 - __main__ - INFO - Fold 2 Epoch 11 Batch 300: Train Loss = 2.3347
2024-01-17 18:32:26,466 - __main__ - INFO - Fold 2, mse = 36.0882, mad = 4.5706
2024-01-17 18:32:26,964 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 1.7556
2024-01-17 18:32:50,677 - __main__ - INFO - Fold 2 Epoch 12 Batch 50: Train Loss = 2.0068
2024-01-17 18:33:14,067 - __main__ - INFO - Fold 2 Epoch 12 Batch 100: Train Loss = 2.1142
2024-01-17 18:33:37,336 - __main__ - INFO - Fold 2 Epoch 12 Batch 150: Train Loss = 2.1373
2024-01-17 18:34:00,422 - __main__ - INFO - Fold 2 Epoch 12 Batch 200: Train Loss = 1.9176
2024-01-17 18:34:23,864 - __main__ - INFO - Fold 2 Epoch 12 Batch 250: Train Loss = 1.5199
2024-01-17 18:34:46,490 - __main__ - INFO - Fold 2 Epoch 12 Batch 300: Train Loss = 1.4483
2024-01-17 18:35:34,739 - __main__ - INFO - Fold 2, mse = 33.3951, mad = 4.3723
2024-01-17 18:35:35,201 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 1.7573
2024-01-17 18:35:58,460 - __main__ - INFO - Fold 2 Epoch 13 Batch 50: Train Loss = 2.3378
2024-01-17 18:36:22,870 - __main__ - INFO - Fold 2 Epoch 13 Batch 100: Train Loss = 1.8513
2024-01-17 18:36:48,281 - __main__ - INFO - Fold 2 Epoch 13 Batch 150: Train Loss = 3.1163
2024-01-17 18:37:14,053 - __main__ - INFO - Fold 2 Epoch 13 Batch 200: Train Loss = 2.2520
2024-01-17 18:37:37,307 - __main__ - INFO - Fold 2 Epoch 13 Batch 250: Train Loss = 1.9361
2024-01-17 18:38:00,468 - __main__ - INFO - Fold 2 Epoch 13 Batch 300: Train Loss = 2.2656
2024-01-17 18:38:49,508 - __main__ - INFO - Fold 2, mse = 35.4786, mad = 4.3090
2024-01-17 18:38:50,014 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 1.7927
2024-01-17 18:39:12,780 - __main__ - INFO - Fold 2 Epoch 14 Batch 50: Train Loss = 2.0335
2024-01-17 18:39:35,397 - __main__ - INFO - Fold 2 Epoch 14 Batch 100: Train Loss = 2.1232
2024-01-17 18:39:58,086 - __main__ - INFO - Fold 2 Epoch 14 Batch 150: Train Loss = 1.9458
2024-01-17 18:40:21,128 - __main__ - INFO - Fold 2 Epoch 14 Batch 200: Train Loss = 1.8440
2024-01-17 18:40:44,221 - __main__ - INFO - Fold 2 Epoch 14 Batch 250: Train Loss = 2.4683
2024-01-17 18:41:10,413 - __main__ - INFO - Fold 2 Epoch 14 Batch 300: Train Loss = 2.6839
2024-01-17 18:42:01,890 - __main__ - INFO - Fold 2, mse = 32.9867, mad = 4.3529
2024-01-17 18:42:02,312 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 2.3033
2024-01-17 18:42:25,296 - __main__ - INFO - Fold 2 Epoch 15 Batch 50: Train Loss = 1.9546
2024-01-17 18:42:48,630 - __main__ - INFO - Fold 2 Epoch 15 Batch 100: Train Loss = 1.8235
2024-01-17 18:43:11,596 - __main__ - INFO - Fold 2 Epoch 15 Batch 150: Train Loss = 2.2370
2024-01-17 18:43:35,168 - __main__ - INFO - Fold 2 Epoch 15 Batch 200: Train Loss = 2.4515
2024-01-17 18:43:58,214 - __main__ - INFO - Fold 2 Epoch 15 Batch 250: Train Loss = 2.2684
2024-01-17 18:44:21,418 - __main__ - INFO - Fold 2 Epoch 15 Batch 300: Train Loss = 2.5937
2024-01-17 18:45:11,457 - __main__ - INFO - Fold 2, mse = 35.3621, mad = 4.6462
2024-01-17 18:45:11,889 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 2.0153
2024-01-17 18:45:35,842 - __main__ - INFO - Fold 2 Epoch 16 Batch 50: Train Loss = 1.9785
2024-01-17 18:46:00,762 - __main__ - INFO - Fold 2 Epoch 16 Batch 100: Train Loss = 1.7418
2024-01-17 18:46:25,380 - __main__ - INFO - Fold 2 Epoch 16 Batch 150: Train Loss = 2.1967
2024-01-17 18:46:49,670 - __main__ - INFO - Fold 2 Epoch 16 Batch 200: Train Loss = 3.2877
2024-01-17 18:47:12,798 - __main__ - INFO - Fold 2 Epoch 16 Batch 250: Train Loss = 2.0979
2024-01-17 18:47:35,537 - __main__ - INFO - Fold 2 Epoch 16 Batch 300: Train Loss = 1.6475
2024-01-17 18:48:24,943 - __main__ - INFO - Fold 2, mse = 38.7357, mad = 4.8101
2024-01-17 18:48:25,420 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 2.6500
2024-01-17 18:48:48,609 - __main__ - INFO - Fold 2 Epoch 17 Batch 50: Train Loss = 2.3756
2024-01-17 18:49:11,454 - __main__ - INFO - Fold 2 Epoch 17 Batch 100: Train Loss = 1.7573
2024-01-17 18:49:34,806 - __main__ - INFO - Fold 2 Epoch 17 Batch 150: Train Loss = 1.8829
2024-01-17 18:49:58,870 - __main__ - INFO - Fold 2 Epoch 17 Batch 200: Train Loss = 2.1736
2024-01-17 18:50:23,559 - __main__ - INFO - Fold 2 Epoch 17 Batch 250: Train Loss = 1.5705
2024-01-17 18:50:48,437 - __main__ - INFO - Fold 2 Epoch 17 Batch 300: Train Loss = 1.7576
2024-01-17 18:51:38,957 - __main__ - INFO - Fold 2, mse = 36.3581, mad = 4.6558
2024-01-17 18:51:39,417 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 2.1935
2024-01-17 18:52:03,106 - __main__ - INFO - Fold 2 Epoch 18 Batch 50: Train Loss = 2.3600
2024-01-17 18:52:25,588 - __main__ - INFO - Fold 2 Epoch 18 Batch 100: Train Loss = 2.2948
2024-01-17 18:52:48,640 - __main__ - INFO - Fold 2 Epoch 18 Batch 150: Train Loss = 1.8988
2024-01-17 18:53:11,912 - __main__ - INFO - Fold 2 Epoch 18 Batch 200: Train Loss = 2.9100
2024-01-17 18:53:35,305 - __main__ - INFO - Fold 2 Epoch 18 Batch 250: Train Loss = 2.1683
2024-01-17 18:53:58,341 - __main__ - INFO - Fold 2 Epoch 18 Batch 300: Train Loss = 3.4069
2024-01-17 18:54:47,990 - __main__ - INFO - Fold 2, mse = 33.2974, mad = 4.4893
2024-01-17 18:54:48,487 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 2.2793
2024-01-17 18:55:13,206 - __main__ - INFO - Fold 2 Epoch 19 Batch 50: Train Loss = 1.8860
2024-01-17 18:55:36,880 - __main__ - INFO - Fold 2 Epoch 19 Batch 100: Train Loss = 2.5309
2024-01-17 18:56:00,839 - __main__ - INFO - Fold 2 Epoch 19 Batch 150: Train Loss = 2.1921
2024-01-17 18:56:24,622 - __main__ - INFO - Fold 2 Epoch 19 Batch 200: Train Loss = 1.8198
2024-01-17 18:56:49,069 - __main__ - INFO - Fold 2 Epoch 19 Batch 250: Train Loss = 2.3880
2024-01-17 18:57:11,417 - __main__ - INFO - Fold 2 Epoch 19 Batch 300: Train Loss = 2.3323
2024-01-17 18:57:59,805 - __main__ - INFO - Fold 2, mse = 36.7668, mad = 4.8525
2024-01-17 18:58:00,599 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 72.7201
2024-01-17 18:58:23,250 - __main__ - INFO - Fold 3 Epoch 0 Batch 50: Train Loss = 2.7824
2024-01-17 18:58:45,972 - __main__ - INFO - Fold 3 Epoch 0 Batch 100: Train Loss = 2.0919
2024-01-17 18:59:08,904 - __main__ - INFO - Fold 3 Epoch 0 Batch 150: Train Loss = 2.5990
2024-01-17 18:59:33,910 - __main__ - INFO - Fold 3 Epoch 0 Batch 200: Train Loss = 2.5073
2024-01-17 18:59:58,168 - __main__ - INFO - Fold 3 Epoch 0 Batch 250: Train Loss = 2.4146
2024-01-17 19:00:21,009 - __main__ - INFO - Fold 3 Epoch 0 Batch 300: Train Loss = 2.6351
2024-01-17 19:01:10,992 - __main__ - INFO - Fold 3, epoch 0: Loss = 3.5105 Valid loss = 2.2618 MSE = 32.0661 AUROC = 0.6144
2024-01-17 19:01:10,993 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 32.0661 ------------
2024-01-17 19:01:11,206 - __main__ - INFO - ------------ Save best model - MSE: 32.0661 ------------
2024-01-17 19:01:11,207 - __main__ - INFO - Fold 3, mse = 32.0661, mad = 4.2377
2024-01-17 19:01:11,635 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 2.5516
2024-01-17 19:01:35,638 - __main__ - INFO - Fold 3 Epoch 1 Batch 50: Train Loss = 2.9723
2024-01-17 19:01:58,704 - __main__ - INFO - Fold 3 Epoch 1 Batch 100: Train Loss = 2.8284
2024-01-17 19:02:21,768 - __main__ - INFO - Fold 3 Epoch 1 Batch 150: Train Loss = 2.4284
2024-01-17 19:02:44,700 - __main__ - INFO - Fold 3 Epoch 1 Batch 200: Train Loss = 2.5269
2024-01-17 19:03:07,650 - __main__ - INFO - Fold 3 Epoch 1 Batch 250: Train Loss = 2.6998
2024-01-17 19:03:31,267 - __main__ - INFO - Fold 3 Epoch 1 Batch 300: Train Loss = 2.1622
2024-01-17 19:04:21,877 - __main__ - INFO - Fold 3, mse = 33.3636, mad = 4.4532
2024-01-17 19:04:22,383 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 2.4168
2024-01-17 19:04:46,238 - __main__ - INFO - Fold 3 Epoch 2 Batch 50: Train Loss = 2.0645
2024-01-17 19:05:09,488 - __main__ - INFO - Fold 3 Epoch 2 Batch 100: Train Loss = 1.9856
2024-01-17 19:05:32,486 - __main__ - INFO - Fold 3 Epoch 2 Batch 150: Train Loss = 2.3928
2024-01-17 19:05:55,818 - __main__ - INFO - Fold 3 Epoch 2 Batch 200: Train Loss = 3.0989
2024-01-17 19:06:20,099 - __main__ - INFO - Fold 3 Epoch 2 Batch 250: Train Loss = 2.0831
2024-01-17 19:06:43,734 - __main__ - INFO - Fold 3 Epoch 2 Batch 300: Train Loss = 2.1779
2024-01-17 19:07:34,500 - __main__ - INFO - Fold 3, mse = 37.0938, mad = 4.7524
2024-01-17 19:07:35,121 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 2.3761
2024-01-17 19:08:00,649 - __main__ - INFO - Fold 3 Epoch 3 Batch 50: Train Loss = 1.8917
2024-01-17 19:08:26,984 - __main__ - INFO - Fold 3 Epoch 3 Batch 100: Train Loss = 2.7354
2024-01-17 19:08:57,479 - __main__ - INFO - Fold 3 Epoch 3 Batch 150: Train Loss = 1.5958
2024-01-17 19:09:25,408 - __main__ - INFO - Fold 3 Epoch 3 Batch 200: Train Loss = 2.1988
2024-01-17 19:09:49,006 - __main__ - INFO - Fold 3 Epoch 3 Batch 250: Train Loss = 2.3174
2024-01-17 19:10:13,061 - __main__ - INFO - Fold 3 Epoch 3 Batch 300: Train Loss = 2.0991
2024-01-17 19:11:14,380 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.9428 ------------
2024-01-17 19:11:14,763 - __main__ - INFO - ------------ Save best model - MSE: 30.9428 ------------
2024-01-17 19:11:14,764 - __main__ - INFO - Fold 3, mse = 30.9428, mad = 4.2254
2024-01-17 19:11:15,378 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 1.7378
2024-01-17 19:11:45,645 - __main__ - INFO - Fold 3 Epoch 4 Batch 50: Train Loss = 1.8503
2024-01-17 19:12:13,713 - __main__ - INFO - Fold 3 Epoch 4 Batch 100: Train Loss = 2.4223
2024-01-17 19:12:37,999 - __main__ - INFO - Fold 3 Epoch 4 Batch 150: Train Loss = 2.0587
2024-01-17 19:13:03,154 - __main__ - INFO - Fold 3 Epoch 4 Batch 200: Train Loss = 1.8125
2024-01-17 19:13:31,134 - __main__ - INFO - Fold 3 Epoch 4 Batch 250: Train Loss = 2.8489
2024-01-17 19:14:00,251 - __main__ - INFO - Fold 3 Epoch 4 Batch 300: Train Loss = 1.9388
2024-01-17 19:14:57,408 - __main__ - INFO - Fold 3, mse = 36.5599, mad = 4.7272
2024-01-17 19:14:57,868 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 3.4983
2024-01-17 19:15:20,343 - __main__ - INFO - Fold 3 Epoch 5 Batch 50: Train Loss = 2.3341
2024-01-17 19:15:42,990 - __main__ - INFO - Fold 3 Epoch 5 Batch 100: Train Loss = 2.3351
2024-01-17 19:16:06,662 - __main__ - INFO - Fold 3 Epoch 5 Batch 150: Train Loss = 2.1793
2024-01-17 19:16:30,077 - __main__ - INFO - Fold 3 Epoch 5 Batch 200: Train Loss = 2.0298
2024-01-17 19:16:54,149 - __main__ - INFO - Fold 3 Epoch 5 Batch 250: Train Loss = 2.3051
2024-01-17 19:17:17,519 - __main__ - INFO - Fold 3 Epoch 5 Batch 300: Train Loss = 2.1616
2024-01-17 19:18:07,196 - __main__ - INFO - Fold 3, mse = 33.1285, mad = 4.3713
2024-01-17 19:18:07,625 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 2.2661
2024-01-17 19:18:31,725 - __main__ - INFO - Fold 3 Epoch 6 Batch 50: Train Loss = 1.8301
2024-01-17 19:18:55,744 - __main__ - INFO - Fold 3 Epoch 6 Batch 100: Train Loss = 2.1246
2024-01-17 19:19:19,020 - __main__ - INFO - Fold 3 Epoch 6 Batch 150: Train Loss = 2.3465
2024-01-17 19:19:41,565 - __main__ - INFO - Fold 3 Epoch 6 Batch 200: Train Loss = 1.8942
2024-01-17 19:20:04,549 - __main__ - INFO - Fold 3 Epoch 6 Batch 250: Train Loss = 2.2714
2024-01-17 19:20:27,113 - __main__ - INFO - Fold 3 Epoch 6 Batch 300: Train Loss = 1.8359
2024-01-17 19:21:19,595 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.8537 ------------
2024-01-17 19:21:19,937 - __main__ - INFO - ------------ Save best model - MSE: 30.8537 ------------
2024-01-17 19:21:19,938 - __main__ - INFO - Fold 3, mse = 30.8537, mad = 4.0885
2024-01-17 19:21:20,545 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 3.3148
2024-01-17 19:21:49,930 - __main__ - INFO - Fold 3 Epoch 7 Batch 50: Train Loss = 1.8930
2024-01-17 19:22:17,630 - __main__ - INFO - Fold 3 Epoch 7 Batch 100: Train Loss = 2.6123
2024-01-17 19:22:42,071 - __main__ - INFO - Fold 3 Epoch 7 Batch 150: Train Loss = 1.8056
2024-01-17 19:23:05,752 - __main__ - INFO - Fold 3 Epoch 7 Batch 200: Train Loss = 1.8319
2024-01-17 19:23:31,889 - __main__ - INFO - Fold 3 Epoch 7 Batch 250: Train Loss = 1.9767
2024-01-17 19:24:00,955 - __main__ - INFO - Fold 3 Epoch 7 Batch 300: Train Loss = 2.7937
2024-01-17 19:25:00,295 - __main__ - INFO - Fold 3, mse = 34.0752, mad = 4.5758
2024-01-17 19:25:00,878 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 2.6552
2024-01-17 19:25:25,956 - __main__ - INFO - Fold 3 Epoch 8 Batch 50: Train Loss = 2.1038
2024-01-17 19:25:51,186 - __main__ - INFO - Fold 3 Epoch 8 Batch 100: Train Loss = 2.1792
2024-01-17 19:26:19,203 - __main__ - INFO - Fold 3 Epoch 8 Batch 150: Train Loss = 2.4266
2024-01-17 19:26:47,788 - __main__ - INFO - Fold 3 Epoch 8 Batch 200: Train Loss = 2.7039
2024-01-17 19:27:16,515 - __main__ - INFO - Fold 3 Epoch 8 Batch 250: Train Loss = 3.7676
2024-01-17 19:27:40,436 - __main__ - INFO - Fold 3 Epoch 8 Batch 300: Train Loss = 1.8140
2024-01-17 19:28:30,063 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 30.3313 ------------
2024-01-17 19:28:30,277 - __main__ - INFO - ------------ Save best model - MSE: 30.3313 ------------
2024-01-17 19:28:30,278 - __main__ - INFO - Fold 3, mse = 30.3313, mad = 4.0395
2024-01-17 19:28:30,705 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 2.4653
2024-01-17 19:28:54,902 - __main__ - INFO - Fold 3 Epoch 9 Batch 50: Train Loss = 2.2574
2024-01-17 19:29:17,912 - __main__ - INFO - Fold 3 Epoch 9 Batch 100: Train Loss = 2.3296
2024-01-17 19:29:41,114 - __main__ - INFO - Fold 3 Epoch 9 Batch 150: Train Loss = 2.6705
2024-01-17 19:30:03,601 - __main__ - INFO - Fold 3 Epoch 9 Batch 200: Train Loss = 2.4313
2024-01-17 19:30:26,225 - __main__ - INFO - Fold 3 Epoch 9 Batch 250: Train Loss = 1.8440
2024-01-17 19:30:48,904 - __main__ - INFO - Fold 3 Epoch 9 Batch 300: Train Loss = 2.4089
2024-01-17 19:31:38,154 - __main__ - INFO - Fold 3, mse = 30.6396, mad = 4.0199
2024-01-17 19:31:38,597 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 2.4093
2024-01-17 19:32:01,443 - __main__ - INFO - Fold 3 Epoch 10 Batch 50: Train Loss = 2.5178
2024-01-17 19:32:25,514 - __main__ - INFO - Fold 3 Epoch 10 Batch 100: Train Loss = 1.6541
2024-01-17 19:32:51,071 - __main__ - INFO - Fold 3 Epoch 10 Batch 150: Train Loss = 1.7168
2024-01-17 19:33:15,120 - __main__ - INFO - Fold 3 Epoch 10 Batch 200: Train Loss = 1.9219
2024-01-17 19:33:39,170 - __main__ - INFO - Fold 3 Epoch 10 Batch 250: Train Loss = 2.5167
2024-01-17 19:34:01,744 - __main__ - INFO - Fold 3 Epoch 10 Batch 300: Train Loss = 3.7198
2024-01-17 19:34:51,361 - __main__ - INFO - Fold 3, epoch 10: Loss = 2.2979 Valid loss = 2.4141 MSE = 34.6109 AUROC = 0.6946
2024-01-17 19:34:51,362 - __main__ - INFO - Fold 3, mse = 34.6109, mad = 4.5119
2024-01-17 19:34:51,824 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 3.1499
2024-01-17 19:35:14,760 - __main__ - INFO - Fold 3 Epoch 11 Batch 50: Train Loss = 1.9166
2024-01-17 19:35:37,614 - __main__ - INFO - Fold 3 Epoch 11 Batch 100: Train Loss = 1.8062
2024-01-17 19:36:00,182 - __main__ - INFO - Fold 3 Epoch 11 Batch 150: Train Loss = 1.8909
2024-01-17 19:36:22,839 - __main__ - INFO - Fold 3 Epoch 11 Batch 200: Train Loss = 2.0775
2024-01-17 19:36:46,255 - __main__ - INFO - Fold 3 Epoch 11 Batch 250: Train Loss = 3.3625
2024-01-17 19:37:10,139 - __main__ - INFO - Fold 3 Epoch 11 Batch 300: Train Loss = 2.7659
2024-01-17 19:38:02,571 - __main__ - INFO - Fold 3, mse = 31.1878, mad = 4.2045
2024-01-17 19:38:03,013 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 1.5785
2024-01-17 19:38:26,369 - __main__ - INFO - Fold 3 Epoch 12 Batch 50: Train Loss = 2.4340
2024-01-17 19:38:49,443 - __main__ - INFO - Fold 3 Epoch 12 Batch 100: Train Loss = 2.1312
2024-01-17 19:39:12,441 - __main__ - INFO - Fold 3 Epoch 12 Batch 150: Train Loss = 2.2445
2024-01-17 19:39:35,233 - __main__ - INFO - Fold 3 Epoch 12 Batch 200: Train Loss = 2.3202
2024-01-17 19:39:58,268 - __main__ - INFO - Fold 3 Epoch 12 Batch 250: Train Loss = 2.4855
2024-01-17 19:40:21,461 - __main__ - INFO - Fold 3 Epoch 12 Batch 300: Train Loss = 1.9352
2024-01-17 19:41:10,493 - __main__ - INFO - Fold 3, mse = 34.1236, mad = 4.5339
2024-01-17 19:41:10,917 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 2.2029
2024-01-17 19:41:33,264 - __main__ - INFO - Fold 3 Epoch 13 Batch 50: Train Loss = 2.3592
2024-01-17 19:41:56,949 - __main__ - INFO - Fold 3 Epoch 13 Batch 100: Train Loss = 2.6167
2024-01-17 19:42:22,251 - __main__ - INFO - Fold 3 Epoch 13 Batch 150: Train Loss = 1.9809
2024-01-17 19:42:46,528 - __main__ - INFO - Fold 3 Epoch 13 Batch 200: Train Loss = 1.9071
2024-01-17 19:43:09,543 - __main__ - INFO - Fold 3 Epoch 13 Batch 250: Train Loss = 2.0837
2024-01-17 19:43:32,455 - __main__ - INFO - Fold 3 Epoch 13 Batch 300: Train Loss = 2.1731
2024-01-17 19:44:22,014 - __main__ - INFO - Fold 3, mse = 42.4428, mad = 5.1822
2024-01-17 19:44:22,478 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 1.7409
2024-01-17 19:44:45,171 - __main__ - INFO - Fold 3 Epoch 14 Batch 50: Train Loss = 1.7057
2024-01-17 19:45:08,046 - __main__ - INFO - Fold 3 Epoch 14 Batch 100: Train Loss = 1.9430
2024-01-17 19:45:30,678 - __main__ - INFO - Fold 3 Epoch 14 Batch 150: Train Loss = 2.0546
2024-01-17 19:45:53,078 - __main__ - INFO - Fold 3 Epoch 14 Batch 200: Train Loss = 1.8920
2024-01-17 19:46:15,213 - __main__ - INFO - Fold 3 Epoch 14 Batch 250: Train Loss = 2.1393
2024-01-17 19:46:38,429 - __main__ - INFO - Fold 3 Epoch 14 Batch 300: Train Loss = 1.9599
2024-01-17 19:47:29,738 - __main__ - INFO - Fold 3, mse = 31.4970, mad = 4.2581
2024-01-17 19:47:30,339 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 1.6237
2024-01-17 19:47:54,073 - __main__ - INFO - Fold 3 Epoch 15 Batch 50: Train Loss = 2.0568
2024-01-17 19:48:16,713 - __main__ - INFO - Fold 3 Epoch 15 Batch 100: Train Loss = 1.9151
2024-01-17 19:48:39,542 - __main__ - INFO - Fold 3 Epoch 15 Batch 150: Train Loss = 3.5058
2024-01-17 19:49:02,559 - __main__ - INFO - Fold 3 Epoch 15 Batch 200: Train Loss = 2.2928
2024-01-17 19:49:25,757 - __main__ - INFO - Fold 3 Epoch 15 Batch 250: Train Loss = 2.4261
2024-01-17 19:49:49,082 - __main__ - INFO - Fold 3 Epoch 15 Batch 300: Train Loss = 2.3540
2024-01-17 19:50:38,120 - __main__ - INFO - Fold 3, mse = 30.6996, mad = 4.1579
2024-01-17 19:50:38,574 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 1.9281
2024-01-17 19:51:01,359 - __main__ - INFO - Fold 3 Epoch 16 Batch 50: Train Loss = 1.8433
2024-01-17 19:51:26,566 - __main__ - INFO - Fold 3 Epoch 16 Batch 100: Train Loss = 2.0725
2024-01-17 19:51:51,090 - __main__ - INFO - Fold 3 Epoch 16 Batch 150: Train Loss = 2.4720
2024-01-17 19:52:15,765 - __main__ - INFO - Fold 3 Epoch 16 Batch 200: Train Loss = 2.5692
2024-01-17 19:52:39,794 - __main__ - INFO - Fold 3 Epoch 16 Batch 250: Train Loss = 2.1941
2024-01-17 19:53:06,183 - __main__ - INFO - Fold 3 Epoch 16 Batch 300: Train Loss = 2.2749
2024-01-17 19:54:06,585 - __main__ - INFO - Fold 3, mse = 34.7392, mad = 4.6084
2024-01-17 19:54:07,007 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 2.3242
2024-01-17 19:54:30,523 - __main__ - INFO - Fold 3 Epoch 17 Batch 50: Train Loss = 2.0672
2024-01-17 19:54:54,586 - __main__ - INFO - Fold 3 Epoch 17 Batch 100: Train Loss = 2.0883
2024-01-17 19:55:22,012 - __main__ - INFO - Fold 3 Epoch 17 Batch 150: Train Loss = 2.1072
2024-01-17 19:55:46,895 - __main__ - INFO - Fold 3 Epoch 17 Batch 200: Train Loss = 1.8833
2024-01-17 19:56:15,520 - __main__ - INFO - Fold 3 Epoch 17 Batch 250: Train Loss = 2.0980
2024-01-17 19:56:43,932 - __main__ - INFO - Fold 3 Epoch 17 Batch 300: Train Loss = 1.8209
2024-01-17 19:57:37,588 - __main__ - INFO - Fold 3, mse = 34.3559, mad = 4.5823
2024-01-17 19:57:38,058 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 2.5225
2024-01-17 19:58:02,175 - __main__ - INFO - Fold 3 Epoch 18 Batch 50: Train Loss = 2.0587
2024-01-17 19:58:24,990 - __main__ - INFO - Fold 3 Epoch 18 Batch 100: Train Loss = 2.7301
2024-01-17 19:58:49,002 - __main__ - INFO - Fold 3 Epoch 18 Batch 150: Train Loss = 2.1245
2024-01-17 19:59:12,641 - __main__ - INFO - Fold 3 Epoch 18 Batch 200: Train Loss = 1.6084
2024-01-17 19:59:36,247 - __main__ - INFO - Fold 3 Epoch 18 Batch 250: Train Loss = 2.0882
2024-01-17 19:59:59,967 - __main__ - INFO - Fold 3 Epoch 18 Batch 300: Train Loss = 3.4660
2024-01-17 20:00:52,577 - __main__ - INFO - Fold 3, mse = 30.9539, mad = 4.1696
2024-01-17 20:00:53,219 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 1.7422
2024-01-17 20:01:21,784 - __main__ - INFO - Fold 3 Epoch 19 Batch 50: Train Loss = 2.4233
2024-01-17 20:01:50,856 - __main__ - INFO - Fold 3 Epoch 19 Batch 100: Train Loss = 2.0786
2024-01-17 20:02:18,039 - __main__ - INFO - Fold 3 Epoch 19 Batch 150: Train Loss = 2.0042
2024-01-17 20:02:43,050 - __main__ - INFO - Fold 3 Epoch 19 Batch 200: Train Loss = 2.2561
2024-01-17 20:03:07,285 - __main__ - INFO - Fold 3 Epoch 19 Batch 250: Train Loss = 2.1535
2024-01-17 20:03:30,551 - __main__ - INFO - Fold 3 Epoch 19 Batch 300: Train Loss = 1.7707
2024-01-17 20:04:20,100 - __main__ - INFO - Fold 3, mse = 34.5940, mad = 4.6652
2024-01-17 20:04:20,101 - __main__ - INFO - mse 32.5068(1.8797)
2024-01-17 20:04:20,101 - __main__ - INFO - mad 4.1472(0.0817)
2024-01-17 20:04:20,102 - __main__ - INFO - auroc 0.6737(0.0630)
2024-01-17 20:04:20,102 - __main__ - INFO - auprc 0.2671(0.0628)
