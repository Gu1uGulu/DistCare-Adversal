2024-03-21 11:01:28,335 - __main__ - INFO - 这是希望输出的info内容
2024-03-21 11:01:28,335 - __main__ - WARNING - 这是希望输出的warning内容
2024-03-21 11:02:45,920 - __main__ - INFO - 32269
2024-03-21 11:02:45,920 - __main__ - INFO - 4034
2024-03-21 11:02:45,920 - __main__ - INFO - 4033
2024-03-21 11:03:13,598 - __main__ - INFO - Batch 0: Test Loss = 0.0931
2024-03-21 11:03:18,802 - __main__ - INFO - 
==>Predicting on test
2024-03-21 11:03:18,802 - __main__ - INFO - Test Loss = 0.1227
2024-03-21 11:03:18,993 - __main__ - INFO - load target data
2024-03-21 11:03:19,908 - __main__ - INFO - Batch 0: Test Loss = 0.1131
2024-03-21 11:03:26,840 - __main__ - INFO - Batch 20: Test Loss = 0.1478
2024-03-21 11:03:33,261 - __main__ - INFO - Batch 40: Test Loss = 0.1526
2024-03-21 11:03:40,150 - __main__ - INFO - Batch 60: Test Loss = 0.1207
2024-03-21 11:03:47,280 - __main__ - INFO - Batch 80: Test Loss = 0.0851
2024-03-21 11:03:54,302 - __main__ - INFO - Batch 100: Test Loss = 0.0724
2024-03-21 11:04:00,888 - __main__ - INFO - Batch 120: Test Loss = 0.1097
2024-03-21 11:04:03,454 - __main__ - INFO - last saved model is in epoch 26
2024-03-21 11:04:04,028 - __main__ - INFO - Batch 0: Test Loss = 0.3375
2024-03-21 11:04:09,024 - __main__ - INFO - 
==>Predicting on test
2024-03-21 11:04:09,025 - __main__ - INFO - Test Loss = 0.2609
2024-03-21 11:29:04,762 - __main__ - INFO - Transfer Target Dataset & Model
2024-03-21 11:29:04,867 - __main__ - INFO - [[-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8285543981909917, 2.770245567717861, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8427648213988651, 0.3744020210323477, 0.6796123704286434, -1.398975413973587, -0.4831419202847951, -0.2120300841305121, 1.5887596625600091, 0.7945789587268225, -0.8612693268611251, -0.4819729243606949, -0.6745224313841819, 0.7137208435891645, -1.447089954740047, -0.7710128163592748, -1.4231815568069368, -0.5851405270139463, -0.5641898854144399, 0.5775106850669863, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -0.9752387057279804, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.0343615044915247, -1.3314821107815475, 0.3379315521074886, -0.3880554131475662, 0.8374745525934485, 2.8093811444689463, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 2.9568603317603377, 4.808428260230306, -0.1299430434915742, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5338330207864587, 0.431156978499758, 0.8964660630930379, -1.342421748214616, -0.6946051308187712, -0.2120300841305121, 0.8952797203562032, 0.0387041303378994, -0.8612693268611251, 0.0726334784031089, -0.5051102137388988, -0.2865727201409924, 0.3988826585206329, -1.3794377690564883, -0.8237089728907875, -0.9221119476695248, -1.6036614531597553, -0.2379932071009605, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, 0.1700684310067274, -0.2031244129114749, -1.1501193786706505, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, 1.0826051686869729, -0.609009148503521, 0.9684393533852332, -0.3880554131475662, 0.8439788318452395, 2.837917502516613, 0.0776143028335215, -0.1259757336723928, 0.0863841325254607, -0.1474826847594624, -0.3999358135056357, -0.0116277505661367, -0.0886088512738706, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.211245000207003, -0.1237195765566573, -0.1259757336723928, 0.0421701504838569, -0.1474826847594624, -0.5354516373428909, -0.0075043080636137, -0.0934220672822521, -0.1491049589303728, 0.0552791522161626, -0.0357876785866217, -0.1716333969449267, 0.278477698357045, 1.2505677424119097, -0.6305581644917595, -0.9062745442328174, -0.7536814885080627, -0.4374103947888615, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, -0.7710128163592748, -0.6738408269117502, -2.438483340619628, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8608899578998963, 2.912112033440545, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3396023676711391, 1.4769583166408096, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.7113800326326694, 0.0906272336952961, 0.5576321683049205, -1.398975413973587, -0.8637756992459511, -0.2120300841305121, 0.4685228328461679, 0.3356549557764047, -0.8612693268611251, -0.574407324821329, -0.5014273394422621, -0.1711542320182811, -0.2539613144618027, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.5641898854144399, 0.1697587389830128, 0.3939858698913405, -0.2032969502372001, -0.2890718868318484, -0.2977155549892737, -0.2031244129114749, -0.725409172952738, -0.995631448716658, -0.7346136214669141, 0.2047416529938912, -0.7879162404292406, -0.4658827214597087, -0.416884337771832, -1.3888212347718671, 0.4897491553635549, -0.7942874573364652, 0.8763143915541441, 2.979783968239297, 0.0776143028335215, -0.2834309446219887, 0.0790528888855871, -0.1474826847594624, -0.3999358135056357, -0.0186903386836148, -0.0954522063493915, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.2380657795779712, -0.3380281643183785, -0.2834309446219887, -0.1096727847689199, -0.1474826847594624, -0.5354516373428909, -0.0015148687509932, -0.087488649839324, -0.1491049589303728, 0.0386929407875269, -0.0467618786482574, -0.158735060378334, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5125273793649132, 0.601421850901989, 0.5034187451388209, -1.455529079732558, -0.5254345623915906, -0.4163803392884947, 0.7885904984786939, 0.2816638966057675, -0.5890172833864098, 0.5902661209826593, -0.3430637446868887, -0.7482466726318323, -0.1864257310498265, 0.6486454066008902, 0.0754999029834364, -0.5851405270139463, -0.2249939001501796, -0.0341172340589738, 0.3466058957088718, 0.3482841380580905, -0.2890718868318484, 0.1923438589112976, -0.2031244129114749, -1.100153472115602, -0.7598199909551685, -0.6868858002659636, 0.3602180776283341, -0.700061185363224, -0.501359972189453, -0.2332733777972843, -1.36588558517574, 0.3571411263970316, -0.7447833078367583, 0.8765002281041955, 2.9805992927549454, 0.0776143028335215, -0.2769313825285214, 0.0755136678180621, -0.1474826847594624, -0.3999358135056357, -0.0169688050451549, -0.0937841116273902, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, -0.0122781725754626, -0.3292808750219816, -0.2769313825285214, -0.107625012968948, -0.1474826847594624, -0.5354516373428909, -0.0002229391773086, -0.0862088042532133, -0.1491049589303728, 0.0432268846241116, -0.0437620129498739, 0.0766269712424727, 0.278477698357045, 1.2505677424119097, -0.3310448442352384, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.3951350096437179, -0.5239726809327129, -0.8378690925056301, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.8988006141103361, 3.0784382346326584, 0.0776143028335215, -0.3487779453829513, 0.070457637721598, -0.1474826847594624, -0.3999358135056357, 0.0045338857876385, -0.0729488956980065, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.1275947723114954, -0.4255010572823463, -0.3487779453829513, -0.1777070069371955, -0.1474826847594624, -0.5354516373428909, 0.029608437271828, -0.0566564538358881, -0.1491049589303728, 0.031626468995041, -0.0514373812797239, -0.0232601678654147, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.4628142160479743, -0.2499025111091658, -0.3775493813102845, -1.1162070851787322, -1.1739217413624488, 0.196670426185453, 0.6819012766011855, 0.4706326037029981, -0.6071674196180575, -0.1492090827024124, -0.3356979960936155, -0.4404640376379396, -1.2444832045041183, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.4766554376043083, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.2531646991801318, -0.2031244129114749, -0.5255455467325439, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4869285481637944, 0.424665895444844, -1.3314821107815475, 0.3571411263970316, -0.7746182501104947, 0.9106941533136118, 3.1306190036341057, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 0.6993474859923247, -0.6738408269117502, -0.9221119476695248, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9108799898636633, 3.1314343281497536, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 0.285341886203996, -0.7536814885080627, -0.7044462462363678, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.6439121681311092, 0.0338722762278857, -0.174249044437414, -1.3706985810941017, -0.553629657129454, 0.4010206813434356, 1.2420196914581063, 0.4706326037029981, -0.6071674196180575, -0.8701974062953576, -0.4351356021028035, 0.021209914852902, -1.064388315405516, 1.0542620417323658, -0.6738408269117502, -0.0796833960305785, -0.7939678109160361, 0.3736347120249996, 0.6308857408036841, 0.0418502001162623, -0.4309446948506726, -0.565020689844132, -0.2031244129114749, -1.287525621697034, -0.5868915885967425, -0.6218024077192129, -0.0802984588359218, -0.3815866107489131, -0.4454382040900255, 0.424665895444844, -1.3314821107815475, 0.6186392022095215, -0.8622687408969322, 0.9222160194167848, 3.1811691236042576, 0.0776143028335215, -0.3487779453829513, 0.067761088336817, -0.1474826847594624, -0.3999358135056357, -0.0560864903921214, -0.131687526934778, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.3339787568372257, -0.4255010572823463, -0.3487779453829513, -0.1806223481889086, -0.1474826847594624, -0.5354516373428909, -0.036772356694307, -0.1224163589348786, -0.1491049589303728, 0.034748685979544, -0.0493715789733202, -0.2440563017214069, 0.278477698357045, 2.1729760248092718, -0.5107528363891513, 1.4769583166408096, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-1.038066534429697, 0.260892106097527, 0.1645848503507034, -1.455529079732558, -1.6391408045371951, 0.196670426185453, 1.2420196914581063, 0.4706326037029981, -0.6616178283130005, -0.5004598044528213, -0.4609157221792596, 0.021209914852902, -1.1769476210921423, 0.0909225332951111, -1.348247483817418, -1.2590833683251033, -0.9252694826312332, 0.781386658108973, -0.0798138719333467, -0.2032969502372001, -0.5728175028694968, 0.103242147293012, -0.2031244129114749, -0.6504603131201653, -0.5868915885967425, -0.6218024077192129, -0.0284729839577739, -0.3925684926321655, -0.4454382040900255, -0.2791761177909213, -1.5149673075505703, 0.3413397023846657, -0.8352815289623093, 0.9329945393197532, 3.228457945511819, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 2.1729760248092718, -0.4251776020301452, 1.4769583166408096, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.8995798651896526, 0.4879119359671683, 0.1510314945591795, -1.4131138304133295, -0.6523124887119757, 0.196670426185453, 0.148455167213642, 0.4706326037029981, -0.752368509471239, -0.9441449266638648, -0.9507380036319262, 0.1751012323498492, -1.0418764542681906, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -1.3629417216818924, 0.1697587389830128, -0.364093717028159, -0.2645837378255657, -0.2890718868318484, 0.0364158635792983, -0.2031244129114749, -1.8246591171638051, -0.4061028043129334, -0.5350245509902118, 0.0492652283594477, 0.0357249008146661, -0.4929416415078188, -0.5851943844151671, -1.170932563608653, 0.051337096981241, -0.8763418268751098, 0.9447022419729768, 3.279823389997619, 0.0776143028335215, -0.4147760464289071, 0.0627050582403518, -0.1474826847594624, -0.3999358135056357, -0.050494972406442, -0.126269577827358, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.2624212575170594, -0.512973950246314, -0.4147760464289071, -0.2448301849643134, -0.1474826847594624, -0.5354516373428909, -0.0251798677021788, -0.1109322996093401, -0.1491049589303728, -0.3712584158476188, -0.318004544437855, -0.1500627926219946, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.5472412478180213, -0.6738408269117502, -1.0063548028334193, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.962914223877992, 3.359725192531085, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5107528363891513, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.5764443036295489, 0.3176470635649374, -0.1200356212713144, -1.300006498895388, -0.6382149413430442, 0.196670426185453, 0.8686074148868256, 0.4706326037029981, -0.806818918166182, -0.2416434831630464, -1.0022982437848384, -4.71094809817822, 6.994857971756963, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -0.8377350348211018, 0.1697587389830128, -0.364093717028159, -0.1420101626488345, -0.0053262707941999, 0.5487507053844415, -0.2031244129114749, -4.805500187471612, -0.0209440899691673, -0.6001079435369626, 0.5156945022627775, 0.2883081841294641, -0.4929416415078188, -0.9218144777018374, -1.2626751619931649, -0.1547010788662775, -0.8374007701449055, 0.973321070680858, 3.405383365407351, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, 0.2430287714694145, 0.8248406328786231, -1.4275690786528925, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9735069072309092, 3.4061986899229986, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2320031006479001, -0.647187213381038, -0.4182094486848582, -1.1586223344979605, -1.3289947624206973, -0.2120300841305121, 0.2551443890911503, 1.1995119025066026, -0.752368509471239, -0.3340778836236804, -0.93600650644538, -0.0942085732698075, -0.096378286500525, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -1.0237457364176323, -0.0341172340589738, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -1.2375597151419853, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.8912126510394129, -1.182400388406717, 0.1117543064402878, -0.8615402517404147, 0.9980373318376644, 3.5138215259884835, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -1.0752252927078816, -1.0485111918593435, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0340896225475933, 3.671994482024121, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7674785394661693, -0.9062745442328174, -0.7536814885080627, -0.971482097683874, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.2639920224449301, -0.6738408269117502, 2.279116548558471, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0396647190491284, 3.696454217493548, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.7418059691584676, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, -0.6693833821778409, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.057505027854041, 3.774725370995719, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.5792130238763558, -0.9062745442328174, -0.7536814885080627, -0.5709283205126147, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [-0.2142483994632791, -0.0228826812395245, 0.0154979366439325, -1.087930252299247, -0.920165888721678, 0.196670426185453, -1.0517985789083308, 1.1995119025066026, -0.752368509471239, 0.9600037228251952, -0.7039854257572746, -0.5174096963864115, 0.6240012698938863, -0.7710128163592748, -0.6738408269117502, 0.0888023142972107, -0.1702848702688472, 0.781386658108973, -0.553613613758034, -0.387157313002297, -0.4309446948506726, -0.074961275943559, -0.2031244129114749, -0.825340986062835, -0.0602459995960821, -0.6304801933921129, 0.0233524909203738, 0.3761632391954809, -0.3059344385086578, -0.6004952977463793, -1.6411133803292737, 0.9483002835655512, -0.8615402517404147, 1.0760886828591592, 3.856257822560481, 0.0776143028335215, -0.5487987206266679, 0.0559215511942626, -0.1474826847594624, -0.3999358135056357, -0.1124082987567718, -0.1862610241745375, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.2941301345546859, -0.6879197361742494, -0.5487987206266679, -0.3696472341575351, -0.1474826847594624, -0.5354516373428909, -0.0838340991700614, -0.1690379121597101, -0.1491049589303728, -0.0128621414043893, -0.0808730913626546, -0.1490558177303448, 0.278477698357045, 1.2505677424119097, -0.6819033051071632, -0.9062745442328174, -0.7536814885080627, -0.8379641719601209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]
2024-03-21 11:29:04,869 - __main__ - INFO - 69
2024-03-21 11:29:04,869 - __main__ - INFO - 325
2024-03-21 11:29:05,004 - __main__ - INFO - {'los_mean': 1055.0307777880782, 'los_std': 799.0879849276147}
2024-03-21 11:29:06,675 - __main__ - INFO - Fold 1 Epoch 0 Batch 0: Train Loss = 1.0874
2024-03-21 11:29:37,681 - __main__ - INFO - Fold 1, epoch 0: Loss = 0.9856 Valid loss = 0.9554 MSE = 647.8166
2024-03-21 11:29:37,682 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 647.8166 ------------
2024-03-21 11:29:38,065 - __main__ - INFO - ------------ Save best model - MSE: 647.8166 ------------
2024-03-21 11:29:38,066 - __main__ - INFO - Fold 1, mse = 647.8166, mad = 21.1666
2024-03-21 11:29:38,796 - __main__ - INFO - Fold 1 Epoch 1 Batch 0: Train Loss = 0.9590
2024-03-21 11:30:08,922 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 606.7087 ------------
2024-03-21 11:30:09,356 - __main__ - INFO - ------------ Save best model - MSE: 606.7087 ------------
2024-03-21 11:30:09,357 - __main__ - INFO - Fold 1, mse = 606.7087, mad = 19.8228
2024-03-21 11:30:10,237 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 0.8373
2024-03-21 11:30:40,221 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 584.4955 ------------
2024-03-21 11:30:40,628 - __main__ - INFO - ------------ Save best model - MSE: 584.4955 ------------
2024-03-21 11:30:40,629 - __main__ - INFO - Fold 1, mse = 584.4955, mad = 19.5190
2024-03-21 11:30:41,526 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 0.7801
2024-03-21 11:31:11,799 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 571.5231 ------------
2024-03-21 11:31:12,178 - __main__ - INFO - ------------ Save best model - MSE: 571.5231 ------------
2024-03-21 11:31:12,179 - __main__ - INFO - Fold 1, mse = 571.5231, mad = 19.4365
2024-03-21 11:31:12,874 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 0.6976
2024-03-21 11:31:43,042 - __main__ - INFO - Fold 1, mse = 572.4293, mad = 19.5667
2024-03-21 11:31:43,928 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 0.7703
2024-03-21 11:32:14,898 - __main__ - INFO - Fold 1, mse = 575.1583, mad = 19.3264
2024-03-21 11:32:15,846 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 0.7152
2024-03-21 11:32:45,508 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 561.7702 ------------
2024-03-21 11:32:45,795 - __main__ - INFO - ------------ Save best model - MSE: 561.7702 ------------
2024-03-21 11:32:45,796 - __main__ - INFO - Fold 1, mse = 561.7702, mad = 19.2369
2024-03-21 11:32:46,643 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 0.8359
2024-03-21 11:33:17,666 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 561.5436 ------------
2024-03-21 11:33:18,188 - __main__ - INFO - ------------ Save best model - MSE: 561.5436 ------------
2024-03-21 11:33:18,189 - __main__ - INFO - Fold 1, mse = 561.5436, mad = 19.0279
2024-03-21 11:33:19,124 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 0.8340
2024-03-21 11:33:52,445 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 558.0529 ------------
2024-03-21 11:33:52,821 - __main__ - INFO - ------------ Save best model - MSE: 558.0529 ------------
2024-03-21 11:33:52,822 - __main__ - INFO - Fold 1, mse = 558.0529, mad = 18.9948
2024-03-21 11:33:53,655 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 0.5735
2024-03-21 11:34:28,824 - __main__ - INFO - Fold 1, mse = 563.9449, mad = 18.7524
2024-03-21 11:34:29,738 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 0.7294
2024-03-21 11:35:05,198 - __main__ - INFO - Fold 1, epoch 10: Loss = 0.7272 Valid loss = 0.7915 MSE = 565.0425
2024-03-21 11:35:05,200 - __main__ - INFO - Fold 1, mse = 565.0425, mad = 19.1408
2024-03-21 11:35:06,172 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 0.7566
2024-03-21 11:35:39,587 - __main__ - INFO - Fold 1, mse = 561.1979, mad = 18.8975
2024-03-21 11:35:40,485 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 0.6164
2024-03-21 11:36:11,690 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 555.2796 ------------
2024-03-21 11:36:12,108 - __main__ - INFO - ------------ Save best model - MSE: 555.2796 ------------
2024-03-21 11:36:12,109 - __main__ - INFO - Fold 1, mse = 555.2796, mad = 18.8564
2024-03-21 11:36:13,021 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 0.7235
2024-03-21 11:36:43,870 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 530.4614 ------------
2024-03-21 11:36:44,331 - __main__ - INFO - ------------ Save best model - MSE: 530.4614 ------------
2024-03-21 11:36:44,331 - __main__ - INFO - Fold 1, mse = 530.4614, mad = 18.4728
2024-03-21 11:36:45,394 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 0.7733
2024-03-21 11:37:15,451 - __main__ - INFO - Fold 1, mse = 546.3140, mad = 18.5834
2024-03-21 11:37:16,161 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 0.7619
2024-03-21 11:37:45,995 - __main__ - INFO - Fold 1, mse = 558.2915, mad = 19.0120
2024-03-21 11:37:46,747 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 0.7664
2024-03-21 11:38:20,410 - __main__ - INFO - Fold 1, mse = 538.4881, mad = 18.5152
2024-03-21 11:38:21,285 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 0.6367
2024-03-21 11:38:53,917 - __main__ - INFO - Fold 1, mse = 557.8726, mad = 18.7518
2024-03-21 11:38:54,734 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 0.6644
2024-03-21 11:39:29,597 - __main__ - INFO - Fold 1, mse = 543.2425, mad = 18.4965
2024-03-21 11:39:30,704 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 0.5773
2024-03-21 11:40:06,501 - __main__ - INFO - Fold 1, mse = 558.3302, mad = 19.0343
2024-03-21 11:40:07,286 - __main__ - INFO - Fold 1 Epoch 20 Batch 0: Train Loss = 0.6174
2024-03-21 11:40:42,399 - __main__ - INFO - Fold 1, epoch 20: Loss = 0.6564 Valid loss = 0.7503 MSE = 540.4630
2024-03-21 11:40:42,409 - __main__ - INFO - Fold 1, mse = 540.4630, mad = 18.7183
2024-03-21 11:40:43,359 - __main__ - INFO - Fold 1 Epoch 21 Batch 0: Train Loss = 0.6333
2024-03-21 11:41:18,109 - __main__ - INFO - Fold 1, mse = 572.5451, mad = 19.3568
2024-03-21 11:41:19,009 - __main__ - INFO - Fold 1 Epoch 22 Batch 0: Train Loss = 0.7237
2024-03-21 11:41:53,979 - __main__ - INFO - Fold 1, mse = 550.3249, mad = 18.8166
2024-03-21 11:41:54,881 - __main__ - INFO - Fold 1 Epoch 23 Batch 0: Train Loss = 0.5924
2024-03-21 11:42:29,467 - __main__ - INFO - Fold 1, mse = 574.7056, mad = 19.3614
2024-03-21 11:42:30,411 - __main__ - INFO - Fold 1 Epoch 24 Batch 0: Train Loss = 0.6497
2024-03-21 11:43:03,216 - __main__ - INFO - Fold 1, mse = 557.1818, mad = 19.2226
2024-03-21 11:43:04,230 - __main__ - INFO - Fold 1 Epoch 25 Batch 0: Train Loss = 0.7116
2024-03-21 11:43:37,481 - __main__ - INFO - Fold 1, mse = 583.6079, mad = 19.3838
2024-03-21 11:43:38,505 - __main__ - INFO - Fold 1 Epoch 26 Batch 0: Train Loss = 0.7337
2024-03-21 11:44:12,182 - __main__ - INFO - Fold 1, mse = 599.0290, mad = 19.7024
2024-03-21 11:44:13,213 - __main__ - INFO - Fold 1 Epoch 27 Batch 0: Train Loss = 0.5089
2024-03-21 11:44:45,540 - __main__ - INFO - Fold 1, mse = 604.9572, mad = 19.8683
2024-03-21 11:44:46,406 - __main__ - INFO - Fold 1 Epoch 28 Batch 0: Train Loss = 0.5716
2024-03-21 11:45:20,869 - __main__ - INFO - Fold 1, mse = 585.5747, mad = 19.5134
2024-03-21 11:45:21,737 - __main__ - INFO - Fold 1 Epoch 29 Batch 0: Train Loss = 0.5911
2024-03-21 11:45:56,463 - __main__ - INFO - Fold 1, mse = 576.8813, mad = 19.5862
2024-03-21 11:45:57,778 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 1.0251
2024-03-21 11:46:31,562 - __main__ - INFO - Fold 2, epoch 0: Loss = 1.0287 Valid loss = 0.7777 MSE = 561.5856
2024-03-21 11:46:31,564 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 561.5856 ------------
2024-03-21 11:46:31,835 - __main__ - INFO - Fold 2, mse = 561.5856, mad = 19.2492
2024-03-21 11:46:32,843 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 0.9164
2024-03-21 11:47:08,255 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 527.9883 ------------
2024-03-21 11:47:08,864 - __main__ - INFO - ------------ Save best model - MSE: 527.9883 ------------
2024-03-21 11:47:08,865 - __main__ - INFO - Fold 2, mse = 527.9883, mad = 18.4924
2024-03-21 11:47:09,946 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 0.9402
2024-03-21 11:47:44,332 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 511.3322 ------------
2024-03-21 11:47:44,896 - __main__ - INFO - ------------ Save best model - MSE: 511.3322 ------------
2024-03-21 11:47:44,897 - __main__ - INFO - Fold 2, mse = 511.3322, mad = 18.2264
2024-03-21 11:47:45,911 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 0.8241
2024-03-21 11:48:21,122 - __main__ - INFO - Fold 2, mse = 512.5998, mad = 18.2422
2024-03-21 11:48:21,997 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 0.8768
2024-03-21 11:48:54,697 - __main__ - INFO - Fold 2, mse = 512.6863, mad = 18.3602
2024-03-21 11:48:55,568 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 0.7262
2024-03-21 11:49:28,814 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 490.4180 ------------
2024-03-21 11:49:29,252 - __main__ - INFO - ------------ Save best model - MSE: 490.4180 ------------
2024-03-21 11:49:29,254 - __main__ - INFO - Fold 2, mse = 490.4180, mad = 17.9091
2024-03-21 11:49:30,187 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 0.7406
2024-03-21 11:50:04,522 - __main__ - INFO - Fold 2, mse = 495.8190, mad = 17.8388
2024-03-21 11:50:05,355 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 0.8082
2024-03-21 11:50:39,296 - __main__ - INFO - Fold 2, mse = 501.0426, mad = 18.0409
2024-03-21 11:50:40,225 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 0.6779
2024-03-21 11:51:13,639 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 481.8493 ------------
2024-03-21 11:51:14,041 - __main__ - INFO - ------------ Save best model - MSE: 481.8493 ------------
2024-03-21 11:51:14,043 - __main__ - INFO - Fold 2, mse = 481.8493, mad = 17.6234
2024-03-21 11:51:15,082 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 0.7358
2024-03-21 11:51:49,530 - __main__ - INFO - Fold 2, mse = 482.1297, mad = 17.5794
2024-03-21 11:51:50,505 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 0.6988
2024-03-21 11:52:23,463 - __main__ - INFO - Fold 2, epoch 10: Loss = 0.7251 Valid loss = 0.6852 MSE = 505.7885
2024-03-21 11:52:23,466 - __main__ - INFO - Fold 2, mse = 505.7885, mad = 17.9466
2024-03-21 11:52:24,349 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 0.6853
2024-03-21 11:52:59,639 - __main__ - INFO - Fold 2, mse = 502.5415, mad = 18.0014
2024-03-21 11:53:00,462 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 0.6805
2024-03-21 11:53:35,370 - __main__ - INFO - Fold 2, mse = 492.3272, mad = 17.5413
2024-03-21 11:53:36,388 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 0.6682
2024-03-21 11:54:10,483 - __main__ - INFO - Fold 2, mse = 492.4613, mad = 17.6927
2024-03-21 11:54:11,413 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 0.6670
2024-03-21 11:54:46,055 - __main__ - INFO - Fold 2, mse = 483.8366, mad = 17.7405
2024-03-21 11:54:47,057 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 0.7444
2024-03-21 11:55:20,175 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 480.1714 ------------
2024-03-21 11:55:20,653 - __main__ - INFO - ------------ Save best model - MSE: 480.1714 ------------
2024-03-21 11:55:20,655 - __main__ - INFO - Fold 2, mse = 480.1714, mad = 17.6509
2024-03-21 11:55:21,633 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 0.6175
2024-03-21 11:55:55,456 - __main__ - INFO - Fold 2, mse = 508.2281, mad = 18.1003
2024-03-21 11:55:56,415 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 0.6167
2024-03-21 11:56:32,018 - __main__ - INFO - Fold 2, mse = 535.5512, mad = 18.4214
2024-03-21 11:56:32,970 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 0.6694
2024-03-21 11:57:05,825 - __main__ - INFO - Fold 2, mse = 499.0341, mad = 17.6557
2024-03-21 11:57:06,765 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 0.6679
2024-03-21 11:57:39,101 - __main__ - INFO - Fold 2, mse = 539.9933, mad = 18.7889
2024-03-21 11:57:40,066 - __main__ - INFO - Fold 2 Epoch 20 Batch 0: Train Loss = 0.6275
2024-03-21 11:58:13,111 - __main__ - INFO - Fold 2, epoch 20: Loss = 0.6351 Valid loss = 0.7032 MSE = 528.0242
2024-03-21 11:58:13,113 - __main__ - INFO - Fold 2, mse = 528.0242, mad = 18.3082
2024-03-21 11:58:13,969 - __main__ - INFO - Fold 2 Epoch 21 Batch 0: Train Loss = 0.6472
2024-03-21 11:58:46,482 - __main__ - INFO - Fold 2, mse = 533.0406, mad = 18.5381
2024-03-21 11:58:47,393 - __main__ - INFO - Fold 2 Epoch 22 Batch 0: Train Loss = 0.7012
2024-03-21 11:59:22,247 - __main__ - INFO - Fold 2, mse = 544.6987, mad = 18.6576
2024-03-21 11:59:23,189 - __main__ - INFO - Fold 2 Epoch 23 Batch 0: Train Loss = 0.5193
2024-03-21 11:59:57,868 - __main__ - INFO - Fold 2, mse = 522.6158, mad = 18.3419
2024-03-21 11:59:58,671 - __main__ - INFO - Fold 2 Epoch 24 Batch 0: Train Loss = 0.5820
2024-03-21 12:00:32,380 - __main__ - INFO - Fold 2, mse = 512.8986, mad = 17.8369
2024-03-21 12:00:33,434 - __main__ - INFO - Fold 2 Epoch 25 Batch 0: Train Loss = 0.6882
2024-03-21 12:01:08,749 - __main__ - INFO - Fold 2, mse = 538.7300, mad = 18.4927
2024-03-21 12:01:09,608 - __main__ - INFO - Fold 2 Epoch 26 Batch 0: Train Loss = 0.6792
2024-03-21 12:01:43,250 - __main__ - INFO - Fold 2, mse = 539.5857, mad = 18.3020
2024-03-21 12:01:44,135 - __main__ - INFO - Fold 2 Epoch 27 Batch 0: Train Loss = 0.6495
2024-03-21 12:02:16,850 - __main__ - INFO - Fold 2, mse = 578.8955, mad = 19.0488
2024-03-21 12:02:17,639 - __main__ - INFO - Fold 2 Epoch 28 Batch 0: Train Loss = 0.6083
2024-03-21 12:02:51,423 - __main__ - INFO - Fold 2, mse = 580.2313, mad = 19.1258
2024-03-21 12:02:52,337 - __main__ - INFO - Fold 2 Epoch 29 Batch 0: Train Loss = 0.5723
2024-03-21 12:03:26,219 - __main__ - INFO - Fold 2, mse = 559.1021, mad = 18.5282
2024-03-21 12:03:27,670 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 1.0793
2024-03-21 12:04:01,439 - __main__ - INFO - Fold 3, epoch 0: Loss = 1.0000 Valid loss = 0.9130 MSE = 651.7688
2024-03-21 12:04:01,441 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 651.7688 ------------
2024-03-21 12:04:01,779 - __main__ - INFO - Fold 3, mse = 651.7688, mad = 20.8586
2024-03-21 12:04:02,772 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 1.1082
2024-03-21 12:04:36,646 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 649.5886 ------------
2024-03-21 12:04:36,860 - __main__ - INFO - Fold 3, mse = 649.5886, mad = 20.9189
2024-03-21 12:04:37,831 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 0.8506
2024-03-21 12:05:11,544 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 632.4512 ------------
2024-03-21 12:05:11,835 - __main__ - INFO - Fold 3, mse = 632.4512, mad = 20.6919
2024-03-21 12:05:12,810 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 0.8661
2024-03-21 12:05:47,827 - __main__ - INFO - Fold 3, mse = 644.8318, mad = 21.0520
2024-03-21 12:05:48,729 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 0.7589
2024-03-21 12:06:25,803 - __main__ - INFO - Fold 3, mse = 656.9252, mad = 21.2556
2024-03-21 12:06:26,598 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 0.8207
2024-03-21 12:07:02,152 - __main__ - INFO - Fold 3, mse = 636.9864, mad = 20.6270
2024-03-21 12:07:03,061 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 0.7816
2024-03-21 12:07:38,768 - __main__ - INFO - Fold 3, mse = 651.0390, mad = 21.0843
2024-03-21 12:07:39,638 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 0.6782
2024-03-21 12:08:14,717 - __main__ - INFO - Fold 3, mse = 635.3814, mad = 20.6600
2024-03-21 12:08:15,661 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 0.7453
2024-03-21 12:08:49,470 - __main__ - INFO - Fold 3, mse = 647.9228, mad = 20.7318
2024-03-21 12:08:50,296 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 0.7201
2024-03-21 12:09:25,427 - __main__ - INFO - Fold 3, mse = 649.8591, mad = 20.8555
2024-03-21 12:09:26,414 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 0.6682
2024-03-21 12:10:03,083 - __main__ - INFO - Fold 3, epoch 10: Loss = 0.6857 Valid loss = 0.8898 MSE = 632.7979
2024-03-21 12:10:03,084 - __main__ - INFO - Fold 3, mse = 632.7979, mad = 20.5521
2024-03-21 12:10:04,192 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 0.6474
2024-03-21 12:10:37,588 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 623.6233 ------------
2024-03-21 12:10:37,771 - __main__ - INFO - Fold 3, mse = 623.6233, mad = 20.1036
2024-03-21 12:10:38,940 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 0.5720
2024-03-21 12:11:13,204 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 622.2925 ------------
2024-03-21 12:11:13,494 - __main__ - INFO - Fold 3, mse = 622.2925, mad = 20.1714
2024-03-21 12:11:14,426 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 0.6664
2024-03-21 12:11:48,993 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 614.0562 ------------
2024-03-21 12:11:49,278 - __main__ - INFO - Fold 3, mse = 614.0562, mad = 19.8601
2024-03-21 12:11:50,186 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 0.5302
2024-03-21 12:12:24,384 - __main__ - INFO - Fold 3, mse = 638.2865, mad = 20.5618
2024-03-21 12:12:25,352 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 0.6235
2024-03-21 12:13:00,639 - __main__ - INFO - Fold 3, mse = 668.2931, mad = 20.8141
2024-03-21 12:13:01,562 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 0.6403
2024-03-21 12:13:37,188 - __main__ - INFO - Fold 3, mse = 652.5337, mad = 20.7817
2024-03-21 12:13:38,280 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 0.7016
2024-03-21 12:14:12,528 - __main__ - INFO - Fold 3, mse = 643.5452, mad = 20.4711
2024-03-21 12:14:13,412 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 0.6063
2024-03-21 12:14:49,011 - __main__ - INFO - Fold 3, mse = 645.4426, mad = 20.3521
2024-03-21 12:14:49,823 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 0.6673
2024-03-21 12:15:25,018 - __main__ - INFO - Fold 3, mse = 636.8185, mad = 19.9554
2024-03-21 12:15:26,008 - __main__ - INFO - Fold 3 Epoch 20 Batch 0: Train Loss = 0.5470
2024-03-21 12:16:00,912 - __main__ - INFO - Fold 3, epoch 20: Loss = 0.6036 Valid loss = 0.9204 MSE = 653.0172
2024-03-21 12:16:00,913 - __main__ - INFO - Fold 3, mse = 653.0172, mad = 20.5431
2024-03-21 12:16:01,845 - __main__ - INFO - Fold 3 Epoch 21 Batch 0: Train Loss = 0.7153
2024-03-21 12:16:36,881 - __main__ - INFO - Fold 3, mse = 645.3542, mad = 20.2627
2024-03-21 12:16:37,995 - __main__ - INFO - Fold 3 Epoch 22 Batch 0: Train Loss = 0.6171
2024-03-21 12:17:12,777 - __main__ - INFO - Fold 3, mse = 673.0658, mad = 21.0443
2024-03-21 12:17:13,680 - __main__ - INFO - Fold 3 Epoch 23 Batch 0: Train Loss = 0.6288
2024-03-21 12:17:48,627 - __main__ - INFO - Fold 3, mse = 686.0846, mad = 21.2952
2024-03-21 12:17:49,617 - __main__ - INFO - Fold 3 Epoch 24 Batch 0: Train Loss = 0.5550
2024-03-21 12:18:22,701 - __main__ - INFO - Fold 3, mse = 704.8580, mad = 21.3073
2024-03-21 12:18:23,838 - __main__ - INFO - Fold 3 Epoch 25 Batch 0: Train Loss = 0.6269
2024-03-21 12:18:56,645 - __main__ - INFO - Fold 3, mse = 671.9143, mad = 20.8531
2024-03-21 12:18:57,586 - __main__ - INFO - Fold 3 Epoch 26 Batch 0: Train Loss = 0.6928
                                                                                                                                                                                                                                                                                                                2024-03-21 11:01:49,316 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 614.5851 ------------
2024-03-21 11:01:49,779 - __main__ - INFO - ------------ Save best model - MSE: 614.5851 ------------
2024-03-21 11:01:49,780 - __main__ - INFO - Fold 1, mse = 614.5851, mad = 19.8769
2024-03-21 11:01:50,537 - __main__ - INFO - Fold 1 Epoch 2 Batch 0: Train Loss = 0.7985
2024-03-21 11:02:23,863 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 587.6515 ------------
2024-03-21 11:02:24,380 - __main__ - INFO - ------------ Save best model - MSE: 587.6515 ------------
2024-03-21 11:02:24,381 - __main__ - INFO - Fold 1, mse = 587.6515, mad = 19.4649
2024-03-21 11:02:25,193 - __main__ - INFO - Fold 1 Epoch 3 Batch 0: Train Loss = 0.8544
2024-03-21 11:02:57,725 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 568.4706 ------------
2024-03-21 11:02:58,356 - __main__ - INFO - ------------ Save best model - MSE: 568.4706 ------------
2024-03-21 11:02:58,358 - __main__ - INFO - Fold 1, mse = 568.4706, mad = 19.2813
2024-03-21 11:02:59,070 - __main__ - INFO - Fold 1 Epoch 4 Batch 0: Train Loss = 0.6811
2024-03-21 11:03:35,298 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 559.0098 ------------
2024-03-21 11:03:35,801 - __main__ - INFO - ------------ Save best model - MSE: 559.0098 ------------
2024-03-21 11:03:35,802 - __main__ - INFO - Fold 1, mse = 559.0098, mad = 19.0774
2024-03-21 11:03:36,716 - __main__ - INFO - Fold 1 Epoch 5 Batch 0: Train Loss = 0.7977
2024-03-21 11:04:12,821 - __main__ - INFO - Fold 1, mse = 567.9169, mad = 19.0371
2024-03-21 11:04:13,703 - __main__ - INFO - Fold 1 Epoch 6 Batch 0: Train Loss = 0.7469
2024-03-21 11:04:47,174 - __main__ - INFO - Fold 1, mse = 559.2746, mad = 18.8649
2024-03-21 11:04:48,324 - __main__ - INFO - Fold 1 Epoch 7 Batch 0: Train Loss = 0.7855
2024-03-21 11:05:20,822 - __main__ - INFO - Fold 1, mse = 586.2243, mad = 19.3185
2024-03-21 11:05:21,696 - __main__ - INFO - Fold 1 Epoch 8 Batch 0: Train Loss = 0.7528
2024-03-21 11:05:53,863 - __main__ - INFO - Fold 1, mse = 575.2112, mad = 19.2364
2024-03-21 11:05:54,708 - __main__ - INFO - Fold 1 Epoch 9 Batch 0: Train Loss = 0.6179
2024-03-21 11:06:29,436 - __main__ - INFO - Fold 1, mse = 586.8769, mad = 19.0974
2024-03-21 11:06:30,137 - __main__ - INFO - Fold 1 Epoch 10 Batch 0: Train Loss = 0.7501
2024-03-21 11:07:04,481 - __main__ - INFO - Fold 1, epoch 10: Loss = 0.7416 Valid loss = 0.7987 MSE = 573.1918
2024-03-21 11:07:04,483 - __main__ - INFO - Fold 1, mse = 573.1918, mad = 19.0578
2024-03-21 11:07:05,312 - __main__ - INFO - Fold 1 Epoch 11 Batch 0: Train Loss = 0.7105
2024-03-21 11:07:39,297 - __main__ - INFO - Fold 1, mse = 577.3152, mad = 19.0523
2024-03-21 11:07:40,137 - __main__ - INFO - Fold 1 Epoch 12 Batch 0: Train Loss = 0.6198
2024-03-21 11:08:14,983 - __main__ - INFO - Fold 1, mse = 584.7444, mad = 19.3434
2024-03-21 11:08:15,825 - __main__ - INFO - Fold 1 Epoch 13 Batch 0: Train Loss = 0.6806
2024-03-21 11:08:45,620 - __main__ - INFO - Fold 1, mse = 562.6453, mad = 18.9426
2024-03-21 11:08:46,410 - __main__ - INFO - Fold 1 Epoch 14 Batch 0: Train Loss = 0.7067
2024-03-21 11:09:17,052 - __main__ - INFO - Fold 1, mse = 577.8217, mad = 19.1081
2024-03-21 11:09:17,909 - __main__ - INFO - Fold 1 Epoch 15 Batch 0: Train Loss = 0.7341
2024-03-21 11:09:49,065 - __main__ - INFO - Fold 1, mse = 566.8143, mad = 18.9394
2024-03-21 11:09:49,772 - __main__ - INFO - Fold 1 Epoch 16 Batch 0: Train Loss = 0.7653
2024-03-21 11:10:23,689 - __main__ - INFO - Fold 1, mse = 565.8192, mad = 18.8657
2024-03-21 11:10:24,609 - __main__ - INFO - Fold 1 Epoch 17 Batch 0: Train Loss = 0.6775
2024-03-21 11:10:56,452 - __main__ - INFO - Fold 1, mse = 602.8690, mad = 19.5491
2024-03-21 11:10:57,333 - __main__ - INFO - Fold 1 Epoch 18 Batch 0: Train Loss = 0.6523
2024-03-21 11:11:29,025 - __main__ - INFO - Fold 1, mse = 576.7942, mad = 18.9922
2024-03-21 11:11:29,914 - __main__ - INFO - Fold 1 Epoch 19 Batch 0: Train Loss = 0.6260
2024-03-21 11:12:02,213 - __main__ - INFO - Fold 1, mse = 619.7570, mad = 19.7943
2024-03-21 11:12:03,295 - __main__ - INFO - Fold 1 Epoch 20 Batch 0: Train Loss = 0.5398
2024-03-21 11:12:36,056 - __main__ - INFO - Fold 1, epoch 20: Loss = 0.6266 Valid loss = 0.8193 MSE = 601.5258
2024-03-21 11:12:36,059 - __main__ - INFO - Fold 1, mse = 601.5258, mad = 19.6198
2024-03-21 11:12:36,858 - __main__ - INFO - Fold 1 Epoch 21 Batch 0: Train Loss = 0.5733
2024-03-21 11:13:07,549 - __main__ - INFO - Fold 1, mse = 637.6028, mad = 20.1317
2024-03-21 11:13:08,359 - __main__ - INFO - Fold 1 Epoch 22 Batch 0: Train Loss = 0.7467
2024-03-21 11:13:39,245 - __main__ - INFO - Fold 1, mse = 605.6164, mad = 19.7641
2024-03-21 11:13:40,008 - __main__ - INFO - Fold 1 Epoch 23 Batch 0: Train Loss = 0.5897
2024-03-21 11:14:09,918 - __main__ - INFO - Fold 1, mse = 562.1592, mad = 19.0290
2024-03-21 11:14:10,661 - __main__ - INFO - Fold 1 Epoch 24 Batch 0: Train Loss = 0.6671
2024-03-21 11:14:40,434 - __main__ - INFO - Fold 1, mse = 581.8182, mad = 19.1840
2024-03-21 11:14:41,057 - __main__ - INFO - Fold 1 Epoch 25 Batch 0: Train Loss = 0.6334
2024-03-21 11:15:07,191 - __main__ - INFO - Fold 1, mse = 587.6751, mad = 19.4272
2024-03-21 11:15:07,901 - __main__ - INFO - Fold 1 Epoch 26 Batch 0: Train Loss = 0.6068
2024-03-21 11:15:33,908 - __main__ - INFO - Fold 1, mse = 609.4088, mad = 19.6250
2024-03-21 11:15:34,725 - __main__ - INFO - Fold 1 Epoch 27 Batch 0: Train Loss = 0.5266
2024-03-21 11:16:03,478 - __main__ - INFO - Fold 1, mse = 605.9235, mad = 19.9761
2024-03-21 11:16:04,329 - __main__ - INFO - Fold 1 Epoch 28 Batch 0: Train Loss = 0.4810
2024-03-21 11:16:35,063 - __main__ - INFO - Fold 1, mse = 584.0767, mad = 19.4854
2024-03-21 11:16:35,831 - __main__ - INFO - Fold 1 Epoch 29 Batch 0: Train Loss = 0.5919
2024-03-21 11:17:03,159 - __main__ - INFO - Fold 1, mse = 642.7690, mad = 20.4849
2024-03-21 11:17:04,122 - __main__ - INFO - Fold 2 Epoch 0 Batch 0: Train Loss = 1.0285
2024-03-21 11:17:31,660 - __main__ - INFO - Fold 2, epoch 0: Loss = 1.0264 Valid loss = 0.7808 MSE = 564.6123
2024-03-21 11:17:31,662 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 564.6123 ------------
2024-03-21 11:17:31,863 - __main__ - INFO - Fold 2, mse = 564.6123, mad = 19.3110
2024-03-21 11:17:32,550 - __main__ - INFO - Fold 2 Epoch 1 Batch 0: Train Loss = 0.9496
2024-03-21 11:18:00,242 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 521.7949 ------------
2024-03-21 11:18:00,637 - __main__ - INFO - ------------ Save best model - MSE: 521.7949 ------------
2024-03-21 11:18:00,638 - __main__ - INFO - Fold 2, mse = 521.7949, mad = 18.2068
2024-03-21 11:18:01,418 - __main__ - INFO - Fold 2 Epoch 2 Batch 0: Train Loss = 0.9143
2024-03-21 11:18:29,454 - __main__ - INFO - Fold 2, mse = 524.0587, mad = 18.5438
2024-03-21 11:18:30,312 - __main__ - INFO - Fold 2 Epoch 3 Batch 0: Train Loss = 0.8436
2024-03-21 11:18:57,030 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 511.3252 ------------
2024-03-21 11:18:57,454 - __main__ - INFO - ------------ Save best model - MSE: 511.3252 ------------
2024-03-21 11:18:57,456 - __main__ - INFO - Fold 2, mse = 511.3252, mad = 18.2814
2024-03-21 11:18:58,208 - __main__ - INFO - Fold 2 Epoch 4 Batch 0: Train Loss = 0.8748
2024-03-21 11:19:25,314 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 496.1738 ------------
2024-03-21 11:19:25,692 - __main__ - INFO - ------------ Save best model - MSE: 496.1738 ------------
2024-03-21 11:19:25,693 - __main__ - INFO - Fold 2, mse = 496.1738, mad = 17.9042
2024-03-21 11:19:26,401 - __main__ - INFO - Fold 2 Epoch 5 Batch 0: Train Loss = 0.7496
2024-03-21 11:19:54,081 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 495.9236 ------------
2024-03-21 11:19:54,647 - __main__ - INFO - ------------ Save best model - MSE: 495.9236 ------------
2024-03-21 11:19:54,649 - __main__ - INFO - Fold 2, mse = 495.9236, mad = 17.8493
2024-03-21 11:19:55,441 - __main__ - INFO - Fold 2 Epoch 6 Batch 0: Train Loss = 0.7158
2024-03-21 11:20:23,384 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 486.5182 ------------
2024-03-21 11:20:23,845 - __main__ - INFO - ------------ Save best model - MSE: 486.5182 ------------
2024-03-21 11:20:23,847 - __main__ - INFO - Fold 2, mse = 486.5182, mad = 17.5352
2024-03-21 11:20:24,698 - __main__ - INFO - Fold 2 Epoch 7 Batch 0: Train Loss = 0.8092
2024-03-21 11:20:51,683 - __main__ - INFO - Fold 2, mse = 489.7959, mad = 17.6728
2024-03-21 11:20:52,400 - __main__ - INFO - Fold 2 Epoch 8 Batch 0: Train Loss = 0.6506
2024-03-21 11:21:19,781 - __main__ - INFO - Fold 2, mse = 512.0576, mad = 18.0033
2024-03-21 11:21:20,415 - __main__ - INFO - Fold 2 Epoch 9 Batch 0: Train Loss = 0.7784
2024-03-21 11:21:47,475 - __main__ - INFO - Fold 2, mse = 494.3095, mad = 17.7686
2024-03-21 11:21:48,241 - __main__ - INFO - Fold 2 Epoch 10 Batch 0: Train Loss = 0.6793
2024-03-21 11:22:14,020 - __main__ - INFO - Fold 2, epoch 10: Loss = 0.7038 Valid loss = 0.6696 MSE = 496.7934
2024-03-21 11:22:14,022 - __main__ - INFO - Fold 2, mse = 496.7934, mad = 17.7048
2024-03-21 11:22:14,770 - __main__ - INFO - Fold 2 Epoch 11 Batch 0: Train Loss = 0.6887
2024-03-21 11:22:42,030 - __main__ - INFO - Fold 2, mse = 498.7410, mad = 17.9043
2024-03-21 11:22:42,693 - __main__ - INFO - Fold 2 Epoch 12 Batch 0: Train Loss = 0.5541
2024-03-21 11:23:09,316 - __main__ - INFO - Fold 2, mse = 495.0589, mad = 17.8030
2024-03-21 11:23:10,179 - __main__ - INFO - Fold 2 Epoch 13 Batch 0: Train Loss = 0.6182
2024-03-21 11:23:38,268 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 477.4220 ------------
2024-03-21 11:23:38,729 - __main__ - INFO - ------------ Save best model - MSE: 477.4220 ------------
2024-03-21 11:23:38,730 - __main__ - INFO - Fold 2, mse = 477.4220, mad = 17.3197
2024-03-21 11:23:39,475 - __main__ - INFO - Fold 2 Epoch 14 Batch 0: Train Loss = 0.7150
2024-03-21 11:24:07,926 - __main__ - INFO - Fold 2, mse = 501.6623, mad = 17.9577
2024-03-21 11:24:08,659 - __main__ - INFO - Fold 2 Epoch 15 Batch 0: Train Loss = 0.6831
2024-03-21 11:24:35,370 - __main__ - INFO - Fold 2, mse = 499.3103, mad = 18.0998
2024-03-21 11:24:36,036 - __main__ - INFO - Fold 2 Epoch 16 Batch 0: Train Loss = 0.5686
2024-03-21 11:25:02,068 - __main__ - INFO - Fold 2, mse = 485.0597, mad = 17.5394
2024-03-21 11:25:02,743 - __main__ - INFO - Fold 2 Epoch 17 Batch 0: Train Loss = 0.6815
2024-03-21 11:25:30,814 - __main__ - INFO - Fold 2, mse = 513.6578, mad = 18.3732
2024-03-21 11:25:31,556 - __main__ - INFO - Fold 2 Epoch 18 Batch 0: Train Loss = 0.6645
2024-03-21 11:25:59,023 - __main__ - INFO - Fold 2, mse = 482.9744, mad = 17.4887
2024-03-21 11:25:59,831 - __main__ - INFO - Fold 2 Epoch 19 Batch 0: Train Loss = 0.6122
2024-03-21 11:26:26,643 - __main__ - INFO - Fold 2, mse = 490.0744, mad = 17.4871
2024-03-21 11:26:27,417 - __main__ - INFO - Fold 2 Epoch 20 Batch 0: Train Loss = 0.6196
2024-03-21 11:26:54,068 - __main__ - INFO - Fold 2, epoch 20: Loss = 0.6317 Valid loss = 0.6462 MSE = 474.2815
2024-03-21 11:26:54,070 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 474.2815 ------------
2024-03-21 11:26:54,444 - __main__ - INFO - ------------ Save best model - MSE: 474.2815 ------------
2024-03-21 11:26:54,445 - __main__ - INFO - Fold 2, mse = 474.2815, mad = 17.3116
2024-03-21 11:26:55,208 - __main__ - INFO - Fold 2 Epoch 21 Batch 0: Train Loss = 0.5630
2024-03-21 11:27:22,837 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 463.5213 ------------
2024-03-21 11:27:23,284 - __main__ - INFO - ------------ Save best model - MSE: 463.5213 ------------
2024-03-21 11:27:23,285 - __main__ - INFO - Fold 2, mse = 463.5213, mad = 16.8095
2024-03-21 11:27:23,972 - __main__ - INFO - Fold 2 Epoch 22 Batch 0: Train Loss = 0.6848
2024-03-21 11:27:50,167 - __main__ - INFO - Fold 2, mse = 477.6400, mad = 17.1064
2024-03-21 11:27:50,886 - __main__ - INFO - Fold 2 Epoch 23 Batch 0: Train Loss = 0.5369
2024-03-21 11:28:17,989 - __main__ - INFO - Fold 2, mse = 475.0091, mad = 17.0488
2024-03-21 11:28:18,725 - __main__ - INFO - Fold 2 Epoch 24 Batch 0: Train Loss = 0.5589
2024-03-21 11:28:46,404 - __main__ - INFO - Fold 2, mse = 466.3903, mad = 16.8257
2024-03-21 11:28:47,222 - __main__ - INFO - Fold 2 Epoch 25 Batch 0: Train Loss = 0.6090
2024-03-21 11:29:16,498 - __main__ - INFO - Fold 2, mse = 486.2637, mad = 17.3901
2024-03-21 11:29:17,348 - __main__ - INFO - Fold 2 Epoch 26 Batch 0: Train Loss = 0.6566
2024-03-21 11:29:47,793 - __main__ - INFO - Fold 2, mse = 473.1940, mad = 16.8208
2024-03-21 11:29:48,439 - __main__ - INFO - Fold 2 Epoch 27 Batch 0: Train Loss = 0.6268
2024-03-21 11:30:17,265 - __main__ - INFO - Fold 2, mse = 475.6956, mad = 16.9674
2024-03-21 11:30:18,014 - __main__ - INFO - Fold 2 Epoch 28 Batch 0: Train Loss = 0.6120
2024-03-21 11:30:46,983 - __main__ - INFO - Fold 2, mse = 483.5132, mad = 17.3673
2024-03-21 11:30:47,798 - __main__ - INFO - Fold 2 Epoch 29 Batch 0: Train Loss = 0.5835
2024-03-21 11:31:17,511 - __main__ - INFO - Fold 2, mse = 482.3305, mad = 17.1773
2024-03-21 11:31:18,819 - __main__ - INFO - Fold 3 Epoch 0 Batch 0: Train Loss = 1.0817
2024-03-21 11:31:47,834 - __main__ - INFO - Fold 3, epoch 0: Loss = 1.0068 Valid loss = 0.9138 MSE = 652.0561
2024-03-21 11:31:47,835 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 652.0561 ------------
2024-03-21 11:31:48,023 - __main__ - INFO - Fold 3, mse = 652.0561, mad = 20.8861
2024-03-21 11:31:48,847 - __main__ - INFO - Fold 3 Epoch 1 Batch 0: Train Loss = 1.1055
2024-03-21 11:32:17,524 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 648.4135 ------------
2024-03-21 11:32:17,777 - __main__ - INFO - Fold 3, mse = 648.4135, mad = 20.8420
2024-03-21 11:32:18,552 - __main__ - INFO - Fold 3 Epoch 2 Batch 0: Train Loss = 0.8356
2024-03-21 11:32:47,152 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 630.7022 ------------
2024-03-21 11:32:47,311 - __main__ - INFO - Fold 3, mse = 630.7022, mad = 20.7686
2024-03-21 11:32:48,111 - __main__ - INFO - Fold 3 Epoch 3 Batch 0: Train Loss = 0.8200
2024-03-21 11:33:17,180 - __main__ - INFO - Fold 3, mse = 636.3302, mad = 20.9016
2024-03-21 11:33:18,076 - __main__ - INFO - Fold 3 Epoch 4 Batch 0: Train Loss = 0.7267
2024-03-21 11:33:50,034 - __main__ - INFO - Fold 3, mse = 663.1564, mad = 21.4577
2024-03-21 11:33:50,923 - __main__ - INFO - Fold 3 Epoch 5 Batch 0: Train Loss = 0.8181
2024-03-21 11:34:22,584 - __main__ - INFO - Fold 3, mse = 647.2474, mad = 20.9638
2024-03-21 11:34:23,400 - __main__ - INFO - Fold 3 Epoch 6 Batch 0: Train Loss = 0.8177
2024-03-21 11:34:57,404 - __main__ - INFO - Fold 3, mse = 637.9894, mad = 20.8029
2024-03-21 11:34:58,288 - __main__ - INFO - Fold 3 Epoch 7 Batch 0: Train Loss = 0.6908
2024-03-21 11:35:30,828 - __main__ - INFO - Fold 3, mse = 641.6702, mad = 20.6788
2024-03-21 11:35:31,614 - __main__ - INFO - Fold 3 Epoch 8 Batch 0: Train Loss = 0.7010
2024-03-21 11:36:01,921 - __main__ - INFO - Fold 3, mse = 663.6368, mad = 20.9585
2024-03-21 11:36:02,726 - __main__ - INFO - Fold 3 Epoch 9 Batch 0: Train Loss = 0.7539
2024-03-21 11:36:32,805 - __main__ - INFO - Fold 3, mse = 648.1727, mad = 20.8132
2024-03-21 11:36:33,478 - __main__ - INFO - Fold 3 Epoch 10 Batch 0: Train Loss = 0.6738
2024-03-21 11:37:02,611 - __main__ - INFO - Fold 3, epoch 10: Loss = 0.6658 Valid loss = 0.9169 MSE = 655.0736
2024-03-21 11:37:02,613 - __main__ - INFO - Fold 3, mse = 655.0736, mad = 20.7292
2024-03-21 11:37:03,313 - __main__ - INFO - Fold 3 Epoch 11 Batch 0: Train Loss = 0.6505
2024-03-21 11:37:32,044 - __main__ - INFO - Fold 3, mse = 661.6935, mad = 20.7710
2024-03-21 11:37:32,907 - __main__ - INFO - Fold 3 Epoch 12 Batch 0: Train Loss = 0.5996
2024-03-21 11:38:03,944 - __main__ - INFO - Fold 3, mse = 661.0385, mad = 20.6704
2024-03-21 11:38:04,661 - __main__ - INFO - Fold 3 Epoch 13 Batch 0: Train Loss = 0.7217
2024-03-21 11:38:37,264 - __main__ - INFO - Fold 3, mse = 659.7667, mad = 20.7808
2024-03-21 11:38:38,025 - __main__ - INFO - Fold 3 Epoch 14 Batch 0: Train Loss = 0.5473
2024-03-21 11:39:09,346 - __main__ - INFO - Fold 3, mse = 676.5931, mad = 21.1661
2024-03-21 11:39:10,202 - __main__ - INFO - Fold 3 Epoch 15 Batch 0: Train Loss = 0.6300
2024-03-21 11:39:43,244 - __main__ - INFO - Fold 3, mse = 673.6836, mad = 21.0395
2024-03-21 11:39:44,157 - __main__ - INFO - Fold 3 Epoch 16 Batch 0: Train Loss = 0.5940
2024-03-21 11:40:16,561 - __main__ - INFO - Fold 3, mse = 685.4391, mad = 21.2728
2024-03-21 11:40:17,562 - __main__ - INFO - Fold 3 Epoch 17 Batch 0: Train Loss = 0.7269
2024-03-21 11:40:50,410 - __main__ - INFO - Fold 3, mse = 686.7966, mad = 21.4295
2024-03-21 11:40:51,248 - __main__ - INFO - Fold 3 Epoch 18 Batch 0: Train Loss = 0.5956
2024-03-21 11:41:25,009 - __main__ - INFO - Fold 3, mse = 693.9822, mad = 21.5677
2024-03-21 11:41:25,930 - __main__ - INFO - Fold 3 Epoch 19 Batch 0: Train Loss = 0.6399
2024-03-21 11:41:59,246 - __main__ - INFO - Fold 3, mse = 691.3043, mad = 21.4173
2024-03-21 11:42:00,070 - __main__ - INFO - Fold 3 Epoch 20 Batch 0: Train Loss = 0.5965
2024-03-21 11:42:31,995 - __main__ - INFO - Fold 3, epoch 20: Loss = 0.5934 Valid loss = 1.0029 MSE = 712.4042
2024-03-21 11:42:31,996 - __main__ - INFO - Fold 3, mse = 712.4042, mad = 21.7671
2024-03-21 11:42:32,866 - __main__ - INFO - Fold 3 Epoch 21 Batch 0: Train Loss = 0.6769
2024-03-21 11:43:05,654 - __main__ - INFO - Fold 3, mse = 710.4995, mad = 21.6472
2024-03-21 11:43:06,551 - __main__ - INFO - Fold 3 Epoch 22 Batch 0: Train Loss = 0.6059
2024-03-21 11:43:38,619 - __main__ - INFO - Fold 3, mse = 724.9778, mad = 21.9848
2024-03-21 11:43:39,361 - __main__ - INFO - Fold 3 Epoch 23 Batch 0: Train Loss = 0.5568
2024-03-21 11:44:12,756 - __main__ - INFO - Fold 3, mse = 708.6991, mad = 21.7018
2024-03-21 11:44:13,683 - __main__ - INFO - Fold 3 Epoch 24 Batch 0: Train Loss = 0.5286
2024-03-21 11:44:46,961 - __main__ - INFO - Fold 3, mse = 737.0566, mad = 22.2397
2024-03-21 11:44:47,897 - __main__ - INFO - Fold 3 Epoch 25 Batch 0: Train Loss = 0.6191
2024-03-21 11:45:19,511 - __main__ - INFO - Fold 3, mse = 761.2862, mad = 22.4225
2024-03-21 11:45:20,289 - __main__ - INFO - Fold 3 Epoch 26 Batch 0: Train Loss = 0.5872
2024-03-21 11:45:52,897 - __main__ - INFO - Fold 3, mse = 754.1314, mad = 22.2534
2024-03-21 11:45:53,874 - __main__ - INFO - Fold 3 Epoch 27 Batch 0: Train Loss = 0.5377
2024-03-21 11:46:25,854 - __main__ - INFO - Fold 3, mse = 734.7625, mad = 21.9209
2024-03-21 11:46:26,670 - __main__ - INFO - Fold 3 Epoch 28 Batch 0: Train Loss = 0.5863
2024-03-21 11:47:00,274 - __main__ - INFO - Fold 3, mse = 730.9644, mad = 21.9345
2024-03-21 11:47:01,047 - __main__ - INFO - Fold 3 Epoch 29 Batch 0: Train Loss = 0.5480
2024-03-21 11:47:35,336 - __main__ - INFO - Fold 3, mse = 760.0292, mad = 22.4910
2024-03-21 11:47:36,922 - __main__ - INFO - Fold 4 Epoch 0 Batch 0: Train Loss = 0.9121
2024-03-21 11:48:10,067 - __main__ - INFO - Fold 4, epoch 0: Loss = 0.9404 Valid loss = 1.1244 MSE = 805.2690
2024-03-21 11:48:10,068 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 805.2690 ------------
2024-03-21 11:48:10,327 - __main__ - INFO - Fold 4, mse = 805.2690, mad = 22.6428
2024-03-21 11:48:11,272 - __main__ - INFO - Fold 4 Epoch 1 Batch 0: Train Loss = 0.9096
2024-03-21 11:48:44,046 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 747.4302 ------------
2024-03-21 11:48:44,273 - __main__ - INFO - Fold 4, mse = 747.4302, mad = 22.0510
2024-03-21 11:48:45,229 - __main__ - INFO - Fold 4 Epoch 2 Batch 0: Train Loss = 0.6860
2024-03-21 11:49:16,974 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 741.1904 ------------
2024-03-21 11:49:17,227 - __main__ - INFO - Fold 4, mse = 741.1904, mad = 21.6582
2024-03-21 11:49:18,233 - __main__ - INFO - Fold 4 Epoch 3 Batch 0: Train Loss = 0.7399
2024-03-21 11:49:49,316 - __main__ - INFO - Fold 4, mse = 745.1044, mad = 21.4707
2024-03-21 11:49:50,192 - __main__ - INFO - Fold 4 Epoch 4 Batch 0: Train Loss = 0.9011
2024-03-21 11:50:22,391 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 709.5585 ------------
2024-03-21 11:50:22,757 - __main__ - INFO - Fold 4, mse = 709.5585, mad = 21.0547
2024-03-21 11:50:23,676 - __main__ - INFO - Fold 4 Epoch 5 Batch 0: Train Loss = 0.7372
2024-03-21 11:50:56,294 - __main__ - INFO - Fold 4, mse = 715.7248, mad = 21.2025
2024-03-21 11:50:57,193 - __main__ - INFO - Fold 4 Epoch 6 Batch 0: Train Loss = 0.7734
2024-03-21 11:51:29,799 - __main__ - INFO - Fold 4, mse = 758.5839, mad = 21.7790
2024-03-21 11:51:30,570 - __main__ - INFO - Fold 4 Epoch 7 Batch 0: Train Loss = 0.7394
2024-03-21 11:52:02,930 - __main__ - INFO - Fold 4, mse = 747.1771, mad = 21.6259
2024-03-21 11:52:03,740 - __main__ - INFO - Fold 4 Epoch 8 Batch 0: Train Loss = 0.8606
2024-03-21 11:52:36,487 - __main__ - INFO - Fold 4, mse = 783.6628, mad = 22.1739
2024-03-21 11:52:37,408 - __main__ - INFO - Fold 4 Epoch 9 Batch 0: Train Loss = 0.6570
2024-03-21 11:53:10,850 - __main__ - INFO - Fold 4, mse = 816.4172, mad = 22.6916
2024-03-21 11:53:11,693 - __main__ - INFO - Fold 4 Epoch 10 Batch 0: Train Loss = 0.6979
2024-03-21 11:53:44,933 - __main__ - INFO - Fold 4, epoch 10: Loss = 0.6626 Valid loss = 1.0838 MSE = 780.8202
2024-03-21 11:53:44,934 - __main__ - INFO - Fold 4, mse = 780.8202, mad = 22.2982
2024-03-21 11:53:45,921 - __main__ - INFO - Fold 4 Epoch 11 Batch 0: Train Loss = 0.5995
2024-03-21 11:54:19,931 - __main__ - INFO - Fold 4, mse = 743.0013, mad = 21.6869
2024-03-21 11:54:20,779 - __main__ - INFO - Fold 4 Epoch 12 Batch 0: Train Loss = 0.6453
2024-03-21 11:54:53,838 - __main__ - INFO - Fold 4, mse = 746.4914, mad = 21.7471
2024-03-21 11:54:54,792 - __main__ - INFO - Fold 4 Epoch 13 Batch 0: Train Loss = 0.5840
2024-03-21 11:55:26,454 - __main__ - INFO - Fold 4, mse = 763.1277, mad = 22.1196
2024-03-21 11:55:27,411 - __main__ - INFO - Fold 4 Epoch 14 Batch 0: Train Loss = 0.6348
2024-03-21 11:55:59,698 - __main__ - INFO - Fold 4, mse = 751.3760, mad = 22.2514
2024-03-21 11:56:00,545 - __main__ - INFO - Fold 4 Epoch 15 Batch 0: Train Loss = 0.6237
2024-03-21 11:56:33,056 - __main__ - INFO - Fold 4, mse = 779.8821, mad = 22.2382
2024-03-21 11:56:33,879 - __main__ - INFO - Fold 4 Epoch 16 Batch 0: Train Loss = 0.6036
2024-03-21 11:57:05,549 - __main__ - INFO - Fold 4, mse = 739.9811, mad = 21.7067
2024-03-21 11:57:06,285 - __main__ - INFO - Fold 4 Epoch 17 Batch 0: Train Loss = 0.5764
2024-03-21 11:57:37,167 - __main__ - INFO - Fold 4, mse = 775.5730, mad = 22.1433
2024-03-21 11:57:38,045 - __main__ - INFO - Fold 4 Epoch 18 Batch 0: Train Loss = 0.6043
2024-03-21 11:58:08,865 - __main__ - INFO - Fold 4, mse = 724.3849, mad = 21.0987
2024-03-21 11:58:09,618 - __main__ - INFO - Fold 4 Epoch 19 Batch 0: Train Loss = 0.5491
2024-03-21 11:58:41,301 - __main__ - INFO - Fold 4, mse = 750.3049, mad = 21.2393
2024-03-21 11:58:42,158 - __main__ - INFO - Fold 4 Epoch 20 Batch 0: Train Loss = 0.5341
2024-03-21 11:59:14,178 - __main__ - INFO - Fold 4, epoch 20: Loss = 0.6036 Valid loss = 1.0209 MSE = 737.1289
2024-03-21 11:59:14,179 - __main__ - INFO - Fold 4, mse = 737.1289, mad = 21.2354
2024-03-21 11:59:15,039 - __main__ - INFO - Fold 4 Epoch 21 Batch 0: Train Loss = 0.6108
2024-03-21 11:59:49,946 - __main__ - INFO - Fold 4, mse = 742.7049, mad = 21.4733
2024-03-21 11:59:50,945 - __main__ - INFO - Fold 4 Epoch 22 Batch 0: Train Loss = 0.6074
2024-03-21 12:00:23,407 - __main__ - INFO - Fold 4, mse = 718.2150, mad = 21.2111
2024-03-21 12:00:24,402 - __main__ - INFO - Fold 4 Epoch 23 Batch 0: Train Loss = 0.5575
2024-03-21 12:00:57,460 - __main__ - INFO - Fold 4, mse = 740.1127, mad = 21.5024
2024-03-21 12:00:58,474 - __main__ - INFO - Fold 4 Epoch 24 Batch 0: Train Loss = 0.4662
2024-03-21 12:01:32,857 - __main__ - INFO - Fold 4, mse = 778.1909, mad = 22.1861
2024-03-21 12:01:33,782 - __main__ - INFO - Fold 4 Epoch 25 Batch 0: Train Loss = 0.6099
2024-03-21 12:02:05,103 - __main__ - INFO - Fold 4, mse = 768.4002, mad = 22.0045
2024-03-21 12:02:05,862 - __main__ - INFO - Fold 4 Epoch 26 Batch 0: Train Loss = 0.4935
2024-03-21 12:02:37,352 - __main__ - INFO - Fold 4, mse = 737.4804, mad = 21.6549
2024-03-21 12:02:38,026 - __main__ - INFO - Fold 4 Epoch 27 Batch 0: Train Loss = 0.5130
2024-03-21 12:03:11,181 - __main__ - INFO - Fold 4, mse = 791.5570, mad = 22.1938
2024-03-21 12:03:11,993 - __main__ - INFO - Fold 4 Epoch 28 Batch 0: Train Loss = 0.5717
2024-03-21 12:03:47,138 - __main__ - INFO - Fold 4, mse = 882.2573, mad = 23.4118
2024-03-21 12:03:48,131 - __main__ - INFO - Fold 4 Epoch 29 Batch 0: Train Loss = 0.5312
2024-03-21 12:04:21,504 - __main__ - INFO - Fold 4, mse = 908.5737, mad = 23.9560
2024-03-21 12:04:22,885 - __main__ - INFO - Fold 5 Epoch 0 Batch 0: Train Loss = 1.1895
2024-03-21 12:04:57,184 - __main__ - INFO - Fold 5, epoch 0: Loss = 0.9976 Valid loss = 0.9433 MSE = 670.9459
2024-03-21 12:04:57,185 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 670.9459 ------------
2024-03-21 12:04:57,366 - __main__ - INFO - Fold 5, mse = 670.9459, mad = 20.9266
2024-03-21 12:04:58,326 - __main__ - INFO - Fold 5 Epoch 1 Batch 0: Train Loss = 0.9886
2024-03-21 12:05:33,234 - __main__ - INFO - Fold 5, mse = 677.9595, mad = 21.2462
2024-03-21 12:05:34,210 - __main__ - INFO - Fold 5 Epoch 2 Batch 0: Train Loss = 0.9433
2024-03-21 12:06:11,440 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 656.8540 ------------
2024-03-21 12:06:11,790 - __main__ - INFO - Fold 5, mse = 656.8540, mad = 20.4661
2024-03-21 12:06:12,875 - __main__ - INFO - Fold 5 Epoch 3 Batch 0: Train Loss = 0.8413
2024-03-21 12:06:49,163 - __main__ - INFO - Fold 5, mse = 662.7900, mad = 20.2188
2024-03-21 12:06:50,218 - __main__ - INFO - Fold 5 Epoch 4 Batch 0: Train Loss = 0.7969
2024-03-21 12:07:25,962 - __main__ - INFO - ------------ Save FOLD-BEST model - MSE: 647.7318 ------------
2024-03-21 12:07:26,228 - __main__ - INFO - Fold 5, mse = 647.7318, mad = 20.1816
2024-03-21 12:07:27,236 - __main__ - INFO - Fold 5 Epoch 5 Batch 0: Train Loss = 0.8265
2024-03-21 12:08:04,299 - __main__ - INFO - Fold 5, mse = 658.6643, mad = 20.2801
2024-03-21 12:08:05,398 - __main__ - INFO - Fold 5 Epoch 6 Batch 0: Train Loss = 0.9070
2024-03-21 12:08:40,322 - __main__ - INFO - Fold 5, mse = 673.0999, mad = 20.5958
2024-03-21 12:08:41,369 - __main__ - INFO - Fold 5 Epoch 7 Batch 0: Train Loss = 0.8753
2024-03-21 12:09:17,905 - __main__ - INFO - Fold 5, mse = 677.2960, mad = 20.5701
2024-03-21 12:09:18,907 - __main__ - INFO - Fold 5 Epoch 8 Batch 0: Train Loss = 0.7207
2024-03-21 12:09:55,884 - __main__ - INFO - Fold 5, mse = 687.5888, mad = 20.6161
2024-03-21 12:09:56,941 - __main__ - INFO - Fold 5 Epoch 9 Batch 0: Train Loss = 0.7173
2024-03-21 12:10:33,394 - __main__ - INFO - Fold 5, mse = 706.7764, mad = 20.9539
2024-03-21 12:10:34,299 - __main__ - INFO - Fold 5 Epoch 10 Batch 0: Train Loss = 0.6618
2024-03-21 12:11:11,798 - __main__ - INFO - Fold 5, epoch 10: Loss = 0.6817 Valid loss = 0.9583 MSE = 682.4083
2024-03-21 12:11:11,799 - __main__ - INFO - Fold 5, mse = 682.4083, mad = 20.8258
2024-03-21 12:11:12,922 - __main__ - INFO - Fold 5 Epoch 11 Batch 0: Train Loss = 0.6984
2024-03-21 12:11:48,016 - __main__ - INFO - Fold 5, mse = 699.4803, mad = 21.2823
2024-03-21 12:11:49,112 - __main__ - INFO - Fold 5 Epoch 12 Batch 0: Train Loss = 0.6942
2024-03-21 12:12:23,686 - __main__ - INFO - Fold 5, mse = 696.2772, mad = 21.2997
2024-03-21 12:12:24,630 - __main__ - INFO - Fold 5 Epoch 13 Batch 0: Train Loss = 0.6534
2024-03-21 12:13:00,720 - __main__ - INFO - Fold 5, mse = 732.1758, mad = 21.5806
2024-03-21 12:13:01,858 - __main__ - INFO - Fold 5 Epoch 14 Batch 0: Train Loss = 0.7080
2024-03-21 12:13:39,079 - __main__ - INFO - Fold 5, mse = 723.0564, mad = 21.4973
2024-03-21 12:13:40,013 - __main__ - INFO - Fold 5 Epoch 15 Batch 0: Train Loss = 0.6319
2024-03-21 12:14:15,840 - __main__ - INFO - Fold 5, mse = 723.6256, mad = 21.7513
2024-03-21 12:14:16,948 - __main__ - INFO - Fold 5 Epoch 16 Batch 0: Train Loss = 0.5813
2024-03-21 12:14:53,136 - __main__ - INFO - Fold 5, mse = 716.4506, mad = 22.0924
2024-03-21 12:14:54,137 - __main__ - INFO - Fold 5 Epoch 17 Batch 0: Train Loss = 0.6652
2024-03-21 12:15:31,662 - __main__ - INFO - Fold 5, mse = 750.8965, mad = 22.2166
2024-03-21 12:15:32,702 - __main__ - INFO - Fold 5 Epoch 18 Batch 0: Train Loss = 0.6747
2024-03-21 12:16:10,388 - __main__ - INFO - Fold 5, mse = 731.7111, mad = 22.0429
2024-03-21 12:16:11,371 - __main__ - INFO - Fold 5 Epoch 19 Batch 0: Train Loss = 0.7043
2024-03-21 12:16:48,574 - __main__ - INFO - Fold 5, mse = 746.2507, mad = 22.0633
2024-03-21 12:16:49,413 - __main__ - INFO - Fold 5 Epoch 20 Batch 0: Train Loss = 0.5936
2024-03-21 12:17:25,610 - __main__ - INFO - Fold 5, epoch 20: Loss = 0.6122 Valid loss = 1.0648 MSE = 736.1520
2024-03-21 12:17:25,611 - __main__ - INFO - Fold 5, mse = 736.1520, mad = 21.8841
2024-03-21 12:17:26,412 - __main__ - INFO - Fold 5 Epoch 21 Batch 0: Train Loss = 0.7487
2024-03-21 12:18:01,852 - __main__ - INFO - Fold 5, mse = 726.9986, mad = 21.9223
2024-03-21 12:18:02,646 - __main__ - INFO - Fold 5 Epoch 22 Batch 0: Train Loss = 0.5563
2024-03-21 12:18:38,143 - __main__ - INFO - Fold 5, mse = 691.5611, mad = 21.4596
2024-03-21 12:18:39,057 - __main__ - INFO - Fold 5 Epoch 23 Batch 0: Train Loss = 0.5295
2024-03-21 12:19:14,183 - __main__ - INFO - Fold 5, mse = 670.3066, mad = 21.1085
2024-03-21 12:19:15,143 - __main__ - INFO - Fold 5 Epoch 24 Batch 0: Train Loss = 0.6836
2024-03-21 12:19:45,090 - __main__ - INFO - Fold 5, mse = 715.2532, mad = 21.6966
2024-03-21 12:19:45,863 - __main__ - INFO - Fold 5 Epoch 25 Batch 0: Train Loss = 0.6509
2024-03-21 12:20:14,460 - __main__ - INFO - Fold 5, mse = 704.2849, mad = 21.3825
2024-03-21 12:20:15,170 - __main__ - INFO - Fold 5 Epoch 26 Batch 0: Train Loss = 0.5535
2024-03-21 12:20:43,246 - __main__ - INFO - Fold 5, mse = 699.1513, mad = 21.5223
2024-03-21 12:20:43,896 - __main__ - INFO - Fold 5 Epoch 27 Batch 0: Train Loss = 0.6139
2024-03-21 12:21:11,975 - __main__ - INFO - Fold 5, mse = 733.2910, mad = 22.0013
2024-03-21 12:21:12,794 - __main__ - INFO - Fold 5 Epoch 28 Batch 0: Train Loss = 0.5404
2024-03-21 12:21:41,623 - __main__ - INFO - Fold 5, mse = 702.0078, mad = 21.5744
2024-03-21 12:21:42,483 - __main__ - INFO - Fold 5 Epoch 29 Batch 0: Train Loss = 0.6170
2024-03-21 12:22:11,956 - __main__ - INFO - Fold 5, mse = 721.2018, mad = 21.8966
2024-03-21 12:22:11,957 - __main__ - INFO - mse 602.1047(84.2716)
2024-03-21 12:22:11,958 - __main__ - INFO - mad 19.5784(1.5413)
